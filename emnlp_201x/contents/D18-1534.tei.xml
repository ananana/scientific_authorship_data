<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When data permutations are pathological: the case of neural natural language inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">IT University Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Varab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">IT University Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When data permutations are pathological: the case of neural natural language inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4935" to="4939"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4935</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Consider two competitive machine learning models, one of which was considered state-of-the art, and the other a competitive baseline. Suppose that by just permut-ing the examples of the training set, say by reversing the original order, by shuffling, or by mini-batching, you could report substantially better/worst performance for the system of your choice, by multiple percentage points. In this paper, we illustrate this scenario for a trending NLP task: Natural Language Inference (NLI). We show that for the two central NLI corpora today , the learning process of neural systems is far too sensitive to permutations of the data. In doing so we reopen the question of how to judge a good neural architecture for NLI, given the available dataset and perhaps, further, the soundness of the NLI task itself in its current state.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is increased interest today in the detection of information quality: whether a statement is true or false, or equivalently, whether one statement (the premise) is entailed by, contradicts or has no rela- tion to another statement (the hypothesis). This is the Natural Language Inference task. The timely development of the Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> and more recently the Multi-Genre NLI (MULTI- NLI) corpus ( <ref type="bibr" target="#b11">Williams et al., 2017</ref>) has lead to a steady increase in contributions to research on NLI. Recent research, however, indicates these central datasets to be trivially annotated to a large degree ( <ref type="bibr" target="#b4">Gururangan et al., 2018)</ref>. In this paper, we give evidence of an unrelated problem.</p><p>Deep neural network approaches provide the state-of-the-art today for the tasks. We show a pathological sensitivity of these systems to permu- tations of the training set. This calls into question the soundness of the task in its current state and corresponding development and benchmarking of NLI neural systems. Given the approximate iterative optimisation methods required to induce models, a common practice for statistical learning is to first randomly shuffle the training set. This serves to offset unwanted bias due to accidental ordering of the examples (for example, with respect to time or class). Probably because of this, there is very lit- tle literature or understanding of the effect of order among the training examples-strict ordering of ex- amples has simply been known to be undesirable. However, as neural network approaches increas- ingly dominate in performance across many NLP tasks, the notion of random shuffling has become overshadowed by that of computational efficiency. This seems to lead back to experiment parameters from <ref type="bibr" target="#b10">Sutskever et al. (2014)</ref>'s work, which demon- strates that grouping examples of similar sentence length into batches results in a halved training time. But what is the effect on accuracy? Indeed, a curiosity of neural network models induced over the NLI datasets is the ease in acquiring rather in- stable results as a result of simple example order permutations of the training set.</p><p>In this paper, we present an investigation into these questions. In particular, we consider two simple but competitive neural network topologies in order to investigate the effect of training ex- ample order from these datasets on performance. One of these achieves close to state-of-art results over SNLI and state-of-the-art for MULTI-NLI (for simple systems, Cf. Section 4.1), while the other is a simpler variant of the first, characterised by fewer parameters. Out of the standard deep learn- ing, standard machine learning and other plausi-ble orderings of the dataset, we show that only the original ordering of the training examples leads to state-of-the-art model induction. We also show that the gap in performance between the two neu- ral architectures described here generally drops over all other permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NLI task and datasets</head><p>The NLI task input is two sentences, a = (a 1 , . . . , a la ) and b = (b 1 , . . . , b l b ) of lengths l a and l b respectively. Each a i (resp. b j ) for i ∈ [l a ] (resp. j ∈ [l b ]) corresponds to the word em- bedding with dimension d for the ith (resp. jth) word. The task dataset consists of labeled pairs of sentences,</p><formula xml:id="formula_0">{a (n) , b (n) , y (n) } N n=1</formula><p>, where y (n) ∈ {entailment, contradiction, neutral} is the class.</p><p>We use two central datasets for our study here: SNLI and MULTI-NLI ( <ref type="bibr" target="#b1">Bowman et al., 2015;</ref><ref type="bibr" target="#b11">Williams et al., 2017)</ref>, containing over 570K and 433K sentence pairs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our neural network architectures</head><p>For the research presented in this paper, we choose two related and relatively simple neural network architectures corresponding roughly to <ref type="bibr" target="#b7">Parikh et al. (2016)</ref> and a simplified version of <ref type="bibr" target="#b2">Chen et al. (2017)</ref> consisting of five components. 1 Note that we also downloaded <ref type="bibr" target="#b2">Chen et al. (2017)</ref>'s code 2 and ran it over the datasets; since it did not perform with the same accuracy reported in the paper for our run (87.77% instead of the re- ported 88%), and since it took several hours longer than our implementation to run (+7 hours longer), we concentrated on our own implementation. We also attempted to run Gong et al. (2017)'s system 3 ; for our runs, the system halted without comple- tion after several hours. Moreover, there are re- ports on the difficulty in getting the architecture to achieve the accuracy score of 88% reported in the original paper, <ref type="bibr" target="#b5">Mirakyan et al. (2018)</ref> reporting that their re-implementation could only achieve 86.38%. This is significantly worst-performing than our best system, which we present now.</p><p>(1) Pre-projection. To compensate approxi- mately for not updating the original embeddings during learning, we first carry out a preliminary <ref type="bibr">1</ref> We make the systems publicly available https://github.com/natschluter/ nli-data-permutations.</p><p>2 https://github.com/lukecq1231/nli 3 https://github.com/YichenGong/ Densely-Interactive-Inference-Network projection of the embeddings, to the same dimen- sions using a feed-forward network.</p><p>(2) Embedding projection. We further project embeddings via either a simple feed-forward (FF) with a ReLU activation function or a bidirectional LSTM (BiLSTM) layer. The result is then sent to the attention component. This corresponds pre- cisely therefore to <ref type="bibr" target="#b7">Parikh et al. (2016)</ref>'s computa- tionally efficient approximation of the vector prod- uct before soft-alignment.</p><p>(3) Attention. The attention mechanism, first in- troduced by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, is based on a matrix of all-pairs scores between the elements of two sequences a i and b j : <ref type="bibr">4</ref> e</p><formula xml:id="formula_1">ij := F (a i , b j ) := F (a i ) T F (b j )</formula><p>We follow ( <ref type="bibr" target="#b7">Parikh et al., 2016</ref>) and later attention- based models for NLI, by representing the impor- tance of a i with respect to b as the normalised sum</p><formula xml:id="formula_2">β i := l b j=1 exp(e ij ) l b k=1 exp(e ik ) b j .</formula><p>The result is concatenated with a i , to create [a i , β i ] which is then projected down to original embed- ding dimension. The same is done for b j with re- spect to a. (5) Prediction. Finally we feed a vector con- catenation of both sentence vectors as input to a component consisting of three feed-forward lay- ers with dropout and regularisation, followed by a linear softmax layer for prediction.</p><p>Instantiated architectures. The two topologies we adopt for this study consist of the above com- ponents with embedding projection and aggrega- tion as follows:</p><p>• FF/SUM corresponding to the embedding pro- jection instantiated with a feed-forward network and the aggregation carried out through vector summation, and</p><p>• Bi/LSTM corresponding to the embedding pro- jection instantiated with a BiLSTM and the ag- gregation carried out via an LSTM.</p><p>Other hyperparameters. We use 300 dimen- sional GloVe embeddings trained on the Com- mon Crawl 840B tokens dataset ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>), which remain fixed during training. Out of vocabulary (OOV) words are represented as zero vectors. <ref type="bibr">6</ref> We use a 0.2 dropout rate and L2 regular- isation, applied in all feed-forward layers. We op- timise the network using categorical cross-entropy loss and employ the RMSprop optimizer with ρ set to 0.9, a 0.001 learning rate, with a batch size of 512 and use early stopping over the develop- ment set after no improvement in accuracy after 4 epochs.</p><p>State-of-the-art for simple systems. The mod- els are simple in that no information above word embeddings is taken as input, for example, no POS-tags or syntactic relations (See Section 4.1).</p><p>Our Bi/LSTM system also currently sets the state- of-the-art for simple systems on the MULTI-NLI dataset (See Section 4.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Previous work is related either in terms of the neu- ral architectures for NLI (Section 4.1), or in terms of work on training data permutations in learning (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">State-of-the-art NLI</head><p>There are different types of neural network sys- tems in the literature with respect to the simplic-ity of input data required for modeling and in- terdependence of the internal modules. In this work, we only consider simple system approaches that use only word embeddings (no character rep- resentations, POS-tags, word-position, syntactic tree, external resources, etc.) and consist only of interdependent modules (not ensembles). We make no claims regarding linguistic or ensemble- complex systems, but make the straightforward hypothesis that the conclusions presented here can be extended to cover more complex frame- works as well, especially given that our systems are strongly competitive or even better perform- ing than two highly complex state-of-the-art neu- ral systems (Cf. Section 3).</p><p>For simple systems, the state-of-the-art is cur- rently set by <ref type="bibr" target="#b9">Sha et al. (2016)</ref> at 87.5%, on SNLI. They use a standard BiLSTM to read the premise, and propose a Bi-rLSTM to read the hypothesis. Their proposed rLSTM-"re-read" LSTM unit- takes the attention vector of one sentence as an inner state while reading the other sentence. The output of the standard BiLSTM is also taken as the general input of the bidirectional rLSTM. The cur- rently published next best performing simple sys- tem, <ref type="bibr" target="#b7">Parikh et al. (2016)</ref> at 86.3% accuracy, intro- duced use of the attention mechanism for the NLI task, the way it is generally being used today. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Example permutation in learning</head><p>Morishita et al. (2017) explored the effect of mini- batching on the learning of Neural Machine Trans- lation models, carrying out their experiments on two datasets (two language-pairs). In particular, they studied the strategies of (1) sorting by length of the source sentence, (2) target sentence, or (3) both, among other things. They empirically com- pare their efficiency on two translation tasks and find that some strategies in wide use are not neces- sarily optimal for accuracy and convergence-wise. In contrast to the work described here however, one of the sorting strategies produced best results, though no comparison was made with the original ordering of examples. By contrast, we show that it is by making non-canonical (semi-non-random) orderings of the data that best results are achieved in NLI, for the two available datasets.  <ref type="table">Table 2</ref>: Performance of the FF/SUM and Bi/LSTM neural networks for the training example permuta- tions, evaluated over the development set, along with the difference (diff) in performance between the two architectures.</p><formula xml:id="formula_3">SNLI MULTI-NLI permutation FF/SUM Bi/LSTM diff FF/SUM Bi/LSTM diff</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Data permutations. We consider the following original and sorted orderings of the training set ex- amples to learn our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>orig:</head><p>The original order in which the dataset is currently distributed. conf: Sorting by increasing score, where the scores for each example are generating in training Bi/LSTM over orig.</p><p>shuffle-once: Randomly shuffle once before training, averaged over 5 runs.</p><p>shuffle-epoch: Randomly shuffle on each epoch during training, for a single run.</p><p>In addition to each of these, we consider the rever- sal of the order (indicated by the suffix -r.) In order to not exhaust the test set, we generate the results over data permutations on the develop- ment set. These are given in <ref type="table">Table 2</ref>.</p><p>Discussion. For both datasets, we observe that all other training example permutations result in a substantial drop in performance, by approximately 3-4% on SNLI and 1-6% on MULTI-NLI. Even a simple reversal of the original order leads to a substantial drop in performance. Shuffling con- sistently the data provides the strongest alternative training conditions to the original ordering. Moreover, the difference in performance of the two separate architectures is generally much lower on all other permutations of the training data, call- ing into question the significance of the more com- plex components. These observations apply to both randomly shuffling (as advised in statistical learning practise) and ordering the data by length (as advised in deep learning for NLP practise).</p><p>For an analysis of the sorting permutations, we looked into whether hypothesis sentence lengths differed by class to such an extent that the dataset became sorted by class label. We cannot include the results here due to space constraints. How- ever, we observed that ordering by class results in small (and quite interesting) drops in performance, so long as the original order is preserved. If we first randomly shuffle the examples before order- ing them by class, similar drops in performance result to those in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have shown that models induced over the SNLI and MULTI-NLI datasets are greatly af- fected by the permutation of the training data in- stances at hand: recommended statistical learning or deep learning engineering strategies for order- ing the training examples result in significantly and even substantially worse performance over these datasets. Our models are simple (no infor- mation over word embedding representations of sentences and no ensembles), but strongly compet- itive with (or better performing than) both SOTA (re-)implementations of much more complex neu- ral systems. We make the straightforward hypoth- esise that these observations will extend to more complex models; we leave this to be verified by future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 4 )</head><label>4</label><figDesc>Aggregation. Following the attention mech- anism we aggregate over words for a sentence rep- resentation. Either we sum over the attended word vectors Parikh et al. (2016) or use the final state of a single LSTM layer (Chen et al., 2017) 5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>prem:</head><label></label><figDesc>Sorting by increasing premise length. hypo: Sorting by increasing hypothesis length. lengths: Sorting by increasing premise + hypo- thesis length.</figDesc></figure>

			<note place="foot" n="4"> Note that we also experimented with eij := F (a i , b j ) where F was a component-wise multiplication and F a feed-forward network with ReLU activations. However, we observed no benefits to doing so performance-wise, and substantial loss in computational efficiency. 5 For Chen et al. (2017), aggregation consists of several layers of BiLSTMS as opposed to our single LSTM layer.</note>

			<note place="foot" n="6"> We experimented with alternative OOV representations, such as the the mean vector of most frequent words and random vectors, with insignificant effects.</note>

			<note place="foot" n="7"> Parikh et al. (2016) also have a result with intrasentential attention that performs with 86.8% accuracy. However this attention model is dependent on word positional information.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>A large annotated corpus for learning natural language inference</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhange</surname></persName>
		</author>
		<idno>ArXiv:1709.04348</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. ACL</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Mirakyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<title level="m">Natural Language Inference over Interaction Space: ICLR 2018 Reproducibility Report</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of mini-batch creation strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Workshop on Neural Machine Translation (NMT)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading and thinking: Re-read lstm unit for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2870" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 28th Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
