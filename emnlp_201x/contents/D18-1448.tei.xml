<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSMO: Multimodal Summarization with Multimodal Output</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MSMO: Multimodal Summarization with Multimodal Output</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4154" to="4164"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4154</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multimodal summarization has drawn much attention due to the rapid growth of multime-dia data. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informa-tiveness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this task, we first collect a large-scale dataset for MSMO research. We then propose a multi-modal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate mul-timodal outputs, we construct a novel multi-modal automatic evaluation (MMAE) method which considers both intramodality salience and intermodality relevance. The experimental results show the effectiveness of MMAE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization is to extract the important in- formation from source documents. With the in- crease of multimedia data on the internet, some researchers ( <ref type="bibr" target="#b18">Li et al., 2016b;</ref><ref type="bibr" target="#b30">Shah et al., 2016;</ref>) focus on multimodal summariza- tion in recent years. Existing experiments ( <ref type="bibr" target="#b15">Li et al., , 2018a</ref>) have proven that, compared to text summarization, multimodal summarization can improve the quality of generated summary by using information in visual modality.</p><p>However, the output of existing multimodal summarization systems is usually represented in a single modality, such as textual or visual ( <ref type="bibr" target="#b8">Evangelopoulos et al., 2013;</ref><ref type="bibr" target="#b22">Mademlis et al., 2016)</ref>. In this paper, we argue that multimodal output 1 is necessary for the follow- ing three reasons: 1) It is much easier and faster for users to get critical information from the im- ages ( ). 2) According to our ex- periments, the multimodal output (text+image) in- creases users' satisfaction by 12.4% compared to the single-modality output (text) (more details can be found in Sec. 4.2). 3) Images help users to grasp events while texts provide more details re- lated to the events. Thus the images and text can complement each other, assisting users to gain a more visualized understanding of events ( <ref type="bibr" target="#b2">Bian et al., 2013)</ref>. We give an example in <ref type="figure">Fig. 1</ref> to il- lustrate this phenomenon. For the output with only the text summary, user will be confused about the description of "four-legged creatures"; while with a relevant image, user will have a clearer under- standing of the text.</p><p>In recent years, some researchers( <ref type="bibr" target="#b2">Bian et al., 2013</ref><ref type="bibr" target="#b3">Bian et al., , 2015</ref><ref type="bibr" target="#b34">Wang et al., 2016</ref>) focus on incorpo- rating multimedia contents into the output of sum- marization which all treat the image-text pair as a basic summarization unit. But in our work, our input comes from a document and a collection of images where there is no alignment between texts and images. So our biggest challenge is how to bridge the semantic gaps between texts and im- ages. Based on the above discussion, in this work, we propose a novel task which we refer to as Mul- timodal Summarization with Multimodal Output (MSMO). To explore this task, we focus on the simplicity, we first consider only one image) and a piece of text. We leave the other multimodal content (like videos) as future work. following three questions: 1) how to acquire the relevant data; 2) how to generate the multimodal output; 3) how to automatically evaluate the qual- ity of the multimodal output in MSMO.</p><p>For the first question, similar to <ref type="bibr" target="#b10">Hermann et al. (2015)</ref>, we collect a large-scale multimodal dataset 2 from Daily Mail website and annotate some pictorial summaries. For the second ques- tion, we propose a multimodal attention model to jointly generate text and the most relevant im- age, in which the importance of images is deter- mined by the visual coverage vector. For the last question, we construct a novel multimodal auto- matic evaluation (MMAE) which jointly considers salience of text, salience of image, and image-text relevance.</p><p>Our main contributions are as follows:</p><p>• We present a novel multimodal summariza- tion task, which takes the news with images as input, and finally outputs a pictorial sum- mary. We construct a large-scale corpus for MSMO studying.</p><p>• We propose an abstractive multimodal sum- marization model to jointly generate sum- mary and the most relevant image.</p><p>• We propose a multimodal automatic evalua- tion (MMAE) method which mainly consid- ers three aspects: salience of text, salience of image, and relevance between text and im- age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>We begin by defining the MSMO task. The in- put of the task is a document and a collection of images and the output is a pictorial summary. As shown in <ref type="figure">Fig. 2</ref>, our proposed model con- sists of four modules: text encoder, image en- coder, multimodal attention layer, and summary decoder. The text encoder is a BiLSTM used to encode text. Our image encoder is VGG19 3 pre- trained on ImageNet ( <ref type="bibr" target="#b31">Simonyan and Zisserman, 2015</ref>) used to extract global or local features. The multimodal attention layer aims to fuse textual and visual information during decoding. Our summary decoder, which is a unidirectional LSTM, makes use of information from two modalities to gener- ate the text summary and select the most relevant image according to visual coverage vector. Our text encoder and summary decoder are based on pointer-generator network which we will describe in Sec. 2.2. We then describe image encoder and multimodal attention layer in our multimodal at- tention model (Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pointer-Generator Network</head><p>See </p><formula xml:id="formula_0">cov t = t−1 ˜ t=0 α ˜ t .</formula><p>The coverage vector is used as an extra input to the attention vector (Eq. 1) and is also used to calculate the coverage loss (Eq. 6). Next, the attention distribution is used to calculate the context vector as follows.</p><formula xml:id="formula_1">e t i = v T tanh(W h h i + W s s t + W c cov t ) (1) α t = softmax(e t )<label>(2)</label></formula><formula xml:id="formula_2">c t = i α t i h i<label>(3)</label></formula><p>The important part in this model is the calcula- tion of the generation probability p g . It represents the probability of generating a word from the vo- cabulary distribution p v , and (1 − p g ) represents the probability of copying a word from the source by sampling from the attention distribution α t . p g is determined by c t , s t , and the decoder input x t in Eq. 4. The final probability distribution over the extended vocabulary, which denotes the union of the vocabulary and all words in the source, is cal- culated in Eq. 5. Finally, the loss for timestep t is the sum of the negative log likelihood of the target  <ref type="figure">Figure 2</ref>: The framework of our model. word w * t and the coverage loss (Eq. 6):</p><formula xml:id="formula_3">p g = σ(W * h c t + W * s s t + W x x t )<label>(4)</label></formula><formula xml:id="formula_4">p w = p g p v (w) + (1 − p g ) w i =w α t i<label>(5)</label></formula><formula xml:id="formula_5">L t = −logp w * t + i min(α t i , cov t i )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimodal Attention Model</head><p>We incorporate visual information into the pointer-generator network and propose a novel multimodal attention model. As shown in <ref type="figure">Fig. 2</ref>, there are three main differences between our model and pointer-generator network: 1) We have an extra image encoder and a corresponding visual attention layer; 2) To achieve the fusion of textual and visual information, we introduce a multimodal attention mechanism; 3) We add a visual cover- age ( <ref type="bibr" target="#b15">Li et al., 2018a</ref>) to both alleviate visual rep- etition and measure the salience of image. More details are as follows.</p><p>Image Encoder. We apply the VGG19 to ex- tract global and local image feature vectors for all images. The global features g are 4096- dimensional activations of the pre-softmax fully- connected layer fc7. The local features l are the 7 × 7 × 512 feature maps of the last pooling layer (pool5). We flatten the local feature into a ma-</p><formula xml:id="formula_6">trix A = (a 1 , · · · , a L )(L = 49)</formula><p>where a l ∈ R 512 corresponds to a patch of an image.</p><p>Visual Attention. The attention mechanism is learned to focus on different parts of input text while decoding. Attention mechanisms have also shown to work with other modalities, like images, where they can learn to attend the salient parts of an image ( <ref type="bibr" target="#b35">Xu et al., 2015)</ref>. We then explore to use images with a visual attention to learn text-image alignment. Concretely, we extend attention mech- anism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b21">Luong et al., 2015)</ref> to visual attention mechanisms, which attend vi- sual signals. There are three variants of our visual attention mechanisms: 1) attention on global fea- tures (ATG), 2) attention on local features (ATL), and 3) hierarchical visual attention on local fea- tures (HAN). We take the calculation of ATG as an example. To attend to the salient parts of a collec- tion of images with size M , we flatten the global feature set g into a matrix g = (g 1 , · · · , g M ). In addition to calculating the text context vector in Sec. 2.2, we also obtain a visual context vector. We first project the image feature into the same dimension as the text context vector. The visual attention is calculated as follows:</p><formula xml:id="formula_7">g * = W 2 I (W 1 I g + b 1 I ) + b 2 I (7) e t a = v T a tanh(W a g * i + U a s t + cov t a ) (8) α t a = softmax(e t a )<label>(9)</label></formula><p>where W 1 I ∈ R 4096×4096 and W 2 I ∈ R 4096×d h are the image transformation matrices, b 1 I ∈ R 4096 and b 2 I ∈ R d h are bias vectors, and cov t a denotes the visual coverage vector and is initialized to zero vector in the beginning. Then the visual attention distribution α t a is used to obtain the visual context</p><formula xml:id="formula_8">vector c t img through c t img = i α t a,i g * i .</formula><p>Similar is the ATL, we flatten the local feature set A into a matrix A = (a 1 , · · · , a M ×49 ). The calculation of attention in ATL is the same as in ATG. There is a bit difference in the HAN model, which first attend to the 49 image patches and get an interme- diate visual context vector to represent the image, and then attend to the intermediate visual context vectors to get the visual context vector.</p><p>Multimodal Attention. To fuse the text and vi- sual context information, we add a multimodal at- tention layer ( <ref type="bibr" target="#b15">Li et al., 2018a</ref>), as shown in <ref type="figure">Fig. 2</ref>. And the attention distribution is calculated as fol- lows:</p><formula xml:id="formula_9">e t txt = v T txt (W txt c t txt + U txt s t )<label>(10)</label></formula><formula xml:id="formula_10">e t img = v T img (W img c t img + U img s t )<label>(11)</label></formula><formula xml:id="formula_11">α t txt = softmax(e t txt ) (12) α t img = softmax(e t img )<label>(13)</label></formula><formula xml:id="formula_12">c t mm = α t txt c t txt + α t img c t img (14)</formula><p>where α t txt is the attention weight for text context vector and α t img is the attention weight for visual context vector.</p><p>Visual Coverage. In addition to the calculation of the text coverage vector as in Sec. 2.2, we also obtain a visual coverage vector cov t img , which is the sum of visual attention distributions. To help reduce repeated attention to multimodal informa- tion, we incorporate a text coverage loss and a vi- sual coverage loss into the loss function. The final loss function is as follows:</p><formula xml:id="formula_13">L t = −logp w * t + i min(α t i , cov t i ) + j min(α t j , cov t img,j ) (15)</formula><p>The attention mechanism can attend the salient parts of texts or images. Meanwhile, the coverage mechanism sums up all the historical attention dis- tributions. Therefore, we regard the coverage vec- tor as a global salience measure of the source be- ing attended. We then use the visual coverage vec- tor in the last decoding timestep to select the most relevant image. Concretely, we choose the image whose coverage score is the largest. The process is a bit different for the local features. An image corresponds to 49 patches, the coverage scores of these patches are summed up to get the salience score of the image as follows:</p><formula xml:id="formula_14">S j = patch cov t * patch,j<label>(16)</label></formula><p>where S j denotes the salience of the j-th image and cov t * patch,j denotes the coverage score of each corresponding image patch in the last decoding timestep t * . For the HAN, we introduce an ex- tra coverage vector for the image patches attention and calculate coverage loss for it as follows:</p><formula xml:id="formula_15">L t = −logp w * t + k min(α t k , cov t patch,k ) + i min(α t i , cov t i ) + j min(α t j , cov t img,j )<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal Automatic Evaluation</head><p>To evaluate the quality of a pictorial summary, we propose the MMAE method which is defined as y = f (m 1 , m 2 , m 3 ). In this definition, m 1 , m 2 , and m 3 denote scores measured by three met- rics which consider salience of text (Sec. 3.1), salience of image (Sec. 3.2), and image-text rele- vance (Sec. 3.3) respectively, f (·) denotes a map- ping function, and y denotes the score of the pic- torial summary.</p><p>In our experiments, the reference pictorial sum- mary consists of a text summary and a reference image set 4 ref img . In MMAE, m 1 is obtained by comparing the text summary in reference with that in model output, m 2 is obtained by comparing the image set in reference with the image in model output, and m 3 considers the image-text similar- ity in model output. To learn MMAE, we choose three simple methods to fit y with human judg- ment scores. These methods include Linear Re- gression (LR), and two nonlinear methods: Logis- tic Regression (Logis), and Multilayer Perceptron (MLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Salience of Text</head><p>ROUGE <ref type="bibr" target="#b20">(Lin, 2004b</ref>) is widely used to automat- ically assess the quality of text summarization systems. It has been shown that ROUGE cor- relates well with human judgments <ref type="bibr" target="#b19">(Lin, 2004a;</ref><ref type="bibr" target="#b25">Owczarzak et al., 2012;</ref><ref type="bibr" target="#b24">Over and Yen, 2004</ref>). Therefore, we directly apply ROUGE to assess the salience of the text units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Salience of Image</head><p>We propose a metric, namely, image precision (IP), to measure the salience of image. The im- age precision is defined as follows:</p><formula xml:id="formula_16">IP = |{ref img } ∩ {rec img }| |{rec img }|<label>(18)</label></formula><p>where ref img , rec img denote reference images and recommended images by MSMO systems respec- tively. The reasons for this metric are as follows.</p><p>A good summary should have good coverage of the events for both texts and images. The im- age in the output should be closely related to the events. So we formulate the image selection pro- cess as an image recommendation -instead of recommending items to users as in a recommen- dation system, we recommend the most salient image to an event. It can also be viewed as an image retrieval task, which retrieves the image most relevant to an event. Precision and recall are commonly used to evaluate recommendation systems (Karypis, 2001) and information retrieval task ( <ref type="bibr" target="#b38">Zuva and Zuva, 2012</ref>). However, we only care about whether the image appears in the ref- erence image set. Thus in our case, we are only interested in calculating precision metric. There- fore, we adapt the precision here as IP to measure image salience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image-Text Relevance</head><p>A prerequisite for a pictorial summary to help users accurately acquire information is that the im- age must be related to the text. Therefore, we re- gard the image-text relevance as one of metrics to measure the quality of the pictorial summary. We consider using visual-semantic embedding <ref type="bibr" target="#b9">(Faghri et al., 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2018)</ref> to calculate the cosine similarity between visual feature and tex- tual feature, which we use as image-text relevance. Visual-semantic embedding has been widely used in cross-modal retrieval ( <ref type="bibr" target="#b13">Kiros et al., 2014</ref>) and image captioning <ref type="bibr" target="#b11">(Karpathy and Fei-Fei, 2015)</ref>.</p><p>We apply VSE0 model of <ref type="bibr" target="#b9">Faghri et al. (2018)</ref>, which achieves state-of-the-art performance for image-caption retrieval task on the Flickr30K dataset ( <ref type="bibr" target="#b36">Young et al., 2014</ref>). The difference is that instead of training a CNN model to encode the image, we use the pretrained VGG19 to ex- tract global features. The text is encoded by a uni- directional Gated Recurrent Unit (GRU) to a se- quence of vector representations. Then we apply the max-over-time pooling <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>) to get a single vector representation. Next, the vi- sual features and text features are projected to a joint semantic space by two feed-forward neural networks. The whole network is trained using a max-margin loss:</p><formula xml:id="formula_17">L = ˆ c max(β − s(i, c) + s(i, ˆ c), 0) + ˆ i max(β − s(i, c) + s( ˆ i, c), 0)<label>(19)</label></formula><p>The loss comprises two symmetric terms, with i and c being images and captions repectively. The first term is taken over negative captionsˆccaptionsˆ captionsˆc image i in a batch. The second is over negative imagesîimagesî given caption c. If i and c are closer to each other in the joint embedding space than to any other neg- ative pairs, by a margin β, the loss is zero. We choose to use image-caption pairs in our dataset to train the VSE0 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct the following five sets of experiments: 1) To verify our motivation of the multimodal out- put (pictorial summary), we design an experiment for user satisfaction test (Sec. 4.2); 2) We compare our multimodal summarization with text summa- rization from both ROUGE score and manual eval- uation (Sec. 4.3); 3) To verify the effectiveness of our evaluation metrics, we calculate the correla- tion between these metrics and human judgments (Sec. 4.4); 4) We conduct two experiments to show the effectiveness of our proposed MMAE and the generalization of MMAE respectively (Sec. 4.5); 5) Finally, we evaluate our multimodal attention model with MMAE (Sec. 4.6).</p><p>The hyperparameters in our model are similar to <ref type="bibr" target="#b29">See et al. (2017)</ref>, except that we set the maximum number of images to 10, 7, and 7 for ATG, ATL, and HAN respectively, because different articles have the image collection of different sizes. The images are sorted in the order of the position in the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>There is no large-scale benchmark dataset for MSMO. We follow <ref type="bibr" target="#b10">Hermann et al. (2015)</ref> to con- struct a corpus from Daily Mail website 5 . Similar to <ref type="bibr" target="#b10">Hermann et al. (2015)</ref>, we use the manually- written highlights offered by Daily Mail as a ref- erence text summary. From Daily Mail, we ran- domly select articles within a week and find that 2,917 out of 2,930 articles contain images. More details are illustrated in <ref type="table">Table 1</ref> To get the pictorial reference, we employ 10 graduate students to select the relevant images from the article for each reference text summary. We allow annotators to select up to three images to reduce the difference between different annota- tors. If the annotators find that there is no relevant image, they will select none of them. Each arti- cle is annotated by at least two students <ref type="bibr">6</ref> . Since we use the text reference to guide the generation of the pictorial summary, we do not use the ref- erence image during training. Therefore, we only conduct the annotation on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User Satisfaction Test</head><p>We conduct an experiment to investigate whether a pictorial summary can improve the user satis- faction for the informativeness of the summary. For a fair comparison, we propose a novel strat- egy to compare text summaries and pictorial sum- maries. We take an example to illustrate our strat- egy. Given 100 source news pages, we have their corresponding reference text summaries and picto- rial summaries. We divide them into two parts of the same size, part 1 and part 2. In part 1, human annotator A evaluates the text summaries accord- ing to the input news, and human annotator B eval- uates the pictorial summaries. In part 2, annotator A evaluates the pictorial summaries and annota- tor B evaluates the text summaries. All annotators will give a score of 1 to 5. The input news is the same for annotator A and annotator B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Format</head><p>AnnotatorA AnnotatorB Overall   <ref type="table" target="#tab_4">Table 2</ref> shows our results for user satisfac- tion test. User ratings of pictorial summaries are 12.4% higher than text summaries. It shows that users prefer this way of presenting information. It also confirms our motivation for MSMO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Text Summarization</head><p>Our user satisfaction test in Sec. 4.2 is done in an ideal situation, comparing the text reference with the pictorial reference. To show the effec- tiveness of our model, we also compare our model with text summarization from ROUGE and human judgment scores. We compare several abstrac- tive summarization methods with our multimodal summarization methods. PGC 7 ( <ref type="bibr" target="#b29">See et al., 2017)</ref> refers to the pointer-generator network (Sec. 2.2). AED ( <ref type="bibr" target="#b23">Nallapati et al., 2016</ref>) uses an attentional encoder-decoder framework and adds some lin- guistic features such as POS, named-entities, and TF-IDF into the encoder. We also implement a seq2seq model with attention (S2S+attn). To compare the multimodal output with our multi- modal model, we propose an extractive method based on GuideRank (GR) ( <ref type="bibr" target="#b14">Li et al., 2016a</ref><ref type="bibr" target="#b17">Li et al., , 2018b</ref>. GuideRank applies LexRank ( <ref type="bibr" target="#b7">Erkan and Radev, 2004</ref>) with guidance strategy. In this strat- egy, captions recommend the sentences related to them. The rankings of sentences and captions are obtained through GR; we extract sentences that satisfy the length limit as a text summary accord- ing to the ranking of text. We select an image whose caption ranks the first in the captions. And finally, the pictorial summary is obtained. We evaluate different summarization models with the standard ROUGE metric, reporting the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L. Our ROUGE results are given in <ref type="table" target="#tab_6">Table 3</ref>, and human judgment scores are given in <ref type="table" target="#tab_7">Table 4</ref>   From <ref type="table" target="#tab_6">Table 3</ref>, all multimodal models lead to a decrease in ROUGE scores which can attribute to the following reasons. There are 6.56 images on average in each article and not every image is closely related to the event of the article. In other words, some images are noise. On the other hand, our text input is long text, and it contains enough information for text generation. In <ref type="table" target="#tab_7">Table 4</ref>, multi- modal models are better than text model in human judgments. It further illustrates our motivation, and also proves the effectiveness of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Correlation Test</head><p>To illustrate the effectiveness of our evaluation metrics, we conduct an experiment on correla- tions between these metrics and human judgment scores. Human annotators give a score which ranges from 1 to 5 to a pictorial summary accord-ing to the reference 8 . The reference consists of a text summary and up to three relevant images se- lected by humans. We randomly extract the pic- torial summaries from the output of different sys- tems. In response to the three aspects we proposed in Section 3, we propose some related metrics re- spectively. For text salience, we apply ROUGE-1, ROUGE-2, ROUGE-L, and BLEU. For image- text relevance of candidate pictorial summaries, we propose two ways. One is to calculate the similarity (Img-Sum) between the image and the whole text summary. The other is to calculate the similarities between the image and each sen- tence in the text summary. Then we take the maxi- mum and average values as two metrics: MAX sim and AVG sim . For image salience, in addition to the IP metric mentioned in Section 3.2, we try to calculate the similarity between the candidate im- age and each reference image in three ways: 1) I-I: similarities between the global fc7 features, 2) Hist: Bhattacharyya distance 9 for histogram comparison <ref type="bibr" target="#b1">(Bhattacharyya, 1943)</ref>, and 3) Temp: Fourier Analysis template matching <ref type="bibr" target="#b4">(Briechle and Hanebeck, 2001</ref>).</p><p>We employ annotators to evaluate 600 samples (randomly selected from the outputs of each model on the validation set). Each sample is scored by two persons and we take the average score as the final score. We use 450 of them as training set to train the MMAE model in Sec. 4.5, the rest is used as test set. The scores calculated by each evalua- tion metric are then tested on the training set to see how well they correlate with human judgments. The correlation is evaluated with three metrics, in- cluding 1) Pearson correlation coefficient (r), 2) Spearman rank coefficient (ρ), and 3) Kendall rank coefficient (τ ). Our results of correlation test are given in <ref type="table" target="#tab_9">Table 5</ref>.</p><p>As shown in <ref type="table" target="#tab_9">Table 5</ref>, IP (Image Precision) cor- relates best with human assessments according to the three correlation coefficients. It illustrates that people pay more attention to images when assess- ing pictorial summaries. If we choose the right image for the summary, people are more likely to assign a high score. We also note that the corre- lation score of IP is significantly higher than four <ref type="bibr">8</ref> Some articles are annotated with no relevant images (about 3.9%), we directly skipped these articles without man- ual scoring <ref type="bibr">9</ref> In statistics, the Bhattacharyya distance measures the similarity of two discrete or continuous probability distribu- tions. For a distance d, we take (1 − d) as the similarity.  text metrics. Because it is easy for a person to judge the importance of images based on refer- ence, such as to see whether the image appears in reference. However, measuring the semantic sim- ilarity of two texts is difficult. The four metrics all measure the degree of n-gram overlap which can- not accurately measure semantic similarity. For the image-text relevance, MAX sim performs best and is comparable to the several ROUGE met- rics. It shows that in a good pictorial summary, the image and text should be relevant. In some cases, even though the generated text is not so im- portant, the image is closely related to the text. At this time, people can also be satisfied. On the other hand, our VSE0 (Sec. 3.3) model can cap- ture some fluency of sentences by adopting GRU. Compare MAX sim , AVG sim , and Img-Sum, this is very intuitive. Once people find a sentence (or a part) relevant to the image, they will think the image is related to the text. Besides, the worst performance of Img-Sum metric is probably be- cause the average length of captions used to train VSE0 model is about 22, far less than the length of the summary. We find the I-I (max) and I-I (avg) nearly do not correlate with human assess- ments. It shows that the visual features extracted from VGG19 are not suitable for calculating the similarity between news images. The analysis of Hist (Temp) avg and Hist (Temp) max is similar to the analysis of MAX sim and AVG sim above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness and Generalization of MMAE</head><p>We then select the best-performing metrics sep- arately from the three sets of metrics, namely ROUGE-L, MAX sim , and IP. We apply LR, MLP, and Logis to learn our MMAE model that com- bines the three metrics. We calculate the three co- efficients for the three metrics on the test set as a comparison. The correlation results are given in  It is crucial that MMAE can generalize for a previously unseen system. To test the generaliza- tion of MMAE, we use MMAE to evaluate a new system and calculate the correlation with human judgment scores. The new system is a naive model which applies LexRank to extract sentences and randomly select an image from source. We can ob- serve that MMAE still correlates well with human judgment scores, as shown in <ref type="table" target="#tab_13">Table 7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Performances</head><p>According to our analyses above, we have proved MMAE can evaluate multimodal output. In this section, we report the MMAE scores for our pro- posed multimodal attention model, as shown in <ref type="table" target="#tab_15">Table 8</ref>. <ref type="bibr">10</ref> The weight for ROUGE-L, MAXsim, and IP is  Surprisingly, the model ATG achieves the high- est MMAE score despite the mediocre perfor- mance in three individual metrics. The MAX sim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn bet- ter image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our pro- posed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Different from text summarization ( <ref type="bibr" target="#b32">Wan and Yang, 2006;</ref><ref type="bibr" target="#b28">Rush et al., 2015;</ref><ref type="bibr" target="#b29">See et al., 2017;</ref><ref type="bibr" target="#b5">Celikyilmaz et al., 2018;</ref><ref type="bibr" target="#b26">Paulus et al., 2018)</ref>, Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text informa- tion, but it can also utilize the rich visual content from the images.</p><p>In recent years, much work has focused on mul- timodal summarization. <ref type="bibr" target="#b8">Evangelopoulos et al. (2013)</ref> detect salient events in a movie based on the saliency of individual features for aural, visual, and linguistic representations.  generate the text summary from an asynchronous collection of text, image, audio, and video. There has also been some work <ref type="bibr" target="#b2">(Bian et al., 2013</ref><ref type="bibr" target="#b3">(Bian et al., , 2015</ref><ref type="bibr" target="#b34">Wang et al., 2016;</ref><ref type="bibr" target="#b27">Qian et al., 2016</ref>) focused on producing multimodal output for summarization. <ref type="bibr" target="#b2">Bian et al. (2013</ref><ref type="bibr" target="#b3">Bian et al. ( , 2015</ref> aim to produce a visual- ized summary for microblogs. <ref type="bibr" target="#b34">Wang et al. (2016)</ref> generate a pictorial storyline for summarization. <ref type="bibr" target="#b27">Qian et al. (2016)</ref> generate the multimedia topics for social events. But these researches all treat image-text pairs, in which texts and images are aligned, as a basic summarization unit. For exam- ple, the images are aligned with the text in a mi-croblog post; <ref type="bibr" target="#b34">Wang et al. (2016)</ref> obtain the image- text pairs by using image search engine. None of the above works focuses on generating multimodal output from a collection of texts and images that are not explicitly aligned. This is one of the goals in this paper. Another difference is that they sep- arately evaluate texts and images when evaluating the final results. In our work, we propose a new automatic evaluation which jointly considers two aspects of textual and visual modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we focus on a novel task which aims to automatically generate a multimodal summary from multimodal news, where the images and the texts are not explicitly aligned. We provide a mul- timodal summarization method to jointly gener- ate text and the most relevant image, which can be referred as the baseline for further study. Our proposed metrics have been proved to be effective in evaluating the multimodal output. Moreover, the idea of constructing our MMAE can be eas- ily extended to other modalities. That is, we both consider the intramodality salience and the inter- modality relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: The illustration of our proposed taskMultimodal Summarization with Multimodal Output (MSMO). The image can help better understand the text in the red font.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>train 
valid 
test 

#Documents 
293,965 10,355 10,261 
#ImgCaps 
1,928,356 68,520 71,509 
#AvgTokens(S) 
720.87 766.08 730.80 
#AvgTokens(R) 
70.12 
70.02 
72.16 
#AvgCapTokens 
22.07 
22.64 
22.34 
#AvgImgCaps 
6.56 
6.62 
6.97 

Table 1: Corpus statistics. Each image on the website 
is paired with a caption. #ImgCaps denotes the number 
of image-caption pairs. #AvgTokens(S), #AvgTokens(R) 
and #AvgCapTokens denote the average number of to-
kens in articles, highlights, and captions respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>User satisfaction test results. In total, we use 
the strategy mentioned in Section 4.2 to evaluate 400 
randomly selected source news pages. Overall denotes 
the average score on these 400 samples. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Model 
ROUGE-1 ROUGE-2 ROUGE-L 

S2S+attn 
32.32 
12.44 
29.65 
Base AED 
34.78 
13.10 
32.24 
PGC 
41.11 
18.31 
37.74 

ATG 
40.63 
18.12 
37.53 

MM 
ATL 
40.86 
18.27 
37.75 
HAN 
40.82 
18.30 
37.70 
GR 
37.13 
15.03 
30.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>ROUGE F1 scores on our test set. All our 
ROUGE scores are reported by official ROUGE script. 

Model PGC ATG ATL HAN 

HS 
3.07 
3.30 3.22 
3.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Human judgment scores for our multimodal 
model and PGC. We randomly select 400 articles and 
use the same strategy as Sec. 4.2. HS denotes the aver-
age human judgment scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Correlation with human judgment scores 
(training set), measured with Pearson r, Spearman ρ, 
and Kendall τ coefficients. The max and avg denote 
the maximum and average value of the scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Metric 
r 
ρ 
τ 

ROUGE-L 
.3488 .3554 .2669 
MAXsim 
.2541 .2339 .1773 
IP 
.5982 .5966 .5485 

MMAELR 
.6646 .6644 .5265 
MMAEMLP .6632 .6646 .5265 
MMAELogis .6630 .6653 .5277 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Correlation with human judgment scores (test 
set). 

As shown in Table 6, the MMAE learned by 
three methods correlates better with human judg-
ments. Although MMAE Logis gets a slightly 
higher correlation score according to Spear-
man and Kendall coefficients, we choose the 
MMAE LR 
10 as our final MMAE model due to Oc-
cam's Razor 11 . 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>.</head><label></label><figDesc></figDesc><table>It illustrates 
that MMAE generalize well for a new model. We 
give some examples of MMAE in supplementary 
material. 

Metric 
r 
ρ 
τ 

ROUGE-L .3223 .3514 .2615 
MMAE 
.6352 .6318 .4728 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Correlation results for the new model on the 
same 150 test samples as in Sec. 4.4. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 8 : Results evaluated by our MMAE method.</head><label>8</label><figDesc></figDesc><table>We 
</table></figure>

			<note place="foot" n="1"> Note that in this work, the multimodal output refers to a pictorial summary which contains one image (for the sake of</note>

			<note place="foot" n="2"> Our dataset has been released to the public, which can be found in http://www.nlpr.ia.ac.cn/cip/ jjzhang.htm. 3 http://www.robots.ox.ac.uk/ ˜ vgg/ research/very_deep</note>

			<note place="foot" n="4"> More details can be found in Sec. 4.1</note>

			<note place="foot" n="5"> http://www.dailymail.co.uk 6 A third annotator will be asked to decide the final annotation for the case of divergence for the first two annotators.</note>

			<note place="foot" n="7"> https://github.com/abisee/ pointer-generator</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On a measure of divergence between two statistical populations defined by their probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Calcutta Math. Soc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimedia summarization for trending topics in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1807" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimedia summarization for social events in microblog stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Template matching using fast normalized cross correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Briechle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical Pattern Recognition XII</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4387</biblScope>
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep communicating agents for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1662" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasia</forename><surname>Zlatintsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1553" to="1568" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Konstantinos Rapantzikos, Georgios Skoumas, and Yannis Avrithis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vse++: Improving visualsemantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation of item-based top-n recommendation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Guiderank: A guided ranking graph model for multilingual multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifth Conference on Natural Language Processing and Chinese Computing &amp; The Twenty Fourth International Conference on Computer Processing of Oriental Languages (NLPCC-ICCPOL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="608" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-modal sentence summarization with modality attention and image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4152" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal summarization for asynchronous collection of text, image, audio and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1092" to="1102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Read, watch, listen and summarize: Multi-modal summarization for asynchronous text, image, audio and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimedia news summarization in search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Looking for a few good metrics: Automatic summarization evaluation-how many samples are enough?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NII Testbeds and Community for information access Research (NTCIR)</title>
		<meeting>NII Testbeds and Community for information access Research (NTCIR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal stereoscopic movie summarization conforming to narrative characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mademlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anastasios Tefas, Nikos Nikolaidis, and Ioannis Pitas</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5828" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An introduction to duc 2004 intrinsic evaluation of generic new text summarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Document Understanding Conference (DUC)</title>
		<meeting>the Document Understanding Conference (DUC)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An assessment of the accuracy of automatic evaluation in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-modal multi-view topic-opinion mining for social event analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Shengsheng Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Leveraging multimodal information for event summarization and concept-level sentiment analysis. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajiv Ratn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhua</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anwar Dilawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved affinity graph based multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL</title>
		<meeting>the Human Language Technology Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
	<note>Jing Huang, and Svetlana Lazebnik</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Augmenting neural sentence summarization through extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Natural Language Processing and Chinese Computing (NLPCC)</title>
		<meeting>the 6th Conference on Natural Language Processing and Chinese Computing (NLPCC)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of information retrieval systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keneilwe</forename><surname>Zuva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tranos</forename><surname>Zuva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
