<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Similar but not the Same: Word Sense Disambiguation Improves Event Detection via Neural Representation Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Lu</surname></persName>
							<email>weiyi.lu@nyu.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><forename type="middle">Huu</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">University of Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Oregon</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename></persName>
						</author>
						<title level="a" type="main">Similar but not the Same: Word Sense Disambiguation Improves Event Detection via Neural Representation Matching</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4822" to="4828"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4822</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Event detection (ED) and word sense disam-biguation (WSD) are two similar tasks in that they both involve identifying the classes (i.e. event types or word senses) of some word in a given sentence. It is thus possible to extract the knowledge hidden in the data for WSD, and utilize it to improve the performance on ED. In this work, we propose a method to transfer the knowledge learned on WSD to ED by matching the neural representations learned for the two tasks. Our experiments on two widely used datasets for ED demonstrate the effectiveness of the proposed method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important aspect of natural language process- ing involves understanding events mentioned in text. Towards this end, event detection (ED) is the task of locating event triggers (usually verbs or nouns) within a given text, and classifying them among a given set of event types. This task re- mains challenging due to the inherent ambiguity and flexibility of natural languages. The current state-of-the-art methods for ED have involved ap- plying deep learning (DL) models to automatically extract feature representations of the text, and then treating the task as a classification problem <ref type="bibr" target="#b3">(Chen et al., 2015;</ref><ref type="bibr">Nguyen and Grishman, 2015b)</ref>.</p><p>The major intuition in this paper is that the task of ED is closely related to the task of word sense disambiguation (WSD) whose datasets can help to improve the performance of the DL models for ED. This is due to the goal of WSD to determine the sense of a word within a particular context, given a set of possible senses that the word can take on. Our intuition is based on the two follow- ing aspects:</p><p>(i) Similar Context Modeling: Given a word in a context/sentence, both ED and WSD models need to select/predict a correct label in a list of candi- date labels for the word. For WSD, the candi- date labels are the possible senses (e.g, sense ids in WordNet) that the word of interest can have, while for ED, they are the set of predetermined event types (e.g, the event subtypes in the ACE 2005 dataset 1 ). Consider the word "fired" in the following sentence as an example:</p><p>The boss fired his secretary today.</p><p>For WSD, there are 12 possible senses for the verb "fire" in WordNet in which the correct la- bel for the word "fired" in this case is the sense id "fire%2:41:00::" (i.e, "terminate the employ- ment of "). The ED task in the ACE 2005 dataset, on the other hand, involves 33 possible event sub- types with "End-Position" as the correct event sub- type/label for the word "fired" in our example.</p><p>In order to make such label predictions, both ED and WSD need to model the word itself and its context (i.e, the words "fired", "boss", and "secre- tary" in the example). This similar modeling al- lows the same DL model to be adopted for both ED and WSD, facilitating the use of WSD data to improve the feature representations for ED via pa- rameter/representation tying.</p><p>(ii) Close Semantic Consideration: As there are some overlaps between the semantic differentia- tion in WSD and ED, the knowledge/information from WSD about a particular word in a context can help to make a better prediction for that word in ED. For instance, in the example above, the knowledge from WSD that the word "fired" is referring to a termination of employment would clearly help ED to identify "End-Position" as the correct event type (rather than the incorrect event type "Attack") for "fired" in this case.</p><p>How can we exploit this intuition to improve the performance of the DL models for ED with WSD data? In this work, we propose a novel method based on representation matching to transfer the knowledge learned from the WSD data to the DL models for ED. In particular, two separate deep learning models are employed to model the con- text for WSD and ED. The two models share the network architecture, but involve different param- eters that are specific to the tasks. We then trans- fer the knowledge from the WSD network to the ED network by ensuring that the feature represen- tations learned by the two networks on the same contexts are similar to each other.</p><p>We demonstrate the effectiveness of the pro- posed method on two widely used datasets for ED. To the best of our knowledge, this is the first work to study the transfer learning/multi-task learning methods for WSD and ED with DL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We consider the typical setting where <ref type="bibr">we</ref>  ∈ Y wsd and y ed i ∈ Y ed ). Our goal is to trans- fer the knowledge learned from the D wsd dataset to improve the performance of the ED models trained on the D ed dataset (multi-task learning).</p><p>In the following, we will first describe the deep learning architectures to transform the sentences W in the datasets D wsd and D ed into representa- tion vectors. We only focus on the deep learning architectures proposed for ED in the literature to achieve compatible comparisons for ED. The pro- posed multi-task learning method for ED with the WSD dataset will follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Computing the Feature Representations</head><p>Consider a sentence W in the datasets D wsd or D ed that is represented as a sequence of tokens W = [w 0 , w 1 , . . . , w t ]. Let p be the index of the word of interest in this sentence. The con- text for w p in W is constructed by taking the word itself, the n preceding words, and the n follow- ing words (padding or truncating when necessary). The tokens in the context are re-indexed to form</p><formula xml:id="formula_0">an instance V = [v 0 , v 1 , . . . , v n , . . . , v 2n−1 , v 2n ],</formula><p>where v n corresponds to w p in W .</p><p>Encoding The first step to prepare the instance V for the deep learning models is to map each token v j in V into two real-valued vectors, which are then con- catenated to form a vector representation x j for v j <ref type="bibr">(Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b3">Chen et al., 2015</ref>):</p><p>1. The word embedding of v j obtained by look- ing up the token v j in the pre-trained word embed- ding table <ref type="bibr" target="#b24">(Mikolov et al., 2013a</ref>).</p><p>2. The position embedding vector for v j : ob- tained by looking up the relative distance j − n of v j with respect to the token of interest v n in a posi- tion embedding table (randomly initialized) <ref type="bibr" target="#b3">(Chen et al., 2015;</ref><ref type="bibr">Nguyen and Grishman, 2015a)</ref>.</p><p>It is important to note that, different from the prior works <ref type="bibr">(Nguyen and Grishman, 2015b;</ref>), we do not include the entity type la- bel of each token into its representation. This is a more realistic setting for our work as the golden entity mentions do not always exist in practice, es- pecially for the datasets in WSD.</p><p>Once each token v j is converted into the representation vector x j , the in- stance V becomes a sequence of vectors</p><formula xml:id="formula_1">X = [x 0 , x 1 , . . . , x n , . . . , x 2n−1 , x 2n</formula><p>] that would be fed into the one of the following deep learning models to learn a feature representation R for V .</p><p>Typical Deep Learning Models for ED 1. CNN: This is the convolutional neural net- works in <ref type="bibr">(Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b3">Chen et al., 2015)</ref>. It features convolution op- erations that are performed over the k consec- utive vectors (k-grams) in X and followed by a max-pooling layer to generate the represen- tation vector R for V . Multiple window val- ues k are used to enhance the coverage of the model over the hidden k-grams in the con- text.</p><p>2. NCNN (Nguyen and Grishman, 2016d): This model is similar to CNN. The only differ- ence is instead of running the convolution over the k consecutive vectors, NCNN con- volutes over the k arbitrarily non-consecutive k vectors in V . This helps NCNN to explic- itly model the non-consecutive words in the context to improve ED.</p><p>3. BiRNN: This is the bidirectional recurrent neural network (RNN) for event extraction in ( <ref type="bibr">Nguyen et al., 2016a</ref>). The model is composed of two recurrent neural networks (RNN), where one runs forward and the other runs backward through the input sequence V . The hidden vectors produced by the two net- works are then concatenated at each position in the context. The vector at the position of n for the word of interest is used as the repre- sentation vector R for V . Due to the property of RNN, R encodes the information over the whole input V with a greater focus on v n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CNN+BiRNN:</head><p>In this model <ref type="bibr" target="#b8">(Feng et al., 2016</ref>), X is passed through both a CNN and a BiRNN whose results are concatenated to produce the hidden representation R for ED. The expectation is to take advantage of the modeling abilities from both the CNN and BiRNN architectures for ED.</p><p>In practice, the representation vector R (ob- tained from one of the deep learning models above) is also concatenated with the word embed- dings of the tokens surrounding the token of inter- est w n to improve its expressiveness ( <ref type="bibr" target="#b3">Chen et al., 2015;</ref><ref type="bibr">Nguyen and Grishman, 2016d</ref>). We would use this extended version when we refer to R in the following.</p><p>In the final step, the representation vector R is fed into a feed-forward neural network followed by a softmax layer to perform predictions for ED and WSD.</p><p>For convenience, we denote the whole process that a DL model M is used to compute the repre- sentation vector R for the input sentence W with the token index p of interest as: R = M (W, p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task Learning Models</head><p>The previous section has described the deep learn- ing methods that can be employed to train the models for ED and WSD separately. This sec- tion presents our proposed method to transfer the knowledge from the WSD dataset to improve the performance for ED.</p><p>A typical method for transfer learning/multi- task learning in NLP is to alternate the training process for the parameter-shared models of the re- lated tasks (possibly with different datasets) ( <ref type="bibr" target="#b9">Guo et al., 2016;</ref><ref type="bibr" target="#b17">Li et al., 2015;</ref>). For instance, in <ref type="bibr" target="#b9">(Guo et al., 2016)</ref>, the authors use the same deep learning model to learn the feature rep- resentations for the text inputs of two related tasks. This is then followed by task-specific output lay- ers to perform the corresponding tasks. Note that the two tasks in ( <ref type="bibr" target="#b9">Guo et al., 2016)</ref> are provided with two different datasets of different text inputs, thereby being similar to the setting we consider in this work. In order to learn the parameters for this model, in each iteration, ( <ref type="bibr" target="#b9">Guo et al., 2016</ref>) se- lect one of the tasks with some probabilities, sam- ple a mini-batch of examples in the dataset of the chosen task, and update the model parameters us- ing the objective function specific to the chosen task. Consequently, the model parameters for fea- ture representation learning are updated at every iteration while only the model parameters in the output layer for the chosen task are updated at the current iteration.</p><p>It has been demonstrated in ( <ref type="bibr" target="#b9">Guo et al., 2016</ref>) that the alternating method (called ALT) is more effective than pre-training the network on a related task and fine-tuning it on the expected task. We thereby consider ALT as the baseline for multi- task learning in our work. However, we argue that this baseline is not effective enough to trans- fer the knowledge from the WSD dataset to ED in our case. This stems from its employment of a single DL model to induce the representations for the text inputs in both tasks. In our case of WSD and ED, although there are some overlap be- tween the semantic differentiation of the two tasks, the labels in the WSD datasets (i.e, the sense ids) tend to be more fine-grained and exhaustive than those in ED. For instance, for the word "fire", there might be 12 WSD labels for it in WordNet while the number of possible event types for "fire" in the ACE 2005 dataset is only 2 (i.e, "End-Position" and "Attack"). Eventually, if a single DL model is used to compute the representations for the text inputs in both WSD and ED, the model would suf- fer from a confusion to distinguish such subtlety in the semantic differentiation.</p><p>In order to overcome this issue, we propose to employ two versions M wsd and M ed of the same DL model (with different model parameters) to compute the feature representations for WSD and ED respectively. We then transfer the knowledge from M wsd to M ed by encouraging the represen- tations generated by the two versions M wsd and M ed on the same text inputs to be similar. For- mally, let (W t , p t , y t ) be an example in the D wsd or D ed dataset (t ∈ {wsd, ed}). Also, let R wsd and R ed be the representations for (W t , p t ) in- duced by M wsd and M ed respectively: Such representation vectors are then followed by a task-specific output layer F t (i.e, feed-forward neural networks followed by a softmax layer) to compute the probability distribution over the pos- sible labels for (W t , p t ): P t (Y t |R t ) = F t (R t ) where Y t is the label set for the t task.</p><p>If the two models M wsd and M ed were trained separately, the objective function for the t task for the current example would be the negative log- likelihood: C t (W t , p t , y t ) = − log P t (y t |R t ). In this work, instead of just optimizing this objective, we optimize the joint function:</p><formula xml:id="formula_2">C t (W t , p t , y t ) = − log P t (y t |R t ) + λ 1 d R d R i=0 R wsd i − R ed i 2</formula><p>where λ is a trade-off parameter and d R is the di- mension of the representation vectors. The second term in the joint objective function enforces that the feature representations learned by M wsd and M ed on the same input context (W t , p t ) are close to each other (t ∈ {wsd, ed}). One the one hand, this representation matching schema helps the two models to communicate to each other so the knowledge from one model can be passed to the other one. On the other hand, the use of two separate models leaves a flexibility for the models to induce the task-specific structures.</p><p>Presumably, the objective function (2.2) can si- multaneously improve the performance for both tasks of consideration. However, in our case of ED and WSD, it turns out this mechanism actu- ally worsen the performance of the WSD models that were trained separately. We attribute this to the fact that the semantic differentiation in ED is more coarse-grained that that of WSD, causing the ineffectiveness of the datasets for ED to improve WSD performance. Eventually, we will just focus on the ED performance in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameters and Datasets</head><p>We use the Semcor dataset <ref type="bibr" target="#b25">(Miller et al., 1994)</ref> as the dataset for WSD in this work. This dataset was extracted from the Brown Corpus, and manually annotated with WordNet senses. We evaluate the models on two different datasets for ED:</p><p>1. ACE 2005: This dataset has 33 event sub- types. We use the same data split with the prior work ( <ref type="bibr" target="#b3">Chen et al., 2015;</ref><ref type="bibr">Nguyen and Grishman, 2015b</ref>). In particular, 40 newswire documents are used for testing, 30 other documents are reserved for validation, and the 529 remaining documents form the training data.</p><p>2. We use the pre-trained word embeddings pro- vided by <ref type="bibr">(Nguyen and Grishman, 2016d</ref>). For CNN, NCNN and CNN+BiRNN, we employ filter sizes of {2, 3, 4, 5} with 300 filters for each size as in <ref type="bibr">(Nguyen and Grishman, 2015b)</ref>, while Gated Recurrent Units ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>) with 300 hid- den units are applied in <ref type="bibr">BiRNN and CNN+BiRNN (as do (Nguyen and Grishman, 2016d)</ref>). For the other parameters, the best values suggested by the development data include: a dropout rate of 0.5, a feed-forward neural network with one hidden layer of 1200 hidden units for the out- put layers, and the penalty rate λ of 0.01 for both CNN and BiRNN, 0.6 for NCNN, and 0.7 for CNN+BiRNN in the proposed transfer learn- ing method (called MATCHING). For simplicity, the same hyper-parameters are used for the two versions of the same network architecture in the MATCHING method. We utilize Adadelta <ref type="bibr">(Zeiler, 2012</ref>) with back-propagation to train the models in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>In this section, we compare the proposed MATCH- ING method with the transfer learning baseline ALT in ( <ref type="bibr" target="#b9">Guo et al., 2016</ref>) and the separate training mechanism for ED (called SEPARATE) employed in the previous work for ED <ref type="bibr" target="#b3">(Chen et al., 2015;</ref><ref type="bibr">Nguyen and Grishman, 2015b)</ref>. Note that in the SEPARATE method, the models are only trained on the datasets for ED without utilizing any trans- fer learning techniques with external datasets. We report the performance when each of the DL meth- ods in Section 2.1 is used as the network to learn the feature representations for ED and WSD. <ref type="table" target="#tab_3">Tables 1 and 2</ref> present the performance (i.e, F1 scores) of the models on the ACE 2005 and TAC 2015 datasets respectively. The first observa- tion is that the proposed transfer learning method MATCHING is consistently better than the base- line method ALT across different deep learning models and datasets with large performance gap. This is significantly with p &lt; 0.05 and confirms our hypothesis in Section 2.2 about the advantage of the proposed MATCHING over the alternating training method ALT for ED and WSD. In fact, the performance of the ALT method is even worse than the traditional SEPARATE method also over different network architectures and datasets. Con- sequently, training a single deep learning model on a combination of ED and WSD data (as in ALT) does not automatically enable the model to learn to exploit the similar structures of the two tasks. In contrast, it hinders the model's ability to effec- tively extract hidden representations for ED.</p><p>Comparing MATCHING and SEPARATE, we see that MATCHING helps to improve SEPARATE with respect to difference choices of the DL mod- els. The performance improvement is significant for CNN and BiRNN on ACE 2005 and for all the models on TAC 2015. Such results demonstrate the effectiveness of the WSD dataset for ED and the ability of the proposed method MATCHING to promote knowledge transferring between WSD and ED to improve ED performance.</p><p>Regarding the best reported performance, our best performance on ACE (i.e, 71.2% with CNN) is comparable with the recent state-of-the-art per- formance (i.e, <ref type="table" target="#tab_3">Table 1</ref>). However, we note that such work heavily relies on the manual anno- tation of the entity mentions in the documents. Our current work do not employ such informa- tion to better reflect the realistic setting. For the TAC 2015 dataset, our best performance is 60.7% with CNN+BiRNN although the performance of the other models is also very close. This perfor- mance is better than the best performance that has been reported on the TAC 2015 (i.e, <ref type="table" target="#tab_4">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Prior works on ED include statistical models with manual feature engineering <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b14">Ji and Grishman, 2008;</ref><ref type="bibr" target="#b12">Hong et al., 2011;</ref><ref type="bibr" target="#b16">Li et al., 2013;</ref><ref type="bibr">Venugopal et al., 2014;</ref><ref type="bibr" target="#b17">Li et al., 2015)</ref>, followed by neural network models, such as CNNs ( <ref type="bibr">Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b3">Chen et al., 2015</ref>  <ref type="bibr">Nguyen and Grishman, 2016d)</ref> 71.3* (  71.9* ( <ref type="bibr" target="#b19">Liu et al., 2018)</ref> 72.4* ( <ref type="bibr">Nguyen and Grishman, 2018a)</ref> 73.1*   <ref type="bibr">, 2016)</ref>, and attention-based methods ( <ref type="bibr" target="#b27">Nguyen and Nguyen, 2018b)</ref>.</p><p>A similar trend exists in methods proposed for WSD, with feature based methods <ref type="bibr" target="#b25">(Miller et al., 1994;</ref><ref type="bibr">Zhong and Ng, 2010;</ref><ref type="bibr">Taghipour and Ng, 2015)</ref> succeeded recently by deep learning meth- ods ( <ref type="bibr">Yuan et al., 2016;</ref><ref type="bibr">Raganato et al., 2017</ref>).</p><p>For multi-task learning in NLP, methods have been proposed for jointly modeling structured prediction tasks ( <ref type="bibr" target="#b10">Hatori et al., 2012;</ref><ref type="bibr" target="#b18">Li et al., 2011;</ref><ref type="bibr" target="#b1">Bohnet and Nivre, 2012;</ref><ref type="bibr" target="#b11">Henderson et al., 2013;</ref><ref type="bibr" target="#b22">Lluís et al., 2013;</ref><ref type="bibr" target="#b7">Duong et al., 2015)</ref>, and for sequence-to-sequence problems ( <ref type="bibr" target="#b6">Dong et al., 2015;</ref><ref type="bibr" target="#b23">Luong et al., 2015;</ref><ref type="bibr" target="#b15">Klerke et al., 2016)</ref>. The prior work to solve multiple NLP tasks using an unified architecture includes <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b9">Guo et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a method that improves the perfor- mance of deep learning models for ED by training two different versions of the same network archi- tecture for ED and WSD, while encouraging the knowledge transfer between the two versions via representation matching. The proposed method produces better results across a variety of deep learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TAC 2015 :</head><label>2015</label><figDesc></figDesc><table>This dataset was released in the 
Event Nugget Detection Evaluation of the 
2015 Text Analysis Conference (TAC) (Mi-
tamura et al., 2015). It comes with 38 event 
subtypes. We follow the data split in the of-
ficial evaluation to achieve compatible com-
parison. As TAC 2015 does not have a devel-
opment set, we use the best parameters tuned 
on ACE 2005 for the experiments with TAC 
2015. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance on the ACE 2005 dataset. * indi-
cates the use of entity mention annotation. 

Method 
CNN 
BiRNN 
NCNN CNN+BiRNN 
SEPARATE 
57.6 
59.4 
58.3 
58.0 
ALT 
57.6 
54.9 
48.5 
57.5 
MATCHING 60.0 
60.4 
60.0 
60.7 
TAC TOP (Mitamura et al., 2015) 
58.4* 
(Nguyen and Grishman, 2018a) 
58.8* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on the TAC 2015 dataset. * indi-
cates the use of entity mention annotation. 

et al., 2016b,e; Chen et al., 2017), RNNs (Nguyen 
et al., 2016a; Jagannatha and Yu</table></figure>

			<note place="foot" n="1"> https://www.ldc.upenn.edu/collaborations/past-projects/ ace</note>

			<note place="foot">R wsd = M wsd (W t , p t ), R ed = M ed (W t , p t )</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatically labeled data generation for large scale event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A languageindependent neural network for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for semantic role labeling and relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hatori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Musillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="949" to="998" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional rnn for medical event detection in electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abhyuday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jagannatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving sentence compression by learning to predict gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigrid</forename><surname>Klerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03357</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving event detection with abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP Workshop on Computing News Storylines (CNewS)</title>
		<meeting>ACL-IJCNLP Workshop on Computing News Storylines (CNewS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint models for chinese pos tagging and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Event detection via gated multilingual attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Joint arc-factored parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<title level="m">Multi-task sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using a semantic concordance for sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shari</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert G</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Overview of tac kbp 2015 event nugget track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Who is killed by police: Introducing supervised attention for hierarchical lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien Huu</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">New york university 2016 system for kbp event nugget: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Thien Huu Nguyen</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Meyers</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
