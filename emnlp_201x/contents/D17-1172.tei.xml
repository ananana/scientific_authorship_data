<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caio</forename><surname>Corro</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratoire d&apos;Informatique de Paris Nord</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">Université Paris 13 -SPC</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-93430</postCode>
									<settlement>Villetaneuse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratoire d&apos;Informatique de Paris Nord</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">Université Paris 13 -SPC</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-93430</postCode>
									<settlement>Villetaneuse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roux</forename><forename type="middle">Mathieu</forename><surname>Lacroix</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratoire d&apos;Informatique de Paris Nord</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">Université Paris 13 -SPC</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-93430</postCode>
									<settlement>Villetaneuse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1644" to="1654"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new method for the joint task of tagging and non-projective dependency parsing. We demonstrate its usefulness with an application to discontinu-ous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly. The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through Lagrangian relaxation. We evaluate this model and obtain state-of-the-art results despite strong independence assumptions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discontinuous phrase-structure parsing relies ei- ther on formal grammars such as LCFRS, which suffer from a high complexity, or on reductions to non-projective dependency parsing with complex labels to encode phrase combinations. We pro- pose an alternative approach based on a variant of spinal TAGs, which allows parses with disconti- nuity while grounding this work on a lexicalized phrase-structure grammar. Contrarily to previous approaches, ( <ref type="bibr" target="#b16">Hall and Nivre, 2008;</ref><ref type="bibr">Versley, 2014;</ref><ref type="bibr" target="#b15">Fernández-González and Martins, 2015)</ref>, we do not model supertagging nor spine interactions with a complex label scheme. We follow <ref type="bibr" target="#b4">Carreras et al. (2008)</ref> but drop projectivity.</p><p>We first show that our discontinuous variant of spinal TAG reduces to the Generalized Max- imum Spanning Arborescence (GMSA) problem <ref type="bibr" target="#b31">(Myung et al., 1995)</ref>. In a graph where vertices are partitioned into clusters, GMSA consists in finding the arborescence of maximum weight in- cident to exactly one vertex per cluster. This prob- lem is NP-complete even for arc-factored models. In order to bypass complexity, we resort to La- grangian relaxation and propose an efficient res- olution based on dual decomposition which com- bines a simple non-projective dependency parser on a contracted graph and a local search on each cluster to find a global consensus.</p><p>We evaluated our model on the discontinuous PTB ( <ref type="bibr" target="#b13">Evang and Kallmeyer, 2011</ref>) and the Tiger ( <ref type="bibr" target="#b2">Brants et al., 2004</ref>) corpora. Moreover, we show that our algorithm is able to quickly parse the whole test sets.</p><p>Section 2 presents the parsing problem. Sec- tion 3 introduces GMSA from which we derive an effective resolution method in Section 4. In Sec- tion 5 we define a parameterization of the parser which uses neural networks to model local prob- abilities and present experimental results in Sec- tion 6. We discuss related work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Joint Supertagging and Spine Parsing</head><p>In this section we introduce our problem and set notation. The goal of phrase-structure parsing is to produce a derived tree by means of a se- quence of operations called a derivation. For in- stance in context-free grammars the derived tree is built from a sequence of substitutions of a non- terminal symbol with a string of symbols, whereas in tree adjoining grammars (TAGs) a derivation is a sequence of substitutions and adjunctions over elementary trees. We are especially interested in building discontinuous phrase-structure trees which may contain constituents with gaps. <ref type="bibr">1</ref> We follow <ref type="bibr">Shen (2006)</ref> and build derived trees from adjunctions performed on spines. Spines are lexicalized unary trees where each level represents a lexical projection of the anchor. <ref type="bibr" target="#b4">Carreras et al. (2008)</ref> showed how spine-based parsing could be reduced to dependency parsing: since spines are attached to words, equivalent derivations can be represented as a dependency tree where arcs are la- beled by spine operations, an adjunction together with information about the adjunction site. How- ever, we depart from previous approaches <ref type="bibr">(Shen and Joshi, 2008;</ref><ref type="bibr" target="#b4">Carreras et al., 2008</ref>) by relaxing the projectivity constraint to represent all discon- tinuous phrase-structure trees (see <ref type="figure" target="#fig_0">Figure 1</ref>). We assume a finite set of spines S. A spine s can be defined as a sequence of grammatical cat- egories, beginning at root. For a sentence w = (w 0 , w 1 , . . . , w n ) where w k is the word at position k and w 0 is a dummy root symbol, a derivation is a triplet (d, s, l) defined as follows. Adjunc- tions are described by a dependency tree rooted at 0 written as a sequence of arcs d. If (h, m) ∈ d with h ∈ {0, . . . , n} and m ∈ {1, . . . , n}, then the derivation contains an adjunction of the root of the spine at position m to a node from the spine at position h. Supertagging, the assignment of a spine to each word, is represented by a sequence s = (s 0 , s 1 , . . . , s n ) of n + 1 spines, each spine s k being assigned to word w k . Finally, labeling l = (l 1 , . . . , l n ) is a sequence where l k is the label of the k th arc (h, m) of d. The label consists of a couple (op, i) where op is the type of adjunction, here sister or regular 2 , and i is the index of the adjunction node in s h .</p><p>Each derivation is assigned an arc-factored score σ which is given by:</p><formula xml:id="formula_0">σ(d, s, l; w) = (h,m)∈d Ω(h, m, s h , s m , l hm ; w)</formula><p>For instance, following score functions de- <ref type="bibr">2</ref> The distinction is not crucial for the exposition. We refer readers to <ref type="bibr">(Shen and Joshi, 2008;</ref><ref type="bibr" target="#b4">Carreras et al., 2008</ref>). veloped in <ref type="bibr" target="#b4">(Carreras et al., 2008)</ref>, this func- tion could read s h [i], s h [i + 1] and s m <ref type="bibr">[0]</ref>, where s[i] denotes the i-th grammatical category of the spine s. The score of the derivation in <ref type="figure" target="#fig_0">Fig- ure 1</ref> could then reflect that the spine WHNP-WP associated with What is adjoined on the spine SBARQ-SQ-VP-VB associated with do on a site with the grammatical triple <ref type="bibr">[VP WHNP VB]</ref>.</p><p>We assume that Ω accounts for the contribution of arcs, spines and labels to the score. The de- tails of the contribution depend on the model. We choose the following:</p><formula xml:id="formula_1">σ(d, s, l; w) = (h,m)∈d (α(h, m; w) +ν(s m ; h, m, w) +γ(l hm ; h, m, s h , w))</formula><p>where α is the score related to the dependency tree, ν is the supertagging score and γ the label- ing score. Note that functions α, ν and γ have access to the entire input string w. Score func- tion σ can be parameterized in many ways and we discuss our implementation in Section 5. In this setting, parsing a sentence w amounts to find- ing the highest-scoring derivation</p><formula xml:id="formula_2">(d * , s * , l * ) = arg max (d,s,l) σ(d, s, l; w).</formula><p>Recovering the derived tree from a derivation is performed by recursively mapping each spine and its dependencies to a possibly gappy constituent. Given a spine s h and site index i, we look for the leftmost s l and rightmost s r dependents at- tached with regular adjunction. The complexity of the parsing problem depends on the type of dependency trees. In the case of projective trees, it has been shown <ref type="bibr" target="#b12">(Eisner, 2000;</ref><ref type="bibr" target="#b4">Carreras et al., 2008;</ref><ref type="bibr" target="#b27">Li et al., 2011</ref>) that this could be performed in cubic worst-case time complex- ity with dynamic programming, whether supertags are fixed beforehand or not. However, the mod- ification of the original Eisner algorithm requires that chart cells must be indexed not only by spans, or pairs of positions, but also by pairs of supertags. In practice the problem is intractable unless heavy pruning is performed first in order to select a sub- set of spines at each position.</p><p>In the case of non-projective dependency trees, the problem has quadratic worst-case time com- plexity when supertags are fixed, since the prob- lem then amounts to non-projective parsing and reduces to the Maximum Spanning Arborescence problem (MSA) as in ( <ref type="bibr" target="#b30">McDonald et al., 2005</ref>). Unfortunately, the efficient algorithm for MSA is greedy and does not store potential substructure candidates. Hence, when supertags are not fixed beforehand, a new arborescence must be recom- puted for each choice of supertags. This problem can be seen as instance of the Generalized Max- imum Spanning Arborescence problem, an NP- complete problem, which we review in the next section. Note that arc labels do not impact the asymptotic complexity of an arc-factored model. Indeed, only the labeled arc with maximum weight between two vertices is considered when parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Generalized Maximum Spanning Arborescence</head><p>In this section, we first define GMSA introduced by <ref type="bibr" target="#b31">Myung et al. (1995)</ref>. We formulate this prob- lem as an integer linear program. We then ex- plain the reduction from the joint supertagging and spine parsing task to this problem. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem definition</head><p>Let D = (V, A) be a directed graph. Given a sub- set T ⊆ A of arcs, V [T ] denotes the set of ver- tices of V which are the tail or the head of at least one arc of T . These vertices are said to be cov- ered by T . A subset T ⊆ A of arcs is called an arborescence if the graph (V [T ], T ) is connected, acyclic and each vertex has at most one entering arc. The vertex with no entering arc is called the root of T . An arborescence covering all vertices is called a spanning arborescence. Let π = {V 0 , . . . , V n }, n ∈ N be a partition of V . Each element of π is called a cluster. An arborescence T of D covering exactly one vertex per cluster of π is called a generalized spanning arborescence (GSA). <ref type="figure">Figure 2</ref> gives an example of a GSA. The partition of V is composed of a cluster having one vertex and six clusters having four vertices. Each cluster is depicted by a hatched area. The GSA is depicted by the dashed arcs.</p><p>Let W be a vertex subset of V . We denote δ − (W ) (resp. δ + (W )) the set of arcs entering (resp. leaving) W and δ(W ) = δ − (W )∪δ + (W ). <ref type="bibr">4</ref> Contracting W consists in replacing in D all ver- tices in W by a new vertex w, replacing each arc uv ∈ δ − (W ) by the arc uw and each arc vu ∈ δ + (W ) by wu. Let D π be the graph ob- tained by contracting each cluster of π in D. Note that a GSA of D and π induces a spanning arbores- cence of D π . 5 For instance, contracting each clus- ter in the graph given by <ref type="figure">Figure 2</ref> leads to a graph D π having 7 vertices and the set of dashed arcs corresponds to a spanning arborescence of D π .</p><p>Given arc weights φ ∈ R A , the weight of an ar- borescence T is a∈T φ a . Given (D, π, φ), the Generalized Maximum Spanning Arborescence problem (GMSA) consists in finding a GSA of D and π of maximum weight whose root is in V 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integer linear program</head><p>Given a set S, z ∈ R S is a vector indexed by ele-</p><formula xml:id="formula_3">ments in S. For S ⊆ S, z(S ) = s∈S z s . A GSA T ⊆ A is represented by variables x ∈ {0, 1} V and y ∈ {0, 1} A such that x v (resp. y a ) is equal to 1 iff v ∈ V [T ] (resp. a ∈ T ).</formula><p>Since a GSA of D and π induces a spanning arborescence of D π , the arc-incidence vector y ∈ {0, 1} A of a GSA with root in V 0 satisfies the fol- lowing, adapted from MSA ( <ref type="bibr" target="#b39">Schrijver, 2003)</ref>:</p><formula xml:id="formula_4">y(δ − (V 0 )) = 0 (1) y(δ − (V k )) = 1 ∀1 ≤ k ≤ n,<label>(2)</label></formula><formula xml:id="formula_5">y(δ − ( ∪ V k ∈π V k )) ≥ 1 ∀π ⊆ π \ {V 0 }.<label>(3)</label></formula><p>Let Y denote all the arc-incidence vectors on D corresponding to a spanning arborescence in D π whose root is the contraction of V 0 . Then,</p><formula xml:id="formula_6">Y = {y ∈ {0, 1} A |y satisfies (1)-(3)}.</formula><p>GMSA can be formulated with the following in- teger linear program:</p><formula xml:id="formula_7">max x,y φ · y (4) s.t. y ∈ Y (5) x v ≥ y a ∀v ∈ V, a ∈ δ(v), (6) x v (V k ) = 1 ∀0 ≤ k ≤ n,<label>(7)</label></formula><formula xml:id="formula_8">x v ∈ {0, 1} ∀v ∈ V.<label>(8)</label></formula><p>Let W and T be the vertex and arc sets given by x v = 1 and y a = 1 respectively. Since T is a spanning arborescence of D π by (5), (V [T ], T ) is an acyclic directed graph with n arcs such that V 0 has no entering arc and V i , i ∈ {1, . . . , n}, has one entering arc. By constraints <ref type="formula" target="#formula_7">(7)</ref>, W contains one vertex per cluster of π. Moreover, by inequal-</p><formula xml:id="formula_9">ities (6), V [T ] ⊆ W . Since |W | = n + 1 and |T | = n, W = V [T ] and (V [T ], T ) is connected,</formula><p>so it is a GSA. Because its root is in V 0 by <ref type="formula">(5)</ref>, it is an optimal solution for GMSA by (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reduction from joint parsing to GMSA</head><p>Given an instance of the joint parsing problem, we construct an instance of GMSA as follows. With every spine s of every word w k different from w 0 , we associate a vertex v. For k = 1, . . . , n, we denote by V k the set of vertices associated with the spines of w k . We associate with w 0 a set V 0 containing only one vertex and V 0 will now refer both the cluster and the vertex it contains depend- ing on the context. Let π = {V 0 , . . . , V n } and V = ∪ n k=0 V k . For every couple u, v of vertices such that u ∈ V h and v ∈ V m , h = m and m = 0, we associate an arc uv corresponding to the best adjunction of the root of spine s m associated with v of V m to spine s h associated with vertex u of V h . The weight of this arc is given by</p><formula xml:id="formula_10">φ uv = α(h, m; w) + ν(s m ; h, m, w) + max l hm γ(l hm ; h, m, s h , w)</formula><p>which is the score of the best adjunction of s m to s h . This ends the construction of (D, π, φ).</p><p>There is a 1-to-1 correspondence between the solutions to GMSA and those to the joint supertag- ging and spine parsing task in which each adjunc- tion is performed with the label maximizing the score of the adjunction. Indeed, the vertices cov- ered by a GSA T with root V 0 correspond to the spines on which the derivation is performed. By definition of GSAs, one spine per word is chosen. Each arc of T corresponds to an adjunction. The score of the arborescence is the sum of the scores of the selected spines plus the sum of the scores of the best adjunctions with respect to T . Hence, one can solve GMSA to perform joint parsing.</p><p>As an illustration, the GSA depicted in <ref type="figure">Figure 2</ref> represents the derivation tree of <ref type="figure" target="#fig_0">Figure 1</ref>: the ver- tices of V \ V 0 covered by the GSA are those as- sociated with the spines of <ref type="figure" target="#fig_0">Figure 1</ref> and the arcs represent the different adjunctions. For instance</p><formula xml:id="formula_11">V0 (ROOT) V1 (What) V2 (I) V3 (said) V4 (should) V5 (I) V6 (do)</formula><p>Figure 2: The generalized spanning arborescence inducing the derivation tree in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>the arc from V 3 to V 2 represents the adjunction of spine NP-PRP to spine S-VP-VB at index 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Efficient Decoding</head><p>Lagrangian relaxation has been successfully ap- plied to various NLP tasks ( <ref type="bibr" target="#b24">Koo et al., 2010;</ref><ref type="bibr" target="#b25">Le Roux et al., 2013;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013;</ref><ref type="bibr" target="#b10">Das et al., 2012;</ref><ref type="bibr" target="#b8">Corro et al., 2016)</ref>. Intuitively, given an integer linear program, it consists in re- laxing some linear constraints which make the program difficult to solve and penalizing their vi- olation in the objective function. We propose a new decoding method for GMSA based on dual decomposition, a special flavor of Lagrangian relaxation where the problem is de- composed in several independent subproblems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dual decomposition</head><p>To perform the dual decomposition, we first refor- mulate the integer linear program (4)-(8) before relaxing linear constraints. For this purpose, we replace the variables y by three copies {y i } = {y 0 , y 1 , y 2 }, y i ∈ {0, 1} A . We also consider vari- ables z ∈ R A . Let φ 0 , φ 1 and φ 2 be arc weight vectors such that i φ i = φ. 6 GMSA can then be reformulated as:</p><formula xml:id="formula_12">max x,{y i },z i φ i · y i<label>(9)</label></formula><p>s.t. y 0 ∈ Y (10)</p><formula xml:id="formula_13">x v ≥ y 1 a ∀v ∈ V, a ∈ δ − (v),<label>(11)</label></formula><formula xml:id="formula_14">x v ≥ y 2 a ∀v ∈ V, a ∈ δ + (v),<label>(12)</label></formula><formula xml:id="formula_15">x v (V k ) = 1 ∀0 ≤ k ≤ n,<label>(13)</label></formula><formula xml:id="formula_16">x v ∈ {0, 1} ∀v ∈ V,<label>(14)</label></formula><formula xml:id="formula_17">z = y i ∀i.<label>(15)</label></formula><p>Note that variables z only appear in equa- tions <ref type="bibr">(15)</ref>. Their goal is to ensure equality be- tween copies y 0 , y 1 and y 2 . Variables z are usually called witness variables ( <ref type="bibr" target="#b22">Komodakis et al., 2007)</ref>. Equality between y 0 , y 1 and y 2 implies that (10)- (12) are equivalent to (5) and (6). We now relax constraints (15) and build the dual objective <ref type="bibr" target="#b26">(Lemaréchal, 2001)</ref> </p><formula xml:id="formula_18">L * ({λ i }): max x,{y i },z i φ i · y i + i∈{0,1,2} λ i · (z − y i ) s.t. (10) − (14)</formula><p>where {λ i } = {λ 0 , λ 1 , λ 2 }, λ i ∈ R A for i = 0, 1, 2, is the set of Lagrangian multipliers. The dual problem is then:</p><formula xml:id="formula_19">min {λ i } L * ({λ i })</formula><p>Note that, as there is no constraint on z, if</p><formula xml:id="formula_20">i λ i = 0 then L * ({λ i }) = +∞.</formula><p>Therefore, we can re- strict the domain of {λ i } in the dual problem to the set Λ = {{λ i }| i λ i = 0}. This implies that z may be removed in the dual objective. This latter can be rewritten as:</p><formula xml:id="formula_21">L * ({λ i }) = max x,{y i } i ¯ φ i · y i s.t. (10) − (14)</formula><p>where ¯ φ i = φ i − λ i for i = 0, 1, 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computing the dual objective</head><p>Given {λ i } ∈ Λ, computing the dual objective L * ({λ i }) can be done by solving the two follow- ing distinct subproblems:</p><formula xml:id="formula_22">P 1 ( ¯ φ 0 ) = max y 0 ¯ φ 0 · y 0 s.t. y 0 ∈ Y P 2 ( ¯ φ 1 , ¯ φ 2 ) = max x,y 1 ,y 2 ¯ φ 1 · y 1 + ¯ φ 2 · y 2 s.t. (11) −<label>(14)</label></formula><p>y i a ∈ {0, 1} ∀a ∈ A, i = 1, 2.</p><p>Subproblem P 1 can be solved by simply running the MSA algorithm on the contracted graph D π . Subproblem P 2 can be solved in a combinato- rial way. Indeed, observe that each value of y 1 and y 2 is only constrained by a single value of x. The problem amounts to selecting for each cluster a vertex as well as all the arcs with positive weight covering it. More precisely, for each vertex v ∈ V , compute the local weight c v defined by:</p><formula xml:id="formula_23">a∈δ − (v) max{0, ¯ φ 1 } + a∈δ + (v) max{0, ¯ φ 2 }.</formula><p>Let V max be the set of vertices defined as fol- lows. For k = 0, . . . , n, add in V max the ver- tex v ∈ V k with the maximum weight c v . Let A 1 and A 2 be the sets of arcs such that A 1 (resp. A 2 ) contains all the arcs with positive weights entering (resp. leaving) a vertex of V max . The vectors x, y 1 and y 2 corresponding respectively to the incidence vectors of V max , A 1 and A 2 form an optimal so- lution to P 2 .</p><p>Hence, both supbroblems can be be solved with a O(|π| 2 ) time complexity, that is quadratic w.r.t. the length of the input sentence. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoding algorithm</head><p>Our algorithm seeks for a solution to GMSA by solving the dual problem since its solution is opti- mal to GMSA whenever it is a GSA. If not, a so- lution is constructed by returning the highest GSA on the spines computed during the resolution of the dual problem.</p><p>We solve the dual problem using a projected subgradient descent which consists in iteratively updating {λ i } in order to reduce the distance to the optimal assignment. Let {λ i,t } denotes the value of {λ i } at iteration t. {λ i,0 } is initially set to 0. At each iteration, the value of {λ i,t+1 } is computed from the value of {λ i,t } thanks to a subgradient of the dual objective. More precisely, we have</p><formula xml:id="formula_24">{λ i,t+1 } = {λ i,t } − η t × L * ({λ i,t })}</formula><p>where L * ({λ i,t }) is a subgradient of L * ({λ i,t }) and η t ∈ R is the stepsize at iteration t. We use the projected subgradient from <ref type="bibr" target="#b22">Komodakis et al. (2007)</ref>. Hence, at iteration t, we must solve repa- rameterized subproblems P 1 and P 2 to obtain the current solution (¯ x t , ¯ y 0,t , ¯ y 1,t , ¯ y 2,t ) of the dual ob- jective. Then each multiplier is updated following</p><formula xml:id="formula_25">λ i,t+1 = λ i,t − η t ×   ¯ y i,t − 2 j=0 ¯ y j,t 3   .</formula><p>Note that for any value of {λ i }, L * ({λ i }) gives an upper bound for GMSA. So, whenever the optimal solution ¯ x t , {¯ y i,t } to the dual objective L * ({λ i,t }) at iteration t is a primal feasible solu- tion, that is ¯ y 0,t = ¯ y 1,t = ¯ y 2,t , it is an optimal solution to GMSA and the algorithm ends. Other- wise, we construct a pipeline solution by perform- ing a MSA on the vertices given by ¯ x t .</p><p>If after a fixed number of iterations we have not found an optimal solution to GMSA, we return the pipeline solution with maximum weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Lagrangian enhancement</head><p>The previsouly defined Lagrangian dual is valid but may lead to slow convergence. Thus, we propose three additional techniques which empir- ically improve the decoding time and the conver- gence rate: constraint tightening, arc reweighing and problem reduction.</p><p>Constraint tightening: In subproblem P 2 , we consider a vertex and all of its adjacent arcs of pos- itive weight. However, we know that our optimal solution must satisfy tree-shape constraints (5). Thus, every cluster except the root must have ex- actly one incoming arc and there is at most one arc between two clusters. Both constraints are added to P 2 without hurting its time complexity.</p><p>Reweighing: By modifying weights such that less incoming arcs have a positive weight, the so- lution of P 2 tends to be an arborescence. For each cluster V k ∈ π \ V 0 , letˆAletˆ letˆA k be the set of incoming arcs with the highest weightˆφweightˆ weightˆφ k . Then, let γ k be a value such that φ a − γ k is positive only for arcs inˆAinˆ inˆA k . Subtracting γ k from the weight φ a of each arc of δ − (V k ) and adding γ k to the objective score does not modify the weight of the solution because only one entering arc per cluster is selected.</p><p>Problem reduction: We use the pipeline solu- tions computed at each iteration to set the value of some variables. Let ¯ x, {¯ y i } be the optimal solu- tion of L * ({λ i }) computed at any iteration of the subgradient algorithm. For k = 1, . . . , n, let ¯ v be the vertex of V k such that ¯ x ¯ v = 1. Using the local weights (Section 4.2), for all v ∈ V k \ {¯ v}, L * ({λ i })+c v −c ¯ v is an upper bound on the weight of any solution (x, y) to GMSA with x v = 1. Hence, if it is lower than the weight of the best pipeline solution found so far, we can guarantee that x v = 0 in any optimal solution. We can check the whole graph in linear time if we keep local weights c in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Parameterization</head><p>We present a probabilistic model for our frame- work. We implement our probability distributions with neural networks, more specifically we build a neural architecture on top of bidirectional recur- rent networks that compute context sensitive rep- resentations of words. At each step, the recurrent architecture is given as input a concatenation of word and part-of-speech embeddings. We refer the reader to <ref type="bibr" target="#b20">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b11">Dozat and Manning)</ref> for further explanations about bidi- rectional LSTMs <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997</ref>). In the rest of this section, b m denotes the context sensitive representation of word w m .</p><p>We now describe the neural network models used to learn and assign weight functions α, ν and γ under a probabilistic model. Given a sentence w of length n, we assume a derivation <ref type="bibr">(d, s, l)</ref> is generated by three distinct tasks. By chain rule, P (d, s, l|w) = P α (d|w) × P ν (s|d, w) × P γ (l|d, s, w). We follow a common approach in dependency parsing and assign labels l in a post- processing step, although our model is able to in- corporate label scores directly. Thus, we are left with jointly decoding a dependency structure and assigning a sequence of spines. We note s i the i th spine: 8</p><formula xml:id="formula_26">P α (d|w) × P ν (s|d, w) = (h,m)∈d P α (h|m, w) × P ν (s m |m, d, w) = (h,m)∈d P α (h|m, w) × P ν (s m |m, h, w)</formula><p>We suppose that adjunctions are generated by an arc-factored model, and that a spine prediction de- pends on both current position and head position.</p><p>Then parsing amounts to finding the most prob- able derivation and can be realized in the log space, which gives following weight functions:</p><p>α(h, m; w) = log P α (h|m, w) ν(s m ; h, m, w) = log P ν (s m |m, h, w) where α represents the arc contribution and ν the spine contribution (cf. Section 2).</p><p>Word embeddings b k are first passed through specific feed-forward networks depending on the distribution and role. The result of the feed- forward transformation parameterized by set of parameters ρ of a word embedding b s is a vector denoted b (ρ) s . We first define a biaffine attention networks weighting dependency relations (Dozat and Manning):</p><formula xml:id="formula_27">o (α) h,m = b (α 1 ) m W (α) b (α 2 ) h + V (α) b (α 2 ) h</formula><p>where W (α) and V (α) are trainable parameters. Moreover, we define a biaffine attention classifier networks for class c as:</p><formula xml:id="formula_28">o (τ ) c,h,m = b (τ 1 ) m W (τc) b (τ 2 ) h + V (τc) b (τ 1 ) m ⊕ b (τ 2 ) h + u (τc)</formula><p>where ⊕ is the concatenation. W (τc) , V (τc) and u (τc) are trainable parameters. Then, we define the weight of assigning spine s to word at position m with head h as o</p><formula xml:id="formula_29">(ν)</formula><p>s,h,m . Distributions P α and P ν are parameterized by these biaffine attention networks followed by a softmax layer:</p><formula xml:id="formula_30">P α (h|m, w) = exp o (α) h,m h exp o (α) h ,m P ν (s|h, m, w) = exp o (ν) s,h,m s exp o (ν) s ,h,m</formula><p>Now we move on to the post-processing step predicting arc labels. For each adjunction of spine s at position m to spine t at position h, instead of predicting a site index i, we predict the non- terminal nt at t[i] with a biaffine attention classi- fier. <ref type="bibr">9</ref> The probability of the adjunction of spine s at position m to a site labeled with nt on spine t at position h with type a ∈ {regular, sister} is:</p><formula xml:id="formula_31">P γ (nt, a|h, m) = P γ (nt|h, m, w)</formula><p>× P γ (a|h, mw) P γ and P γ are again defined as distributions from the exponential family using biaffine atten- tion classifiers:</p><formula xml:id="formula_32">P γ (nt|h, m, t) = exp o (γ ) nt,h,m nt exp o (γ ) nt,h,m P γ (a|h, m, t) = exp o (γ ) t,h,m a exp o (γ ) a ,h,m</formula><p>We use embeddings of size 100 for words and size 50 for parts-of-speech tags. We stack two bidirectional LSTMs with a hidden layer of size 300, resulting in a context sensitive embedding of size 600. Embeddings are shared across distribu- tions. All feed-forward networks have a unique elu-activated hidden layer of size 100 <ref type="bibr" target="#b5">(Clevert et al., 2016)</ref>. We regularize parameters with a dropout ratio of 0.5 on LSTM input. We es- timate parameters by maximizing the likelihood of the training data through stochastic subgradi- ent descent using Adam ( <ref type="bibr" target="#b19">Kingma and Ba, 2015)</ref>. Our implementation uses the Dynet library (Neu- big et al., 2017) with default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We ran a series of experiments on two corpora an- notated with discontinuous constituents.</p><p>English We used an updated version of the Wall Street Journal part of the Penn Treebank corpus <ref type="bibr" target="#b29">(Marcus et al., 1994)</ref> which introduces discontinu- ity ( <ref type="bibr" target="#b13">Evang and Kallmeyer, 2011</ref>). Sections 2-21 are used for training, 22 for developpement and 23 for testing. We used gold and predicted POS tags by the Stanford tagger, 10 trained with 10- jackknifing. Dependencies are extracted following the head-percolation table of <ref type="bibr" target="#b7">Collins (1997)</ref>.</p><p>German We used the Tiger corpus ( <ref type="bibr" target="#b2">Brants et al., 2004</ref>) with the split defined for the SPMRL 2014 shared task <ref type="bibr" target="#b28">(Maier, 2015;</ref><ref type="bibr">Seddah et al., 2013)</ref>. Following Maier (2015) and <ref type="bibr" target="#b6">Coavoux and Crabbé (2017)</ref>, we removed sentences num- ber 46234 and 50224 as they contain anno- tation errors. We only used the given gold POS tags. Dependencies are extracted following the head-percolation table distributed with Tulipa ( <ref type="bibr" target="#b18">Kallmeyer et al., 2008)</ref>.</p><p>We emphasize that long sentences are not fil- tered out. Our derivation extraction algorithm is similar to the one proposed in <ref type="bibr" target="#b4">Carreras et al. (2008)</ref>. Regarding decoding, we use a beam of size 10 for spines w.r.t. P ν (s m |m, w) = h P ν (s m |h, m, w) × P α (h|m, w) but allow ev- ery possible adjunction. The maximum number of iterations of the subgradient descent is set to 500 and the stepsize η t is fixed following the rule of <ref type="bibr" target="#b33">Polyak (1987)</ref>.</p><p>Parsing results and timing on short sentences only (≤ 40 words) and full test set using the de-fault discodop 11 eval script are reported on <ref type="table" target="#tab_1">Table 1  and Table 2</ref>. <ref type="bibr">12</ref> We report labeled recall (LR), pre- cision (LP), F-measure (LF) and time measured in minutes. We also report results published by van <ref type="bibr" target="#b9">Cranenburgh et al. (2016)</ref> for the discontin- uous PTB and <ref type="bibr" target="#b6">Coavoux and Crabbé (2017)</ref> for Tiger. Moreover, dependency unlabeled attach- ment scores (UAS) and tagging accuracies (Spine acc.) are given on <ref type="table" target="#tab_3">Table 3</ref>. We achieve signif- icantly better results on the discontinuous PTB, while being roughly 36 times faster together with a low memory footprint. <ref type="bibr">13</ref> On the Tiger corpus, we achieve on par results. Note however that <ref type="bibr" target="#b6">Coavoux and Crabbé (2017)</ref> rely on a greedy parser com- bined with beam search.</p><p>Fast and efficient parsing of discontinuous con- stituent is a challenging task. Our method can quickly parse the whole test set, without any par- allelization or GPU, obtaining an optimality cer- tificate for more than 99% of the sentences in less than 500 iterations of the subgradient descent. When using a non exact decoding algorithm, such as a greedy transition based method, we may not be able to deduce the best opportunity for improv- ing scores on benchmarks, such as the parameter- ization method or the decoding algorithm. Here the behavior may be easier to interpret and direc- tions for future improvement easier to see. We stress that our method is able to produce an op- timality certificate on more than 99% of the test examples thanks to the enhancement presented in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Spine-based parsing has been investigated in <ref type="bibr">(Shen and Joshi, 2005</ref>) for Lexicalized TAGs with a left-to-right shift-reduce parser which was sub- sequently extended to a bidirectional version in <ref type="bibr">(Shen and Joshi, 2008)</ref>. A graph-based algorithm was proposed in <ref type="bibr" target="#b4">(Carreras et al., 2008</ref>) for second- order projective dependencies, and for a form of non-projectivity occurring in machine translation (i.e. projective parses of permutated input sen- tences) in <ref type="bibr" target="#b3">(Carreras and Collins, 2009</ref>   <ref type="bibr" target="#b6">Coavoux and Crabbé (2017)</ref>.</p><p>been explored in <ref type="bibr" target="#b16">(Hall and Nivre, 2008;</ref><ref type="bibr">Versley, 2014;</ref><ref type="bibr" target="#b15">Fernández-González and Martins, 2015)</ref>. The first two encode spine information as arc la- bels while the third one relaxes spine information by keeping only the root and height of the adjunc- tion, thus avoiding combinatorial explosion. La- beling is performed as a post-processing step in these approaches, since the number of labels can be very high. Our model also performs labeling after structure construction, but it could be per- formed jointly without major issue. This is one way our model could be improved. GMSA has been studied mostly as a way to solve the non directed version (i.e. with symet- ric arc weights) ( <ref type="bibr" target="#b31">Myung et al., 1995)</ref>, see <ref type="bibr" target="#b35">(Pop, 2009;</ref><ref type="bibr" target="#b14">Feremans et al., 1999</ref>) for surveys on res- olution methods. <ref type="bibr" target="#b31">Myung et al. (1995)</ref> proposed an exact decoding algorithm through branch-and- bound using a dual ascent algorithm to compute bounds. <ref type="bibr" target="#b34">Pop (2002)</ref>   it is the optimal solution <ref type="bibr" target="#b1">(Beasley, 1993)</ref>, and thus the stopping criterion for the subgradient descent is usually slow to obtain. To our knowledge, our system is the first time that GMSA is used to solve a NLP problem. Dual decomposition has been used to derive ef- ficient practical resolution methods in NLP, mostly for machine translation and parsing, see  for an overview and ( <ref type="bibr" target="#b24">Koo et al., 2010)</ref> for an application to dependency parsing.</p><p>To accelerate the resolution, our method re- lies heavily on problem reduction <ref type="bibr" target="#b1">(Beasley, 1993)</ref>, which uses the primal/dual bounds to filter out suboptimal assignments. Exact pruning based on duality has already been studied in parsing, with branch and bound ( <ref type="bibr" target="#b8">Corro et al., 2016</ref>) or column generation ( <ref type="bibr" target="#b36">Riedel et al., 2012)</ref> and in machine translation with beam search ( <ref type="bibr" target="#b37">Rush et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a novel framework for the joint task of supertagging and parsing by a reduction to GMSA. Within this framework we developed a model able to produce discontinuous constituents. The scoring model can be decomposed into tag- ging and dependency parsing and thus may rely on advances in those active fields.</p><p>This work could benefit from several exten- sions. Bigram scores on spines could be added at the expense of a third subproblem in the dual objective. High-order scores on arcs like grand- parent or siblings can be handled in subproblem P 2 with the algorithms described in ( <ref type="bibr" target="#b24">Koo et al., 2010)</ref>. In this work, the parameters are learned as separate models. Joint learning in the max-margin framework <ref type="bibr" target="#b21">(Komodakis, 2011;</ref><ref type="bibr" target="#b23">Komodakis et al., 2015</ref>) may model interactions between vertex and arc weights better and lead to improved accuracy. Finally, we restricted our grammar to spinal trees but it could be possible to allow full lexicalized TAG-like trees, with substitution nodes and even obligatory adjunction sites. Derivations compat- ible with the TAG formalism (or more generally LCFRS) could be recovered by the use of a con- strained version of MSA ( <ref type="bibr" target="#b8">Corro et al., 2016</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A derivation with spines and adjunctions (dashed arrows). The induced dependency tree is non-projective. Each color corresponds to a spine. We omit punctuation to simplify figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>If any, we insert a new node between s h [i] and s h [i + 1] with the same grammatical category as the first one. This new node fills the role of the foot node in TAGs. Every dependent of s h [i] with anchor in interval [l + 1, r − 1] is moved to the newly created node. Remaining sister and regular adjunctions are sim- ply attached to s h [i].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parsing results and processing time on 
the german Tiger corpus. C2017 indicates results 
of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>also used Lagrangian relax- ation -in the non directed case -where a single subproblem is solved in polynomial time. How- ever, the relaxed constraints are inequalities: if the dual objective returns a valid primal solution, it is not a sufficient condition in order to guarantee that</figDesc><table>UAS Spine acc. 
English 93.70 
97.32 
English  † 93.04 
96.81 
German 92.25 
96.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Dependency parsing and tagging re-
sults. Results marked with  † use predicted part-
of-speech tags. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Libin Shen and Aravind Joshi. 2005. Incremental 
ltag parsing. In Proceedings of Human Language 
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 
811-818, Vancouver, British Columbia, Canada. 
Association for Computational Linguistics. 

Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 495-504, Honolulu, Hawaii. Association 
for Computational Linguistics. 

Yannick Versley. 2014. Experiments with easy-first 
nonprojective constituent parsing. In Proceedings 
of the First Joint Workshop on Statistical Parsing 
of Morphologically Rich Languages and Syntactic 
Analysis of Non-Canonical Languages, pages 39-
53, Dublin, Ireland. Dublin City University. </table></figure>

			<note place="foot" n="1"> Although we will borrow concepts from TAGs, we do not require derivations to be TAG compatible (i.e. well-nested dependencies with a bounded number of gaps).</note>

			<note place="foot" n="3"> A similar reduction can be obtained in the reverse direction, thus proving the NP-completeness of our problem.</note>

			<note place="foot" n="4"> By an abuse of notation, we identify any singleton {v} with its element v. 5 The converse does not hold: an arc subset of A corresponding to a spanning arborescence of D π may not be a GSA of D and π since it may not induce a connected graph.</note>

			<note place="foot" n="6"> In our implementation, we choose φ 0 = φ 1 = φ 2 = 1 3 φ.</note>

			<note place="foot" n="7"> In the general case, the time complexity is O(|V | 2 ). But in our problem, the number of vertices per cluster is bounded by the grammar size: O(|V | 2 ) = O(|Sπ| 2 ) = O(|π| 2 ).</note>

			<note place="foot" n="8"> We assume that the spine for the root w0 is unique.</note>

			<note place="foot" n="9"> If a spine contains repeated non-terminal sequences, we select the lowest match.</note>

			<note place="foot" n="10"> http://nlp.stanford.edu/software/tagger.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their in-sightful comments. We thank Laura Kallmeyer and Kilian Evang for providing us with the script for the discontinuous PTB. First author is sup-ported by a public grant overseen by the French National Research Agency (ANR) as part of the Investissements d'Avenir program (ANR-10-LABX-0083). Second author, supported by a pub-lic grant overseen by the French ANR (ANR-16-CE33-0021), completed this work during a CNRS research leave at LIMSI, CNRS / Université Paris Saclay.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and robust compressive summarization with dual decomposition and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modern heuristic techniques for combinatorial problems, chapter Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Beasley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tiger: Linguistic interpretation of a german corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen-Schirra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on language and computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="597" to="620" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonprojective parsing for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting><address><addrLine>Manchester, England. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Learning Representations</title>
		<meeting>the 2016 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental discontinuous phrase structure parsing with the gap transition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1259" to="1270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 35th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency parsing with bounded block degree and well-nestedness via lagrangian relaxation and branch-and-bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caio</forename><surname>Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Rozenknop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">Wolfler</forename><surname>Calvo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-oriented parsing with discontinuous constituents and function tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Van Cranenburgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Modelling</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="111" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An exact dual decomposition algorithm for shallow semantic parsing with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 International Conference on Learning Representations</title>
		<meeting>the 2017 International Conference on Learning Representations</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilexical grammars and their cubic-time parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Developments in Natural Language Parsing</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="29" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PLCFRS parsing of english discontinuous constituents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Kallmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Parsing Technologies</title>
		<meeting>the 12th International Conference on Parsing Technologies<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="104" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The generalized minimum spanning tree: Polyhedra and branch-and-cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinne</forename><surname>Feremans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martine</forename><surname>Labbé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Laporte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Notes in Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parsing as reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-González</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1523" to="1533" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing discontinuous phrase structure with grammatical functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Language Processing: 6th International Conference</title>
		<meeting><address><addrLine>Gothenburg, Sweden; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2008-08-25" />
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
	<note>GoTAL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tulipa: Towards a multi-formalism parsing environment for grammar engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Kallmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Lichte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Parmentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Dellert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks</title>
		<meeting><address><addrLine>Manchester, England. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The International Conference on Learning Representations</title>
		<meeting>The International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient training for pairwise or higher order crfs via dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1841" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MRF optimization via dual decomposition: Message-passing revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A framework for efficient structured maxmargin learning of high-order mrf models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1425" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1298" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining PCFG-LA models with dual decomposition: A case study with function labels and binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Rozenknop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1158" to="1169" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Lemaréchal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational combinatorial optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="112" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint models for chinese pos tagging and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1180" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discontinuous incremental shift-reduce parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1202" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The penn treebank: annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT&apos;94: Proceedings of the workshop on Human Language Technology</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the generalized minimum spanning tree problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Soo</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Wan</forename><surname>Tcha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="231" to="241" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introduction to optimization. Optimization Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boris T Polyak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The generalized minimum spanning tree problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrica Claudiu Pop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Twente University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of different integer programming formulations of the generalized minimum spanning tree problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrica Claudiu Pop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Carpathian Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="118" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parse, price and cut-delayed column and row generation for graph based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="732" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimal beam search for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On dual decomposition and linear programming relaxations for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Combinatorial OptimizationPolyhedra and Efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
