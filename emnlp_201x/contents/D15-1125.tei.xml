<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Continuous Space Language Models for Machine Translation Quality Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">W M</forename><surname>Ng</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
							<email>fethi.bougares@lium.univ-lemans.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">§Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Le Mans</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Continuous Space Language Models for Machine Translation Quality Estimation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present novel features designed with a deep neural network for Machine Translation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target segments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including post-editing effort, human translation edit rate, post-editing time and METEOR. Results show significant improvements in prediction over the baseline, as well as over systems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Quality Estimation (QE) is concerned with pre- dicting the quality of Machine Translation (MT) output without reference translations. QE is ad- dressed with various features indicating fluency, adequacy and complexity of the translation pair. These features are used by a machine learning al- gorithm along with quality labels given by humans to learn models to predict the quality of unseen translations.</p><p>A variety of features play a key role in QE. A wide range of features from source segments and their translated segments, extracted with the help of external resources and tools, have been proposed. These go from simple, language- independent features, to advanced, linguistically motivated features. They include features that summarise how the MT systems generate transla- tions, as well as features that are oblivious to the systems. The majority of the features in the lit- erature are extracted from each sentence pair in isolation, ignoring the context of the text. QE performance usually differs depending on the lan- guage pair, the specific quality score being opti- mised (e.g., post-editing time vs translation ad- equacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks ( <ref type="bibr" target="#b15">Shah et al., 2013b</ref>). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling.</p><p>Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words <ref type="bibr" target="#b13">(Schwenk, 2012;</ref><ref type="bibr" target="#b7">Mikolov et al., 2013</ref>). The assumption of these models is that semantically or grammatically related words are mapped to simi- lar geometric locations in a high-dimensional con- tinuous space. The probability distribution is thus much smoother and therefore the model has a bet- ter generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multi- ple (called deep networks) hidden layers. Deep neural networks have been shown to perform bet- ter than shallow ones due to their capability to learn higher-level, abstract representations of the input ( <ref type="bibr" target="#b0">Arisoy et al., 2012)</ref>. In this paper, we ex- plore the potential of these models in context of QE for MT. We obtain more robust features with CSLM and improve the overall prediction power for translation quality.</p><p>The paper is organised as follows: In Section 2 we briefly present the related work. Section 3 describes the CSLM model training and its vari- ous settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For a detailed overview of various features and algorithms for QE, we refer the reader to the WMT12-14 shared tasks on QE <ref type="bibr" target="#b5">(Callison-Burch et al., 2012;</ref><ref type="bibr" target="#b4">Bojar et al., 2013;</ref><ref type="bibr" target="#b6">Ling et al., 2014</ref>). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive.</p><p>Since their introduction in ( <ref type="bibr" target="#b3">Bengio et al., 2003)</ref>, neural network language models have been successfully exploited in many speech and language processing problems, including auto- matic speech recognition <ref type="bibr" target="#b10">(Schwenk and Gauvain, 2005;</ref><ref type="bibr" target="#b12">Schwenk, 2007)</ref> and machine trans- lation <ref type="bibr" target="#b13">(Schwenk, 2012)</ref>.</p><p>Recently, ( <ref type="bibr" target="#b1">Banchs et al., 2015</ref>) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. ( <ref type="bibr" target="#b18">Tan et al., 2015)</ref> pro- posed to train models with deep regression for ma- chine translation evaluation in a task to measure semantic similarity between sentences. They re- ported positive results on simple features; larger feature sets did not improve these results.</p><p>In this paper, we propose to estimate the prob- abilities of source and target segments with con- tinuous space language models based on a deep architecture and to use these estimated probabili- ties as features along with standard feature sets in a supervised learning framework. To the best of our knowledge, such approach has not been stud- ied before in the context of QE for MT. The result shows significant improvements in many predic- tion tasks, despite its simplicity. Monolingual data for source and target language is the only resource required to extract these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Continuous Space Language Models</head><p>A key factor for quality inference of a translated text is to determine the fluency of such a text and how well it conforms to the linguistic regularities of the target language. It involves grammatical correctness, idiomatic and stylistic word choices that can be derived by using n-gram language models. However, in high-order n-grams, the pa- rameter space is sparse and conventional mod- elling is inefficient. Neural networks model the non-linear relationship between the input features and target outputs. They often outperform con- ventional techniques in difficult machine learning tasks. Neural network language models (CSLM) alleviate the curse of dimensionality by projecting words into a continuous space, and modelling and estimating probabilities in this space.</p><p>The architecture of a deep CSLM is illus- trated in <ref type="figure" target="#fig_0">Figure 1</ref>. The inputs to a CSLM model are the (K − 1) left-context words (w i−K+1 , . . . , w i−2 , w i−1 ) to predict w i . A one- hot vector encoding scheme is used to repre- sent the input w i−k with an N -dimensional vec- tor. The output of CSLM is a vector of pos- terior probabilities for all words in vocabulary, P (w i |w i−1 , w i−2 , . . . , w i−K+1 ). Due to the large output layer (vocabulary size), the complexity of a basic neural network language model is very high. <ref type="bibr" target="#b12">Schwenk (2007)</ref> proposed efficient training strate- gies in order to reduce the computational complex- ity and speed up the training time. They process several examples at once and use a short-list vo- cabulary V with only the most frequent words. Following the settings mentioned in ( <ref type="bibr" target="#b11">Schwenk et al., 2014</ref>), all CSLM experiments described in this paper are performed using deep networks with four hidden layers: first layer for the projec- tion (320 units for each context word) and three hidden layers of 1024 units with tanh activation. At the output layer, we use a softmax activation function applied to a short-list of the 32k most frequent words. The probabilities of the out-of- vocabulary words are obtained from a standard back-off n-gram language model. The projection of the words onto the continuous space and the training of the neural network is done by the stan- dard back-propagation algorithm and outputs are the converged posterior probabilities. The model parameters are optimised on a development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CSLM and Quality Estimation</head><p>In the context of MT, CSLMs are generally trained on the target side of a given language pair to ex-press the probability that the generated sentence is "correct" or "likely", without looking at the source sentence. However, QE is also concerned with how well the source segments can be trans- lated. Therefore, we trained two models, one for each side of a given language pair. We extracted the probabilities for QE training and test sets for both source and its translation with their respec- tive models and used them as features, along with other features, in a supervised learning setting.</p><p>Finally, we also used CSLM in a spoken lan- guage translation (SLT) task. In SLT, an auto- matic speech recogniser (ASR) is used to decode the source language text from audio. This creates an extra source of variability, where different ASR models and configurations give different outputs. In this paper, we use QE to exploit different ASR outputs (i.e. MT inputs) which in turn can lead to different MT outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We focus on experiments with sentence level QE tasks. Our English-Spanish experiments are based on the WMT QE shared task data from 2012 to 2015. 1 These tasks are diverse in nature, with dif- ferent sizes and labels such as post-editing effort (PEE), post-editing time (PET) and human trans- lation error rate (HTER). The results reported in Section 5.5 are directly comparable with the of- ficial systems submitted for each of the respec- tive tasks. We also performed experiments on the IWSLT 2014 English-French SLT task 2 to study the applicability of our models on n-best ASR (MT inputs) comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">QE Datasets</head><p>In <ref type="table">Table 1</ref> we summarise the data and tasks for our experiments. We refer readers to the WMT and IWSLT websites for detailed descriptions of these datasets. All datasets are publicly available.</p><p>WMT12: English-Spanish news sentence trans- lations produced by a Moses "baseline" statisti- cal MT (SMT) system, and judged for perceived post-editing effort in 1-5 (highest-lowest), taking a weighted average of three annotators <ref type="bibr">(CallisonBurch et al., 2012</ref>).</p><p>WMT13 (Task-1): English-Spanish sentence translations of news texts produced by a Moses "baseline" SMT system. These were then post- edited by a professional translator and labelled using HTER. This is a superset of the WMT12 dataset, with 500 additional sentences for test, and a different quality label ( <ref type="bibr" target="#b4">Bojar et al., 2013</ref>).</p><p>WMT14 (Task-1.1): English-Spanish news sentence translations. The dataset contains source sentences and their human translations, as well as three versions of machine translations: by an SMT system, a rule-based system system and a hybrid system. Each translation was labelled by professional translators with 1-3 (lowest-highest) scores for perceived post-editing effort. IWSLT14: English-French dataset containing source language data from the 10-best (sentences) ASR system output. On the target side, the 1- best MT translation is used. The ASR system leads to different source segments, which in turn lead to different translations. METEOR ( <ref type="bibr" target="#b2">Banerjee and Lavie, 2005</ref>) is used to label these alternative translations against a reference (human) transla- tion. Both ASR and MT outputs come from a sys- tem submission in IWSLT 2014 ( <ref type="bibr" target="#b9">Ng et al., 2014</ref>). The ASR system is a multi-pass deep neural net- work tandem system with feature and model adap- tation and rescoring. The MT system is a phrase- based SMT system produced using Moses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Lang  <ref type="table">Table 1</ref>: QE datasets: # sentences and labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CSLM Dataset</head><p>The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl cor- pus. We used a data selection method <ref type="bibr" target="#b8">(Moore and Lewis, 2010)</ref> to select the most relevant train- ing data with respect to a development set. For English-Spanish, the development data is the con- catenation of newstest2012 and newstest2013 of the WMT translation track. For English-French, the development set is the concatenation of the IWSLT dev2010 and eval2010. In <ref type="table" target="#tab_2">Table 2</ref> we show statistics on the selected monolingual data used to train back-off LM and CSLM.</p><p>Lang  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Feature Sets</head><p>We use the QuEst 3 toolkit ( <ref type="bibr" target="#b14">Shah et al., 2013a</ref>) to extract two feature sets for each dataset:</p><p>• BL: 17 features used as baseline in the WMT shared tasks on QE.</p><p>• AF: 80 augmented MT system-independent features <ref type="bibr">4</ref> (superset of BL). For the En-Fr SLT task, we have additional 36 features (21 ASR + 15 MT-dependent features) The resources used to extract these features (cor- pora, etc.) are also available as part of the WMT shared tasks on QE. The CSLM features for each of the source and target segments are extracted us- ing the procedure described in Section 3 with the CSLM toolkit. <ref type="bibr">5</ref> We trained QE models with following combina- tion of features:</p><p>• BL + CSLM src,tgt : CSLM features for source and target segments, plus the baseline features.</p><p>• AF + CSLM src,tgt : CSLM features for source and target segments, plus all available features. For the WMT12 task, we performed further exper- iments to analyse the improvements with CSLM:</p><p>• CSLM src : Source side CSLM feature only.</p><p>• CSLM tgt : Target side CSLM feature only.</p><p>• • FS(AF) + CSLM src,tgt : CSLM features in addition to the best performing feature set (FS(AF)) selected as described in <ref type="bibr" target="#b15">(Shah et al., 2013b;</ref><ref type="bibr" target="#b16">Shah et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learning algorithms</head><p>We use the Support Vector Machines implementa- tion of the scikit-learn toolkit to perform re- gression (SVR) with either Radial Basis Function (RBF) or linear kernel and parameters optimised via grid search. To evaluate the prediction models we use Mean Absolute Error (MAE), its squared version -Root Mean Squared Error (RMSE), and Pearson's correlation (r) score.    <ref type="table" target="#tab_5">Table 3</ref> presents the results with different feature sets for data from various shared tasks. It can be noted that CSLM features always bring significant improvements whenever added to either baseline or augmented feature set. A reduction in both error scores (MAE and RMSE) as well as an increase in Pearson's correlation with human labels can be observed on all tasks. It is also worth noticing that the CSLM features bring improvements over all tasks with different labels, evidencing that dif- ferent optimisation objectives and language pairs can benefit from these features. However, the im- provements are more visible when predicting post- editing effort for WMT12 and WMT14's Task 1.1. For these two tasks, we are able to achieve state- of-the-art performance by adding the two CSLM features to all available or selected feature sets. For WMT12, we performed another set of ex- periments to study the effect of CSLM features by themselves and in combination. The results in <ref type="table" target="#tab_6">Table 4</ref> show that the target side CSLM fea- ture bring larger improvements than its source side counterpart. We believe that it is because the tar- get side feature directly reflects the fluency of the translation, whereas the source side feature (re- garded as a translation complexity feature) only has indirect effect on quality. Interestingly, the two CSLM features alone give comparable re- sults (slightly worse) than the BL feature set 6 de- spite the fact that these 17 features cover many complexity, adequacy and fluency quality aspects. CSLM features bring further improvements on pre-selected feature sets, as shown in <ref type="table" target="#tab_5">Table 3</ref>. We also performed feature selection over the full fea- ture set along with CSLM features, following the procedure in <ref type="bibr" target="#b15">(Shah et al., 2013b</ref>). Interestingly, both CSLM features were selected among the top ranked features, confirming their relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results</head><p>In order to investigate whether our CSLM fea- tures results hold for other feature sets, we ex- perimented with the feature sets provided by most teams participating in the WMT12 QE shared task. These feature sets are very diverse in terms of the types of features, resources used, and their sizes. <ref type="table">Table 5</ref> shows the official results from the shared task (Off.) <ref type="bibr" target="#b5">(Callison-Burch et al., 2012)</ref>, those from training an SVR on these features with and without CSLM features. Note that the official scores are often different from the results obtained with our SVR models because of differences in <ref type="bibr">6</ref> We compare results in terms of MAE scores only. the learning algorithms. As shown in <ref type="table">Table 5</ref>  <ref type="table">Table 5</ref>: MAE score on official WMT12 feature sets using SVR with and without CSLM features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed novel features for machine transla- tion quality estimation obtained using a deep con- tinuous space language models. The proposed fea- tures led to significant improvements over stan- dard feature sets for a variety of datasets, outper- forming the state-of-art on two official WMT QE tasks. These results showed that different opti- misation objectives and language pairs can bene- fit from the proposed features. The proposed fea- tures have been shown to also perform well on QE within a spoken language translation task. Both source and target CSLM features improve prediction quality, either when used separately or in combination. They proved complementary when used together with other feature sets and produce comparable results to high performing baseline features when used alone for prediction. Finally, results comparing all official WMT12 QE feature sets showed significant improvements in the predictions when CSLM features were added to those submitted by participating teams. These findings provide evidence that the proposed fea- tures bring valuable information into prediction models, despite their simplicity and the fact that they require only monolingual data as resource, which is available in abundance for many lan- guages.</p><p>As future work, it would be interesting to ex- plore various distributed word representations for quality estimation and joint models that look at both the source and the target sentences simulta- neously.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deep CSLM architecture.</figDesc><graphic url="image-1.png" coords="2,312.33,310.80,208.15,145.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>English-Spanish news sentence translations post-edited by a professional translator, with the post-editing time collected on a sentence-basis and used as label (in milliseconds). WMT15 (Task-1): Large English-Spanish news dataset containing source sentences, their machine translations by an online SMT system, and the post-editions of the translation by crowdsourced translators, with HTER used as label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training data size (number of tokens) and 
language models perplexity (ppl). The values in 
parentheses in last column shows percentage de-
crease in perplexity. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for datasets with various feature 
sets. Figures with  *  beat the official best systems, 
and with  *  *  are second best. Results with CSLM 
features are significantly better than BL and AF on 
all tasks (paired t-test with p ≤ 0.05). 

Task System 
#feats 
MAE 
RMSE 
r 

WMT12 

BL + CSLM src 
18 
0.6751 
0.8125 
0.5626 
BL + CSLM tgt 
18 
0.6694 
0.8023 
0.5815 
CSLM src,tgt 
2 
0.6882 
0.8430 
0.5314 
FS(AF) 
19 
0.6131 
0.7598 
0.6296 
FS(AF) + CSLM src,tgt 
21 
0.5950  *  0.7442  *  0.6482  *  

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Impact of different combinations of 
CSLM features on the WMT12 task. Figures with 
 *  beat the official best system. Results with CSLM 
features are significantly better than BL and AF on 
all tasks (paired t-test with p ≤ 0.05). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>, we observed similar improvements with additional CSLM features over all of these feature sets.</figDesc><table>System 
#feats Off. 
SVR 
SVR 
without CSLM with CSLM 
SDL 
15 
0.61 
0.6115 
0.5993 
UU 
82 
0.64 
0.6513 
0.6371 
Loria 
49 
0.68 
0.6978 
0.6729 
UEdin 
56 
0.68 
0.6879 
0.6724 
TCD 
43 
0.68 
0.6972 
0.6715 
WL-SH 
147 
0.69 
0.6791 
0.6678 
UPC 
57 
0.84 
0.8419 
0.8310 
DCU 
308 
0.75 
0.6825 
0.6812 
PRHLT 
497 
0.70 
0.6699 
0.6649 

</table></figure>

			<note place="foot" n="1"> http://www.statmt.org/wmt[12,13,14, 15]/quality-estimation-task.html 2 https://sites.google.com/site/ iwsltevaluation2014/slt-track</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the QT21 (H2020 No. 645452), Cracker (H2020 No. 645357) and DARPA Bolt projects.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adequacy-fluency metrics: Evaluating mt in the continuous space model framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael E Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F D&amp;apos;</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="472" to="482" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Audio, Speech, and Language Processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Findings of the 2013 Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Eighth Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Findings of the 2012 WMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crowdsourcing high-quality parallel data extraction from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="426" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers, ACLShort &apos;10</title>
		<meeting>the ACL 2010 Conference Short Papers, ACLShort &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The USFD spoken language translation system for IWSLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mortaza</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Doulaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madina</forename><surname>Saz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Shaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="86" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training neural network language models on very large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient training strategies for deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quest-design, implementation and extensions of a framework for machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleftherios</forename><surname>Avramidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergun</forename><surname>Biçicic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An investigation on the effectiveness of features for translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation Summit</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A bayesian non-linear method for feature selection in machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">QuEst-A translation quality estimation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Meeting of the Association for Computational Linguistics: Demo Session</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Usaar-sheffield: Semantic textual similarity with deep regression and machine translation evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liling</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
