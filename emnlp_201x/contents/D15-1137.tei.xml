<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Logic</orgName>
								<orgName type="department" key="dep2">Language and Computation</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
							<email>{p.le,zuidema}@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Logic</orgName>
								<orgName type="department" key="dep2">Language and Computation</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>According to the principle of composi-tionality, the meaning of a sentence is computed from the meaning of its parts and the way they are syntactically combined. In practice, however, the syntactic structure is computed by automatic parsers which are far-from-perfect and not tuned to the specifics of the task. Current re-cursive neural network (RNN) approaches for computing sentence meaning therefore run into a number of practical difficulties, including the need to carefully select a parser appropriate for the task, deciding how and to what extent syntactic context modifies the semantic composition function , as well as on how to transform parse trees to conform to the branching settings (typically, binary branching) of the RNN. This paper introduces a new model, the Forest Convolutional Network, that avoids all of these challenges, by taking a parse forest as input, rather than a single tree, and by allowing arbitrary branching factors. We report improvements over the state-of-the-art in sentiment analysis and question classification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For many natural language processing tasks we need to compute meaning representations for sen- tences from meaning representations of words. In a recent line of research on 'recursive neural net- works' (e.g., <ref type="bibr" target="#b23">Socher et al. (2010)</ref>), both the word and sentence representations are vectors, and the word vectors ("embeddings") are borrowed from work in distributional semantics or neural lan- guage modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure pro- vided by an external parser). The network, which thus implements a 'composition function', is opti- mized for delivering sentence representations that support a given semantic task: sentiment analysis ( <ref type="bibr" target="#b9">Irsoy and Cardie, 2014;</ref><ref type="bibr" target="#b17">Le and Zuidema, 2015)</ref>, paraphrase detection <ref type="bibr" target="#b25">(Socher et al., 2011</ref>), seman- tic relatedness <ref type="bibr" target="#b29">(Tai et al., 2015)</ref> etc. Studies with recursive neural networks have yielded promising results on a variety of such tasks.</p><p>In this paper, we represent a new recursive neu- ral network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work. In particular, the model we propose addresses three issues:</p><p>1. how to make the composition functions adap- tive, in the sense that they operate adequately for the many different types of combina- tions (e.g., adjective-noun combinations are of a very different type than VP-PP combina- tions);</p><p>2. how to deal with different branching factors of nodes in the relevant syntactic trees (i.e., we want to avoid having to binarize syntac- tic trees, 1 but also do not want ternary pro- ductions to be completely independent from binary productions);</p><p>3. how to deal with uncertainty about the correct parse inside the neural architecture (i.e., we don't want to work with just the best or k-best parses for a sentence according to an external model, but receive an entire distribution over possible parsers). To solve these challenges we take inspiration from two other traditions: the convolutional neu- ral networks and classic parsing algorithms based on dynamic programming. Including convolution in our network provides a direct solution for is- sue (2), and turns out, somewhat unexpectedly, to also provide a solution for issue (1). Introduc- ing the chart representation from classic parsing into our architecture then allows us to tackle issue (3). The resulting model, the Forest Convolutional Network, outperforms all other models on a senti- ment analysis and question classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This section is to introduce the recursive neural network (RNN) and convolutional neural network (CNN) models, on which our work is based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recursive Neural Network</head><p>A recursive neural network (RNN) <ref type="bibr" target="#b5">(Goller and Küchler, 1996</ref>) is a feed-forward neural network where, given a tree structure, we recursively ap- ply the same weight matrices at each inner node in a bottom-up manner. In order to see how an RNN works, consider the following exam- ple. Assume that there is a constituent with parse tree (S I (V P like it)) <ref type="figure" target="#fig_0">(Figure 1</ref>), and that x I , x like , x it ∈ R d are the vectorial representa- tions of the three words I, like and it, respec- tively. We use a neural network which consists of a weight matrix W 1 ∈ R d×d for left children and a weight matrix W 2 ∈ R d×d for right children to compute the vector for a parent node in a bottom up manner. Thus, we compute x V P</p><formula xml:id="formula_0">x V P = f (W 1 x like + W 2 x it + b)<label>(1)</label></formula><p>where b is a bias vector and f is an (non-linear) activation function. Having computed x V P , we can then move one level up in the hierarchy and compute x S</p><formula xml:id="formula_1">x S = f (W 1 x I + W 2 x V P + b)</formula><p>This process is continued until we reach the root node.</p><p>For classification tasks, we put a softmax layer on the top of the root node, and compute the prob- ability of assigning a class c to an input x by</p><formula xml:id="formula_2">P r(c|x) = softmax(c) = e u(c,ytop) c ∈C e u(c ,ytop) (2)</formula><p>where u(c 1 , y top ), ..., u(c |C| , y top ) T = W u y top + b u ; C is the set of all possible classes; W u ∈ R |C|×d , b u ∈ R |C| are a weight matrix and a bias vector.</p><p>Training an RNN uses the gradient descent method to minimize an objective function J(θ). The gradient ∂J/∂θ is efficiently computed thanks to the back-propagation through structure algo- rithm <ref type="bibr" target="#b5">(Goller and Küchler, 1996)</ref>.</p><p>Departing from the original RNN model, many extensions have been proposed to enhance its compositionality ( <ref type="bibr" target="#b26">Socher et al., 2013;</ref><ref type="bibr" target="#b9">Irsoy and Cardie, 2014;</ref><ref type="bibr" target="#b17">Le and Zuidema, 2015)</ref> and appli- cability ( <ref type="bibr" target="#b16">Le and Zuidema, 2014b</ref>). The model we are going to propose can be considered as an ex- tension of RNN with an ability to solve the three issues introduced in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Neural Network</head><p>A convolutional neural network (CNN) ( <ref type="bibr" target="#b18">LeCun et al., 1998</ref>) is also a feed-forward neural network; it consists of one or more convolutional layers (of- ten with a pooling operation) followed by one or more fully connected layers. This architecture was invented for computer vision. It then has been widely applied to solve natural language process- ing tasks <ref type="bibr" target="#b2">(Collobert et al., 2011;</ref><ref type="bibr" target="#b10">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b12">Kim, 2014)</ref>.</p><p>To illustrate how a CNN works, the following example uses a simplified model proposed by <ref type="bibr" target="#b2">Collobert et al. (2011)</ref> which consists of one con- volutional layer with the max pooling operation, followed by one fully connected layer <ref type="figure" target="#fig_1">(Figure 2)</ref>. This CNN uses a kernel with window size 3; when we slide this kernel along the sentence "s I like it very much /s", we get five vectors:</p><formula xml:id="formula_3">u (1) = W 1 x &lt;s&gt; + W 2 x I + W 3 x like + b c u (2) = W 1 x I + W 2 x like + W 3 x it + b c ... u (5) = W 1 x very + W 2 x much + W 3 x &lt;/s&gt; + b c where W 1 , W 2 , W 3 ∈ R d×m are weight matri- ces, b c ∈ R m</formula><p>is a bias vector. The max pooling operation is then applied to those resulted vectors in an element-wise manner:</p><formula xml:id="formula_4">x = max 1≤i≤5 u (i) 1 , ..., max 1≤i≤5 u (i) j , ... T</formula><p>Finally, a fully connected layer is employed</p><formula xml:id="formula_5">y = f (Wx + b)</formula><p>where W, b are a real weight matrix and bias vec- tor, respectively; f is an activation function. Intuitively, a window-size-k kernel extracts (lo- cal) features from k-grams, and is thus able to cap- ture k-gram composition. The max pooling oper- ation is for reducing dimension, forcing the net- work to discriminate important features from oth- ers by assigning high values to them. For instance, if the network is used for sentiment analysis, local features corresponding to k-grams containing the word "like" should receive high values in order to be propagated to the top layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Forest Convolutional Network</head><p>We now first propose a solution to the issues (1) and (2) (i.e., making the composition functions adaptive and dealing with different branching fac- tors), called Recursive convolutional neural net- work (RCNN), and then a solution to the third is- sue (i.e., dealing with uncertainty about the cor- rect parse), called Chart Neural Network (ChNN). A combination of them, Forest Convolutional Net- work (FCN), will be introduced lastly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recursive Convolutional Neural</head><p>Network 2</p><p>Given a subtree</p><formula xml:id="formula_6">p → x 1 ... x l , an RCNN (Fig- ure 3)</formula><p>, like a CNN, slides a window-size-k kernel along the sequence of children (x 1 , ..., x l ) to com- pute a pool of vectors. The max pooling operation followed by a fully connected layer is then applied to this pool to compute a vector for the parent p. This RCNN differs from the CNN introduced in Section 2.2 at two points. First, we use a non-linear kernel: after linearly transforming in- put vectors, an activation function is applied. Sec- ond, we put k − 1 padding tokens &lt;b&gt; at the be- ginning of the children sequence and k−1 padding tokens &lt;e&gt; at the end. This thus guarantees that all the children contribute equally to the resulted vector pool, which now has l + k − 1 vectors.</p><p>It is obvious that this RCNN can solve the sec- ond issue (i.e., dealing with different branching factors), we now show how it can make the com- position functions adaptive. We first see what hap- pens if the window size k is larger than the number of children l, for instance k = 3 and l = 2. There are four vectors in the pool</p><formula xml:id="formula_7">u (1) = f (W 1 x &lt;b&gt; + W 2 x &lt;b&gt; + W 3 x 1 + b c ) u (2) = f (W 1 x &lt;b&gt; + W 2 x 1 + W 3 x 2 + b c ) u (3) = f (W 1 x 1 + W 2 x 2 + W 3 x &lt;e&gt; + b c ) u (4) = f (W 1 x 2 + W 2 x &lt;e&gt; + W 3 x &lt;e&gt; + b c )</formula><p>where W 1 , W 2 , W 3 are weight matrices, b c is a 2 While finalizing the current paper we discovered a pa- per by <ref type="bibr" target="#b32">Zhu et al. (2015)</ref> proposing a similar model which is evaluated on syntactic parsing. Our work goes substantially beyond theirs, however, as it takes a parse forest rather than a single tree as input. bias vector, f is an activation function. These four resulted vectors correspond to four ways of com- posing the two children:</p><p>(1) the first child stands alone (e.g., when the in- formation of the second child is not impor- tant, it is better to ignore it),</p><p>(2,3) the two children are composed with two dif- ferent weight matrix sets, (4) the second child stands alone.</p><p>Now, imagine that we must handle binary syn- tactic rules with different head positions such as S → N P V P (e.g. "Jones runs") where the second child is the head and V P → V BD N P (e.g., "ate spaghetti") where the first child is the head. We can set those weight matrices such that when multiplying W 2 by the vector of a head, we have a vector with high-value entries. And when multiplying W 2 by the vector of a non-head, or when multiplying W 1 or W 3 by a vector, the re- sulted vector has low-value entries. This is possi- ble thanks to the max pooling operation and that heads are often more informative than non-heads. If the window size k is smaller than the number of children l, the argument above is still valid in some cases such as head position. However, there is no longer a direct interaction between any two children whose distance is larger than k. 3 In prac- tice, this problem is not serious because rules with a large number of children are very rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chart Neural Network</head><p>Unseen sentences are always parsed by an auto- matic parser, which is far from perfect and task- independent. Therefore, a good solution is to give <ref type="bibr">3</ref> An indirect interaction can be set up through pooling.</p><p>the system a set of parses and let it decide which parse is the best or to combine some of them. The RNN model handles one extreme where this set contains only one parse. We now consider the other extreme where the set contains all possible parses. Because the number of all possible bi- nary parse trees of a length-n sentence is the n- th Catalan number, processing individual parses is not practical. We thus propose a new model work- ing on charts in the CKY style <ref type="bibr" target="#b30">(Younger, 1967)</ref>, called Chart Neural Network <ref type="figure">(ChNN)</ref>. We describe this model by the following exam- ple. Given a phrase "ate pho with Milos", a ChNN will process its parse chart as in <ref type="figure" target="#fig_3">Figure 4</ref>. Be- cause any 2-word constituent has only one parse, the computation for p 1 , p 2 , p 3 is identical to Equa- tion 1. For 3-word constituent p 4 , because there are two possible productions p 4 → ate p 2 and p 4 → p 1 with, we compute one vector for each production</p><formula xml:id="formula_8">u (1) = f (W 1 x ate + W 2 p 2 + b) u (2) = f (W 1 p 1 + W 2 x with + b)<label>(3)</label></formula><p>and then apply the max pooling operation to these two vectors to compute p 4 . We do the same to compute p 5 . Finally, at the top, there are three productions p 6 → ate p 5 , p 6 → p 1 p 3 and p 6 → p 4 M ilos. Similarly, we compute one vec- tor for each production and employ the max pool- ing operation to compute p 6 . Because this ChNN processes a chart like the CKY algorithm, its time complexity is O(n 2 d 2 + n 3 d) where n and d are the sentence length and the dimension of vectors, respectively. 4 A ChNN is thus notably more complex than an RNN, whose complexity is O(nd 2 ). Like chart parsing, the complexity can be reduced significantly by prun- ing the chart before applying the ChNN. This will be discussed right below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Forest Convolutional Network</head><p>We now introduce the Forest Convolutional Net- work (FCN) model, which is a combination of the RCNN and the ChNN. The idea is to use an au- tomatic parser to prune a chart 5 , debinarize pro- ductions (if applicable), and then apply a ChNN <ref type="figure">Figure 5</ref>: Forest of parses (left) and Forest Convolutional Network (right). ⊗ denotes a convolutional layer followed by the max pooling operation and a fully connected layer as in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>where the computation in Equation 3 is replaced by a convolutional layer followed by the max pool- ing operation and a fully connected layer as in the RCNN. <ref type="figure">Figure 5</ref> shows an illustration how the FCN works on the phrase "ate pho with Milos".</p><p>A forest of parses, given by an external parser, comprises two parses (V P ate pho (P P with M ilos)) (solid lines) and (V P ate (N P pho (P P with M ilos))) (dash-dotted lines). The first parse is the preferred reading if Milos is a person, but the second one is a possible reading (for instance, if Milos is the name of a sauce). Instead of forcing the external parser to decide which one is correct, we let the FCN network do that because it has more information about the context and domain, which are embedded in training data. What the network should do is depicted in <ref type="figure">Figure 5</ref>-right.</p><p>Training Training an FCN is similar to train- ing an RNN. We use the mini-batch gradient de- scent method to minimize an objective function J, which depends on which task this network is ap- plied to. For instance, if the task is sentiment anal- ysis, J is the cross-entropy over the training sen- tence set D plus an L2-norm regularization term</p><formula xml:id="formula_9">J(θ) = − 1 |D| s∈D p∈s log P r(c p |p) + λ 2 ||θ|| 2</formula><p>where θ is the parameter set, c p is the sentiment class of phrase p, p is the vector representation at the node covering p, P r(c p |p) is computed by score on section 23 of the Penn Treebank whereas resulted forests are very compact: the average number of hyperedges per forest is 123.1.</p><p>the softmax function, and λ is the regularization parameter.</p><p>The gradient ∂J/∂θ is computed efficiently thanks to the back-propagation through structure <ref type="bibr" target="#b5">(Goller and Küchler, 1996)</ref>. We use the AdaGrad method <ref type="bibr" target="#b3">(Duchi et al., 2011</ref>) to automatically up- date the learning rate for each parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the FCN model with two tasks: ques- tion classification and sentiment analysis. The evaluation metric is the classification accuracy.</p><p>Our networks were initialized with the 300-D GloVe word embeddings trained on a corpus of 840B words <ref type="bibr">6 (Pennington et al., 2014</ref>). The ini- tial values for a weight matrix were uniformly sampled from the symmetric interval</p><formula xml:id="formula_10">− 1 √ n , 1 √ n</formula><p>where n is the number of total input units. In each experiment, a development set was used to tune the model. We run the model ten times and chose the run with the highest performance on the devel- opment set. We employed early stopping: training is halted if performance on the development set does not improve after three consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Analysis</head><p>The Stanford Sentiment Treebank (SST) <ref type="bibr">7</ref>  also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for develop- ment, and 1821 for testing. All sentences were parsed by Liang Huang's de- pendency parser <ref type="bibr">8 (Huang and Sagae, 2010)</ref>. We used this parser because it generates parse forests and that dependency trees are less deep than con- stituent trees. In addition, because the SST was annotated in a constituency manner, we also em- ployed the Charniak's constituent parser <ref type="bibr" target="#b0">(Charniak and Johnson, 2005</ref>) with Huang (2008)'s for- est pruner. We found that the beam width 16 for the dependency parser and the log probability beam 10 for the other worked best. Lower values harmed the system's performance and higher val- ues were not beneficial.</p><p>Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh. It was trained with the learning rate 0.01, the regu- larization parameter 10 −4 , and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see <ref type="figure" target="#fig_2">Figure 3</ref>).</p><p>We compare the FCN against other models: the Recursive neural tensor network (RNTN) <ref type="bibr" target="#b26">(Socher et al., 2013)</ref>, the Convolutional neural net- work (CNN) <ref type="bibr" target="#b12">(Kim, 2014)</ref>, the Dynamic convolu- tional neural network (DCNN) <ref type="bibr" target="#b10">(Kalchbrenner et al., 2014</ref>), the Paragraph vectors (PV) ( <ref type="bibr" target="#b14">Le and Mikolov, 2014)</ref>, the Deep recursive neural net- work (DRNN) <ref type="bibr" target="#b9">(Irsoy and Cardie, 2014)</ref>, the Re- cursive neural network with Long short term mem- ory (LSTM-RNN) ( <ref type="bibr" target="#b17">Le and Zuidema, 2015)</ref> and the Constituent Tree LSTM (CT-LSTM) <ref type="bibr" target="#b29">(Tai et al., 2015</ref>). 9 <ref type="table">Table 1</ref> shows the results. Our FCN using con- stituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the dif- ference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%).</p><p>Constituent parsing is clearly more helpful than dependency parsing: the improvements that the FCN got are 0.6% in the fine-grained task and 0.9% in the binary task. We conjecture that, be- cause sentences in the treebank were parsed by a constituent parser (here is the Stanford parser), training with constituent forests is easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Classification</head><p>In this task we used the TREC question dataset <ref type="bibr">10</ref> ( <ref type="bibr" target="#b19">Li and Roth, 2002</ref>) which contains 5952 ques- tions (5452 questions for training and 500 ques- tions for testing). The task is to assign a ques- tion to one in six types: ABBREVIATION, EN- TITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC. The average length of the questions in the training set is 10.2 whereas in the test set is 7.5. This difference is due to the fact that those questions are from different sources. All questions were parsed by Liang Huang's dependency parser with the beam width 16.</p><p>We randomly picked 5% of the training set (272 questions) for validation. Our FCN has the dimen- sion of vectors at inner nodes 200, a window size for the convolutional kernel of 5, and the activa- tion function tanh. It was trained with the learn- ing rate 0.01, the regularization parameter 10 −4 , and the mini batch size 1. The vectors represent- ing the two padding tokens &lt;b&gt;, &lt;e&gt; were fixed to 0.</p><p>We compare the FCN against the Convolutional neural network (CNN) <ref type="bibr" target="#b12">(Kim, 2014)</ref>   <ref type="bibr" target="#b13">(Klein and Manning, 2003)</ref>. This network was also initialized by the 300-D GloVe word embeddings. <ref type="table" target="#tab_2">Table 2</ref> shows the results. <ref type="bibr">13</ref> The FCN achieved the second best accuracy, only lightly lower than SVM S (0.2%). This is a promising result because our network used only parse forests, unsupervis- edly pre-trained word embeddings whereas SVM S used heavily engineered resources. The differ- ence between FCN and the third best is remark- able (1.2%). Interestingly, LSTM-RNN did not perform well on this dataset. This is likely be- cause the questions are short and the parse trees quite shallow, such that the two problems that the LSTM was invented for (long range dependency and vanishing gradient) do not play much of a role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization</head><p>We visualize the charts we obtained in the sen- timent analysis task as in <ref type="figure" target="#fig_4">Figure 6</ref>. To identify how important each cell is for determining the fi- nal vector at the root, we compute the number of features of each that are actually propagated all the way to the root in the successive max pooling op- 11 https://github.com/lephong/lstm-rnn 12 http://nlp.stanford.edu/software/lex-parser.shtml <ref type="bibr">13</ref> While finalizing the current paper we discovered a paper by <ref type="bibr" target="#b20">Ma et al. (2015)</ref> proposing a convolutional network model for dependency trees. They report a new state-of-the-art ac- curacy of 95.6%.</p><p>erations. The circles in a graph are proportional to this number. Here, to make the contribution of each individual cell clearer, we have set the win- dow size to 1 to avoid direct interactions between cells.</p><p>At the lexical level, we can see that the FCN can discriminate important words from the others. Two words "most" and "incoherent" are the key of the sentiment of this sentence: if one of them is replaced by another word (e.g. replacing "most" by "few" or "incoherent" by "coherent"), the sen- timent will flip. The punctuation "." however also has a high contribution to the root. This happens to other charts as well. We conjecture that the net- work uses the vector of "." to store neutral features and propagate them to the root whenever it can not find more useful features in other vectors. Our fu- ture work is to examine this.</p><p>At the phrasal level, the network tends to group words in grammatical constituents, such as "most of the action setups", "are incoherent". Ill-formed constituents such as "of the action" and "incoher- ent ." receive little attention from the network.</p><p>Interestingly, we can see that the circle of "inco- herent" is larger than the circles of any inner cells, suggesting that the network is able to make use of parses containing direct links from that word to the root. This is evidence that the network has an ability of selecting (or combining) parses that are beneficial to this sentiment analysis task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The idea that a composition function must be able to change its behaviour on the fly according to in- put vectors is explored by <ref type="bibr" target="#b26">Socher et al. (2013)</ref>, <ref type="bibr" target="#b17">Le and Zuidema (2015)</ref>, among others. The tensor in the former is multiplied with the vector repre- sentations of the phrases it is going to combine to define a composition function (a matrix) on the fly, and then multiplies again with these vectors to yield a compound representation. In the LSTM architecture of the latter, there is one input gate for each child in order to control how the vector of the child affects the composition at the parent node. Because the input gate is a function of the vector of the child, the composition function has an infinite number of behaviours. In this paper, we instead slide a kernel function along the sequence of children to generate different ways of composi- tion. Although the number of behaviours is limited (and depends on the window size), it simultane- ously provides us with a solution to handle rules with different branching sizes.</p><p>Some approaches try to overcome the prob- lem of varying branching sizes. <ref type="bibr" target="#b16">Le and Zuidema (2014b)</ref> use different sets of weight matrices for different branching sizes, thus requiring a large number of parameters. Because large branching- size rules are rare, many parameters are infre- quently updated during training. <ref type="bibr" target="#b21">Socher et al. (2014)</ref>, for dependency trees, use a weight matrix for each relative position to the head word (e.g., first-left, second-right). <ref type="bibr" target="#b15">Le and Zuidema (2014a)</ref> replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly de- pend on input parse trees and are very sensitive to parsing errors. The approach presented in this pa- per, on the other hand, does not need the informa- tion about the head word position and is less sensi- tive to parsing errors. Moreover, its number of pa- rameters is independent from the maximal branch- ing size.</p><p>Convolutional networks have been widely ap- plied to solve natural language processing tasks.  <ref type="formula" target="#formula_0">(2014)</ref> by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by <ref type="bibr" target="#b32">Zhu et al. (2015)</ref>, which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional network and syn- tactic information, goes beyond them: we address the issue of how to deal with uncertainty about the correct parse inside the neural architecture. There- fore, instead of using a single parse, our proposed FCN model takes as input a forest of parses.</p><p>Related to our FCN is the Gated recursive con- volutional neural network model proposed by <ref type="bibr" target="#b1">Cho et al. (2014)</ref> which is stacking n − 1 convolutional neural layers using a window-size-2 gated kernel (where n is the sentence length). Mapping their network into a chart, each cell is only connected to the two cells right below it. What makes this network special is the gated kernel which is a 3- gate switcher for choosing one of three options: directly transmit the left/right child's vector to the parent node, or compose the vectors of the two children. Thanks to this, the network can capture any binary parse trees by setting those gates prop- erly. However, because only one gate is allowed to open in a cell, the network is not able to capture an arbitrary forest. Our FCN is thus more expressive and flexible than their model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed the Forest Convolutional Network (FCN) model that addresses the three issues: (1) how to make the composition functions adaptive, (2) how to deal with different branching factors of nodes in the relevant syntactic trees, (3) how to deal with uncertainty about the correct parse in- side the neural architecture. The key principle is to carry out many different ways of computation and then choose or combine some of them. For more details, the two first issues are solved by em- ploying a convolutional net for composition. To the third issue, the network takes input as a forest of parses instead of a single parse as in traditional approaches.</p><p>Our future work is to focus on how to choose/combine different ways of computation. For instance, we might replace the max pooling by different pooling operations such as mean pool- ing, k-max pooling <ref type="bibr" target="#b10">(Kalchbrenner et al., 2014)</ref>, and stochastic pooling <ref type="bibr" target="#b31">(Zeiler and Fergus, 2013)</ref>. We can even bias the selection/combination to- ward grammatical constituents by weighing cells by their inside probabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recursive Neural Network. For simplicity, bias vectors are removed.</figDesc><graphic url="image-1.png" coords="2,101.76,62.81,158.75,124.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convolutional Neural Network (one convolutional layer and one fully connected layer) with a window-size-3 kernel.</figDesc><graphic url="image-2.png" coords="2,314.36,62.81,204.09,130.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recursive Convolutional Neural Network with a nonlinear window-size-3 kernel.</figDesc><graphic url="image-3.png" coords="3,337.04,62.81,158.74,158.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Chart Neural Network.</figDesc><graphic url="image-4.png" coords="4,79.09,62.81,204.09,154.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Chart of sentence "Most of the action setups are incoherent ." The size of a circle is propositional to the number of the cell's features that are propagated to the root.</figDesc><graphic url="image-6.png" coords="8,140.03,62.81,317.48,169.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracies on the TREC question type 
classification dataset. The accuracies of DCNN, 
MaxEnt H , CNN-non-static, and SVM S are copied 
from the corresponding papers (see text). 

brenner et al., 2014), MaxEnt H (Huang et al., 
2008) (which uses MaxEnt with uni-bi-trigrams, 
POS, wh-word, head word, word shape, parser, 
hypernyms, WordNet) and the SVM S (Silva et al., 
2011) (which uses SVM with, in addition to fea-
tures used by MaxEnt H , 60 hand-coded rules). We 
also include the LSTM-RNN (Le and Zuidema, 
2015) whose accuracy was computed by running 
their published source code 11 on binary trees from 
the Stanford Parser 12 </table></figure>

			<note place="foot" n="1"> Eisner (2001, Chapter 2) shows that using flat rules is linguistically beneficial, &quot;most crucially, a flat lexical entry corresponds to the local domain of a headword-the word together with all its semantic arguments and modifiers&quot;. From the computational perspective, flat rules make trees less deep, thus avoiding the vanishing gradient problem and capturing long range dependencies.</note>

			<note place="foot" n="4"> In each cell, we apply the matrix-vector multiplication two times and (if the cell is not a leaf) apply the max pooling to a pool of maximally n d-D vectors. 5 Pruning a chart by an automatic parser is also not perfect. However, the quality of a pruned chart can get very close to human annotation. For instance, the chart pruner proposed by Huang (2008) has a forest oracle of 97.8% F</note>

			<note place="foot" n="6"> http://nlp.stanford.edu/projects/GloVe/ 7 http://nlp.stanford.edu/sentiment/treebank.html</note>

			<note place="foot" n="8"> http://acl.cs.qc.edu/ ∼ lhuang/software</note>

			<note place="foot" n="9"> LSTM-RNN and CT-LSTM are very similar: they are RNNs using LSTMs for composition. Their difference is that LSTM-RNN uses one input gate for each child where as CTLSTM uses only one input gate for all children. 10 http://cogcomp.cs.illinois.edu/Data/QA/QC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our three anonymous reviewers for their comments. This work was funded by the Faculty of Humanities of the University of Amsterdam, through a Digital Humanities fellowship to PL and a position in the New Generation Initiative (NGO) for WZ.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Smoothing a Probabilistic Lexicon via Syntactic Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">318</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Question classification using head words and their hypernyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Thint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="927" to="936" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inside-outside semantics: A framework for neural models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional distributional semantics with long short term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics</title>
		<meeting>the Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency-based convolutional neural networks for sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From symbolic to subsymbolic information in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognition and parsing of context-free languages in time n 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A re-ranking model for dependency parser with recursive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
