<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<addrLine>8 Somapah Road</addrLine>
									<postCode>487372</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Syntactic language models and N-gram language models have both been used in word ordering. In this paper, we give an empirical comparison between N-gram and syntactic language models on word order task. Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic models. Both of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>N-gram language models have been used in a wide range of the generation tasks, such as machine translation ( <ref type="bibr" target="#b9">Koehn et al., 2003;</ref><ref type="bibr" target="#b3">Chiang, 2007;</ref><ref type="bibr" target="#b7">Galley et al., 2004</ref>), text summarization ( <ref type="bibr" target="#b0">Barzilay and McKeown, 2005</ref>) and realization <ref type="bibr" target="#b8">(Guo et al., 2011</ref>). Such models are trained from large-scale raw text, capturing distributions of local word N- grams, which can be used to improve the fluency of synthesized text.</p><p>More recently, syntactic language models have been used as a complement or alternative to N- gram language models for machine translation <ref type="bibr" target="#b1">(Charniak et al., 2003;</ref><ref type="bibr" target="#b15">Shen et al., 2008;</ref><ref type="bibr" target="#b14">Schwartz et al., 2011</ref>), syntactic analysis ( <ref type="bibr" target="#b2">Chen et al., 2012</ref>) and tree linearization ( ). Compared with N-gram models, syntactic mod- els capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and over- all sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to N- gram models.</p><p>In this paper, we make an empirical compari- son between syntactic and N-gram language mod- els on the task of word ordering ( <ref type="bibr" target="#b17">Wan et al., 2009;</ref><ref type="bibr" target="#b18">Zhang and Clark, 2011a;</ref><ref type="bibr" target="#b5">De Gispert et al., 2014)</ref>, which is to order a set of input words into a gram- matical and fluent sentence. The task can be re- garded as an abstract language modeling problem, although methods have been explored extending it for tree linearization <ref type="bibr" target="#b22">(Zhang, 2013)</ref>, broader text generation ( ) and machine trans- lation ( .</p><p>We choose the model of <ref type="bibr" target="#b10">Liu et al.(2015)</ref> as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit empha- sis on word orders <ref type="bibr" target="#b15">(Shen et al., 2008;</ref><ref type="bibr" target="#b2">Chen et al., 2012)</ref>. As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type mod- els syntactic structures incrementally, thereby can be used to directly score surface orders ( <ref type="bibr" target="#b14">Schwartz et al., 2011;</ref><ref type="bibr" target="#b10">Liu et al., 2015)</ref>. We choose the dis- criminative model of <ref type="bibr" target="#b10">Liu et al. (2015)</ref>, which gives state-of-the-art results for word ordering.</p><p>We try to answer the following research ques- tions by comparing the syntactic model and the N- gram model using the same search algorithm.</p><p>• What is the influence of automatically- parsed training data on the performance of syntactic models.</p><p>Because manual syntac- tic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syn- tactic language models. As a result, the syntac- tic structures that a word ordering system learns can be inaccurate. However, this might not affect Initial State</p><formula xml:id="formula_0">([], set(1...n), Ø) Final State ([], Ø, A)</formula><p>Induction Rules:</p><formula xml:id="formula_1">SHIFT (σ,ρ,A) ([σ|i],ρ−{i},A) L-ARC ([σ|j i],ρ,A) ([σ|i],ρ,A∪{j←i}) R-ARC ([σ|j i],ρ,A) ([σ|i],ρ,A∪{j→i})</formula><p>Figure 1: Deduction system for transition-based linearization.</p><p>the quality of the synthesized output, which is a string only. We quantitatively study the influence of parsing accuracy of syntactic training data on word ordering output.</p><p>• What is the influence of data scale on the performance. N-gram language models can be trained efficiently over large numbers of raw sen- tences. In contrast, syntactic language models can be much slower to train due to rich features. We compare the output quality of the two models on different scales of training data, and also on differ- ent amounts of training time.</p><p>• What are the errors characteristics of each model. Syntactic language models can poten- tially be better in capturing larger constituents and overall sentence structures. However, compared with N-gram models, little work has been done to quantify the difference between the two mod- els. We characterise the outputs using a set of dif- ferent measures, and show empirically the relative strength and weakness of each model.</p><p>• What is the effect of model combination. Finally, because the two models make different types of errors, they can be combined to give bet- ter outputs. We develop a combined model by dis- cretizing probability from N-gram model, and us- ing them as features in the syntactic model. The combined model gives the best results in a stan- dard benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Syntactic word ordering</head><p>Syntactic word ordering algorithms take a multi- set of input words constructing an output sen- tence and its syntactic derivation simultaneously. Transition-based syntactic word ordering can be modelled as an extension to transition-based pars- ing ( <ref type="bibr" target="#b10">Liu et al., 2015)</ref>, with the main difference be-</p><formula xml:id="formula_2">step action σ ρ A init [] (0 1 2) Ø 0 shift [1] (0 2) 1 shift [1 2] (0) 2 L-arc [2] (0) A ∪ {1 ← 2} 3 shift [2 0] () 4 R-arc [2] () A ∪ {2 → 0}</formula><p>Figure 2: Transition-based process for ordering {"potatoes 0 ", "Tom 1 ", "likes 2 "}.</p><p>ing that the order of words is not given in the input, which leads to a much larger search space. We take the system of Liu, et al. <ref type="bibr">1</ref> , which gives state-of-the-art performance and efficiencies in standard word ordering benchmark. It maintains outputs in stack σ, and orders the unprocessed in- coming words in a set ρ. Given an input bag of words, ρ is initialized to the input and σ is ini- tialized as empty. The system repeatedly applies transition actions to consume words from ρ and construct output on σ. <ref type="figure">Figure 1</ref> shows the deduction system, where ρ is unordered and any word in ρ can be shifted onto the stack σ. The set of actions are SHIFT, L-ARC and R-ARC. The SHIFT actions add a word to the stack. For the L-ARC and R-ARC actions, new arcs {j ← i} and {j → i} are constructed re- spectively. Under these possible actions, the un- ordered word set "potatoes 0 Tom 1 likes 2 " is gen- erated as shown in <ref type="figure">Figure 2</ref>, and the result is "Tom 1 ←likes 2 →potatoes 0 ".</p><p>We apply the learning and search framework of <ref type="bibr" target="#b18">Zhang and Clark (2011a)</ref>. Pseudocode of the search algorithm is shown in Algorithm 1. [] refers to an empty stack, and set(1...n) represents the full set of input words W and n is the number of distinct words. candidates stores possible states, and agenda stores temporary states transited from possible actions. GETACTIONS generates a set of possible actions depending on the current state s. APPLY generates a new state by applying action on the current state s. N-BEST produces the top k can- didates in agenda. Finally, the algorithm returns the highest-score state best in the agenda.</p><p>A global linear model is used to score search hypotheses. Given a hypothesis h, its score is cal- culated by:</p><formula xml:id="formula_3">Score(h) = Φ(h) · θ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Transition-based linearisation</head><p>Input: W, a set of input word Output: the highest-scored final state</p><formula xml:id="formula_4">1: candidates ← ([], set(1..n), Ø) 2: agenda ← Ø 3: N ← 2n 4: for i ← 1..N do 5:</formula><p>for s in candidates do <ref type="bibr">6:</ref> for action in GETACTIONS(s) do where Φ(h) is the feature vector of h, extracted by using the same feature templates as <ref type="bibr" target="#b10">Liu et al.(2015)</ref>, which are shown in <ref type="table" target="#tab_1">Table 1</ref> and θ is the parameter vector of the model. The feature templates essentially represents a syntactic lan- guage model. As shown in <ref type="figure">Figure 2</ref>, from the hy- potheses produced in steps 2 and 4, the features "T om 1 ← likes 2 " and "likes 2 → potatoes 0 " are extracted, which corresponds to P (T om 1 |likes 2 ) and P (potatoes 0 |likes 2 ) respectively in the de- pendency language model of <ref type="bibr" target="#b2">Chen et al.,(2012)</ref>. Training. We apply perceptron with early-update ( <ref type="bibr" target="#b4">Collins and Roark, 2004)</ref>, and iteratively tune re- lated parameters on a set of development data. For each iteration, we measure the performance on the development data, and choose best parameters for final tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">N-gram word ordering</head><p>We build an N-gram word ordering system under the same beam-search framework as the syntac- tic word ordering system. In particular, search is performed incrementally, from left to right, adding one word at each step. The decoding process can be regarded as a simplified version of Algorithm 1, with only SHIFT being returned by GETACTIONS, and the score of each transition is given by a stan- dard N-gram language model. We use the same beam size for both N-gram and the syntactic word ordering. Compared with the syntactic model, the N-gram model has less information for disam- biguation, but also has less structural ambiguities, and therefore a smaller search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unigram</head><p>S0w; S0p; S 0,l w; S 0,l p; S0,rw; S0,rp; S 0,l2 w; S 0,l2 p; S0,r2w; S0,r2p;</p><p>S1w; S1p; S 1,l w; S 1,l p; S1,rw; S1,rp; S 1,l2 w; S 1,l2 p; S1,r2w; S1,r2p;</p><formula xml:id="formula_5">Bigram S0wS 0,l w; S0wS 0,l p; S0pS 0,l w; S0pS 0,l pS 0,l p;</formula><p>S0wS0,rw; S0wS0,rp; S0pS0,rw; S0pS0,rpS0,rp; S1wS 1,l w; S1wS 1,l p; S1pS 1,l w; S1pS 1,l pS 1,l p; S1wS1,rw; S1wS1,rp; S1pS1,rw; S1pS1,rpS1,rp;</p><p>S0wS1w; S0wS1p; S0pS1w; S0pS1p;</p><formula xml:id="formula_6">Trigram S0wS0pS 0,l w; S0wS 0,l wS 0,l p; S0wS0pS 0,l p; S0pS 0,l wS 0,l p; S0wS0pS0,rw; S0wS 0,l wS0,rp;</formula><p>S0wS0pS0,rp; S0pS0,rwS0,rp;</p><formula xml:id="formula_7">S1wS1pS 1,l w; S1wS 1,l wS 1,l p; S1wS1pS 1,l p; S1pS 1,l wS 1,l p; S1wS1pS1,rw; S1wS 1,l wS1,rp;</formula><p>S1wS1pS1,rp; S1pS1,rwS1,rp; Linearization w0; p0; w−1w0; p−1p0; w−2w−1w0; p−2p−1p0; S 0,l S 0,l2 w; S 0,l pS 0,l2 p; S0,r2wS0,rw; S0,r2pS0,rp; S 1,l S 1,l2 w; S 1,l pS 1,l2 p; S1,r2wS1,rw; S1,r2pS1,rp;  Training. We train N-gram language models from raw text using modified Kneser-Ney smooth- ing without pruning. The text is true-case tok- enized, and we train 4-gram language modes using KenLM 2 , which gives high efficiencies in standard N-gram language model construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental settings 3.1 Data</head><p>For training data, we use the Wall Street Journal (WSJ) sections 1-22 of the Penn Treebank (Mar-  The $ 409 million bid includes the assum- ption of an estimated $ 300 million in sec- ured liabilities on those properties , accor- ding to those making the bid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News</head><p>But after rising steadily during the quarter- century following World War II , wages ha- ve stagnated since the manufacturing sector began to contract .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blog</head><p>The freaky thing here is that these bozos are seriously claiming the moral high grou- nd ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation metrics</head><p>We follow previous work and use the BLEU met- ric ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We addition- ally use METEOR 3 <ref type="bibr" target="#b6">(Denkowski and Lavie, 2010</ref>) to evaluate the system performances. The BLEU metric measures the fluency of generated sentence without considering long range ordering. The ME- TEOR metric can potentially fix this problem us- ing a set of mapping between generated sentences and references to evaluate distortion. The fol- lowing example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is (1) <ref type="bibr">[</ref>  the METEOR gives a score of 61.34 out of 100. This is because that METEOR is based on ex- plicit word-to-word matches over the whole sen- tence. For word ordering, word-to-word matches are unique, which facilitates METEOR evaluation between generated sentences and references. As can bee seen from the example, long range dis- tortion can highly influence the METEOR scores making the METEOR metric more suitable for evaluating word ordering distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data preparation</head><p>For all the experiments, we assume that the in- put is a bag of words without order, and the out- put is a fully ordered sentence. The syntactic model requires that the train- ing sentences have syntactic dependency struc- ture. However, only the WSJ data contains gold- standard annotations. In order to obtain automati- cally annotated dependency trees, we train a con- stituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data. The results are turned into de- pendency trees using Penn2Malt 4 , after base noun phrases are extracted. In our experiments, we use ZPar 5 (Zhu et al., 2013) for automatic constituent parsing.</p><p>In order to study the influence of parsing ac- curacy of the training data, we also use ten-fold jackknifing to construct WSJ training data with different accuracies. The data is randomly split into ten equal-size subsets, and each subset is auto- matically parsed with a parser trained on the other</p><note type="other">in-domain on WSJ test cross-domain on WPB test cross-domain on SANCL test</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU (%) METEOR (%) BLEU (%) METEOR (%) BLEU (%) METEOR (%)</head><p>syntax-set57  <ref type="table">Table 5</ref>: Influence result of parsing accuracy.</p><p>nine subset. In order to obtain datasets with dif- ferent parsing accuracies, we randomly sample a small number of sentences from each training sub- set, as shown in <ref type="table" target="#tab_5">Table 4</ref>. The dependency trees of each set are derived from these bracketed sen- tences using Penn2Malt after base noun phrase are extracted as a single word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Influence of parsing accuracy 4.1 In-domain word ordering</head><p>We train the syntactic models on the WSJ training parsing data with different accuracies. The WSJ development data are used to find out the optimal number of training iterations for each experiments, and the WSJ test results are shown in <ref type="table">Table 5</ref>. <ref type="table">Table 5</ref> shows that the parsing accuracy can af- fect the performance of the syntactic model. A higher parsing accuracy can lead to a better syn- tactic language model. It conforms to the intu- ition that syntactic quality affects the fluency of surface texts. On the other hand, the influence is not huge, the BLEU scores decrease by 1.0 points as the parsing accuracy decreases from 88.10% to 57.31%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-domain word ordering</head><p>The influence of parsing accuracy of the training data on cross-domain word ordering is measured by using the same training settings, but testing on the WPB and SANCL test sets. <ref type="table">Table 5</ref> shows that the performance on cross-domain word order- ing cannot reach that of in-domain word ordering using the syntactic models. Compared with the cross-domain experiments, the influence of pars- ing accuracy becomes smaller. In the WPB test, the fluctuation of performance decline to about 0.9 BLEU points, and in the SANCL test, the fluctua- tion is about 1.1 BLEU points.</p><p>In conclusion, the experiments show that pars- ing accuracies have a relatively small influence on the syntactic models. This suggests that it is possi- ble to use large automatically-parsed data to train syntactic models. On the other hand, when the training data scale increases, syntactic models can become much slower to train compared with N- gram models. The influence on data scale, which includes output quality and training time, is further studied in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Influence of data scale</head><p>We use the AFP news data as the training data for the experiments of this section. The syntac- tic models are trained using automatically-parsed trees derived from ZPar, as described in Section 3.3. The WPB test data is used to measure in- domain performance, and the SANCL blog data is used to measure cross-domain performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Influence on BLEU and METEOR</head><p>The <ref type="figure">Figure 3</ref> and 4 shows that using both the BLEU and the METEOR metrics, the perfor- mance of the syntactic model is better than that of the N-gram models. It suggests that sentences generated by the syntactic model have both bet- ter fluency and better ordering. The performance of the syntactic models is not highly weakened in cross-domain tests. The grey dot in each figure shows the perfor- mance of the syntactic model trained on the gold WSJ training data, and evaluated on the same WPB and SANCL test data sets. A comparison between the grey dots and the dashed lines shows that the syntactic model trained on the WSJ data perform better than the syntactic model trained on similar amounts of AFP data. This again shows the effect of syntactic quality of the training data.</p><p>On the other hand, as the scale of automatically- parsed AFP data increases, the performance of the  syntactic model rapidly increases, surpassing the syntactic model trained on the high-quality WSJ data. This observation is important, showing that large-scale data can be used to alleviate the prob- lem of lower syntactic quality in automatically- parsed data, which can be leveraged to address the scarcity issue of manually annotated data in both in-domain and cross-domain settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Influence on training time</head><p>The training time of both syntactic models and N-gram models increases as the size of training data increases. <ref type="figure" target="#fig_3">Figure 5</ref> shows the BLEU of the two systems under different amounts of training time. There is no result reported for the syntac- tic model beyond 1 million training sentences, be- cause training becomes infeasibly slow <ref type="bibr">6</ref> . On the other hand, the N-gram model can be trained using all the WSJ, AFP, XIN training sentences, which are 53 millions, within 10 3.2 seconds. As a result, there is no overlap between the syntactic model and the N-gram model curves.</p><p>As can be seen from the figure, the syntactic model is much slower to train. However, it ben- efits more from the scale of the training data, with the slope of the dashed curve being steeper than that of the solid curve. The N-gram model can be trained with more data thanks to the fast train- ing speed. However, the performance of the N- gram model flattens when the training data size reaches beyond 3 million. Projection of the solid curve suggests that the performance of the N-gram model may not surpass that of the syntactic model even if sufficiently large data is available for train- ing the N-gram model in more time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error analysis</head><p>Although giving overall better performance, the syntactic model does not perform better than the N-gram model in all cases. Here we analyze the strength of each model via more fine-grained com- parison.</p><p>In this set of experiments, the syntactic model is trained using gold-standard annotated WSJ train- ing parse trees, and the N-gram model is trained using the data containing WSJ training data, AFP and XIN. The WSJ test data, which contains  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sentence length</head><p>The BLEU and METEOR scores of the two sys- tems on various sentence lengths are shown in <ref type="figure">Figure 6</ref>. The results are measured by binning sentences according to their lengths, so that each bin contains about the same number of sentences. As shown by the figure, the N-gram model per- forms better on short sentences (less than 8 to- kens), and the syntactic model performs better on longer sentences. This can be explained by the fact that longer sentences have richer underlying syntactic structures, which can better captured by the syntactic model. In contrast, for shorter sen- tences, the syntactic structure is relatively simple, and therefore the N-gram model can give better performance based on string patterns, which form smaller search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Distortion range</head><p>We measure the average distortion rate of output word w using the following metric:</p><formula xml:id="formula_8">distortion(w) = |i w − i w | len(S w ) ,</formula><p>where i w is index of word w in the output sentence S w , i w is the index of the word w in the refer- ence sentence. len(S w ) is the number of tokens in  sentence S w . <ref type="figure">Figure 7</ref> shows distributions of dis- tortion respectively by the syntactic and N-gram model. The N-gram model makes relatively fewer short-range distortions, but more long-range dis- tortions. This can be explained by the local scor- ing nature of the N-gram model. In contrast, the syntactic model makes less long-range distortions, which can suggest better sentence structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Constituent span</head><p>We further evaluate sentence structure correctness by evaluating the recalls of discovered constituent span in output two systems, respectively. As shown in <ref type="figure">Figure 8</ref>. The syntactic model performs better in most constituent labels. However, the N-gram model performs better in WHPP, SBARQ and WHNP. In the test data, WHPP, SBARQ and WHNP are much less than PP, NP, VP, ADJP, ADVP and CONJP, on which the syntactic model gives bet- ter recalls. WHNP spans are small and most of them consist of a question word (WP$) and one or two nouns (e.g. "whose (WP$) parents (NNS)"). WHPP spans are also small and usually consist of a preposition (IN) and a WHNP span (e.g "at (IN) what level (WHNP)"). The N-gram model performs better on these small spans. The syntac- tic model also performs better on S, which covers the whole sentence structure. This verifies the hy- pothesis introduce that syntactic language models better capture overall sentential grammaticality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C O N J P W H A D V P W H P P F R A G S B A R Q W H A D J P W H N P P P N P V P Q P A D J P A D V P S S B A R</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Combining the syntactic and N-gram models</head><p>The results above show the respective error char- acteristics of each model, which are complimen- tary. This suggests that better results can be achieved by model combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">N-gram language model feature</head><p>We integrate the two types of models by using N-gram language model probabilities as features in the syntactic model. N-gram language model probabilities, which ranges from 0 to 1. Direct use of real value probabilities as features does not work well in our experiments, and we use dis- cretized features instead. For the L-ARC and R- ARC actions, because no words are pushed onto the stack, The NLM feature is set to NULL by de- fault. For the SHIFT action, different feature values are extracted depending on the NLM from 0 to 1. In order to measure the N-gram probabilities on our data, we train the 4-gram language model WSJ, AFP and XIN data, and randomly sample 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10 −12.5 . In this way, if p lower than 10 −12.5 , NLM feature value is set to LOW. As for p larger than 10 −12.5 , we extract the discrete features by assigning them into different bins. We bin the 4-gram probabil- ities with different granularities without overlap features. As shown in <ref type="table" target="#tab_9">Table 6</ref>, NLM-20, NLM- 10, NLM-5 and NLM-2 respectively use 20, 10, 5    <ref type="table" target="#tab_14">Table 9</ref> In addition, <ref type="table" target="#tab_11">Table 7</ref> shows that the N-gram model is the fastest among the models due to its small search space. The running time of the com- bined system is larger than the pure syntactic sys- tem, because of N-gram probability computation. <ref type="table" target="#tab_13">Table 8</ref> compare our results with different previ- ous methods on word ordering. Our combined model gives the best reported performance on this standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We empirically compared the strengths and er- ror distributions of syntactic and N-gram lan- guage models on word ordering, showing that both can benefit from large-scale raw text. The influ- ence of parsing accuracies has relatively small im- pact on the syntactic language model trained on automatically-parsed data, which enables scaling up of training data for syntactic language mod- els. However, as the size of training data in- creases, syntactic language models can become in- tolerantly slow to train, making them benefit less from the scale of training data, as compared with N-gram models.</p><p>Syntactic models give better performance com- pared with N-gram models, despite trained with less data. On the other hand, the two models lead to different error distributions in word ordering. As a result, we combined the advantages of both systems by integrating a syntactic model trained with relatively small data and an N-gram model trained with relatively large data. The resulting model gives better performance than both single models and achieves the best reported scores in a standard benchmark for word ordering.</p><p>We release our code under GPL at https:// github.com/SUTDNLP/ZGen. Future work includes application of the system on text-to-text problem such as machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Finance</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Following previ- ous work (Wan et al., 2009 ;</head><label>2009</label><figDesc>Zhang, 2013; Liu et al., 2015), we treat base noun phrases (i.e. noun phrases do not contains other noun phrases, such as 'Pierre Vinken' and 'a big cat') as a single word. This avoids unnecessary ambiguities in combination between their subcomponents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: BLEU on different training times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Performance on sentences with different length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Template distribution NLM-LOW set 1 if p &lt; e −12.5 , else 0 NLM-20 use 20 bins to scatter probability NLM-10 use 10 bins to scatter probability NLM-5 use 5 bins to scatter probability NLM-2 use 2 bins to scatter probability</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Feature templates.</head><label>1</label><figDesc></figDesc><table>name 
domain 
# of sents 
# of tokens 

training data 

AFP 
News 
35,390,025 844,395,322 

XIN 
News 
18,095,371 401,769,616 

WSJ 
Finance 
39,832 
950,028 

testing data 

WSJ 
Finance 
2,416 
56,684 

WPB 
News 
2,000 
43,712 

SANCL Blog 
1,015 
20,356 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Data.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Domain examples. 

cus et al., 1993), and the Agence France-Presse 
(AFP) and Xinhua News Agency (XIN) subsets of 
the English Giga Word Fifth Edition (Parker et al., 
2011). As the development data, we use WSJ sec-
tion 0 for parameter tuning. For testing, we use 
data from various domain, which consist of WSJ 
section 23, Washington Post/Bloomberg(WPB) 
subsets of the English Giga Word Fifth Edition and 
SANCL blog data, as shown in Table 2. Example 
sentence in various test domains are shown in Ta-
ble 3. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parsing accuracy settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 : NLM feature templates.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Final results on various domains.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Final results of all systems, where "*" 
means that the system uses extra POS input. 

and 2 bins to capture NLM feature values. 

7.2 Final results 

We use the WSJ, AFP and XIN for training the N-
gram model 7 . The same WSJ, WPB and SANCL 
test data are used to measure performances on dif-
ferent domains. 
The experimental results are shown in Tables 
7 and 8. In both in-domain and cross-domain 
test data, the combined system outperforms all 
other systems, with a BLEU score of 52.38 been 
achieved in the WSJ domain. It would be overly 
expensive to obtain a human oracle on discusses. 
However, according to Papineni (2002), a BLEU </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Output samples. 

score of over 52.38 indicate an easily understood 
sentence. Some sample outputs with different 
BLEU scores are shown in </table></figure>

			<note place="foot" n="1"> http://sourceforge.net/projects/zgen/</note>

			<note place="foot" n="2"> https://kheafield.com/code/kenlm/</note>

			<note place="foot" n="4"> http://stp.lingfil.uu.se/∼nivre/research/Penn2Malt.html 5 http://people.sutd.edu.sg/∼yue zhang/doc/doc/conparser.html</note>

			<note place="foot" n="6"> Our experiments are carried on a single thread of 3.60GHz CPU. If the training time is over 90 hours for a model, we consider it infeasible.</note>

			<note place="foot" n="7"> For the combined model, we used the WSJ training data for training, because the syntactic model is slower to train using large data. However, we did a set of experiments to scale up the training data by sampling 900k sentences from AFP. Results show that the combined model gives BLEU scores of 42.86 and 44.44 on the WPB and SANCL tests, respectively. Cross-domain BLEU on WSJ, however falls to 49.84.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research is funded by the Singapore min-istry of education (MOE) ACRF Tier 2 project T2MOE201301. We thank the anonymous review-ers for their detailed comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentence fusion for multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="328" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Syntax-based language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit IX</title>
		<meeting>MT Summit IX</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="40" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Utilizing dependency language models for graphbased dependency parsing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">111</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word ordering with phrase-based grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>A De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tomalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extending the meteor machine translation evaluation metric to the phrase level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="250" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency-based n-gram models for general purpose sentence realisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="455" to="483" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transition-based syntactic linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
		<meeting>NAACL/HLT<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">English gigaword fifth edition, june. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incremental syntactic language models for phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/HLT</title>
		<meeting>ACL/HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="620" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new string-to-dependency machine translation algorithm with a target dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint morphological generation and syntactic linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cécile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="852" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Syntax-based grammaticality improvement using ccg and guided search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1147" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Syntax-based word ordering incorporating a large-scale language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="736" to="746" />
		</imprint>
	</monogr>
	<note>Proceedings of EACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Syntactic smt using a discriminative text generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="177" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Partial-tree linearization: generalized word ordering for text synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2232" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
