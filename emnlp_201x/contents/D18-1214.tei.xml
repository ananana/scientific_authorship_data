<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gromov-Wasserstein Alignment of Word Embedding Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez-Melis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MIT</roleName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola Csail</surname></persName>
						</author>
						<title level="a" type="main">Gromov-Wasserstein Alignment of Word Embedding Spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1881" to="1890"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1881</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently , purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsu-pervised word translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many key linguistic tasks, within and across lan- guages or domains, including machine translation, rely on learning cross-lingual correspondences be- tween words or other semantic units. While the as- sociated alignment problem could be solved with access to large amounts of parallel data, broader applicability relies on the ability to do so with largely mono-lingual data, from Part-of-Speech (POS) tagging ( <ref type="bibr" target="#b26">Zhang et al., 2016)</ref>, dependency parsing ( <ref type="bibr" target="#b9">Guo et al., 2015)</ref>, to machine translation ( <ref type="bibr" target="#b4">Lample et al., 2018</ref>). The key subtask of bilingual lexical induction, for example, while long stand- ing as a problem <ref type="bibr" target="#b8">(Fung, 1995;</ref><ref type="bibr" target="#b18">Rapp, 1995</ref><ref type="bibr" target="#b19">Rapp, , 1999</ref>), has been actively pursued recently <ref type="bibr" target="#b0">(Artetxe et al., 2016;</ref><ref type="bibr" target="#b24">Zhang et al., 2017a;</ref><ref type="bibr" target="#b4">Conneau et al., 2018)</ref>.</p><p>Current methods for learning cross-domain cor- respondences at the word level rely on distributed representations of words, building on the observa- tion that mono-lingual word embeddings exhibit similar geometric properties across languages <ref type="bibr" target="#b12">Mikolov et al. (2013)</ref>. While most early work assumed some, albeit minimal, amount of paral- lel data ( <ref type="bibr" target="#b12">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Dinu et al., 2014;</ref><ref type="bibr" target="#b26">Zhang et al., 2016)</ref>, recently fully-unsupervised methods have been shown to perform on par with their supervised counterparts ( <ref type="bibr" target="#b4">Conneau et al., 2018;</ref><ref type="bibr" target="#b1">Artetxe et al., 2018)</ref>. While successful, the mappings arise from multiple steps of process- ing, requiring either careful initial guesses or post- mapping refinements, including mitigating the ef- fect of frequent words on neighborhoods. The as- sociated adversarial training schemes can also be challenging to tune properly ( <ref type="bibr" target="#b1">Artetxe et al., 2018)</ref>.</p><p>In this paper, we propose a direct optimization approach to solving correspondences based on re- cent generalizations of optimal transport (OT). OT is a general mathematical toolbox used to evalu- ate correspondence-based distances and establish mappings between probability distributions, in- cluding discrete distributions such as point-sets. However, the nature of mono-lingual word embed- dings renders the classic formulation of OT inap- plicable to our setting. Indeed, word embeddings are estimated primarily in a relational manner to the extent that the algorithms are naturally in- terpreted as metric recovery methods ( <ref type="bibr">Hashimoto et al., 2016)</ref>. In such settings, previous work has sought to bypass this lack of registration by jointly optimizing over a matching and an or- thogonal mapping ( <ref type="bibr" target="#b17">Rangarajan et al., 1997;</ref><ref type="bibr" target="#b25">Zhang et al., 2017b</ref>). Due to the focus on distances rather than points, we instead adopt a relational OT for- mulation based on the Gromov-Wasserstein dis- tance that measures how distances between pairs of words are mapped across languages. We show that the resulting mapping admits an efficient so- lution and requires little or no tuning.</p><p>In summary, we make the following contribu- tions:</p><p>• We propose the use of the Gromov- Wasserstein distance to learn correspon- dences between word embedding spaces in a fully-unsupervised manner, leading to a theoretically-motivated optimization prob- lem that can be solved efficiently, robustly, in a single step, and requires no post-processing or heuristic adjustments.</p><p>• To scale up to large vocabularies we realize an extended mapping to words not part of the original optimization problem.</p><p>• We show that the proposed approach per- forms on par with state-of-the-art neural net- work based methods on benchmark word translation tasks, while requiring a frac- tion of the computational cost and/or hyper- parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>In the unsupervised bilingual lexical induction problem we consider two languages with vocabu- laries V x and V y , represented by word embeddings</p><formula xml:id="formula_0">X = {x (i) } n i=1 and Y = {y (j) } m j=1</formula><p>, respectively, where</p><formula xml:id="formula_1">x (i) ∈ X ⊂ R dx corresponds to w x i ∈ V x and y (j) ∈ Y ⊂ R dy to w y j ∈ V y .</formula><p>For simplicity, we let m = n and d x = d y , although our meth- ods carry over to the general case with little or no modifications. Our goal is to learn an alignment between these two sets of words without any par- allel data, i.e., we learn to relate x (i) ↔ y (j) with the implication that w x i translates to w y j . As background, we begin by discussing the problem of learning an explicit map between em- beddings in the supervised scenario. The associ- ated training procedure will later be used for ex- tending unsupervised alignments (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Maps: Procrustes</head><p>In the supervised setting, we learn a map T : X → Y such that T (x (i) ) ≈ y (j) whenever w y j is a translation of w x i . Let X and Y be the matrices whose columns are vectors x (i) and y (j) , respec- tively. Then we can find T by solving</p><formula xml:id="formula_2">min T ∈F X − T (Y) 2 F (1)</formula><p>where · F is the Frobenius norm A F = i,j |a ij | 2 . Naturally, both the difficulty of finding T and the quality of the resulting align- ment depend on the choice of space F. A classic approach constrains T to be orthonormal matrices, i.e., rotations and reflections, resulting in the or- thogonal Procrustes problem</p><formula xml:id="formula_3">min P∈O(n) X − PY 2 F (2)</formula><p>where O(n) = {P ∈ R n×n | P P = I}.</p><p>One key advantage of this formulation is that it has a closed-form solution in terms of a sin- gular value decomposition (SVD), whereas for most other choices of constraint set F it does not. Given an SVD decomposition UΣV of XY , the solution to problem (2) is P * = UV <ref type="bibr" target="#b21">(Schönemann, 1966)</ref>. Besides obvious compu- tational advantage, constraining the mapping be- tween spaces to be orthonormal is justified in the context of word embedding alignment because orthogonal maps preserve angles (and thus dis- tances), which is often the only information used by downstream tasks (e.g., for nearest neighbor search) that rely on word embeddings. ( <ref type="bibr" target="#b22">Smith et al., 2017)</ref> further show that orthogonality is re- quired for self-consistency of linear transforma- tions between vector spaces.</p><p>Clearly, the Procrustes approach only solves the supervised version of the problem as it requires a known correspondence between the columns of X and Y. Steps beyond this constraint include using small amounts of parallel data ( <ref type="bibr" target="#b26">Zhang et al., 2016)</ref> or an unsupervised technique as the initial step to generate pseudo-parallel data ( <ref type="bibr" target="#b4">Conneau et al., 2018</ref>) before solving for P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Maps: Optimal Transport</head><p>Optimal transport formalizes the problem of find- ing a minimum cost mapping between two point sets, viewed as discrete distributions. Specifically, we assume two empirical distributions over em- beddings, e.g.,</p><formula xml:id="formula_4">µ = n i=1 p i δ x (i) , ν = m j=1 q j δ y (i)<label>(3)</label></formula><p>where p and q are vectors of probability weights associated with each point set. In our case, we usually consider uniform weights, e.g., p i = 1/n and q j = 1/m, although if additional information were provided (such as in the form of word fre- quencies), those could be naturally incorporated via p and q (see discussion at the end of Section 3). We find a transportation map T realizing</p><formula xml:id="formula_5">inf T X c(x, T (x))dµ(x) | T # µ = ν ,<label>(4)</label></formula><p>where the cost c(x, T (x)) is typically just x − T (x) and T # µ = ν implies that the source points must exactly map to the targets. However, such a map need not exist in general and we instead fol- low a relaxed Kantorovich's formulation. In this case, the set of transportation plans is a polytope:</p><formula xml:id="formula_6">Π(p, q) = {Γ ∈ R n×m + | Γ1 n = p, Γ 1 n = q}.</formula><p>The cost function is given as a matrix C ∈ R n×m , e.g., C ij = x (i) − y (j) . The total cost incurred by Γ is Γ, C := ij Γ ij C ij . Thus, the discrete optimal transport (DOT) problem consists of find- ing a plan Γ that solves</p><formula xml:id="formula_7">min Γ∈Π(p,q) Γ, C.<label>(5)</label></formula><p>Problem <ref type="formula" target="#formula_7">(5)</ref> is a linear program, and thus can be solved exactly in O(n 3 log n) with interior point methods. However, regularizing the objective leads to more efficient optimization and often bet- ter empirical results. The most common such reg- ularization, popularized by Cuturi (2013), involves adding an entropy penalization:</p><formula xml:id="formula_8">min Γ∈Π(p,q) Γ, C − λH(Γ).<label>(6)</label></formula><p>The solution of this strictly convex optimization problem has the form Γ * = diag (a) K diag (b), with K = e − C λ (element-wise), and can be ob- tained efficiently via the Sinkhorn-Knopp algo- rithm, a matrix-scaling procedure which itera- tively computes:</p><p>a ← p Kb and b ← q K a,</p><p>where denotes entry-wise division. The deriva- tion of these updates is immediate from the form of Γ * above, combined with the marginal con- straints Γ1 n = p, Γ 1 n = q ( <ref type="bibr" target="#b14">Peyré and Cuturi, 2018)</ref>. Although simple, efficient and theoretically- motivated, a direct application of discrete OT for unsupervised word translation is not appropriate. One reason is that the mono-lingual embeddings are estimated in a relative manner, leaving, e.g., an overall rotation unspecified. Such degrees of freedom can dramatically change the entries of the cost matrix C ij = x (i) − y (j) and the resulting transport map. One possible solution is to simulta- neously learn an optimal coupling and an orthog- onal transformation ( <ref type="bibr" target="#b25">Zhang et al., 2017b</ref>). The transport problem is then solved iteratively, using</p><formula xml:id="formula_10">C ij = x (i) − Py (j)</formula><p>, where P is in turn cho- sen to minimize the transport cost (via Procrustes). While promising, the resulting iterative approach is sensitive to initialization, perhaps explaining why <ref type="bibr" target="#b25">Zhang et al. (2017b)</ref> used an adversarially learned mapping as the initial step. The compu- tational cost can also be prohibitive ( <ref type="bibr" target="#b1">Artetxe et al., 2018</ref>) though could be remedied with additional development.</p><p>We adopt a theoretically well-founded gener- alization of optimal transport for pairs of points (their distances), thus in line with how the embed- dings are estimated in the first place. We explain the approach in detail in the next Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transporting across unaligned spaces</head><p>In this section we introduce the Gromov- Wasserstein distance, describe an optimization al- gorithm for it, and discuss how to extend the ap- proach to out-of-sample vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Gromov Wasserstein Distance</head><p>The classic optimal transport requires a distance between vectors across the two domains. Such a metric may not be available, for example, when the sample sets to be matched do not be- long to the same metric space (e.g., different dimension). The Gromov-Wasserstein distance (Mémoli, 2011) generalizes optimal transport by comparing the metric spaces directly instead of samples across the spaces. In other words, this framework operates on distances between pairs of points calculated within each domain and mea- sures how these distances compare to those in the other domain. Thus, it requires a weaker but easy to define notion of distance between distances, and operates on pairs of points, turning the problem from a linear to a quadratic one.</p><p>Formally, in its discrete version, this framework considers two measure spaces expressed in terms of within-domain similarity matrices (C, p) and (C , q) and a loss function defined between simi- larity pairs:</p><formula xml:id="formula_11">L : R × R → R, where L(C ik , C jl ) measures the discrepancy between the distances d(x (i) , x (k) ) and d (y (j) , y (l) ). Typical choices for L are L(a, b) = 1 2 (a − b) 2 or L(a, b) = KL(a|b). In this framework, L(C ik , C</formula><p>jl ) can also be under- stood as the cost of "matching" i to j and k to l.</p><p>All the relevant values of L(·, ·) can be put in a 4-th order tensor  <ref type="figure" target="#fig_0">Figure 1</ref>: The Gromov-Wasserstein distance is well suited for the task of cross-lingual alignment be- cause it relies on relational rather than positional similarities to infer correspondences across domains. Computing it requires two intra-domain similarity (or equivalently cost) matrices <ref type="bibr">(left &amp; center)</ref>, and it produces an optimal coupling of source and target points with minimal discrepancy cost (right).</p><formula xml:id="formula_12">L ∈ R N 1 ×N 1 ×N 2 ×N 2 , where L ijkl = L(C ik , C jl ).</formula><p>pling Γ specifying how much mass to transfer be- tween each pair of points from the two spaces. The Gromov-Wasserstein problem is then defined as solving</p><formula xml:id="formula_13">GW(C, C , p, q) = min Γ∈Π(p,q) i,j,k,l L ijkl Γ ij Γ kl (8)</formula><p>Compared to problem (5), this version is sub- stantially harder since the objective is now not only non-linear, but non-convex too. <ref type="bibr">1</ref> In addi- tion, it requires operating on a fourth-order ten- sor, which would be prohibitive in most settings. Surprisingly, this problem can be optimized ef- ficiently with first-order methods, whereby each iteration involves solving a traditional optimal transport problem <ref type="bibr" target="#b15">(Peyré et al., 2016)</ref>. Fur- thermore, for suitable choices of loss function L, <ref type="bibr" target="#b15">Peyré et al. (2016)</ref> show that instead of the O(N 2 1 N 2 2 ) complexity implied by naive fourth- order tensor product, this computation reduces to</p><formula xml:id="formula_14">O(N 2 1 N 2 + N 1N 2 2 ) cost.</formula><p>Their approach con- sists of solving (5) by projected gradient descent, which yields iterations that involve projecting onto Π(p, q) a pseudo-cost matrix of the formˆC</p><formula xml:id="formula_15">formˆ formˆC Γ (C, C , Γ) = C xy − h 1 (C)Γh 2 (C ) (9)</formula><p>where</p><formula xml:id="formula_16">C xy = f 1 (C)p1</formula><p>m + 1 n q f 2 (C ) and f 1 , f 2 , h 2 , h 2 are functions that depend on the loss L. We provide an explicit algorithm for the case L = L 2 at the end of this section. <ref type="bibr">1</ref> In fact, the discrete (Monge-type) formulation of the problem is essentially an instance of the well-known (and NP-hard) quadratic assignment problem (QAP).</p><p>Once we have solved (8), the optimal trans- port coupling Γ * provides an explicit (soft) match- ing between source and target samples, which for the problem of interest can be interpreted as a probabilistic translation: for every pair of words (w</p><formula xml:id="formula_17">(i) src , w (j) trg ), Γ *</formula><p>ij provides a likelihood that these two words are translations of each other. This itself is enough to translate, and we show in the experiments section that Γ * by itself, with- out any further post-processing, provides high- quality translations. This stands in sharp con- trast to mapping-based methods, which rely on nearest-neighbor computation to infer transla- tions, and thus become prone to hub-word effects which have to be mitigated with heuristic post- processing techniques such as Inverted Softmax ( <ref type="bibr" target="#b22">Smith et al., 2017)</ref> and Cross-Domain Similarity Scaling (CSLS) ( <ref type="bibr" target="#b4">Conneau et al., 2018</ref>). The trans- portation coupling Γ, being normalized by con- struction, requires no such artifacts.</p><p>The Gromov-Wasserstein problem (8) pos- sesses various desirable theoretical properties, in- cluding the fact that for a suitable choice of the loss function it is indeed a distance: Solving problem (8) therefore yields a fas- cinating accompanying notion: the Gromov- Wasserstein distance between languages, a mea- sure of semantic discrepancy purely based on the relational characterization of their word embed- dings. Owing to Theorem 3.1, such values can be interpreted as distances, so that, e.g., the triangle inequality holds among them. In Section 4.4 we compare various languages in terms of their GW- distance.</p><formula xml:id="formula_18">Theorem 3.1 (Mémoli 2011). With the choice L = L 2 , GW</formula><p>Finally, we note that whenever word frequency counts are available, those would be used for p and q. If they are not, but words are sorted ac- cording to occurrence (as they often are in popu- lar off-the-shelf embedding formats), one can es- timate rank-probabilities such as Zipf power laws, which are known to accurately model multiple lan- guages <ref type="bibr" target="#b16">(Piantadosi, 2014)</ref>. In order to provide a fair comparison to previous work, throughout our experiments we use uniform distributions so as to avoid providing our method with additional in- formation not available to others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scaling Up</head><p>While the pure Gromov-Wasserstein approach leads to high quality solutions, it is best suited to small-to-moderate vocabulary sizes, 2 since its optimization becomes prohibitive for very large problems. For such settings, we propose a two- step approach in which we first match a subset of the vocabulary via the optimal coupling, after which we learn an orthogonal mapping through a modified Procrustes problem. Formally, suppose we solve problem (8) for a reduced matrices X 1:k and Y i:k consisting of the first columns k of X and Y, respectively, and let Γ * be the optimal coupling. We seek an orthogonal matrix that best recovers the barycentric mapping implied by Γ * . Namely, we seek to find P which solves:</p><formula xml:id="formula_19">min P∈O(n) XΓ * − PY 2 2 (10)</formula><p>Just as problem (2), it is easy to show that this Procrustes-type problem has a closed form solu- tion in terms of a singular value decomposition. Namely, the solution to (10) is P * = UV , where</p><formula xml:id="formula_20">UΣV * = X 1:m Γ * Y 1:m .</formula><p>After obtaining this pro- jection, we can immediately map the rest of the embeddings viâ y (j) = P * y (j) . We point out that this two-step procedure re- sembles that of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref>. Both ul- timately produce an orthogonal mapping obtained by solving a Procrustes problems, but they differ in the way they produce pseudo-matches to allow for such second-step: while their approach relies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Gromov-Wasserstein Computation for Word Embedding Alignment</head><p>Input: Source and target embeddings X, Y. Regularization λ. Probability vectors p, q. // Compute intra-language similarities</p><formula xml:id="formula_21">C s ← cos(X, X), C t ← cos(Y, Y) C st ← C 2 s p1 m + 1 n q(C 2 t ) while not converged do // Compute pseudo-cost matrix (Eq. (9)) ˆ C Γ ← C st − 2C s ΓC t // Sinkhorn iterations (Eq. (7)) a ← 1, K ← exp{− ˆ C Γ /λ} while not converged do a ← p Kb, b ← q K a end while Γ ← diag (a) K diag (b) end while // Optional step: Learn explicit projection U, Σ, V ← SVD(XΓY ) P = UV return Γ, P</formula><p>on an adversarially-learned transformation, we use an explicit optimization problem.</p><p>We end this section by discussing parameter and configuration choices. To leverage the fast algo- rithm of <ref type="bibr" target="#b15">Peyré et al. (2016)</ref>, we always use the L 2 distance as the loss function L between cost ma- trices. On the other hand, we observed throughout our experiments that the choice of cosine distance as the metric in both spaces consistently leads to better results, which agrees with common wis- dom on computing distances between word em- beddings. This leaves us with a single hyper- parameter to control: the entropy regularization term λ. By applying any sensible normalization on the cost matrices (e.g., dividing by the mean or median value), we are able to almost entirely eliminate sensitivity to that parameter. In prac- tice, we use a simple scheme in all experiments: we first try the same fixed value (λ = 5 × 10 −5 ), and if the regularization proves too small (by lead- ing to floating point errors), we instead use λ = 1 × 10 −4 . We never had to go beyond these two values in all our experiments. We emphasize that at no point we use train (let alone test) supervision available with many datasets-model selection is done solely in terms of the unsupervised objective. Pseudocode for the full method (with L = L 2 and cosine similarity) is shown here as Algorithm 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Through this experimental evaluation we seek to: (i) understand the optimization dynamics of the proposed approach ( §4.2), evaluate its perfor- mance on benchmark cross-lingual word embed- ding tasks ( §4.3), and (iii) qualitatively investi- gate the notion of distance-between-languages it computes ( §4.4). Rather than focusing solely on prediction accuracy, we seek to demonstrate that the proposed approach offers a fast, principled, and robust alternative to state-of-the-art multi-step methods, delivering comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Tasks and Methods</head><p>Datasets We evaluate our method on two stan- dard benchmark tasks for cross-lingual embed- dings. First, we consider the dataset of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref>, which consists of word embeddings trained with FASTTEXT ( <ref type="bibr" target="#b2">Bojanowski et al., 2017)</ref> on Wikipedia and parallel dictionaries for 110 lan- guage pairs. Here, we focus on the language pairs for which they report results: English (EN) from/to Spanish (ES), French (FR), German (DE), Russian (RU) and simplified Chinese (ZH). We do not report results on Esperanto (EO) as dictionar- ies for that language were not provided with the original dataset release. For our second set of experiments, we con- sider the-substantially harder <ref type="bibr">3</ref> -dataset of ( <ref type="bibr" target="#b6">Dinu et al., 2014)</ref>, which has been extensively compared against in previous work. It consists of embed- dings and dictionaries in four pairs of languages; EN from/to ES, IT, DE, and FI (Finnish). <ref type="bibr">3</ref> We discuss the difference in hardness of these two bench- mark datasets in Section 4.3.</p><p>Methods To see how our fully-unsupervised method compares with methods that require (some) cross-lingual supervision, we follow <ref type="bibr" target="#b4">(Conneau et al., 2018)</ref> and consider a simple but strong baseline consisting of solving a procrustes prob- lem directly using the available cross-lingual em- bedding pairs. We refer to this method sim- ply as PROCRUSTES. In addition, we compare against the fully-unsupervised methods of <ref type="bibr" target="#b24">Zhang et al. (2017a)</ref>, <ref type="bibr" target="#b1">Artetxe et al. (2018)</ref> and <ref type="bibr" target="#b4">Conneau et al. (2018)</ref>. <ref type="bibr">4</ref> As proposed by the latter, we use CSLS whenever nearest neighbor search is re- quired, which has been shown to improve upon naive nearest-neighbor retrieval in multiple work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Dynamics of G-W</head><p>As previously mentioned, our approach involves only two optimization choices, one of which is required only for very large settings. When run- ning Algorithm 1 for the full set of embeddings is infeasible (due to memory limitations), one must decide what fraction of the embeddings to use during optimization. In our experiments, we use the largest possible size allowed by memory con- straints, which was found to be K = 20, 000 for the personal computer we used.</p><p>The other-more interesting-optimization choice involves the entropy regularization pa- rameter λ used within the Sinkhorn iterations.</p><p>Large regularization values lead to denser optimal coupling Γ * , while less regularization leads to sparser solutions, 5 at the cost of a harder (more  <ref type="table">Table 1</ref>: Performance (P@1) of unsupervised and minimally-supervised methods on the dataset of Con- neau et al. (2018). The time columns shows the average runtime in minutes of an instance (i.e., one language pair) of the method in this task on the same quad-core CPU machine.</p><note type="other">EN-ES EN-FR EN-DE EN-IT EN-RU Supervision</note><p>non-convex) optimization problem. In <ref type="figure" target="#fig_1">Figure 2</ref> we show the training dynamics of our method when learning correspondences be- tween word embeddings from the dataset of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref>. As expected, larger values of λ lead to smoother improvements with faster runtime-per-iteration, at a price of some drop in performance. In addition, we found that comput- ing GW distances between closer languages (such as EN and FR) leads to faster convergence than for more distant ones (such as EN and RU, in <ref type="figure" target="#fig_1">Fig. 2c)</ref>.</p><p>Worth emphasizing are three desirable opti- mization properties that set apart the Gromov- Wasserstein distance from other unsupervised alignment approaches, particularly adversarial- training ones: (i) the objective decreases mono- tonically (ii) its value closely follows the true metric of interest (translation, which naturally is not available during training) and (iii) there is no risk of degradation due to overtraining, as is the case for adversarial-based methods trained with stochastic gradient descent ( <ref type="bibr" target="#b4">Conneau et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmark Results</head><p>We report the results on the dataset of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> in <ref type="table">Table 1</ref>. The strikingly high per- formance of all methods on this task belies the hardness of the general problem of unsupervised cross-lingual alignment. Indeed, as pointed out by <ref type="bibr" target="#b1">Artetxe et al. (2018)</ref>, the FASTTEXT embed- dings provided in this task are trained on very large and highly comparable-across languages- corpora (Wikipedia), and focuses on closely re- lated pairs of languages. Nevertheless, we carry out experiments here to have a broad evaluation of our approach in both easier and harder settings.</p><p>Next, we present results on the more challeng- to a permutation matrix, which gives a hard-matching solu- tion to the transportation problem <ref type="bibr" target="#b14">(Peyré and Cuturi, 2018)</ref>. ing dataset of ( <ref type="bibr" target="#b6">Dinu et al., 2014</ref>) in <ref type="table" target="#tab_4">Table 2</ref>. Here, we rely on the results reported by <ref type="bibr" target="#b1">(Artetxe et al., 2018)</ref> since by the time of writing the present work their implementation was not available yet. Part of what makes this dataset hard is the wide discrepancy between word distance across lan- guages, which translates into uneven distance ma- trices <ref type="figure" target="#fig_2">(Figure 3)</ref>, and in turn leads to poor results for G-W. To account for this, previous work has relied on an initial whitening step on the embed- dings. In our case, it suffices to normalize the pairwise similarity matrices to the same range to obtain substantially better results. While we have observed that careful choice of the regularization parameter λ can obviate the need for this step, we opt for the normalization approach since it allows us to optimize without having to tune λ.</p><p>We compare our method (with and without nor-   <ref type="formula">(2018)</ref> as-is, which are obtained by running on a Titan XP GPU, while our runtimes are, as be- fore, obtained purely by CPU computation.</p><formula xml:id="formula_22">EN-IT EN-DE EN-FI EN-ES P@1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>As mentioned earlier, Theorem 3.1 implies that the optimal value of the Gromov-Wasserstein problem can be legitimately interpreted as a distance be- tween languages, or more explicitly, between their word embedding spaces. This distributional no- tion of distance is completely determined by pair- wise geometric relations between these vectors. In <ref type="figure" target="#fig_3">Figure 4</ref> we show the values GW(C s , C t , p, q) computed on the FASTTEXT word embeddings of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> corresponding to the most frequent 2000 words in each language. Overall, these distances conform to our intu- itions: the cluster of romance languages exhibits some of the shortest distances, while classical Chi- nese (ZH) has the overall largest discrepancy with all other languages. But somewhat surprisingly, Russian is relatively close to the romance lan- guages in this metric. We conjecture that this could be due to Russian's rich morphology (a trait shared by romance languages but not English). Furthermore, both Russian and Spanish are pro- drop languages <ref type="bibr">(Haspelmath, 2001</ref>) and share syn- tactic phenomena, such as dative subjects <ref type="bibr" target="#b13">(Moore and Perlmutter, 2000;</ref><ref type="bibr" target="#b10">Melis et al., 2013</ref>) and dif- ferential object marking <ref type="bibr" target="#b3">(Bossong, 1991)</ref>, which might explain why ES is closest to RU overall.</p><p>On the other hand, English appears remarkably isolated from all languages, equally distant from its germanic (DE) and romance (FR) cousins. In- deed, other aspects of the data (such as corpus size) might be underlying these observations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Study of the problem of bilingual lexical induction goes back to <ref type="bibr" target="#b18">Rapp (1995)</ref> and <ref type="bibr" target="#b8">Fung (1995)</ref>. While the literature on this topic is extensive, we focus here on recent fully-unsupervised and minimally- supervised approaches, and refer the reader to one of various existing surveys for a broader panorama ( <ref type="bibr" target="#b23">Upadhyay et al., 2016;</ref><ref type="bibr" target="#b20">Ruder et al., 2017)</ref>.</p><p>Methods with coarse or limited parallel data. Most of these fall in one of two categories: meth- ods that learn a mapping from one space to the other, e.g., as a least-squares objective (e.g., <ref type="bibr" target="#b12">(Mikolov et al., 2013)</ref>) or via orthogonal transfor- mations <ref type="bibr" target="#b26">Zhang et al. (2016)</ref>; <ref type="bibr" target="#b22">Smith et al. (2017)</ref>; <ref type="bibr" target="#b0">Artetxe et al. (2016)</ref>, and methods that find a com-mon space on which to project both sets of embed- dings <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr">Lu et al., 2015)</ref>.</p><p>Fully Unsupervised methods. <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> and <ref type="bibr" target="#b24">Zhang et al. (2017a)</ref> rely on adversarial training to produce an initial alignment between the spaces. The former use pseudo-matches de- rived from this initial alignment to solve a Pro- crustes (2) alignment problem. Our Gromov- Wasserstein framework can be thought of as pro- viding an alternative to these adversarial training steps, albeit with a concise optimization formula- tion and producing explicit matches (via the op- timal coupling) instead of depending on nearest neighbor search, as the adversarially-learnt map- pings do.</p><p>Zhang et al. (2017b) also leverage optimal transport distances for the cross-lingual embed- ding task. However, to address the issue of non- alignment of embedding spaces, their approach follows the joint optimization of the transportation and procrustes problem as outlined in Section 2.2. This formulation makes an explicit modeling as- sumption (invariance to unitary transformations), and requires repeated solution of Procrustes prob- lems during alternating minimization. Gromov- Wasserstein, on the other hand, is more flexible and makes no such assumption, since it directly deals with similarities rather than vectors. In the case where it is required, such an orthogonal map- ping can be obtained by solving a single procrustes problem, as discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and future work</head><p>In this work we provided a direct optimization approach to cross-lingual word alignment. The Gromov-Wasserstein distance is well-suited for this task as it performs a relational comparison of word-vectors across languages rather than word- vectors directly. The resulting objective is concise, and can be optimized efficiently. The experimen- tal results show that the resulting alignment frame- work is fast, stable and robust, yielding near state- of-the-art performance at a computational cost or- ders of magnitude lower than that of alternative fully unsupervised methods.</p><p>While directly solving Gromov-Wasserstein problems of reasonable size is feasible, scaling up to large vocabularies made it necessary to learn an explicit mapping via Procrustes. GPU computa- tions or stochastic optimization could help avoid this secondary step.</p><p>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), vol- ume 1, pages 1234-1244. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 2</head><label>1</label><figDesc>is a distance on the space of metric measure spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training dynamics for the Gromov-Wasserstein alignment problem. The algorithm provably makes progress in each iteration, and the objective (red dashed line) closely follows the metric of interest (translation accuracy, not available during training). More related languages (e.g., EN →FR in 2b,2a) lead to faster optimization, while more distant pairs yield slower learning curves (EN →RU, 2c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top: Word embeddings trained on noncomparable corpora can lead to uneven distributions of pairwise distances as shown here for the EN-FI pair of (Dinu et al., 2014). Bottom: Normalizing the cost matrices leads to better optimization and improved performance.</figDesc><graphic url="image-13.png" coords="7,323.24,325.60,76.82,76.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pairwise language Gromov-Wasserstein distances obtained as the minimal transportation cost (8) between word embedding similarity matrices. Values scaled by 10 2 for easy visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Time P@1 Time P@1 Time P@1 Time</head><label></label><figDesc></figDesc><table>(Zhang et al., 2017a) † 
0 
46.6 
0 
46.0 
0.07 
44.9 
0.07 
43.0 
(Conneau et al., 2018) † 45.40 46.1 47.27 45.4 
1.62 
44.4 36.20 45.3 
(Artetxe et al., 2018) † 
48.53 
8.9 
48.47 
7.3 
33.50 12.9 37.60 
9.1 

G-W 
44.4 
35.2 37.83 36.7 
6.8 
15.6 
12.5 
18.4 
G-W + NORMALIZE 
49.21 
36 
46.5 
33.2 
18.3 
42.1 37.60 38.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of unsupervised methods on the dataset of Dinu et al. (2014) with runtimes in min-
utes. Those marked with  † are from (Artetxe et al., 2018). Note that their runtimes correspond to GPU 
computation, while ours are CPU-minutes, so the numbers are not directly comparable. 

malization) against alternative approaches in Ta-
ble 2. Note that we report the runtimes of Artetxe 
et al. </table></figure>

			<note place="foot" n="2"> As shown in the experimental section, we are able to run problems of size in the order of |Vs| ≈ 10 5 ≈ |Vt| on a single machine without relying on GPU computation.</note>

			<note place="foot" n="4"> Despite its relevance, we do not include the OT-based method of Zhang et al. (2017b) in the comparison because their implementation required use of proprietary software. 5 In the limit λ → 0, when n = m, the solution converges</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous re-viewers for helpful feedback. The work was par-tially supported by MIT-IBM grant "Adversarial learning of multimodal and structured data", and Graduate Fellowships from Hewlett Packard and CONACYT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Differential object marking in Romance and beyond. New analyses in Romance linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Bossong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="143" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word Translation Without Parallel Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6568</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Cross-lingual dependency parsing based on distributed representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the historical expansion of noncanonically marked &apos;subjects&apos; in Spanish. The diachronic Typology of Non-Canonical Subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantal</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holvoet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Amsterdam/Philadelphia, Benjamins</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="163" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein distances and the metric approach to object matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facundo</forename><surname>Mémoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of computational mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="487" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168v1</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Perlmutter</surname></persName>
		</author>
		<title level="m">What does it take to be a dative subject? Natural Language and Linguistic Theory</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="373" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computational Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein averaging of kernel and distance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2664" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zipf&apos;s word frequency law in natural language: A critical review and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steven T Piantadosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1112" to="1130" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Softassign Procrustes Matching Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haili</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1230</biblScope>
			<biblScope unit="page" from="29" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying word translations in non-parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="320" to="322" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic identification of word translations from unrelated English and German corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A survey of cross-lingual embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04902</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cross-lingual models of word embeddings: An empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00425</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Earth Mover&apos;s Distance Minimization for Unsupervised Bilingual Lexicon Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ten Pairs to Tag-Multilingual POS Tagging via Coarse Mapping between Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1307" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
