<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Code-switched Language Models Using Dual RNNs and Same-Source Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Garg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Parekh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Code-switched Language Models Using Dual RNNs and Same-Source Pretraining</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3078" to="3083"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3078</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work focuses on building language models (LMs) for code-switched text. We propose two techniques that significantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive significant reductions in perplexity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Code-switching is a widespread linguistic phe- nomenon among multilingual speakers that in- volves switching between two or more languages in the course of a single conversation or within a single sentence <ref type="bibr" target="#b3">(Auer, 2013)</ref>. Building speech and language technologies to handle code-switching has become a fairly active area of research and presents a number of interesting technical chal- lenges <ref type="bibr">(C ¸ etinoglu et al., 2016)</ref>. Language mod- els for code-switched text is an important prob- lem with implications to downstream applications such as speech recognition and machine transla- tion of code-switched data. A natural choice for building such language models would be to use recurrent neural networks (RNNs) ( <ref type="bibr">Mikolov et al., 2010)</ref>, which yield state-of-the-art language mod- els in the case of monolingual text. In this work, we explore mechanisms that can significantly im- prove upon such a baseline when applied to code- switched text. Specifically, we develop two such mechanisms:</p><p>• We alter the structure of an RNN unit to in- clude separate components that focus on each * Joint first authors language in code-switched text separately while coordinating with each other to retain contex- tual information across code-switch boundaries.</p><p>Our new model is called a Dual RNN Language Model (D-RNNLM), described in Section 2.</p><p>• We propose using same-source pretraining - i.e., pretraining the model using data sampled from a generative model which is itself trained on the given training data -before training the model on the same training data (see Section 3). We find this to be a surprisingly effective strat- egy.</p><p>We study the improvements due to these tech- niques under various settings (e.g., with and with- out access to monolingual text in the candidate languages for pretraining). We use perplexity as a proxy to measure the quality of the language model, evaluated on code-switched text in English and Mandarin from the SEAME corpus. Both the proposed techniques are shown to yield significant perplexity improvements (up to 13% relative) over different baseline RNNLM models (trained with a number of additional resources). We also explore how to combine the two techniques effectively.</p><p>Related Work: <ref type="bibr" target="#b2">Adel et al. (2013)</ref> was one of the first works to explore the use of RNNLMs for code-switched text. Many subsequent works explored the use of external sources to enhance code-switched LMs, including the use of part-of- speech (POS) tags, syntactic and semantic fea- tures ( <ref type="bibr" target="#b20">Yeh et al., 2010;</ref><ref type="bibr" target="#b0">Adel et al., 2014</ref><ref type="bibr" target="#b1">Adel et al., , 2015</ref> and the use of machine translation systems to gen- erate synthetic text ( <ref type="bibr" target="#b19">Vu et al., 2012)</ref>. Prior work has also explored the use of interpolated LMs trained separately on monolingual texts <ref type="bibr" target="#b6">(Bhuvanagiri and Kopparapu, 2010;</ref><ref type="bibr" target="#b12">Imseng et al., 2011;</ref><ref type="bibr" target="#b15">Li et al., 2011;</ref><ref type="bibr" target="#b4">Baheti et al., 2017)</ref>. Linguis- tic constraints governing code-switching have also</p><formula xml:id="formula_0">Out LSTM (L1) 0 0 LSTM (L0) #0 #1 Emb0 Emb1 0 0 τ2 b2 Out LSTM (L1) 0 0 LSTM (L0) #0 #1 Emb0 Emb1 0 0 τ1 b1</formula><p>Figure 1: Illustration of the dual RNNLM (see the text for a detailed description). The highlighted left-to-right path (in green) indicates the flow of state information, when b1 = 0 and b2 = 1 (corresponding to token τ1 belonging to language L0 and τ2 belonging to L1). The highlighted bottom-to-top path (in orange) indicates the inputs and outputs.</p><p>been used as explicit priors to model when peo- ple switch from one language to another. Fol- lowing this line of enquiry, <ref type="bibr" target="#b9">(Chan et al., 2004)</ref> used grammar rules to model code-switching; <ref type="bibr">Fung, 2013, 2014</ref>) incorporated syntactic con- straints with the help of a code-switch boundary prediction model; ( <ref type="bibr" target="#b18">Pratapa et al., 2018</ref>) used a lin- guistically motivated theory to create grammati- cally consistent synthetic code-mixed text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dual RNN Language Models</head><p>Towards improving the modeling of code- switched text, we introduce Dual RNN Language Models (D-RNNLMs). The philosophy behind D- RNNLMs is that two different sets of neurons will be trained to (primarily) handle the two languages. (In prior work ( <ref type="bibr" target="#b11">Garg et al., 2018</ref>), we applied sim- ilar ideas to build dual N-gram based language models for code-switched text.) As shown in <ref type="figure">Figure 1</ref>, the D-RNNLM consists of a "Dual LSTM cell" and an input encoding layer. The Dual LSTM cell, as the name indi- cates, has two long short-term memory (LSTM) cells within it. The two LSTM cells are desig- nated to accept input tokens from the two lan- guages L 0 and L 1 respectively, and produce an (unnormalized) output distribution over the tokens in the same language. When a Dual LSTM cell is invoked with an input token τ , the two cells will be invoked sequentially. The first (upstream) LSTM cell corresponds to the language that τ belongs to, and gets τ as its input. It passes on the resulting state to the downstream LSTM cell (which takes a dummy token as input). The unnormalized out- puts from the two cells are combined and passed through a soft-max operation to obtain a distribu- tion over the union of the tokens in the two lan- guages. <ref type="figure">Figure 1</ref> shows a circuit representation of this configuration, using multiplexers (shaded units) controlled by a selection bit b i such that the i th token τ i belongs to L b i .</p><p>The input encoding layer also uses multiplexers to direct the input token to the upstream LSTM cell. Two dummy tokens # 0 and # 1 are added to L 0 and L 1 respectively, to use as inputs to the downstream LSTM cell. The input tokens are en- coded using an embedding layer of the network (one for each language), which is trained along with the rest of the network to minimize a cross- entropy loss function.</p><p>The state-update and output functions of the Dual LSTM cell can be formally described as fol- lows. It takes as input (b, τ ) where b is a bit and τ is an input token, as well as a state vector of the form (h 0 , h 1 ) corresponding to the state vectors produced by its two constituent LSTMs. Below we denote the state-update and output functions of these two LSTMs as H b (τ, h) and O b (τ, h) (for b = 0, 1):</p><formula xml:id="formula_1">H((b, τ ), (h 0 , h 1 )) = (h 0 , h 1 )</formula><p>where</p><formula xml:id="formula_2">(h 0 , h 1 ) = (H 0 (τ, h 0 ), H 1 (# 1 , h 0 )) if b = 0 (H 0 (# 0 , h 1 ), H 1 (τ, h 1 )) if b = 1 O((b, τ ), (h 0 , h 1 )) = softmax(o 0 , o 1 )</formula><p>where </p><formula xml:id="formula_3">(o 0 , o 1 ) = (O 0 (τ, h 0 ), O 1 (# 1 , h 0 )) if b = 0 (O 0 (# 0 , h 1 ), O 1 (τ, h 1 )) if b = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Same-Source Pretraining</head><p>Building robust LMs for code-switched text is challenging due to the lack of availability of large amounts of training data. One solution is to artificially generate code-switched to augment the training data. We propose a variant of this approach -called same-source pretraining -in which the actual training data itself is used to train a generative model, and the data sampled from this model is used to pretrain the language model. Same-source pretraining can leverage powerful training techniques for generative models to train a language model. We note that the generative mod- els by themselves are typically trained to minimize a different objective function (e.g., a discrimina- tion loss) and need not perform well as language models. * Our default choice of generative model will be an RNN (but see the end of this paragraph). To complete the specification of same-source pre- training, we need to specify how it is trained from the given data. Neural language models trained using the maximum likelihood training paradigm tend to suffer from the exposure bias problem during inference when the model gener- ates a text sequence by conditioning on previous tokens that may never have appeared during train- ing. Scheduled sampling ( <ref type="bibr" target="#b5">Bengio et al., 2015)</ref> can help bridge this gap between the training and in- ference stages by using model predictions to syn- thesize prefixes of text that are used during train- ing, rather than using the actual text tokens. A more promising alternative to generate text se- quences was recently proposed by <ref type="bibr" target="#b21">Yu et al. (2017)</ref> where sequence generation is modeled in a genera- tive adversarial network (GAN) based framework. This model -referred to as "SeqGAN" -consists of a generator RNN and a discriminator network trained as a binary classifier to distinguish between real and generated sequences. The main innova- tion of SeqGAN is to train the generative model using policy gradients (inspired by reinforcement learning) and use the discriminator to determine the reward function. We experimented with using both na¨ıvena¨ıve and scheduled sampling based training; using SeqGAN was a consistently better choice (5 points or less in terms of test perplexities) com- pared to these two sampling methods. As such, we use SeqGAN as our training method for the generator. We also experiment with replacing the RNN with a Dual RNN as the generator in the Se- qGAN training and observe small but consistent reductions in perplexity. * In our experiments, we found the preplexity measures for the generative models to be an order of magnitude larger than that of the LMs we construct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Dataset Preparation: For our experiments, we use code-switched text from the SEAME cor- pus ( <ref type="bibr" target="#b16">Lyu et al., 2010</ref>) which contains conversa- tional speech in Mandarin and English. Since there is no standardized task based on this corpus, we construct our own training, development and test sets using a random 80-10-10 split. <ref type="table">Table 1</ref> shows more details about our data sets. (Speakers were kept disjoint across these datasets.)</p><p>Evaluation Metric: We use token-level per- plexity as the evaluation metric where tokens are words in English and characters in Mandarin. The SEAME corpus provides word boundaries for Mandarin text. However, we used Mandarin char- acters as individual tokens since a large proportion of Mandarin words appeared very sparsely in the data. Using Mandarin characters as tokens helped alleviate this issue of data sparsity; also, applica- tions using Mandarin text are typically evaluated at the character level and do not rely on having word boundary markers ( <ref type="bibr" target="#b19">Vu et al., 2012</ref>).</p><p>Outline of Experiments: Section 4.1 will ex- plore the benefits of both our proposed techniques -(1) using D-RNNLMs and (2) using text gen- erated from SeqGAN for pretraining -in isolation and in combination. Section 4.2 will introduce two additional resources (1) monolingual text for pre- training and (2) a set of syntactic features used as additional input to the RNNLMs that further im- prove baseline perplexities. We show that our pro- posed techniques continue to outperform the base- lines albeit with a smaller margin. All these per- plexity results have been summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Improvements Over the Baseline</head><p>This section focuses only on the numbers listed in the first two columns of <ref type="table" target="#tab_2">Table 2</ref>. The Base- line model is a 1-layer LSTM LM with 512 hidden nodes, input and output embedding dimensionality of 512, trained using SGD with an initial learning rate of 1.0 (decayed exponentially after 80 epochs at a rate of 0.98 till 100 epochs) The develop- ment and test set perplexities using the baseline are 89.60 and 74.87, respectively.</p><p>The D-RNNLM is a 1-layer language model with each LSTM unit having 512 hidden nodes. The training paradigm is similar to the above- mentioned setting for the baseline model. <ref type="bibr">†</ref>   Finally, we combine both our proposed tech- niques by replacing the generator with our best- trained D-RNNLM within SeqGAN. Although there are other ways of combining both our pro- posed techniques, e.g. pretraining a D-RNNLM using data sampled from an RNNLM SeqGAN, we found this method of combination to be most effective. We see modest but consistent improve- ments with D-RNNLM SeqGAN over RNNLM SeqGAN in <ref type="table" target="#tab_2">Table 2</ref>, further validating the utility of D-RNNLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Using Additional Resources</head><p>We employed two additional resources to further improve our baseline models. First, we used monolingual text in the candidate languages to pretrain the RNNLM and D-RNNLM models. We used transcripts from the Switchboard corpus ¶ for English; AIShell and THCHS30 * * corpora for ever, increasing the capacity of an RNNLM to exactly match this number makes its test perplexity worse; RNNLM with 720 hidden units gives a development set perplexity of <ref type="bibr">91</ref> Mandarin monolingual text data. This resulted in a total of ≈3.1 million English tokens and ≈2.5 million Mandarin tokens. Second, we used an additional set of input features to the RNNLMs and D-RNNLMs that were found to be useful for code-switching in prior work ( <ref type="bibr" target="#b0">Adel et al., 2014</ref>). The feature set included part-of-speech (POS) tag features and Brown word clusters ( <ref type="bibr" target="#b7">Brown et al., 1992)</ref>, along with a language ID feature. We ex- tracted POS tags using the Stanford POS-tagger † † and we clustered the words into 70 classes using the unsupervised clustering algorithm by <ref type="bibr" target="#b7">Brown et al. (1992)</ref> to get Brown cluster features.</p><p>The last six columns in <ref type="table" target="#tab_2">Table 2</ref> show the util- ity of using either one of these resources or both of them together (shown in the last two columns). The perplexity reductions are largest (compared to the numbers in the first two columns) when com- bining both these resources together. Interestingly, all the trends we observed in Section 4.1 still hold. D-RNNLMs still consistently perform better than their RNNLM counterparts and we obtain the best overall results using D-RNNLM SeqGAN.   <ref type="table" target="#tab_5">Table 3</ref> shows how the perplexities on the de- velopment set from six of our prominent mod- els decompose into the perplexities contributed by English tokens preceded by English tokens (Eng- Eng), Eng-Man, Man-Eng and Man-Man tokens. This analysis reveals a number of interesting ob- servations. 1) The D-RNNLM mainly improves over the baseline on the "switching tokens", Eng-Man and Man-Eng. 2) The RNNLM with mono- lingual data improves most over the baseline on "the monolingual tokens", Eng-Eng and Man- Man, but suffers on the Eng-Man tokens. The D- RNNLM with monolingual data does as well as the baseline on the Eng-Man tokens and performs better than "Mono RNNLM" on all other tokens. 3) RNNLM SeqGAN suffers on the Man-Eng to- kens, but helps on the rest; in contrast, D-RNNLM SeqGAN helps on all tokens when compared with the baseline. <ref type="bibr">SeqGAN</ref>  As an additional measure of the quality of text generated by RNNLM SeqGAN and D-RNNLM SeqGAN, in <ref type="table" target="#tab_7">Table 4</ref>, we measure the diversity in the generated text by looking at the increase in the number of unique n-grams with respect to the SEAME training text. D-RNNLM SeqGAN is clearly better at generating text with larger diver- sity, which could be positively correlated with the perplexity improvements shown in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Analysis</head><p>While we do not claim same-source pretraining may be an effective strategy in general, we show it is useful in low training-data scenarios. Even with only <ref type="bibr">1</ref> 16 th of the original SEAME training data used for same-source pretraining, develop- ment and test perplexities are reduced to 84.45 and 70.59, respectively (compared to 79.16 and 65.96 using the entire training data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>D-RNNLMs and same-source pretraining pro- vide significant perplexity reductions for code- switched LMs. These techniques may be of more general interest. Leveraging generative models to train LMs is potentially applicable beyond code- switching; D-RNNLMs could be generalized be- yond LMs, e.g. speaker diarization. We leave these for future work to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Note that above, the inputs to the downstream LSTM functions H 1−b and O 1−b are expressed in terms of h b which is produced by the upstream LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Tokens 661, 025 101, 076 64, 009 Table 1: Statistics of data splits derived from SEAME.</head><label></label><figDesc></figDesc><table>Train 
Dev 
Test 
# Utterances 
74, 927 
9, 301 
9, 552 
# Tokens 
977, 751 131, 230 114, 546 
# English Tokens 
316, 726 30, 154 
50, 537 
# Mandarin </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Development set and test set perplexities using RNNLMs and D-RNNLMs with various pretraining strategies. 

consistent improvements in test perplexity when 
comparing a D-RNNLM with an RNNLM (i.e. 
74.87 drops to 72.29).  ‡ 
Next, we use text generated from a SeqGAN 
model to pretrain the RNNLM.  § We use our best 
trained RNNLM baseline as the generator within 
SeqGAN. We sample 157,440 sentences (with 
a fixed sentence length of 20) from the Seq-
GAN model; this is thrice the amount of code-
switched training data. We first pretrain the base-
line RNNLM with this sampled text, before train-
ing it again on the code-switched text. This gives 
significant reductions in test perplexity, bringing it 
down to 65.96 (from 74.87). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Decomposed perplexities on the development set 

on all four types of tokens from various models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Percentage of new n-grams generated. 

</table></figure>

			<note place="foot">† D-RNNLMs have a few additional parameters. How</note>

			<note place="foot">† † https://nlp.stanford.edu/software/ tagger.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>The authors thank the anonymous reviewers for their helpful comments and suggestions. The last author gratefully acknowledges financial support from Microsoft Research India (MSRI) for this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining recurrent neural networks and factored language models during decoding of code-switching speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Syntactic and semantic features for code-switching factored language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Transactions on Audio, Speech, and Language Processing</title>
		<meeting>IEEE Transactions on Audio, Speech, and Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="431" to="440" />
		</imprint>
	</monogr>
	<note>Dominic Telaar, and Tanja Schultz</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent neural network language modeling for code switching conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8411" to="8415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Code-switching in conversation: Language, interaction and identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum design for code-switching: Experiments with language identification and language modeling with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICON</title>
		<meeting>ICON</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An approach to mixed language automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhuvanagiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Kopparapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Oriental COCOSDA</title>
		<meeting>Oriental COCOSDA<address><addrLine>Kathmandu, Nepal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Challenges of computational processing of code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ozlem C ¸ Etinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detection of language boundary in code-switching utterances by bi-phone probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Chinese Spoken Language Processing</title>
		<meeting>International Symposium on Chinese Spoken Language Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language informed modeling of code-switched text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Third Workshop on Computational Approaches to Linguistic Code-Switching</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual language models for code mixed speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language dependent universal phoneme posterior estimation for mixed language speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><forename type="middle">Magimai</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5012" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7368" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language modeling with functional head constraint for code switching speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asymmetric acoustic modeling of mixed language speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5004" to="5007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An analysis of a MandarinEnglish code-switching speech corpus: SEAME</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dau-Cheng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng-Siong</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Age</title>
		<meeting>Age</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">JaňJaň Cernock`Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language modeling for code-mixing: The role of linguistic theory based synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Pratapa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayatri</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1543" to="1553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A first speech recognition system for MandarinEnglish code-switch conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dau-Cheng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Blaicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engsiong</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4889" to="4892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An integrated framework for transcribing Mandarin-English code-mixed lectures with improved acoustic and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching Feng</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin Shan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Chinese Spoken Language Processing (ISCSLP)</title>
		<meeting>Chinese Spoken Language Processing (ISCSLP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="214" to="219" />
		</imprint>
	</monogr>
	<note>7th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
