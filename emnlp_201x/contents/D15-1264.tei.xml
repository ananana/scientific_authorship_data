<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Closing the Gap: Domain Adaptation from Explicit to Implicit Discourse Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Gongbo</forename><surname>Yangfeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Closing the Gap: Domain Adaptation from Explicit to Implicit Discourse Relations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many discourse relations are explicitly marked with discourse connectives, and these examples could potentially serve as a plentiful source of training data for recognizing implicit discourse relations. However , there are important linguistic differences between explicit and implicit discourse relations, which limit the accuracy of such an approach. We account for these differences by applying techniques from domain adaptation, treating implicitly and explicitly-marked discourse relations as separate domains. The distribution of surface features varies across these two domains, so we apply a marginalized denoising autoencoder to induce a dense, domain-general representation. The label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse relations reveal the structural orga- nization of text, potentially supporting applica- tions such as summarization ( <ref type="bibr" target="#b13">Louis et al., 2010;</ref><ref type="bibr" target="#b25">Yoshida et al., 2014</ref>), sentiment analysis <ref type="bibr" target="#b23">(Somasundaran et al., 2009)</ref>, and coherence evaluation ( <ref type="bibr" target="#b12">Lin et al., 2011</ref>). While some relations are sig- naled explicitly with connectives such as how- ever ( <ref type="bibr" target="#b18">Pitler et al., 2008</ref>), many more are im- plicit. Expert-annotated datasets of implicit dis- course relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit con- nectives ( <ref type="bibr" target="#b16">Marcu and Echihabi, 2003)</ref>.</p><p>However, <ref type="bibr" target="#b24">Sporleder and Lascarides (2008)</ref> show that models trained on explicitly marked ex- amples generalize poorly to implicit relation iden- tification. They argued that explicit and implicit examples may be linguistically dissimilar, as writ- ers tend to avoid discourse connectives if the dis- course relation could be inferred from context <ref type="bibr" target="#b6">(Grice, 1975)</ref>. Similar observations are made by <ref type="bibr" target="#b22">Rutherford and Xue (2015)</ref>, who attempt to add automatically-labeled instances to improve super- vised classification of implicit discourse relations.</p><p>In this paper, we approach this problem from the perspective of domain adaptation. Specifically, we argue that the reason that automatically-labeled examples generalize poorly is due to domain mis- match from the explicit relations (source domain) to the implicit relations (target domain). We pro- pose to close the gap by using two simple meth- ods from domain adaptation: (1) feature represen- tation learning: mapping the source domain and target domain to a shared latent feature space; (2) resampling: modifying the relation distribution in the explicit relations to match the distribution over implicit relations. Our results on the Penn Dis- course Treebank ( <ref type="bibr" target="#b21">Prasad et al., 2008)</ref> show that these two methods improve the performance on unsupervised discourse relation identification by more than 8.4% on average F 1 score across all relation types, an 82% reduction on the transfer loss incurred by training on explicitly-marked dis- course relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Marcu and Echihabi (2003) train a classifier for implicit intra-sentence discourse relations from explicitly-marked examples in the rhetorical struc- ture theory (RST) treebank, where the relations are automatically labeled by their discourse con- nectives: for example, labeling the relation as CONTRAST if the connective is but. However, <ref type="bibr" target="#b24">Sporleder and Lascarides (2008)</ref> argue that explic- itly marked relations are too different from im- plicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory (SDRT) relations.</p><p>More recent work has focused on the Penn Dis- course Treebank (PDTB), using explicitly-marked relations to supplement, rather than replace, a la- beled corpus of implicit relations. For example, <ref type="bibr" target="#b1">Biran and McKeown (2013)</ref> collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identifica- tion. <ref type="bibr" target="#b10">Lan et al. (2013)</ref> present a multi-task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit re- lation identification. <ref type="bibr" target="#b22">Rutherford and Xue (2015)</ref> explore several selection heuristics for adding automatically-labeled examples from Gigaword to their system for implicit relation detection, obtain- ing a 2% improvement in Macro-F 1 . Our work differs from these previous efforts in that we fo- cus exclusively on training from automatically- labeled explicit instances, rather than supplement- ing a training set of manually-labeled implicit ex- amples.</p><p>Learning good feature representations <ref type="bibr">(BenDavid et al., 2007)</ref> and reducing mismatched la- bel distributions <ref type="bibr" target="#b8">(Joshi et al., 2012</ref>) are two main ways to make a domain adaptation task successful. Structural correspondence learning is an early ex- ample of representation learning for domain adap- tation ( <ref type="bibr" target="#b2">Blitzer et al., 2006</ref>); we build on the more computationally tractable approach of marginal- ized denoising autoencoders <ref type="bibr" target="#b3">(Chen et al., 2012</ref>). Instance weighting is an approach for correct- ing label distribution mismatch <ref type="bibr" target="#b7">(Jiang and Zhai, 2007)</ref>; we apply a simpler approach of resampling the source domain according to an estimate of the target domain label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Domain Adaptation for Implicit Relation Identification</head><p>We employ two domain adaptation techniques: learning feature representations, and resampling to match the target label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning feature representation: Marginalized denoising autoencoders</head><p>The goal of feature representation learning is to obtain dense features that capture feature correla- tions between the source and target domains. De- noising autoencoders <ref type="bibr" target="#b5">(Glorot et al., 2011</ref>) do this by first "corrupting" the original data, x 1 , . . . , x n intõ x 1 , . . . , ˜ x n , either by adding Gaussian noise (in the case of real-valued data) or by randomly ze- roing out features (in the case of binary data). We can then learn a function to reconstruct the origi- nal data, thereby capturing feature correlations and improving resilience to domain shift.</p><p>Chen et al. <ref type="formula">(2012)</ref> propose a particularly sim- ple and elegant form of denoising autencoder, by marginalizing over the noising process. Their single-layer marginalized denoising autoencoder (mDA) solves the following problem:</p><formula xml:id="formula_0">min W E ˜ x i |x i [x i − W˜xW˜x i 2 ] (1)</formula><p>where the parameter W ∈ R d×d is a projection matrix. After learning the projection matrix, we use tanh(Wx) as the representation for our rela- tion identification task.</p><p>Usually, x i ∈ R d is a sparse vector with more than 10 5 dimensions. Solving the optimization problem defined in equation 1 will produce a d × d dense matrix W, and is prohibitively ex- pensive. We employ the trick proposed by <ref type="bibr" target="#b2">Blitzer et al. (2006)</ref> to select κ pivot features to be re- constructed. We then split all features into non- overlapping subsets of size ≤ K. Then, a set of projection matrices are learned, so as to transform each feature subset to the pivot feature set. The final projection matrix W is the stack of all pro- jection matrices learned from the feature subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Handling mismatched label distributions: Resampling with minimal supervision</head><p>There is a notable mismatch between the relation distributions for implicit and explicitly-marked discourse relations in the Penn Discourse Tree- bank: as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the <ref type="bibr">EXPANSION</ref>   way to obtain performance gains in domain adap- tation ( <ref type="bibr" target="#b8">Joshi et al., 2012)</ref>. Specifically, our goal is to modify the relation distribution in the source domain (explicitly-marked relations) and make it as similar as possible to the target domain (im- plicit relations). Given the label distribution from the target domain, we resample training examples from the source domain with replacement, in or- der to match the label distribution in the target do- main. As this requires the label distribution from the target domain, it is no longer purely unsuper- vised domain adaptation; instead, we call it resam- pling with minimal supervision.</p><p>It may also be desirable to ensure that the source and target training instances are similar in terms of their observed features; this is the idea behind the instance weighting approach to domain adap- tation ( <ref type="bibr" target="#b7">Jiang and Zhai, 2007)</ref>. Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least τ with at least one target domain instance (Ruther- ford and Xue, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments test the utility of the two do- main adaptation methods, using the Penn Dis- course Treebank ( <ref type="bibr" target="#b21">Prasad et al., 2008</ref>) and some extra-training data collected from a external re- source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Datasets The test examples are implicit rela- tion instances from section 21-22 in the PDTB. For the domain adaptation setting, the training set consists of the explicitly-marked examples ex- tracted from sections 02-20 and 23-24, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from <ref type="table">Table 2</ref> in <ref type="bibr" target="#b20">(Prasad et al., 2007)</ref>, where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classi- fier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work ( <ref type="bibr" target="#b19">Pitler et al., 2009;</ref><ref type="bibr" target="#b17">Park and Cardie, 2012;</ref><ref type="bibr" target="#b1">Biran and McKeown, 2013)</ref>, we consider the first- level discourse relations in the PDTB -Temporal (TEMP.), Comparison (COMP.), Expansion (EXP.) and Contingency (CONT.). We train binary classi- fiers and report F 1 score on each binary classifica- tion task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work.</p><p>The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Treebank. To test this idea, we collected 1,000 news articles from CNN.com as extra training data. Explicitly-marked dis- course relations from this data are automatically extracted by matching the PDTB discourse con- nectives ( <ref type="bibr" target="#b20">Prasad et al., 2007)</ref>. For this data, we also need to extract the arguments of the iden- tified connectives: for every identified connec- tive, the sentence following this connective is la- beled as Arg2 and the preceding sentence is la- beled as Arg1, as suggested by <ref type="bibr" target="#b1">Biran and McKeown (2013)</ref>. In a pilot study we found that larger amounts of additional training data yielded no fur- ther improvements, which is consistent with the recent results of <ref type="bibr" target="#b22">Rutherford and Xue (2015)</ref>.</p><p>Model selection We use a linear support vec- tor machine <ref type="bibr" target="#b4">(Fan et al., 2008)</ref> as the classifica- tion model. Our model includes five tunable pa- rameters: the number of pivot features κ, the size of the feature subset K, the noise level for the denoising autoencoder q, the cosine similar- ity threshold for resampling τ , and the penalty pa- rameter C for the SVM classifier. We consider κ ∈ {1000, 2000, 3000} for pivot features and C ∈ {0.001, 0.01, 0.1, 1.0, 10.0} for penalty pa- rameters, q ∈ {0.90, 0.95, 0.99} for noise levels. To reduce the free parameters, we set K = 5κ and simply fix the cosine similarity threshold τ =   <ref type="bibr" target="#b17">Park and Cardie, 2012</ref>). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relations</head><p>The FULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that oc- cur fewer than ten times. The PIVOT feature set includes κ high-frequency features from the FULL feature set. To focus on testing the domain adaptation techniques, we use the same FULL and PIVOT set for all four relations, and leave fea- ture set optimization for each relation as a future work <ref type="bibr" target="#b17">(Park and Cardie, 2012)</ref>. To obtain the up- per bound, we employ the same feature categories and frequency threshold to extract features from the in-domain data, hand-annotated implicit dis- course relations. To include the representations from the marginalized denoising autoencoder for relation identification, we concatenate them with the original surface feature representations of the same examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>In experiments, we start with surface feature rep- resentations as baselines, then incorporate the two domain adaptation techniques incrementally. As shown in line 2 of <ref type="table">Table 1</ref>, the performance is poor if directly applying a model trained on the ex- plicit examples with the FULL feature set, which is consistent with the observations of <ref type="bibr" target="#b24">Sporleder and Lascarides (2008)</ref>: there is a 10.28% abso- lute reduction on average F 1 score from the up- per bound obtained with in-domain supervision (line 1). With mDA, the overall performance in- creases by 0.86% (line 4); resampling gives a fur- ther 4.16% improvement mainly because of the performance gain on the EXP. relation (line 5). The resampling method itself (line 3) also gives a better overall performance then mDA (line 4). However, the F 1 scores on the TEMP. and CONT. are even worse than the baseline (line 2).</p><p>Surface representations with the FULL feature set were found to cause serious overfitting in the experiments. To deal with this problem, we pro- pose to use only κ pivot features, which gives a stronger baseline of the cross-domain relation identification, as shown in line 6. Then, by in- corporating resampling and feature representation learning individually, the average F 1 increases from 32.74% to 35.44% (line 7) and 36.69% (line 8) respectively. The combination of these two domain adaptation techniques boosts the average F 1 further to 38.62% (line 9). The additional CNN training data further improves performance to 39.46% (line 10). This represents an 8.42% im- provement of average F 1 from the original result (line 2), for more than 80% reduction on the trans- fer loss incurred by training on explicit discourse relations.</p><p>An additional experiment is to use automatic ar- gument extraction in both the PDTB and the CNN data, which would correspond to more truly un- supervised domain adaptation. (Recall that in the CNN data, we used adjacent sentences as argu- ment spans, while in the PDTB data, we use ex- pert annotations.) When using adjacent sentences as argument spans in both datasets, the average F 1 is 38.52% for the combination of representa- tion learning and resampling. Compared to line 10, this is a 0.94% performance drop, indicating the importance of argument identification in the PDTB data. In future work we may consider bet- ter heuristics for argument extraction, such as ob- taining automatically-labeled examples only from those connectors for whom the arguments usu- ally are the adjacent sentences; for example, the connector nonetheless usually connects adjacent spans (e.g., Bob was hungry. Nonetheless he gave Tina the burger.), while the connector even though may connect two spans that follow the connector in the same sentence (e.g., Even though Bob was hungry, he gave Tina the burger.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented two methods -feature rep- resentation learning and resampling -from do- main adaptation to close the gap of using explicit examples for unsupervised implicit discourse re- lation identification. Experiments on the PDTB show the combination of these two methods elimi- nates more than 80% of the transfer loss caused by training on explicit examples, increasing average F 1 from 31% to 39.5%, against a supervised up- per bound of 41.3%. Future work will explore the combination of this approach with more sophis- ticated techniques for instance selection <ref type="bibr" target="#b22">(Rutherford and Xue, 2015)</ref> and feature selection <ref type="bibr" target="#b17">(Park and Cardie, 2012;</ref><ref type="bibr" target="#b1">Biran and McKeown, 2013)</ref>, while also tackling the more difficult problems of multi-class relation classification and fine-grained level-2 discourse relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The relation distributions of training examples from the source domain (explicitly-marked relations) and target domain (implicit relations) in the PDTB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Features</head><label></label><figDesc>All features are motivated by prior work on implicit discourse relation classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both argu- ments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two argu- ments (Lin et al., 2009); (3) Other features, includ- ing modality, Inquirer tags, Levin verb classes, and argument polarity (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Surface Features +Rep. Learning +Resampling TEMP. COMP. EXP. CONT. Average F 1</head><label></label><figDesc></figDesc><table>Implicit → Implicit 
1. FULL 
24.15 
28.87 
68.84 43.45 
41.32 
Explicit [PDTB] → Implicit 
2. FULL 
No 
No 
17.13 
20.54 
50.55 36.14 
31.04 
3. FULL 
No 
Yes 
15.38 
23.88 
62.04 35.29 
34.14 
4. FULL 
Yes 
No 
17.53 
22.77 
50.85 36.43 
31.90 
5. FULL 
Yes 
Yes 
17.05 
22.00 
63.51 38.23 
35.20 

6. PIVOT 
No 
No 
17.33 
23.89 
53.53 36.22 
32.74 
7. PIVOT 
No 
Yes 
17.73 
25.39 
62.65 36.02 
35.44 
8. PIVOT 
Yes 
No 
18.66 
25.86 
63.37 38.87 
36.69 
9. PIVOT 
Yes 
Yes 
19.26 
25.74 
68.08 41.39 
38.62 
Explicit [PDTB + CNN] → Implicit 
10. PIVOT 
Yes 
Yes 
20.35 
26.32 
68.92 42.25 
39.46 

Table 1: Performance of cross-domain learning for implicit discourse relation identification. 

0.85; pilot studies found that results are not sensi-
tive to the value of τ across a range of values. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aggregated word pair features for implicit discourse relation disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Sophia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Minmin Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Killian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Logic and Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Paul</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Acts</title>
		<editor>P. Cole and J. L. Morgan</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-domain learning: when do domains matter?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">P</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2223</title>
		<meeting>the 2223</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<biblScope unit="page" from="1302" to="1312" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Sophia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically Evaluating Text Coherence Using Discourse Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discourse indicators for content selection in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An unsupervised approach to recognizing discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="368" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving Implicit Discourse Relation Recognition Through Feature Set Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Easily identifiable discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mridhula</forename><surname>Raghupathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hena</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic Sense Prediction for Implicit Discourse Relations in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Suntec, Singapore</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Penn Discourse Treebank 2.0 annotation manual. The PDTB Research Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Penn Discourse Treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving the Inference of Implicit Discourse Relations via Classifying Explicit Discourse Connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="799" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using automatically labelled examples to classify rhetorical relations: An assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="416" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dependency-based Discourse Parser for Single-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
