<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chinese Poetry Generation with Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
							<email>x.zhang@ed.ac.uk, mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chinese Poetry Generation with Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="670" to="680"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form. Our generator jointly performs content selection (&quot;what to say&quot;) and surface realization (&quot;how to say&quot;) by learning representations of individual characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other. Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams. Experimental results show that our model outperforms competitive Chi-nese poetry generation systems using both automatic and manual evaluation methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classical poems are a significant part of China's cultural heritage. Their popularity manifests itself in many aspects of everyday life, e.g., as a means of expressing personal emotion, political views, or communicating messages at festive occasions as well as funerals. Amongst the many differ- ent types of classical Chinese poetry, quatrain and regulated verse are perhaps the best-known ones. Both types of poem must meet a set of structural, phonological, and semantic requirements, render- ing their composition a formidable task left to the very best scholars.</p><p>An example of a quatrain is shown in <ref type="table">Table 1</ref>. Quatrains have four lines, each five or seven char- acters long. Characters in turn follow specific phonological patterns, within each line and across lines. For instance, the final characters in the sec- ond, fourth and (optionally) first line must rhyme, Missing You (* Z P P Z) Red berries born in the warm southland.</p><p>(P P Z Z P) How many branches flush in the spring?</p><p>(* P P Z Z) Take home an armful, for my sake,</p><p>(* Z Z P P) As a symbol of our love. <ref type="table">Table 1</ref>: An example of a 5-char quatrain ex- hibiting one of the most popular tonal patterns. The tone of each character is shown at the end of each line (within parentheses); P and Z are short- hands for Ping and Ze tones, respectively; * indi- cates that the tone is not fixed and can be either. Rhyming characters are shown in boldface. whereas there are no rhyming constraints for the third line. Moreover, poems must follow a pre- scribed tonal pattern. In traditional Chinese, ev- ery character has one tone, Ping (level tone) or Ze (downward tone). The poem in <ref type="table">Table 1</ref> exempli- fies one of the most popular tonal patterns <ref type="bibr" target="#b25">(Wang, 2002)</ref>. Besides adhering to the above formal crite- ria, poems must exhibit concise and accurate use of language, engage the reader/hearer, stimulate their imagination, and bring out their feelings.</p><p>In this paper we are concerned with generat- ing traditional Chinese poems automatically. Al- though computers are no substitute for poetic cre- ativity, they can analyze very large online text repositories of poems, extract statistical patterns, maintain them in memory and use them to gen- erate many possible variants. Furthermore, while amateur poets may struggle to remember and ap- ply formal tonal and structural constraints, it is rel- atively straightforward for the machine to check whether a candidate poem conforms to these re- quirements. Poetry generation has received a fair amount of attention over the past years (see the discussion in Section 2), with dozens of computa- tional systems written to produce poems of vary- ing sophistication. Beyond the long-term goal of building an autonomous intelligent system capa- ble of creating meaningful poems, there are po- tential short-term applications for computer gen- erated poetry in the ever growing industry of elec- tronic entertainment and interactive fiction as well as in education. An assistive environment for poem composition could allow teachers and stu- dents to create poems subject to their require- ments, and enhance their writing experience.</p><p>We propose a model for Chinese poem genera- tion based on recurrent neural networks. Our gen- erator jointly performs content selection ("what to say") and surface realization ("how to say"). Given a large collection of poems, we learn repre- sentations of individual characters, and their com- binations into one or more lines as well as how these mutually reinforce and constrain each other. Our model generates lines in a poem probabilis- tically: it estimates the probability of the current line given the probability of all previously gener- ated lines. We use a recurrent neural network to learn the representations of the lines generated so far which in turn serve as input to a recurrent lan- guage model <ref type="bibr" target="#b14">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b16">Mikolov et al., 2011b;</ref><ref type="bibr" target="#b15">Mikolov et al., 2011a</ref>) which generates the current line. In contrast to previous approaches ( <ref type="bibr" target="#b5">Greene et al., 2010;</ref><ref type="bibr" target="#b8">Jiang and Zhou, 2008)</ref>, our generator makes no Markov assumptions about the dependencies of the words within a line and across lines.</p><p>We evaluate our approach on the task of qua- train generation (see <ref type="table">Table 1</ref> for a human-written example). Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and man- ual evaluation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automated poetry generation has been a popular research topic over the past decades (see <ref type="bibr" target="#b3">Colton et al. (2012)</ref> and the references therein). Most ap- proaches employ templates to construct poems ac- cording to a set of constraints (e.g., rhyme, me- ter, stress, word frequency) in combination with corpus-based and lexicographic resources. For example, the Haiku poem generator presented in <ref type="bibr" target="#b26">Wu et al. (2009)</ref> and <ref type="bibr" target="#b24">Tosa et al. (2008)</ref> produces poems by expanding user queries with rules ex- tracted from a corpus and additional lexical re- sources. <ref type="bibr" target="#b19">Netzer et al. (2009)</ref> generate Haiku with Word Association Norms, <ref type="bibr" target="#b0">Agirrezabal et al. (2013)</ref> compose Basque poems using patterns based on parts of speech and WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, and Oliveira (2012) presents a generation algorithm for Portuguese which leverages seman- tic and grammar templates.</p><p>A second line of research uses genetic algo- rithms for poem generation <ref type="bibr" target="#b12">(Manurung, 2003;</ref><ref type="bibr" target="#b11">Manurung et al., 2012;</ref><ref type="bibr" target="#b28">Zhou et al., 2010)</ref>. <ref type="bibr" target="#b11">Manurung et al. (2012)</ref> argue that at a basic level all (machine-generated) poems must satisfy the constraints of grammaticality (i.e., a poem must syntactically well-formed), meaningfulness (i.e., a poem must convey a message that is meaningful under some interpretation) and poeticness (i.e., a poem must exhibit features that distinguishes it from non-poetic text, e.g., metre). Their model generates several candidate poems and then uses stochastic search to find those which are grammat- ical, meaningful, and poetic.</p><p>A third line of research draws inspiration from statistical machine translation (SMT) and re- lated text-generation applications such as sum- marization. <ref type="bibr" target="#b5">Greene et al. (2010)</ref> infer meters (stressed/unstressed syllable sequences) from a corpus of poetic texts which they subsequently use for generation together with a cascade of weighted finite-state transducers interpolated with IBM Model 1. <ref type="bibr" target="#b8">Jiang and Zhou (2008)</ref> generate Chinese couplets (two line poems) using a phrase- based SMT approach which translates the first line to the second line. <ref type="bibr" target="#b6">He et al. (2012)</ref> extend this al- gorithm to generate four-line quatrains by sequen- tially translating the current line from the previous one. <ref type="bibr">Yan et al. (2013)</ref> generate Chinese quatrains based on a query-focused summarization frame- work. Their system takes a few keywords as input and retrieves the most relevant poems from a cor- pus collection. The retrieved poems are segmented into their constituent terms which are then grouped into clusters. Poems are generated by iteratively selecting terms from clusters subject to phonolog- ical, structural, and coherence constraints.</p><p>Our approach departs from previous work in two important respects. Firstly, we model the tasks of surface realization and content selection jointly   using recurrent neural networks. Structural, se- mantic, and coherence constraints are captured naturally in our framework, through learning the representations of individual characters and their combinations. Secondly, generation proceeds by taking into account multi-sentential context rather than the immediately preceding sentence. Our work joins others in using continuous representa- tions to express the meaning of words and phrases <ref type="bibr" target="#b23">(Socher et al., 2012;</ref><ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) and how these may be combined in a language mod- eling context <ref type="bibr" target="#b13">(Mikolov and Zweig, 2012</ref>). More recently, continuous translation models based on recurrent neural networks have been proposed as a means to map a sentence from the source lan- guage to sentences in the target language <ref type="bibr" target="#b1">(Auli et al., 2013;</ref><ref type="bibr" target="#b9">Kalchbrenner and Blunsom, 2013</ref>). These models are evaluated on the task of rescor- ing n-best lists of translations. We use neural net- works more directly to perform the actual poem generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Poem Generator</head><p>As common in previous work <ref type="bibr">(Yan et al., 2013;</ref><ref type="bibr" target="#b6">He et al., 2012</ref>) we assume that our generator op- erates in an interactive context. Specifically, the user supplies keywords (e.g., spring, lute, drunk ) highlighting the main concepts around which the poem will revolve. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, our generator expands these keywords into a set of re- lated phrases. We assume the keywords are re- stricted to those attested in the ShiXueHanYing po- etic phrase taxonomy ( <ref type="bibr" target="#b6">He et al., 2012;</ref><ref type="bibr">Yan et al., 2013</ref>). The latter contains 1,016 manual clusters of phrases (Liu, 1735); each cluster is labeled with a keyword id describing general poem-worthy top- ics. The generator creates the first line of the poem based on these keywords. Subsequent lines are generated based on all previously generated lines, subject to phonological (e.g., admissible tonal pat- terns) and structural constraints (e.g., whether the quatrain is five or seven characters long).</p><p>To create the first line, we select all phrases corresponding to the user's keywords and gener- ate all possible combinations satisfying the tonal pattern constraints. We use a language model to rank the generated candidates and select the best- ranked one as the first line in the poem. In im- plementation, we employ a character-based recur- rent neural network language model ( <ref type="bibr" target="#b14">Mikolov et al., 2010</ref>) interpolated with a Kneser-Ney trigram and find the n-best candidates with a stack de- coder (see Section 3.5 for details). We then gen- erate the second line based on the first one, the third line based on the first two lines, and so on. Our generation model computes the probability of line S i+1 = w 1 , w 2 , . . . , w m , given all previously generated lines S 1:i (i ≥ 1) as:</p><formula xml:id="formula_0">P(S i+1 |S 1:i ) = m−1 ∏ j=1 P(w j+1 |w 1: j , S 1:i )<label>(1)</label></formula><p>Equation <ref type="formula" target="#formula_0">(1)</ref>, decomposes P(S i+1 |S 1:i ) as the prod- uct of the probability of each character w j in the current line given all previously generated characters w 1: j−1 and lines S 1:i . This means that P(S i+1 |S 1:i ) is sensitive to previously gener- ated content and currently generated characters.</p><p>The estimation of the term P(w j+1 |w 1: j , S 1:i ) lies at the heart of our model. We learn repre- sentations for S 1:i , the context generated so far, using a recurrent neural network whose output serves as input to a second recurrent neural net- work used to estimate P(w j+1 |w 1: j , S 1:i ). <ref type="figure">Figure 2</ref> illustrates the generation process for the ( j + 1)th character w j+1 in the (i + 1)th line S i+1 . First, lines S 1:i are converted into vectors v 1:i with a convolutional sentence model (CSM; described in Section 3.1). Next, a recurrent context model (RCM; see Section 3.2) takes v 1:i as input and outputs u j i , the representation needed for gener- ating w j+1 ∈ S i+1 . Finally, u 1 i , u 2 i , . . . , u j i and the first j characters w 1: j in line S i+1 serve as input to a recurrent generation model (RGM) which esti- mates P(w j+1 = k|w 1: j , S 1:i ) with k ∈ V , the prob- ability distribution of the ( j + 1)th character over all words in the vocabulary V . More formally, to estimate P(w j+1 |w 1: j , S 1:i ) in Equation <ref type="formula" target="#formula_0">(1)</ref>, we ap- ply the following procedure:</p><formula xml:id="formula_1">v i = CSM(S i ) (2a) u j i = RCM(v 1:i , j)<label>(2b)</label></formula><p>P(w j+1 |w 1: j , S 1:i ) = RGM(w 1: j+1 , u</p><p>1: j i ) (2c) We obtain the probability of the (i + 1)th sen- tence P(S i+1 |S 1:i ), by running the RGM in (2c) above m − 1 times (see also Equation <ref type="formula" target="#formula_0">(1)</ref>). In the following, we describe how the different compo- nents of our model are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolutional Sentence Model (CSM)</head><p>The CSM converts a poem line into a vector. In principle, any model that produces vector-based representations of phrases or sentences could be used ( <ref type="bibr" target="#b18">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b23">Socher et al., 2012</ref>). We opted for the convolutional sentence model proposed in <ref type="bibr" target="#b9">Kalchbrenner and Blunsom (2013)</ref> as it is n-gram based and does not make use of any parsing, POS-tagging or segmentation tools which are not available for Chinese poems. Their model computes a continuous representation for a sentence by sequentially merging neighbor- ing vectors (see <ref type="figure" target="#fig_2">Figure 3)</ref>.</p><p>Let V denote the character vocabulary in our corpus; L ∈ R q×|V | denotes a character embed- ding matrix whose columns correspond to char- acter vectors (q represents the hidden unit size). Such vectors can be initialized randomly or ob- tained via a training procedure ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref>. Let w denote a character with index k; e(w) ∈ R |V |×1 is a vector with zero in all positions except e(w) k = 1; T l ∈ R q×N l is the sentence rep- resentation in the lth layer, where N l is the num- ber of columns in the lth layer (N l = 1 in the</p><formula xml:id="formula_2">v i u j i h i h i−1 u k i (k = j) RCM 1-of-N encoding of w j =(0,. . . ,1,. . . ,0)</formula><p>r j r j−1 P(w j+1 |w 1: j , S 1:i ) RGM <ref type="figure">Figure 2</ref>: Generation of the ( j + 1)th charac- ter w j+1 in the (i + 1)th line S i+1 . The recur- rent context model (RCM) takes i lines as in- put (represented by vectors v 1 , . . . , v i ) and cre- ates context vectors for the recurrent generation model (RGM). The RGM estimates the probabil- ity P(w j+1 |w 1: j , S 1:i ).</p><p>top layer); C l,n ∈ R q×n is an array of weight ma- trices which compress neighboring n columns in the lth layer to one column in the (l + 1)th layer. Given a sentence S = w 1 , w 2 , . . . , w m , the first layer is represented as:</p><formula xml:id="formula_3">T 1 = [L · e(w 1 ), L · e(w 2 ), . . . , L · e(w m )] N 1 = m<label>(3)</label></formula><p>The (l + 1)th layer is then computed as follows:</p><formula xml:id="formula_4">T l+1 :, j = σ( n ∑ i=1 T l :, j+i−1 C l,n :,i ) N l+1 = N l − n + 1 1 ≤ j ≤ N l+1<label>(4)</label></formula><p>where T l is the representation of the previous layer l, C l,n a weight matrix, element-wise vec- tor product, and σ a non-linear function. We com- press two neighboring vectors in the first two lay- ers and three neighboring vectors in the remaining layers. Specifically, for quatrains with seven char- acters, we use C 1,2 , C 2,2 , C 3,3 , C 4,3 to merge vec- tors in each layer (see <ref type="figure" target="#fig_2">Figure 3)</ref>; and for quatrains with five characters we use C 1,2 , C 2,2 , C 3,3 .</p><p>Far off I watch the waterfall plunge to the long river. </p><formula xml:id="formula_5">C 1,2 C 2,2 C 3,3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Context Model (RCM)</head><p>The RCM takes as input the vectors representing the i lines generated so far and reduces them to a single context vector which is then used to gener- ate the next character (see <ref type="figure">Figure 2)</ref>. We compress the i previous lines to one vector (the hidden layer) and then decode the compressed vector to different character positions in the current line. The output layer consists thus of several vectors (one for each position) connected together. This way, different aspects of the context modulate the generation of different characters. Let v 1 , . . . , v i (v i ∈ R q×1 ) denote the vectors of the previous i lines; h i ∈ R q×1 is their compressed representation (hidden layer) which is obtained with matrix M ∈ R q×2q ; matrix U j decodes h i to u j i ∈ R q×1 in the (i + 1)th line. The computation of the RCM proceeds as follows:</p><formula xml:id="formula_6">h 0 = 0 0 0 h i = σ(M · v i h i−1 ) u j i = σ(U j · h i ) 1 ≤ j ≤ m − 1<label>(5)</label></formula><p>where σ is a non-linear function such as sigmoid and m the line length. Advantageously, lines in classical Chinese poems have a fixed length of five or seven characters. Therefore, the output layer of the recurrent context model only needs two weight matrices (one for each length) and the number of parameters still remains tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recurrent Generation Model (RGM)</head><p>As shown in <ref type="figure">Figure 2</ref>, the RGM estimates the probability distribution of the next character (over the entire vocabulary) by taking into account the context vector provided by the RCM and the 1-of-N encoding of the previous character. The RGM is essentially a recurrent neural network lan- guage model ( <ref type="bibr" target="#b14">Mikolov et al., 2010</ref>) with an aux- iliary input layer, i.e., the context vector from the RCM. Similar strategies for encoding addi- tional information have been adopted in related language modeling and machine translation work ( <ref type="bibr" target="#b13">Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b9">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b1">Auli et al., 2013)</ref>. Let S i+1 = w 1 , w 2 , . . . , w m denote the line to be generated.</p><p>The RGM must esti- mate P(w j+1 |w 1: j , S 1:i ), however, since the first i lines have been encoded in the context vector u j i , we compute P(w j+1 |w 1: j , u j i ) instead. Therefore, the probability P(S i+1 |S 1:i ) becomes:</p><formula xml:id="formula_7">P(S i+1 |S 1:i ) = m−1 ∏ j=1 P(w j+1 |w 1: j , u j i )<label>(6)</label></formula><p>Let |V | denote the size of the character vocabu- lary. The RGM is specified by a number of ma- trices. Matrix H ∈ R q×q (where q represents the hidden unit size) transforms the context vector to a hidden representation; matrix X ∈ R q×|V | trans- forms a character to a hidden representation, ma- trix R ∈ R q×q implements the recurrent transfor- mation and matrix Y ∈ R |V |×q decodes the hidden representation to weights for all words in the vo- cabulary. Let w denote a character with index k in V ; e(w) ∈ R |V |×1 represents a vector with zero in all positions except e(w) k = 1, r j is the hidden layer of the RGM at step j, and y j+1 the output of the RGM, again at step j. The RGM proceeds as follows:</p><formula xml:id="formula_8">r 0 = 0 0 0 (7a) r j = σ(R · r j−1 + X · e(w j ) + H · u j i )<label>(7b)</label></formula><formula xml:id="formula_9">y j+1 = Y · r j (7c)</formula><p>where σ is a nonlinear function (e.g., sigmoid).</p><p>The probability of the ( j + 1)th word given the previous j words and the previous i lines is esti- mated by a softmax function:</p><formula xml:id="formula_10">P(w j+1 = k|w 1: j , u j i ) = exp(y j+1,k ) ∑ |V | k=1 exp(y j+1,k )<label>(8)</label></formula><p>We obtain P(S i+1 |S 1:i ) by multiplying all the terms in the right hand-side of Equation (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The objective for training is the cross entropy er- rors of the predicted character distribution and the actual character distribution in our corpus. An l 2 regularization term is also added to the objec- tive. The model is trained with back propagation through time <ref type="bibr" target="#b22">(Rumelhart et al., 1988</ref>) with sen- tence length being the time step. The objective is minimized by stochastic gradient descent. Dur- ing training, the cross entropy error in the output layer of the RGM is back-propagated to its hid- den and input layers, then to the RCM and finally to the CSM. The same number of hidden units (q = 200) is used throughout (i.e., in the RGM, RCM, and CSM). In our experiments all param- eters were initialized randomly, with the excep- tion of the word embedding matrix in the CSM which was initialized with word2vec embeddings ( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) obtained from our poem corpus (see Section 4 for details on the data we used).</p><p>To speed up training, we employed word- classing ( <ref type="bibr" target="#b16">Mikolov et al., 2011b</ref>). To compute the probability of a character, we estimate the proba- bility of its class and then multiply it by the proba- bility of the character conditioned on the class. In our experiments we used 82 (square root of |V |) classes which we obtained by applying hierarchi- cal clustering on character embeddings. This strat- egy outperformed better known frequency-based classing methods (Zweig and Makarychev, 2013) on our task.</p><p>Our poem generator models content selection and lexical choice and their interaction, but does not have a strong notion of local coherence, as manifested in poetically felicitous line-to-line transitions. In contrast, machine translation mod- els (Jiang and Zhou, 2008) have been particu- larly successful at generating adjacent lines (cou- plets). To enhance coherence, we thus interpolate our model with two machine translation features (i.e., inverted phrase translation model feature and inverted lexical weight feature). Also note, that in our model surface generation depends on the last observed character and the state of the hidden layer before this observation. This way, there is no explicitly defined context, and history is captured implicitly by the recurrent nature of the model. This can be problematic for our texts which must obey certain stylistic conventions and sound po- etic. In default of a better way of incorporating poeticness into our model, we further interpolate it with a language model feature (i.e., a Kneser-Ney trigram model).</p><p>Throughout our experiments, we use the RNNLM toolkit to train the character-based recur- rent neural network language model ( <ref type="bibr" target="#b14">Mikolov et al., 2010)</ref>. Kneser-Ney n-grams were trained with KenLM (Heafield, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decoding</head><p>Our decoder is a stack decoder similar to <ref type="bibr" target="#b10">Koehn et al. (2003)</ref>. In addition, it implements the tonal pattern and rhyming constraints necessary for gen- erating well-formed Chinese quatrains. Once the first line in a poem is generated, its tonal pattern is determined. During decoding, phrases violat- ing this pattern are ignored. As discussed in Sec- tion 1, the final characters of the second and the fourth lines must rhyme. We thus remove during decoding fourth lines whose final characters do not rhyme with the second line. Finally, we use MERT training <ref type="bibr" target="#b20">(Och, 2003)</ref> to learn feature weights for the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Design</head><p>Data We created a corpus of classical Chinese poems by collating several online resources: Tang Poems, Song Poems, Song Ci, Ming Poems, Qing Poems, and Tai Poems. The corpus consists of 284,899 poems in total. 78,859 of these are quatrains and were used for training and evalu- ating our model. 1 <ref type="table">Table 2</ref> shows the different partitions of this dataset (POEMLM) into train- ing (QTRAIN) 2 , validation (QVALID) and testing (QTEST). Half of the poems in QVALID and QTEST are 5-char quatrains and the other half are 7-char quatrains. All poems except QVALID <ref type="table" target="#tab_2">Poems   Lines Characters  QTRAIN  74,809  299,236  2,004,460  QVALID  2,000  8,000  48,000  QTEST  2,050  8,200  49,200  POEMLM 280,849 2,711,034 15,624,283   Table 2</ref>: Dataset partitions of our poem corpus. and QTEST were used for training the character- based language models (see row POEMLM in Ta- ble 2). We also trained word2vec embeddings on POEMLM. In our experiments, we generated qua- trains following the eight most popular tonal pat- terns according to <ref type="bibr" target="#b25">Wang (2002)</ref>.</p><p>Perplexity Evaluation Evaluation of machine- generated poetry is a notoriously difficult task. Our evaluation studies were designed to assess <ref type="bibr">Manurung et al.'s (2012)</ref> criteria of grammatical- ity, meaningfulness, and poeticness. As a san- ity check, we first measured the perplexity of our model with respect to the goldstandard. Intu- itively, a better model should assign larger proba- bility (and therefore lower perplexity) to goldstan- dard poems.</p><p>BLEU-based Evaluation We also used BLEU to evaluate our model's ability to generate the sec- ond, third and fourth line given previous goldstan- dard lines. A problematic aspect of this evalu- ation is the need for human-authored references (for a partially generated poem) which we do not have. We obtain references automatically follow- ing the method proposed in <ref type="bibr" target="#b6">He et al. (2012)</ref>. The main idea is that if two lines share a similar topic, the lines following them can be each other's ref- erences. Let A and B denote two adjacent lines in a poem, with B following A. Similarly, let line B follow line A in another poem. If lines A and A share some keywords in the same cluster in the Shixuehanying taxonomy, then B and B can be used as references for both A and A . We use this algorithm on the Tang Poems section of our corpus to build references for poems in the QVALID and QTEST data sets. Poems in QVALID (with auto- generated references) were used for MERT train- ing and Poems in QTEST (with auto-generated ref- erences) were used for BLEU evaluation.</p><p>Human Evaluation Finally, we also evaluated the generated poems by eliciting human judg- <ref type="table" target="#tab_2">Models  Perplexity  KN5  172  RNNLM  145  RNNPG  93   Table 3</ref>: Perplexities for different models.</p><p>ments. Specifically, we invited 30 experts 3 on Chinese poetry to assess the output of our gen- erator (and comparison systems). These experts were asked to rate the poems using a 1-5 scale on four dimensions: fluency (is the poem grammati- cal and syntactically well-formed?), coherence (is the poem thematically and logically structured?), meaningfulness (does the poem convey a mean- ingful message to the reader?) and poeticness (does the text display the features of a poem?).</p><p>We also asked our participants to evaluate system outputs by ranking the generated poems relative to each other as a way of determining overall poem quality <ref type="bibr" target="#b2">(Callison-Burch et al., 2012)</ref>. Participants rated the output of our model and three comparison systems. These included <ref type="bibr">He et al.'s (2012)</ref> SMT-based model (SMT), <ref type="bibr">Yan et al.'s (2013)</ref> summarization-based system (SUM), and a random baseline which creates poems by ran- domly selecting phrases from the Shixuehanying taxonomy given some keywords as input. We also included human written poems whose content matched the input keywords. All systems were provided with the same keywords (i.e., the same cluster names in the ShiXueHanYing taxonomy). In order to compare all models on equal footing, we randomly sampled 30 sets of keywords (with three keywords in each set) and generated 30 qua- trains for each system according to two lengths, namely 5-char and 7-char. Overall, we obtained ratings for 300 (5×30×2) poems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results of our perplexity evaluation are sum- marized in <ref type="table">Table 3</ref>. We compare our RNN-based poem generator (RNNPG) against Mikolov's (2010) recurrent neural network language model (RNNLM) and a 5-gram language model with Kneser-Ney smoothing (KN5). All models were trained on QTRAIN and tuned on QVALID. The perplexities were computed on QTEST. Note that Models 1 → 2 2 → 3 3 → 4 Average 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char SMT 0.0559 0.0906 0.0410 0.1837 0.0547 0.1804 0.0505 0.1516 RNNPG 0.0561 0.1868 0.0515 0.2102 0.0572 0.1800 0.0549 0.1923 <ref type="table">Table 4</ref>: BLEU-2 scores on 5-char and 7-char quatrains. Given i goldstandard lines, BLEU-2 scores are computed for the next (i + 1)th lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Fluency Coherence Meaning Poeticness Rank 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char 5-char <ref type="bibr">7</ref>  the RNNPG estimates the probability of a poem line given at least one previous line. Therefore, the probability of a quatrain assigned by the RNNPG is the probability of the last three lines. For a fair comparison, RNNLM and KN5 only leverage the last three lines of each poem during training, vali- dation and testing. The results in <ref type="table">Table 3</ref> indicate that the generation ability of the RNNPG is better than KN5 and RNNLM. Note that this perplexity- style evaluation is not possible for models which cannot produce probabilities for gold standard po- ems. For this reason, other related poem gener- ators ( <ref type="bibr">Yan et al., 2013;</ref><ref type="bibr" target="#b6">He et al., 2012)</ref> are not included in the table.</p><p>The results of our evaluation using BLEU-2 are summarized in <ref type="table">Table 4</ref>. Here, we compare our system against the SMT-based poem generation model of <ref type="bibr" target="#b6">He et al. (2012)</ref>. <ref type="bibr">4</ref> Their system is a linear combination of two translation models (one with five features and another one with six). Our model uses three of their features, namely the in- verted phrase translation model feature, the lexical weight feature, and a Kneser-Ney trigram feature. Unfortunately, it is not possible to evaluate <ref type="bibr">Yan et al.'s (2013)</ref> summarization-based system with BLEU, as it creates poems as a whole and there is no obvious way to generate next lines with their algorithm. The BLEU scores in <ref type="table">Table 4</ref> indicate that, given the same context lines, the RNNPG is better than SMT at generating what to say next. BLEU scores should be, however, viewed with some degree of caution. Aside from being an ap- proximation of human judgment <ref type="bibr" target="#b2">(Callison-Burch et al., 2012</ref>), BLEU might be unnecessarily con- servative for poem composition which by its very nature is a creative endeavor.</p><p>The results of our human evaluation study are shown in <ref type="table" target="#tab_2">Table 5</ref>. Each column reports mean rat- ings for a different dimension (e.g., fluency, co- herence). Ratings for 5-char and 7-char quatrains are shown separately. The last column reports rank scores for each system <ref type="bibr" target="#b2">(Callison-Burch et al., 2012)</ref>. In a ranked list of N items (N = 5 here), the score of the ith ranked item is (N−i) (N−1) . The numer- ator indicates how many times a systems won in pairwise comparisons, while the denominator nor- malizes the score.</p><p>With respect to 5-char quatrains, RNNPG is significantly better than Random, SUM and SMT on fluency, coherence, meaningfulness, poeticness and ranking scores (using a t-test). On all dimen- sions, human-authored poems are rated as signif- icantly better than machine-generated ones, with the exception of overall ranking. Here, the dif- ference between RNNPG and Human is not sig- nificant. We obtain similar results with 7-char quatrains. In general, RNNPG seems to perform better on the shorter poems. The mean ratings , , Egrets stood, peeping fishes.</p><p>Budding branches are full of romance. .</p><p>. Water was still, reflecting mountains. Plum blossoms are invisible but adorable. , , The wind went down by nightfall, With the east wind comes Spring. .</p><p>. as the moon came up by the tower.</p><p>Where on earth do I come from? <ref type="table">Table 6</ref>: Example output produced by our model (RNNPG).</p><p>are higher and the improvements over other sys- tems are larger. Also notice, that the score mar- gins between the human-and machine-written po- ems become larger for 7-char quatrains. This in- dicates that the composition of 7-char quatrains is more difficult compared to 5-char quatrains. Ta- ble 6 shows two example poems (5-char and 7- char) produced by our model which received high scores with respect to poeticness. Interestingly, poems generated by SUM 5 are given ratings similar to Random. In fact SUM is slightly worse (although not significantly) than Random on all dimensions, with the exception of coherence. In the human study reported in <ref type="bibr">Yan et al. (2013)</ref>, SUM is slightly better than SMT. There are several reasons for this discrepancy. We used a more balanced experimental design: all systems generated poems from the same keywords which were randomly chosen. We used a larger dataset to train the SMT model compared to <ref type="bibr">Yan et al. (284,899 poems vs 61,960)</ref>. The Random baseline is not a straw-man; it selects phrases from a taxon- omy of meaningful clusters edited by humans and closely related to the input keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we have presented a model for Chi- nese poem generation based on recurrent neural networks. Our model jointly performs content se- lection and surface realization by learning repre- sentations of individual characters and their com- binations within and across poem lines. Previous work on poetry generation has mostly leveraged contextual information of limited length (e.g., one sentence). In contrast, we introduced two recur- rent neural networks (the recurrent context model and recurrent generation model) which naturally capture multi-sentential content. Experimental re- sults show that our model yields high quality po- ems compared to the state of the art. Perhaps un- surprisingly, our human evaluation study revealed that machine-generated poems lag behind human- generated ones. It is worth bearing in mind that poetry composition is a formidable task for hu- mans, let alone machines. And that the poems against which our output was compared have been written by some of the most famous poets in Chi- nese history! Avenues for future work are many and varied. We would like to generate poems across differ- ent languages and genres (e.g., Engish sonnets or Japanese haiku). We would also like to make the model more sensitive to line-to-line transitions and stylistic conventions by changing its training ob- jective to a combination of cross-entropy error and BLEU score. Finally, we hope that some of the work described here might be of relevance to other generation tasks such as summarization, concept- to-text generation, and machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Poem generation with keywords spring, lute, and drunk. The keywords are expanded into phrases using a poetic taxonomy. Phrases are then used to generate the first line. Following lines are generated by taking into account the representations of all previously generated lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convolutional sentence model for 7-char quatrain. The first layer has seven vectors, one for each character. Two neighboring vectors are merged to one vector in the second layer with weight matrix C 1,2. In other layers, either two or three neighboring vectors are merged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Mean ratings elicited by humans on 5-char and 7-char quatrains. Diacritics ** (p &lt; 0.01) 
and * (p &lt; 0.05) indicate our model (RNNPG) is significantly better than all other systems except Human. 
Diacritics ++ (p &lt; 0.01) and + (p &lt; 0.05) indicate Human is significantly better than all other systems. 

</table></figure>

			<note place="foot" n="1"> The data used in our experiments can be downloaded from http://homepages.inf.ed.ac.uk/mlap/index. php?page=resources. 2 Singleton characters in QTRAIN (6,773 in total) were replaced by &lt;R&gt; to reduce data sparsity.</note>

			<note place="foot" n="3"> 27 participants were professional or amateur poets and three were Chinese literature students who had taken at least one class on Chinese poetry composition.</note>

			<note place="foot" n="4"> Our re-implementation of their system delivered very similar scores to He et al. (2012). For example, we obtained an average BLEU-1 of 0.167 for 5-char quatrains and 0.428 for 7-char quatrains compared to their reported scores of 0.141 and 0.380, respectively.</note>

			<note place="foot" n="5"> We made a good-faith effort to re-implement their poem generation system. We are grateful to Rui Yan for his help and technical advice.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Eva Halser for valuable discussions on the machine translation baseline. We are grateful to the 30 Chinese poetry experts for participating in our rating study. Thanks to Gujing Lu, Chu Liu, and Yibo Wang for their help with translating the poems in <ref type="table">Table 6 and Table 1.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">POS-Tag Based Poetry Generation with WordNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manex</forename><surname>Agirrezabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertol</forename><surname>Arrieta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation</title>
		<meeting>the 14th European Workshop on Natural Language Generation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="162" to="166" />
		</imprint>
	</monogr>
	<note>Aitzol Astigarraga, and Mans Hulden</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint Language and Translation Modeling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2012 Workshop on Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Statistical Machine Translation</title>
		<meeting>the 7th Workshop on Statistical Machine Translation<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Full-FACE Poetry Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Colton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Creativity</title>
		<meeting>the International Conference on Computational Creativity<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic Analysis of Rhythmic Poetry with Applications to Generation and Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugba</forename><surname>Bodrumlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating Chinese Classical Poems with Statistical Machine Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 26th AAAI Conference on Artificial Intelligence<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1650" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">KenLM: Faster and Smaller Language Model Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland; United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating Chinese Couplets using a Statistical MT Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical Phrase-based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using Genetic Algorithms to Create Meaningful Poetic Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="64" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An Evolutionary Algorithm Approach to Poetry Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context Dependent Recurrent Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2012 IEEE Workshop on Spoken Language Technology</title>
		<meeting>2012 IEEE Workshop on Spoken Language Technology<address><addrLine>Miami, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Makuhari, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strategies for Training Large Scale Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU 2011</title>
		<meeting>ASRU 2011<address><addrLine>Hilton Waikoloa Village, Big Island, Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extensions of Recurrent Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 2011 IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Composition in Distributional Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaiku: Generating Haiku with Word Associations Norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Sapporo</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PoeTryMe: a Versatile Platform for Poetry Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo Gonçalo</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Creativity, Concept Invention, and General Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning Representations by Backpropagating Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hitch Haiku: An Interactive Supporting System for Composing Haiku Poem How I Learned to Love the Bomb: Defcon and the Ethics of Computer Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoko</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Obara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiko</forename><surname>Minoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Entertainment Computing</title>
		<meeting>the 7th International Conference on Entertainment Computing<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A Summary of Rhyming Constraints of Chinese Poems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<editor>Shi Ci Ge Lv Gai Yao</editor>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Beijing Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">New Hitch Haiku: An Interactive Renku Poem Composition Supporting Tool Applied for Sightseeing Navigation System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoko</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Entertainment Computing</title>
		<meeting>the 8th International Conference on Entertainment Computing<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2013. I, Poet: Automatic Chinese Poetry Composition Through a Generative Summarization Framework Under Constrained Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Joint Conference on Artificial Intelligence</title>
		<meeting>the 23rd International Joint Conference on Artificial Intelligence<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2197" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Genetic Algorithm and its Implementation of Automatic Generation of Chinese SongCi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Le</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software</title>
		<imprint>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speed Regularization and Optimality in Word Classing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Makarychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8237" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
