<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correcting Keyboard Layout Errors and Homoglyphs in Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Barnes</surname></persName>
							<email>debarnes@ebay.com</email>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc</orgName>
								<address>
									<addrLine>2065 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
							<email>mahesh.joshi@ebay.com</email>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc</orgName>
								<address>
									<addrLine>2065 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sawaf</surname></persName>
							<email>hsawaf@ebay.com</email>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc</orgName>
								<address>
									<addrLine>2065 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correcting Keyboard Layout Errors and Homoglyphs in Queries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="621" to="626"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results. We present a machine learning approach to correcting these errors, based largely on character-level n-gram features. We demonstrate superior performance over rule-based methods, as well as a significant reduction in the number of queries that yield null search results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of an eCommerce site depends on how well users are connected with products and services of interest. Users typically communi- cate their desires through search queries; however, queries are often incomplete and contain errors, which impact the quantity and quality of search results.</p><p>New challenges arise for search engines in cross-border eCommerce. In this paper, we fo- cus on two cross-linguistic phenomena that make interpreting queries difficult: (i) Homoglyphs: <ref type="bibr" target="#b3">(Miller, 2013)</ref>: Tokens such as "case" (underlined letters Cyrillic), in which users mix characters from different character sets that are visually simi- lar or identical. For instance, English and Russian alphabets share homoglyphs such as c, a, e, o, y, k, etc. Although the letters are visually similar or in some cases identical, the underlying character codes are different. (ii) Keyboard Layout Errors (KLEs): (Baytin et al., 2013): When switching one's keyboard between language modes, users at times enter terms in the wrong character set. For instance, "чехол шзфв" may appear to be a Rus- sian query. While "чехол" is the Russian word for "case", "шзфв" is actually the user's attempt to enter the characters "ipad" while leaving their keyboard in Russian language mode. Queries con- taining KLEs or homoglyphs are unlikely to pro- duce any search results, unless the intended ASCII sequences can be recovered. In a test set sam- pled from Russian/English queries with null (i.e. empty) search results (see Section 3.1), we found approximately 7.8% contained at least one KLE or homoglyph.</p><p>In this paper, we present a machine learning approach to identifying and correcting query to- kens containing homoglyphs and KLEs. We show that the proposed method offers superior accuracy over rule-based methods, as well as significant im- provement in search recall. Although we focus our results on Russian/English queries, the techniques (particularly for KLEs) can be applied to other lan- guage pairs that use different character sets, such as Korean-English and Thai-English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In cross-border trade at eBay, multilingual queries are translated into the inventory's source language prior to search. A key application of this, and the focus of this paper, is the translation of Rus- sian queries into English, in order to provide Rus- sian users a more convenient interface to English- based inventory in North America. The presence of KLEs and homoglyphs in multilingual queries, however, leads to poor query translations, which in turn increases the incidence of null search results. We have found that null search results correlate with users exiting our site.</p><p>In this work, we seek to correct for KLEs and homoglyphs, thereby improving query translation, reducing the incidence of null search results, and increasing user engagement. Prior to translation and search, we preprocess multilingual queries by identifying and transforming KLEs and homo- glyphs as follows (we use the query "чехол шзфв 2 new" as a running example):</p><p>(a) Tag Tokens: label each query token with one of the following semantically moti- vated classes, which identify the user's informa- tion need: (i) E: a token intended as an English search term; (ii) R: a Cyrillic token intended as a Russian search term; (iii) K: A KLE, e.g. "шзфв" for the term "ipad". A token intended as an En- glish search term, but at least partially entered in the Russian keyboard layout; (iv) H: A Russian homoglyph for an English term, e.g. "вмw" (un- derlined letters Cyrillic). Employs visually sim- ilar letters from the Cyrillic character set when spelling an intended English term; (v) A: Ambigu- ous tokens, consisting of numbers and punctuation characters with equivalent codes that can be en- tered in both Russian and English keyboard lay- outs. Given the above classes, our example query "чехол шзфв 2 new" should be tagged as "R K A E".</p><p>(b) Transform Queries: Apply a deterministic mapping to transform KLE and homoglyph tokens from Cyrillic to ASCII characters. For KLEs the transformation maps between characters that share the same location in Russian and English keyboard layouts (e.g. ф → a, ы → s). For homoglyphs the transformation maps between a smaller set of vi- sually similar characters (e.g. е → e, м → m). Our example query would be transformed into "чехол ipad 2 new".</p><p>(c) Translate and Search: Translate the trans- formed query (into "case ipad 2 new" for our ex- ample), and dispatch it to the search engine.</p><p>In this paper, we formulate the token-level tag- ging task as a standard multiclass classification problem (each token is labeled independently), as well as a sequence labeling problem (a first order conditional Markov model). In order to provide end-to-end results, we preprocess queries by de- terministically transforming into ASCII the tokens tagged by our model as KLEs or homoglyphs. We conclude by presenting an evaluation of the impact of this transformation on search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features</head><p>Our classification and sequence models share a common set of features grouped into the follow- ing categories:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Language Model Features</head><p>A series of 5-gram, character-level language mod- els (LMs) capture the structure of different types of words. Intuitively, valid Russian terms will have high probability in Russian LMs. In contrast, KLEs or homoglyph tokens, despite appearing on the surface to be Russian terms, will generally have low probability in the LMs trained on valid Russian words. Once mapped into ASCII (see Section 2 above), however, these tokens tend to have higher probability in the English LMs. LMs are trained on the following corpora: English and Russian Vocabulary: based on a collection of open source, parallel En- glish/Russian corpora (∼50M words in all). English Brands: built from a curated list of 35K English brand names, which often have distinctive linguistic properties compared with common En- glish words ( <ref type="bibr" target="#b1">Lowrey et al., 2013)</ref>. Russian Transliterations: built from a col- lection of Russian transliterations of proper names from Wikipedia (the Russian portion of guessed-names.ru-en made available as a part of WMT 2013 1 ).</p><p>For every input token, each of the above LMs fires a real-valued feature -the negated log- probability of the token in the given language model. Additionally, for tokens containing Cyril- lic characters, we consider the token's KLE and homoglyph ASCII mappings, where available. For each mapping, a real-valued feature fires corre- sponding to the negated log-probability of the mapped token in the English and Brands LMs. Lastly, an equivalent set of LM features fires for the two preceding and following tokens around the current token, if applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Token Features</head><p>We include several features commonly used in token-level tagging problems, such as case and shape features, token class (such as letters-only, digits-only), position of the token within the query, and token length. In addition, we include fea- tures indicating the presence of characters from the ASCII and/or Cyrillic character sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Dictionary Features</head><p>We incorporate a set of features that indicate whether a given lowercased query token is a mem- ber of one of the lexicons described below. UNIX: The English dictionary shipped with Cen- tOS, including ∼480K entries, used as a lexicon of common English words. BRANDS: An expanded version of the curated list of brand names used for LM features. Includes ∼58K brands. PRODUCT TITLES: A lexicon of over 1.6M en- tries extracted from a collection of 10M product titles from eBay's North American inventory. QUERY LOGS: A larger, in-domain collection of approximately 5M entries extracted from ∼100M English search queries on eBay.</p><p>Dictionary features fire for Cyrillic tokens when the KLE and/or homoglyph-mapped version of the token appears in the above lexicons. Dictionary features are binary for the Unix and Brands dictio- naries, and weighted by relative frequency of the entry for the Product Titles and Query Logs dic- tionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The following datasets were used for training and evaluating the baseline (see Section 3.2 below) and our proposed systems: Training Set: A training set of 6472 human- labeled query examples (17,239 tokens). In-Domain Query Test Set: A set of 2500 Rus- sian/English queries (8,357 tokens) randomly se- lected from queries with null search results. By focusing on queries with null results, we empha- size the presence of KLEs and homoglyphs, which occur in 7.8% of queries in our test set.</p><p>Queries were labeled by a team of Russian lan- guage specialists. The test set was also indepen- dently reviewed, which resulted in the correction of labels for 8 out of the 8,357 query tokens.</p><p>Although our test set is representative of the types of problematic queries targeted by our model, our training data was not sampled using the same methodology. We expect that the differences in distributions between training and test sets, if anything, make the results reported in Section 3.3 somewhat pessimistic 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dictionary Baseline</head><p>We implemented a rule-based baseline system em- ploying the dictionaries described in Section 2.1.3. In this system, each token was assigned a class k ∈ {E, R, K, H, A} using a set of rules: a token among a list of 101 Russian stopwords 3 is tagged <ref type="bibr">2</ref> As expected, cross-validation experiments on the train- ing data (for parameter tuning) yielded results slightly higher than the results reported in Section 3.3, which use a held-out test set <ref type="bibr">3</ref> Taken from the Russian Analyzer packaged with Lucene -see lucene.apache.org.</p><p>as R. A token containing only ASCII characters is labeled as A if all characters are common to En- glish and Russian keyboards (i.e. numbers and some punctuation), otherwise E. For tokens con- taining Cyrillic characters, KLE and homoglyph- mapped versions are searched in our dictionaries. If found, K or H are assigned. If both mapped ver- sions are found in the dictionaries, then either K or H is assigned probabilistically <ref type="bibr">4</ref> . In cases where neither mapped version is found in the dictionary, the token assigned is either R or A, depending on whether it consists of purely Cyrillic characters, or a mix of Cyrillic and ASCII, respectively.</p><p>Note that the above tagging rules allow tokens with classes E and A to be identified with perfect accuracy. As a result, we omit these classes from all results reported in this work. We also note that this simplification applies because we have restricted our attention to the Russian → English direction. In the bidirectional case, ASCII tokens could represent either English tokens or KLEs (i.e. a Russian term entered in the English keyboard layout). We leave the joint treatment of the bidi- rectional case to future work. We experimented with different combinations of dictionaries, and found the best combination to be UNIX, BRANDS, and PRODUCT TITLES dic- tionaries (see <ref type="table">Table 1</ref>). We observed a sharp de- crease in precision when incorporating the QUERY LOGS dictionary, likely due to noise in the user- generated content.</p><p>Error analysis suggests that shorter words are the most problematic for the baseline system <ref type="bibr">5</ref> . Shorter Cyrillic tokens, when transformed from Cyrillic to ASCII using KLE or homoglyph map- pings, have a higher probability of spuriously mapping to valid English acronyms, model IDs, or short words. For instance, Russian car brand "ваз" maps across keyboard layouts to "dfp",  <ref type="table">Table 2</ref>: Classification and sequence tagging re- sults on the test set a commonly used acronym in product titles for "Digital Flat Panel". Russian words "муки" and "рук" similarly map by chance to English words "verb" and "her". A related problem occurs with product model IDs, and highlights the limits of treating query to- kens independently. Consider Cyrillic query "БМВ e46". The first token is a Russian transliteration for the BMW brand. The second token, "e46", has three possible interpretations: i) as a Russian token; ii) a homoglyph for ASCII "e46"; or iii) a KLE for "t46". It is difficult to discriminate between these options without considering token context, and in this case having some prior knowl- edge that e46 is a BMW model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Machine Learning Models</head><p>We trained linear classification models using lo- gistic regression (LR) <ref type="bibr">6</ref> , and non-linear models us- ing random forests (RFs), using implementations from the Scikit-learn package <ref type="bibr" target="#b4">(Pedregosa et al., 2011</ref>). Sequence models are implemented as first order conditional Markov models by applying a beam search (k = 3) on top of the LR and RF classifiers. The LR and RF models were tuned us- ing 5-fold cross-validation results, with models se- lected based on the mean F1 score across R, K, and H tags. <ref type="table">Table 2</ref> shows the token-level results on our in- domain test set. As with the baseline, we focus the model on disambiguating between classes R, K and H. Each of the reported models performs signifi- cantly better than the baseline (on each tag), with statistical significance evaluated using McNemar's test. The differences between LR and RF mod- els, as well as sequence and classification variants, however, are not statistically significant. Each of the machine learning models achieves a query- level accuracy score of roughly 98% (the LR se-quence model achieved the lowest with 97.78%, the RF sequence model the highest with 97.90%).</p><p>Our feature ablation experiments show that the majority of predictive power comes from the character-level LM features. Dropping LM fea- tures results in a significant reduction in perfor- mance (F1 scores .878 and .638 for the RF Se- quence model on classes K and H). These results are still significantly above the baseline, suggest- ing that token and dictionary features are by them- selves good predictors. However, we do not see a similar performance reduction when dropping these feature groups.</p><p>We experimented with lexical features, which are commonly used in token-level tagging prob- lems. Results, however, were slightly lower than the results reported in this section. We suspect the issue is one of overfitting, due to the limited size of our training data, and general sparsity associated with lexical features. Continuous word presenta- tions ( <ref type="bibr" target="#b2">Mikolov et al., 2013</ref>), noted as future work, may offer improved generalization.</p><p>Error analysis for our machine learning mod- els suggests patterns similar to those reported in Section 3.2. Although errors are significantly less frequent than in our dictionary baseline, shorter words still present the most difficulty. We note as future work the use of word-level LM scores to target errors with shorter words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Search Results</head><p>Recall that we translate multilingual queries into English prior to search. KLEs and homoglyphs in queries result in poor query translations, often leading to null search results.</p><p>To evaluate the impact of KLE and homoglyph correction, we consider a set of 100k randomly se- lected Russian/English queries. We consider the subset of queries that the RF or baseline models predict as containing a KLE or homoglyph. Next, we translate into English both the original query, as well as a transformed version of it, with KLEs and homoglyphs replaced with their ASCII map- pings. Lastly, we execute independent searches using original and transformed query translations. <ref type="table" target="#tab_3">Table 3</ref> provides details on search results for original and transformed queries. The baseline model transforms over 12.6% of the 100k queries. Of those, 24.3% yield search results where the un- modified queries had null search results (i.e. Null → Non-null). In 20.9% of the cases, however, the transformations are destructive (i.e. Non-null → Null), and yield null results where the unmodified query produced results.</p><p>Compared with the baseline, the RF model transforms only 7.4% of the 100k queries; a frac- tion that is roughly in line with the 7.8% of queries in our test set that contain KLEs or homoglyphs. In over 42% of the cases (versus 24.3% for the baseline), the transformed query generates search results where the original query yields none. Only 4.81% of the transformations using the RF model are destructive; a fraction significantly lower than the baseline.</p><p>Note that we distinguish here only between queries that produce null results, and those that do not. We do not include queries for which original and transformed queries both produce (potentially differing) search results. Evaluating these cases requires deeper insight into the relevance of search results, which is left as future work.   <ref type="bibr" target="#b0">Baytin et al. (2013)</ref> first refer to keyboard lay- out errors in their work. However, their focus is on predicting the performance of spell-correction, not on fixing KLEs observed in their data. To our knowledge, our work is the first to introduce this problem and to propose a machine learning solution. Since our task is a token-level tagging problem, it is very similar to the part-of-speech (POS) tagging task <ref type="bibr" target="#b5">(Ratnaparkhi, 1996)</ref>, only with a very small set of candidate tags. We chose a supervised machine learning approach in order to achieve maximum precision. However, this problem can also be approached in an unsuper- vised setting, similar to the method <ref type="bibr" target="#b7">Whitelaw et al. (2009)</ref> use for spelling correction. In that setup, the goal would be to directly choose the correct transformation for an ill-formed KLE or homo- glyph, instead of a tagging step followed by a de- terministic mapping to ASCII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We investigate two kinds of errors in search queries: keyboard layout errors (KLEs) and ho- moglyphs. Applying machine learning methods, we are able to accurately identify a user's intended query, in spite of the presence of KLEs and ho- moglyphs. The proposed models are based largely on compact, character-level language models. The proposed techniques, when applied to multilingual queries prior to translation and search, offer signif- icant gains in search results.</p><p>In the future, we plan to focus on additional fea- tures to improve KLE and homoglyph discrimina- tion for shorter words and acronyms. Although lexical features did not prove useful for this work, presumably due to data sparsity and overfitting issues, we intend to explore the application of continuous word representations ( <ref type="bibr" target="#b2">Mikolov et al., 2013</ref>). Compared with lexical features, we expect continuous representations to be less susceptible to overfitting, and to generalize better to unknown words. For instance, using continuous word rep- resentations, <ref type="bibr" target="#b6">Turian et al. (2010)</ref> show significant gains for a named entity recognition task.</p><p>We also intend on exploring the use of features from in-domain, word-level LMs. Word-level fea- tures are expected to be particularly useful in the case of spurious mappings (e.g. "ваз" vs. "dfp" from Section 3.2), where context from surround- ing tokens in a query can often help in resolving ambiguity. Word-level features may also be useful in re-ranking translated queries prior to search, in order to reduce the incidence of erroneous query transformations generated through our methods. Finally, our future work will explore KLE and ho- moglyph correction bidirectionally, as opposed to the unidirectional approach explored in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Impact of KLE and homoglyph correction 
on search results for 100k queries 

4 Related Work 

</table></figure>

			<note place="foot" n="1"> www.statmt.org/wmt13/ translation-task.html#download</note>

			<note place="foot" n="4"> We experimented with selecting K or H based on a prior computed from training data; however, results were lower than those reported, which use random selection. 5 Stopwords are particularly problematic, and hence excluded from consideration as KLEs or homoglyphs.</note>

			<note place="foot" n="6"> Although CRFs are state-of-the-art for many tagging problems, in our experiments they yielded results slightly lower than LR or RF models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Jean-David Ruvini, Mike Dillinger, Saša Hasan, Irina Borisova and the anonymous reviewers for their valuable feedback. We also thank our Russian language special-ists Tanya Badeka, Tatiana Kontsevich and Olga Pospelova for their support in labeling and review-ing datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speller performance pre625 diction for query autocorrection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Baytin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Galinskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Panina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1821" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Relation Between Brand-name Linguistic Characteristics and Brand-name Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><forename type="middle">M</forename><surname>Lowrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">J</forename><surname>Shrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">M</forename><surname>Dubitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advertising</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="7" to="17" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Russian-English Homoglyphs, Homographs, and Homographic Translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Word Ways: The Journal of Recreational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="168" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Maximum Entropy Model for Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using the Web for Language Independent Spellchecking and Autocorrection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><forename type="middle">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ged</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="890" to="899" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
