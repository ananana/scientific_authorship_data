<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafise</forename><forename type="middle">Sadat</forename><surname>Moosavi</surname></persName>
							<email>moosavi@ukp.informatik.tu-darmstadt.de, michael.strube@h-its.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Training Group AIPHES</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Training Group AIPHES</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="193" to="203"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>193</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore , generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more gen-eralizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization , our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution is the task of recognizing different expressions that refer to the same en- tity. The referring expressions are called mentions. For instance, the sentence " <ref type="bibr">[Susan]</ref> 1 sent <ref type="bibr">[her]</ref> 1 daughter to a boarding school" contains two core- ferring mentions. "her" is an anaphor which refers to the antecedent "Susan".</p><p>The availability of coreference information ben- efits various Natural Language Processing (NLP) tasks including automatic summarization, ques- tion answering, machine translation and informa- tion extraction. Current coreference developments are almost only targeted at improving scores on * This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universität Darmstadt, https://www.ukp.tu-darmstadt. <ref type="bibr">de.</ref> the CoNLL official test set. However, the supe- riority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of <ref type="bibr" target="#b7">Clark and Manning (2016a)</ref>, the reinforcement learning model of <ref type="bibr" target="#b8">Clark and Manning (2016b)</ref> and the end-to-end model of <ref type="bibr" target="#b19">Lee et al. (2017)</ref> are three recent coreference re- solvers, among which the model of <ref type="bibr" target="#b19">Lee et al. (2017)</ref> performs the best and that of <ref type="bibr" target="#b8">Clark and Manning (2016b)</ref> performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset ( <ref type="bibr" target="#b15">Ghaddar and Langlais, 2016a)</ref>, which is consis- tent with CoNLL with regard to coreference def- inition and annotation scheme, the performance ranking would be in a reverse order <ref type="bibr">1</ref> .</p><p>In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolu- tion and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily in- dicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference resolution, the use of coreference res- olution in other applications is mainly limited to the use of simple rule-based systems, e.g. Lapata and <ref type="bibr" target="#b18">Barzilay (2005)</ref>, <ref type="bibr" target="#b40">Yu and Ji (2016)</ref>, and <ref type="bibr" target="#b11">Elsner and Charniak (2008)</ref>.</p><p>In this paper, we explore the role of linguis- tic features for improving generalization. The in- corporation of linguistic features is considered as a potential solution for building more generaliz- able NLP systems 2 . While linguistic features <ref type="bibr">3</ref> were shown to be important for coreference res- olution, e.g. <ref type="bibr" target="#b34">Uryupina (2007)</ref> and <ref type="bibr" target="#b3">Bengtson and Roth (2008)</ref>, state-of-the-art systems no longer use them and mainly rely on word embeddings and deep neural networks. Since all recent sys- tems are using neural networks, we focus on the effect of linguistic features on a neural coreference resolver.</p><p>The contributions of this paper are as follows:</p><p>-We show that linguistic features are more ben- eficial for a neural coreference resolver if we incorporate features and subsets of their values that are informative for discriminating corefer- ence relations. Otherwise, employing linguis- tic features with all their values only slightly affects the performance and generalization.</p><p>-We propose an efficient discriminative pattern mining algorithm, called EPM, for determin- ing (feature, value) pairs that are informative for the given task. We show that while the in- formativeness of EPM mined patterns is on- par with those of its counterparts, it scales best to large datasets. <ref type="bibr">4</ref> -By improving generalization, we achieve state-of-the-art performance on all exam- ined out-of-domain evaluations. Our out-of- domain performance on WikiCoref is on-par with that of Ghaddar and Langlais (2016b)'s coreference resolver, which is a system specif- ically designed for WikiCoref and uses its do- main knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Importance of Features in Coreference</head><p>Uryupina <ref type="formula">(2007)</ref>'s thesis is one of the most thor- ough analyses of linguistically motivated features for coreference resolution. She examines a large set of linguistic features, i.e. string match, syntac- tic knowledge, semantic compatibility, discourse structure and salience, and investigates their inter- action with coreference relations. She shows that even imperfect linguistic features, which are ex- tracted using error-prone preprocessing modules, boost the performance and argues that coreference resolvers could and should benefit from linguistic theories. Her claims are based on analyses on the MUC dataset. <ref type="bibr" target="#b26">Ng and Cardie (2002)</ref>, <ref type="bibr" target="#b39">Yang et al. (2004)</ref>, <ref type="bibr" target="#b28">Ponzetto and Strube (2006)</ref>, Bengtson and itions, e.g. string match, or are acquired from linguistic pre- processing modules, e.g. POS tags, as linguistic features. <ref type="bibr">4</ref> The EPM code is available at https://github. com/ns-moosavi/epm <ref type="bibr" target="#b3">Roth (2008), and</ref><ref type="bibr" target="#b30">Hovy (2009)</ref> also study the importance of features in coreference resolution.</p><p>Apart from the mentioned studies, which are mainly about the importance of individual fea- tures, studies like <ref type="bibr" target="#b4">Björkelund and Farkas (2012)</ref>, , and <ref type="bibr" target="#b35">Uryupina and Moschitti (2015)</ref> generate new features by combining basic features. <ref type="bibr" target="#b4">Björkelund and Farkas (2012)</ref> do not use a systematic approach for combining fea- tures.  use the Entropy guided Feature Induction (EFI) approach ) to automatically generate discriminative feature combinations. The first step is to train a decision tree on a dataset in which each sample consists of features describing a mention pair. The EFI approach traverses the tree from the root in a depth-first order and recursively builds feature combinations. Each pattern that is gener- ated by EFI starts from the root node. As a result, EFI tends to generate long patterns. A decision tree does not represent all patterns of data. There- fore, it is not possible to explore all feature com- binations from a decision tree. <ref type="bibr" target="#b35">Uryupina and Moschitti (2015)</ref> propose an al- ternative approach to EFI. They formulate the problem of generating feature combinations as a pattern mining approach. They use the Jac- card Item Mining (JIM) algorithm <ref type="bibr">5 (Segond and Borgelt, 2011</ref>). They show that the classifier that uses the JIM features significantly outperforms the one that employs the EFI features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Coreference Resolver</head><p>deep-coref <ref type="bibr" target="#b7">(Clark and Manning, 2016a</ref>) and e2e- coref ( <ref type="bibr" target="#b19">Lee et al., 2017</ref>) are among the best per- forming coreference resolvers from which e2e- coref performs better on the CoNLL test set. deep- coref is a pipelined system, i.e. a mention detec- tion first determines the list of candidate men- tions with their corresponding features. It con- tains various coreference models including the mention-pair, mention-ranking, and entity-based models. The mention-ranking model of deep- coref has three variations: (1) "ranking" uses the slack-rescaled max-margin training objective of <ref type="bibr" target="#b38">Wiseman et al. (2015)</ref>, (2) "reinforce" is a varia- tion of the "ranking" model in which the hyper- parameters are set in a reinforcement learning framework <ref type="bibr" target="#b33">(Sutton and Barto, 1998)</ref>, and (3) "top-pairs" is a simple variation of the "ranking" model that uses a probabilistic objective function and is used for pretraining the "ranking" model. e2e-coref is an end-to-end system that jointly models mention detection and coreference reso- lution. It considers all possible (start, end) word spans of each sentence as candidate mentions. Apart from a single model, e2e-coref includes an ensemble of five models.</p><p>We use deep-coref as the baseline in our experi- ments. The reason is that some of the examined features require the head of each mention to be known, e.g. head match, while e2e-coref mentions do not have specific heads and heads are automat- ically determined using an attention mechanism. We also observe that if we limit e2e-coref candi- date spans to those that correspond to deep-coref's detected mentions, the performance of e2e-coref drops to a level on-par with deep-coref 6 . -Acronym, e.g. "Heidelberg Institute for Theo- retical Studies" and "HITS" <ref type="bibr">6</ref> The CoNLL score of the e2e-coref single model on the CoNLL development set drops from 67.36 to 65.81, while that of the deep-coref "ranking" model is 66.09.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Examined Features</head><p>-Compatible pre-modifiers: the set of pre- modifiers of one mention is contained in that of the other, e.g. "the red hat that she is wear- ing" and "the red hat" -Compatible <ref type="bibr">7</ref> gender, e.g. "Mary" and "women" -Compatible number, e.g. "Mary" and "John" -Compatible animacy, e.g. "those hats" and "it" -Compatible attributes: compatible gender, number and animacy, e.g. "Mary" and "she"</p><p>-Closest antecedent that has the same head and compatible premodifiers, e.g. "this new book" and "This book" in "Take a look at this new book. This book is one of the best sellers."</p><p>-Closest antecedent that has compatible at- tributes, e.g. the antecedent "Mary" and the anaphor "she" in the sentence "John saw Mary, and she was in a hurry"</p><p>-Closest antecedent that has compatible at- tributes and is a subject, e.g. the antecedent "Mary" and the anaphor "she" in the sentence "Mary saw John, but she was in a hurry"</p><p>-Closest antecedent that has compatible at- tributes and is an object, e.g. "Mary" and "she" in "John saw Mary, and she was in a hurry"</p><p>The last three features are similar to the discourse- level features discussed by Uryupina (2007), which are created by combining proximity, agree- ment and salience properties. She shows that such features are useful for resolving pronouns. we esti- mate proximity by considering the distance of two mentions. The salience is also incorporated by dis- criminating subject or object antecedents. We do not use any gold information. All features are ex- tracted using Stanford CoreNLP ( <ref type="bibr" target="#b22">Manning et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Impact of Linguistic Features</head><p>In this section, we examine the effect of employ- ing all linguistic features described in Section 4 in a neural coreference resolver, i.e. deep-coref. The results of employing those features in deep- coref's "ranking" and "top-pairs" models on the CoNLL development set are reported in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: Impact of linguistic features on deep-coref models on the CoNLL development set.</p><p>The rows "ranking" and "top-pairs" show the base results of deep-coref's "ranking" and "top- pairs" models, respectively. "+linguistic" rows represents the results for each of the mention- ranking models in which the feature set of Sec- tion 4 is employed. The gender, number, ani- macy and mention type features, which have less than five values, are converted to binary features. Named entity and POS tags, and dependency rela- tions are represented as learned embeddings.</p><p>We observe that incorporating all the linguistic features bridges the gap between the performance of "top-pairs" and "ranking". However, it does not improve significantly over "ranking". Henceforth, we use the "top-pairs" model of deep-coref as the baseline model to incorporate linguistic features.</p><p>To assess the impact on generalization, we eval- uate "top-pairs" and "+linguistic" 8 models that are trained on CoNLL, on WikiCoref (see <ref type="table" target="#tab_3">Table 2</ref>). We observe that the impact on generalization is also not notable, i.e. the CoNLL score improves only by 0.5pp over "ranking".  Based on an ablation study, while our feature set contains numerous features, the resulting im- provements of "linguistic" over "top-pairs" mainly comes from the last four pairwise features in Sec- tion 4, which are carefully designed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Better Exploiting Linguistic Features</head><p>As discussed by <ref type="bibr" target="#b24">Moosavi and Strube (2017a)</ref>, there is a large lexical overlap between the core- ferring mentions of the CoNLL training and eval- uation sets. As a result, lexical features provide a 8 i.e. "top-pairs+linguistic" very strong signal for resolving coreference rela- tions.</p><p>For linguistic features to be more effective in current coreference resolvers, which rely heav- ily on lexical features, they should also provide a strong signal for coreference resolution.</p><p>Additional linguistic features are not necessar- ily all informative for coreference resolution, es- pecially if they are extracted automatically and are noisy. Besides, for features with multiple values, e.g. mention-based features, only a small subset of values may be informative.</p><p>To better exploit linguistic features, we only employ (feature, value) pairs 9 that are informative for coreference resolution. Coreference resolution is a complex task in which features have complex interactions <ref type="bibr" target="#b30">(Recasens and Hovy, 2009)</ref>. As a re- sult, we cannot determine the informativeness of feature-values in isolation.</p><p>We use a discriminative pattern mining ap- proach ( <ref type="bibr" target="#b5">Cheng et al., 2007</ref><ref type="bibr" target="#b6">Cheng et al., , 2008</ref><ref type="bibr" target="#b2">Batal and Hauskrecht, 2010</ref>) that examines all combinations of feature-values, up to a certain length, and deter- mines which feature-values are informative when they are considered in combination.</p><p>Due to the large data size (all mention-pairs of the CoNLL training data) and the high dimension- ality of feature-values, compared to common eval- uation sets of pattern mining methods, the exist- ing discriminative pattern mining approaches were not applicable to our data. In this section, we pro- pose an efficient discriminative pattern mining ap- proach, called Efficient Pattern Miner (EPM), that is scalable to large NLP datasets. The most impor- tant properties of EPM are (1) it examines all fre- quent feature-values combinations, up to the de- sired length, (2) it is scalable to large datasets, and (3) it is only data dependent and independent of the coreference resolver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Notation</head><p>We use the following notations and definitions throughout this section:</p><formula xml:id="formula_0">-D = {X i , c(X i )} n i=1</formula><p>: set of n training sam- ples. X i is the set of feature-values that de- scribes the ith sample. c(X i ) ∈ C is the label of X i , e.g. coreferent and non-coreferent.</p><p>-A = {a 1 , . . . , a l }: set of all feature-values present in D. Each a i ∈ A is called an item, e.g. a i ="anaphor type=proper".</p><p>-p: pattern p = {a i 1 , . . . , a i k } is a set of one or more items, e.g. p ={"anaphor type=proper", "antecedent type=proper"}.</p><p>-support(p, c i ): the number of samples that contain pattern p and are labeled with c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Structure</head><p>For representing the input samples, we use the Fre- quent Pattern Tree (FP-Tree) structure that is the data structure of the FP-Growth algorithm <ref type="bibr" target="#b17">(Han et al., 2004</ref>), i.e. one of the most common algo- rithms for frequent pattern mining. FP-Tree pro- vides a structure for representing all existing pat- terns of data in a compressed form. Using the FP-Tree structure allows an efficient enumeration of all frequent patterns. In the FP-Tree struc- ture, items are arranged in descending order of frequency. Frequency of an item corresponds to c i ∈C support(a i , c i ). Except for the root, which is a null node, each node n contains an item a i ∈ A. It also contains the support values of a i in the subpath of the tree that starts from the root and ends with n, i.e. support n (a i , c j ).</p><p>The FP-Tree construction method ( <ref type="bibr" target="#b17">Han et al., 2004</ref>) is as follows: (a) scan D to collect the set of all items, i.e. A. Compute support(a i , c j ) for each item a i ∈ A and label c j ∈ C. Sort A's members in descending order according to their frequencies, i.e. 1. Order all items a j ∈ X i according to the or- der in A.</p><p>2. Set the current node (T ) to the root.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Consider</head><formula xml:id="formula_1">X i = [a k | ¯ X i ]</formula><p>, where a k is the first (ordered) item of x i , and ¯ X i = X i − a k . If T has a child n that contains a k then in- crement support n (a k , c(X i )) by one. Oth- erwise, create a new node n that contains a k with support n (a k , c(X i )) = 1. Add n to the tree as a child of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">If ¯</head><p>X i is non-empty, set T to n. Assign X i = ¯ X i and go to step 3.</p><p>As an example, assume D contains the follow- ing two samples:</p><formula xml:id="formula_2">X 1 ={ana-type=NAM, ant-type=NAM, head- match=F}, C(X 1 ) = 0 X 2 ={ana-type=NAM, ant-type=NAM, head- match=T}, C(X 2 ) = 1</formula><p>Based on these samples A={ana-type=NAM, ant-type=NAM, head-match=F, head- match=T}, support(a i , 0) a i ∈A = {1,1,1,0}, and support(a i , 1) a i ∈A ={1,1,0,1}.</p><p>If we sort A based on a i 's frequencies (support(a i , 0) + support(a i , 1)), the order- ing of A's items will remain the same.</p><p>The FP-Tree construction steps for the above samples are demonstrated in <ref type="figure" target="#fig_2">Figure 1</ref>. ana-type, ant-type, and head-match features are abbreviated as ana, ant, and head, respectively. From an initial FP-Tree (T ) that represents all existing patterns, one can easily obtain a new FP- Tree in which all patterns include a given pattern p. This can be done by only including sub-paths of T that contain pattern p. The new tree is called conditional FP-Tree of p, T p . An example of con- ditional FP-Tree is included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Informativeness Measures</head><p>We use a discriminative power and an informa- tion novelty measure for determining informative- ness. We also use a frequency measure which determines the required minimum frequency of a pattern in training samples. It helps to avoid over- fitting to the properties of the training data. Discriminative power: We use the G 2 likelihood ratio test <ref type="bibr" target="#b0">(Agresti, 2007</ref>) in order to choose pat- terns whose association with the class variable is statistically significant. <ref type="bibr">10</ref> The G 2 test is success- fully used for text analysis <ref type="bibr" target="#b10">(Dunning, 1993)</ref>. Information Novelty: A large number of redun- dant patterns can be generated by adding irrelevant items to a base pattern that is discriminative itself.</p><p>We consider the pattern p as novel if (1) p predicts the target class label c significantly better than all of its containing items, and (2) p predicts c signifi- cantly better than all of its sub-patterns that satisfy the frequency, discriminative power, and the first information novelty conditions. Similar to <ref type="bibr" target="#b2">Batal and Hauskrecht (2010)</ref>, we employ a binomial dis- tribution to determine information novelty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Mining Algorithm</head><p>The EPM algorithm is summarized in Algo- rithm 1. It takes FP-Tree T , pattern p on which T is conditioned, and set of items (A j ⊂ A) whose combinations with p will be examined. Initially, p is empty and the FP-Tree is constructed based on all frequent items of data and A j = A. Resulting patterns are collected in P .</p><p>For each a i ∈ A j , the algorithm builds new pattern q by combining a i with p. f requent(q) checks whether q meets the frequency condition. If q is frequent, the algorithm continues the search process. Otherwise, it is not possible to build any frequent pattern out of a non-frequent one. Dis- criminative power and the first condition of infor- mation novelty are then checked for pattern q.</p><formula xml:id="formula_3">Algorithm EP M (T , p, A j ) foreach a i ∈ A j do q = p ∪ {a i } if F requent(q) then if Discriminative(q) then if N ovel(q) then P = P ∪ q end end if |q| &gt;= Θ l then continue end construct T q = q's conditional tree EP M (T q , q, ancestors(a i )) end end Algorithm 1: The EPM algorithm.</formula><p>We use a threshold (Θ l ) for the maximum length of mined patterns. Θ l can be set to large values if more complex and specific patterns are desirable.</p><p>If |q| is smaller than Θ l , the conditional FP-Tree T q is built that represents patterns of T that in- clude the pattern q. The mining algorithm then continues to recursively search for more specific patterns by combining q with the items included in ancestors(a i ), which keeps the list of all an- cestors of a i in the original FP-Tree. EPM exam- ines all frequent patterns of up to length Θ l .</p><p>If we use a statistical test multiple times, the risk of making false discoveries increases <ref type="bibr" target="#b37">(Webb, 2006</ref>). To tackle this, we apply the Bonferroni cor- rection for multiple tests in a post-pruning func- tion after the mining process. This function also applies the second information novelty condition on the resulting patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Why Use EPM?</head><p>In this section, we explain why EPM is a better alternative compared to its counterparts for large NLP datasets. We compare EPM with two ef- ficient discriminative pattern mining algorithms, i.e. Minimal Predictive Patterns (MPP) ( <ref type="bibr" target="#b2">Batal and Hauskrecht, 2010)</ref> and Direct Discriminative Pat- tern Mining (DDPMine) ( <ref type="bibr" target="#b6">Cheng et al., 2008</ref>), on standard machine learning datasets.</p><p>MPP selects patterns that are significantly more predictive than all their sub-patterns. It measures significance by the binomial distribution. For each pattern of length l, MPP checks 2 l −1 sub-patterns. DDPMine is an iterative approach that selects the most discriminative pattern at each iteration and reduces the search space of the next iteration by removing all samples that include the selected pat- tern. DDPMine uses the FP-Tree structure.</p><p>We show that EPM scales best and compares fa- vorably based on the informativeness of resulting patterns. Due to its efficiency, EPM can handle large datasets similar to ones that are commonly used in various NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head><p>We use the same FP-Tree implementation for DDPMine and EPM. In all algorithms, we con- sider a pattern as frequent if it occurs in 10% of the samples of one of the classes. We use Θ l = 3 for both MPP and EPM.</p><p>We perform 5-times repeated 5-fold cross vali- dation and the results are averaged. In each vali- dation, all experiments are performed on the same split. We use a linear SVM, i.e. LIBLINEAR 2.11 <ref type="bibr" target="#b12">(Fan et al., 2008)</ref>, as the baseline classifier.</p><p>We use several datasets from the UCI ma- chine learning repository (Lichman, 2013) whose characteristics are presented in the first three columns of  <ref type="table" target="#tab_4">Table 3</ref>: Evaluating the informativeness of DDPMine, MPP and EPM patterns on standard datasets.</p><note type="other">(1/12/28) 76 299284 - - 5618 93.8 - - 93.8 48.4 - - 51.6 poker (0/10/0) 85 1025010 - - 14216 23.1 - - 49.6 22.4 - - 44.5</note><p>(real/integer/nominal) features (#Features), (2) frequent items (#FI), and (3) samples (n). We use one[the minority class]-vs-all technique for datasets with more than two classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">How Informative are EPM Patterns?</head><p>To evaluate the informativeness of mined patterns, the common practice is to add them as new fea- tures to the feature set of the baseline classifier; the more informative the patterns, the greater im- pact they would have on the overall performance. All patterns are added as binary features, i.e. the feature is true for samples that contain all items of the corresponding pattern. The effect of the patterns of DDPMine, MPP and EPM on the overall accuracy is presented in <ref type="table" target="#tab_4">Table 3</ref>. The columns #Patterns show the num- ber of patterns mined by each of the algorithms. The Orig columns show the results of the SVM us- ing the original feature sets. The DDP, MPP, and EPM columns show the results of the SVM on the datasets for which the feature set is extended by the features mined by DDPMine, MPP, and EPM, respectively. The results of the 5-repeated 5-fold cross validation are reported if each single valida- tion takes less than 10 hours.</p><p>Based on the results of <ref type="table" target="#tab_4">Table 3</ref> (1) EPM effi- ciently scales to larger datasets, (2) MPP and EPM patterns considerably improves the performance, and (3) EPM has on-par results with MPP while it mines considerably fewer patterns. 8 Impact of Informative Feature-values</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">How Does it Scale?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experimental Setup</head><p>For determining informative feature-values, we extract all features for all mention-pairs 11 of the CoNLL training data and then apply EPM on this data. In order to prevent learning annotation er- rors and specific properties of the training data, we consider a pattern as frequent if it occurs in coreference relations of at least m different core- ferring anaphors (m = 20). Since the majority of mention-pairs are non-coreferent and we are not interested in patterns for non-coreferring relations, we also consider the coreference probability of each pattern p, i.e. |{X i |p∈X i ∧c(X i )=coref erent}|</p><formula xml:id="formula_4">|{X i |p∈X i }|</formula><p>, in the post-pruning function. The coreference prob- ability should be higher than a threshold (60% in our experiments), so we only mine patterns that are informative for coreferring mentions.</p><p>For the coreference resolution experiments, in- stead of incorporating informative patterns, we in- corporate feature-values that are included in the  <ref type="table">Table 4</ref>: Comparisons on the CoNLL test set. The F1 gains that are statistically significant: (1) "+EPM" compared to "top- pairs", "ranking" and "JIM", (2) "+EPM" compared to "reinforce" based on MUC, B 3 and LEA, (3) "single" compared to "+EPM" based on MUC and B 3 , and (4) "ensemble" compared to other systems. Significance is measured based on the approximate randomization test (p &lt; 0.05) <ref type="bibr" target="#b27">(Noreen, 1989).</ref> informative patterns mined by EPM. The reason is that deep-coref, or any other recent coreference resolver, uses a deep neural network, which has a fully automated feature generation process. We add these feature-values as binary features. By setting Θ l to five, <ref type="bibr">12</ref> EPM results in 13 pair- wise feature-values, 112 POS tags, i.e. 53 POS for anaphors and 59 for antecedents, 25 depen- dency relations, 26 mention types (mention types or fine mention types), and finally, 14 named en- tity tags. <ref type="bibr">13</ref> Based on the observation in Section 5, we use the top-pairs model of deep-coref as the baseline to employ additional features, i.e. "+EPM" is the top-pairs model in which EPM feature-values are incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Impact on In-domain Performance</head><p>The performance of the "+EPM" model compared to recent state-of-the-art coreference models on the CoNLL test set is presented in <ref type="table">Table 4</ref>. The "single" and "ensemble" rows represent the results of the single and ensemble models of e2e-coref.</p><p>We also compare EPM with the pattern mining approach used by <ref type="bibr" target="#b35">Uryupina and Moschitti (2015)</ref>, i.e. Jaccard Item Mining (JIM). For a fair compar- ison, while <ref type="bibr" target="#b35">Uryupina and Moschitti (2015)</ref> used mined patterns for extracting feature templates, we use them for selecting feature-values. We run the JIM algorithm on the same data and with the same setup as that of EPM. <ref type="bibr">14</ref> This results in nine pair- <ref type="bibr">12</ref> We observe that using larger Θ l values will result in many over-specified patterns. <ref type="bibr">13</ref> Following the previous studies that show different fea- tures are of different importance for various types of men- tions, e.g. <ref type="bibr" target="#b9">Denis and Baldridge (2008)</ref> and <ref type="bibr" target="#b25">Moosavi and Strube (2017b)</ref>, we mine a separate set of patterns for each type of anaphor. These resulting feature-values are the union of informative feature-values for all types of anaphora. <ref type="bibr">14</ref> We set the minimum frequency, maximum pattern length and score + threshold parameters of JIM to 20, 5 and wise features, 260 POS tags, 38 dependency rela- tions, 32 mention types, and 18 named entity tags. The "+JIM" row shows the results of deep-coref top-pairs model in which these feature-values are incorporated. As we see, EPM feature-values re- sult in significantly better performance than those of JIM while the number of EPM feature-values is considerably less than JIM.  Feature Ablation <ref type="table" target="#tab_7">Table 5</ref> shows the effect of each group of EPM feature-values, i.e. pairwise features, mention types, dependency relations, named entity tags and POS tags, on the perfor- mance of "+EPM". The performance of "+EPM" from which each of the above feature groups is removed, one feature group at a time, is repre- sented as "-pairwise", "-types", "-dep", "-NER", and "-POS", respectively. The POS and named entity tags have the least and the pairwise features have the most significant effect. Since pairwise features have the most significant effect, we also perform an experiment in which only pairwise fea- tures are incorporated in the "top-pairs" model, i.e. "+pairwise". The results of "-pairwise" compared to "+pairwise" show that pairwise feature-values have a significant impact, but only when they are considered in combination with other EPM   In-domain and out-of-domain evaluations for the pt and wb genres of the CoNLL test set. The highest scores are boldfaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Impact on Generalization</head><p>We use the same setup as that of Moosavi and Strube (2017a) for evaluating generalization in- cluding (1) training on the CoNLL data and test- ing on WikiCoref 15 and (2) excluding a genre of the CoNLL data from training and development sets and testing on the excluded genre. Similar to <ref type="bibr" target="#b24">Moosavi and Strube (2017a)</ref>, we use the pt and wb genres for the latter evaluation setup.</p><p>The results of the first evaluation setup are shown in <ref type="table" target="#tab_9">Table 6</ref>. The best performance on WikiCoref is achieved by Ghaddar and Langlais (2016a) ("G&amp;L" in <ref type="table" target="#tab_9">Table 6</ref>) who introduced Wi- kiCoref and design a domain-specific coreference resolver that makes use of the Wikipedia markups of a document as well as links to Freebase, which are annotated in WikiCoref.</p><p>Incorporating EPM feature-values improves the performance by about three points. While "+EPM" does not use the WikiCoref data dur- ing training, and unlike "G&amp;L", it does not em- ploy any domain-specific features, it achieves on- par performance with that of "G&amp;L". This indeed <ref type="bibr">15</ref> WikiCoref only contains 30 documents, which is not enough for training neural coreference resolvers.</p><p>shows the effectiveness of informative feature- values in improving generalization.</p><p>The second set of generalization experiments is reported in <ref type="table" target="#tab_10">Table 7</ref>. "in-domain" columns show the results when the evaluation genres were in- cluded in training and development sets while the "out-of-domain" columns show the results when the evaluation genres were excluded. As we can see, "+EPM" generalizes best, and in out-of- domain evaluations, it considerably outperforms the ensemble model of e2e-coref, which has the best performance on the CoNLL test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this paper, we show that employing linguistic features in a neural coreference resolver signifi- cantly improves generalization. However, the in- corporated features should be informative enough to be taken into account in the presence of lexi- cal features, which are very strong features in the CoNLL dataset. We propose an efficient algorithm to determine informative feature-values in large datasets. As a result of a better generalization, we achieve state-of-the-art results in all examined out- of-domain evaluations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The examined linguistic features include string match, syntactic, shallow semantic and discourse features. Mention-based features include: -Mention type: proper, nominal or pronominal -Fine mention type: proper, definite or indefi- nite nominal, or the citation form of pronouns -Gender: female, male, neutral, unknown -Number: singular, plural, unknown -Animacy: animate, inanimate, unknown -Named entity type: person, location, organiza- tion, date, time, number, etc. -Dependency relation: enhanced dependency relation (Schuster and Manning, 2016) of the head word to its parent -POS tags of the first, last, head, two words pre- ceding and following of each mention Pairwise features include: -Head match: both mentions have the same head, e.g. "red hat" and "the hat" -String of one mention is contained in the other, e.g. "Mary's hat" and "Mary" -Head of one mention is contained in the other, e.g. "Mary's hat" and "hat"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>c i ∈C support(a i , c i ). (b) cre- ate a null-labeled node as the root, and (c) scan D again. For each (X i , c(X i )) ∈ D:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left to right: (partially) constructed FPTree for the example in Section 6.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 comparesFigure 2 :</head><label>22</label><figDesc>Figure 2 compares EPM mining time (in seconds) with those of DDPMine and MPP. The parameter in the parentheses is the pattern size threshold, e.g. Θ l = 4 for EPM(4). The experiments that take more than two days are terminated and are not included. EPM is notably faster in comparison to the other two approaches. It is notable that the examined datasets are considerably smaller than</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Out-of-domain evaluation of deep-coref 
models on the WikiCoref dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 , i.e. the number of (1)</head><label>3</label><figDesc></figDesc><table>Data characteristics 
# Patterns 
Micro-F 
Macro-F 
Dataset 
#Features 
#FI 
n 
DDP MPP 
EPM Orig DDP MPP EPM 
Orig DDP MPP EPM 
cmc 
(0/2/7) 
24 
1473 
4 
99 
23 77.5 
77.4 
76.2 
77.3 
57.3 
57.1 
57.7 
59.4 
nursery 
(0/0/8) 
27 
12690 
4 
258 
198 97.5 
98.2 
99.9 
99.8 
49.4 
79.4 
99.8 
98.8 
sick 
(6/1/22) 
36 
2800 
5 
627 
89 94.6 
94.7 
96.1 
95.8 
62.6 
64.8 
81.0 
75.6 
kr-v-k 
(0/0/16) 
40 
28056 
7 
71 
63 99.1 
99.1 
99.6 
99.6 
49.8 
49.8 
87.8 
88.4 
german 
(0/7/13) 
51 
1000 
8 
548 
97 70.7 
70.9 
73.1 
72.7 
49.6 
55.2 
65.3 
64.2 
connect-4 
(0/0/42) 
75 
67557 
-
-
907 90.5 
-
-
90.5 
47.5 
-
-
56.6 
census 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Impact of different EPM feature groups 
on the CoNLL development set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Out-of-domain evaluation on the WikiCoref dataset. The highest F 1 scores are boldfaced. 

feature-values. 

in-domain 
out-of-domain 
CoNLL LEA CoNLL 
LEA 
pt (Bible) 

deep-coref 
ranking 
75.61 71.00 
66.06 
57.58 
+EPM 
76.08 71.13 68.14 60.74 

e2e-coref 
single 
77.80 73.73 
65.22 
58.26 
ensemble 78.88 74.88 
65.45 
59.71 
wb (weblog) 

deep-coref 
ranking 
61.46 53.75 
57.17 
48.74 
+EPM 
61.97 53.93 61.52 53.78 

e2e-coref 
single 
62.02 53.09 
60.69 
52.69 
ensemble 64.76 57.54 
60.99 
52.99 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The single model of Lee et al. (2017) is used here. 2 E.g. there is a dedicated workshop for this topic https: //sites.google.com/view/relsnnlp. 3 We refer to features that are based on linguistic intu</note>

			<note place="foot" n="5"> http://www.borgelt.net/jim.html</note>

			<note place="foot" n="7"> One value is unknown, or both values are identical.</note>

			<note place="foot" n="9"> Henceforth, we refer to them as feature-values.</note>

			<note place="foot" n="10"> A pattern is considered discriminative if the corresponding p-value is less than a fixed threshold (0.01).</note>

			<note place="foot" n="11"> Each mention is paired with all the preceding mentions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Mark-Christoph Müller, Benjamin Heinzerling, Alex Judea, Stef-fen Eger and the anonymous reviewers for their helpful comments and feedbacks. This work has been supported by the Klaus Tschira Founda-tion, Heidelberg, Germany and the German Re-search Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Informa-tion from Heterogeneous Sources (AIPHES) un-der grant No. GRK 1994/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An Introduction to Categorical Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Agresti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Language Resources and Evaluation</title>
		<meeting>the 1st International Conference on Language Resources and Evaluation<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05-30" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constructing classification features using minimal predictive patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Batal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="869" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Waikiki, Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Datadriven multilingual coreference resolution using resolver stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning</title>
		<meeting>the Shared Task of the 16th Conference on Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative frequent pattern analysis for effective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 23rd International Conference on Data Engineering (ICDE 2007)</title>
		<meeting>the IEEE 23rd International Conference on Data Engineering (ICDE 2007)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="716" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Direct discriminative pattern mining for effective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 24th International Conference on Data Engineering</title>
		<meeting>the IEEE 24th International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016-01-05" />
			<biblScope unit="page" from="2256" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Specialized models and ranking for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Waikiki, Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACL-HLT 2008 Conference Short Papers</title>
		<meeting>ACL-HLT 2008 Conference Short Papers<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entropy-guided feature generation for structured learning of Portuguese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eraldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy L</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Processing of the Portuguese Language</title>
		<meeting>the International Conference on Computational Processing of the Portuguese Language</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="146" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy Luiz</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning</title>
		<meeting>the Shared Task of the 16th Conference on Computational Natural Language Learning<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Korea</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coreference in Wikipedia: Main concept resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference on Computational Natural Language Learning</title>
		<meeting>the 20th Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-11" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WikiCoref: An English coreference-annotated corpus of Wikipedia articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation</title>
		<meeting>the 10th International Conference on Language Resources and Evaluation<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation: A frequent-pattern tree approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runying</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1-5</biblScope>
			<biblScope unit="page" from="53" to="87" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic evaluation of text coherence: Models and representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 19th International Joint Conference on Artificial Intelligence<address><addrLine>Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07-30" />
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, B.C., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-08" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Which coreference evaluation metric do you trust? A proposal for a link-based entity aware metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexical features in coreference resolution: To be used with caution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, B.C., Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Use generalized representations, but do not forget surface features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</title>
		<meeting>the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia<address><addrLine>Penn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Computer Intensive Methods for Hypothesis Testing: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York, N.Y.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>New York, N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scoring coreference partitions of predicted mentions: A reference implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Md</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deeper look into features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Discourse Anaphora and Anaphor Resolution</title>
		<meeting>the 7th Discourse Anaphora and Anaphor Resolution</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhanced English universal dependencies: An improved representation for natural language understanding tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Item set mining based on cover similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Segond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="493" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Knowledge acquisition for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Saarland University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A state-of-the-art mention-pair model for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of STARSEM 2015: The Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>STARSEM 2015: The Fourth Joint Conference on Lexical and Computational Semantics<address><addrLine>Denver, Col</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Message Understanding Conference (MUC-6)</title>
		<meeting>the 6th Message Understanding Conference (MUC-6)<address><addrLine>San Mateo, Cal</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discovering significant patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving pronoun resolution by incorporating coreferential information of candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodung</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised person slot filling based on graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="44" to="53" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
