<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-grained Candidate Generation and Fine-grained Re-ranking for Chinese Abbreviation Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-grained Candidate Generation and Fine-grained Re-ranking for Chinese Abbreviation Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1881" to="1890"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Correctly predicting abbreviations given the full forms is important in many natural language processing systems. In this paper we propose a two-stage method to find the corresponding abbreviation given its full form. We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk. This coarse-grained rank list fixes the search space inside the top-ranked candidates. Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result. Our method achieves good results and outperforms the state-of-the-art systems. One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data. The candidate generation and coarse-grained ranking is totally unsupervised. The re-ranking phase can use a very small amount of training data to get a reasonably good result.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abbreviation Prediction is defined as finding the meaningful short subsequence of characters given the original fully expanded form. As an example, "HMM" is the abbreviation for the correspond- ing full form "Hidden Markov Model". While the existence of abbreviations is a common lin- guistic phenomenon, it causes many problems like spelling variation <ref type="bibr" target="#b13">(Nenadi´cNenadi´c et al., 2002</ref>). The dif- ferent writing manners make it difficult to identify the terms conveying the same concept, which will hurt the performance of many applications, such as information retrieval (IR) systems and machine translation (MT) systems.</p><p>Previous works mainly treat the Chinese ab- breviation generation task as a sequence labeling problem, which gives each character a label to in- dicate whether the given character in the full form should be kept in the abbreviation or not. These methods show acceptable results. However they rely heavily on the character-based features, which means it needs lots of training data to learn the weights of these context features. The perfor- mance is good on some test sets that are similar to the training data, however, when it moves to an un- seen context, this method may fail. This is always true in real application contexts like the social me- dia where there are tremendous new abbreviations burst out every day.</p><p>A more intuitive way is to find the full- abbreviation pairs directly from a large text cor- pus. A good source of texts is the news texts. In a news text, the full forms are often mentioned first. Then in the rest of the news its corresponding abbreviation is mentioned as an alternative. The co-occurrence of the full form and the abbrevia- tion makes it easier for us to mine the abbreviation pairs from the large amount of news texts. There- fore, given a long full form, we can generate its abbreviation candidates from the given corpus, in- stead of doing the character tagging job.</p><p>For the abbreviation prediction task, the candi- date abbreviation must be a sub-sequence of the given full form. An intuitive way is to select all the sub-sequences in the corpus as the can- didates. This will generate large numbers of ir- relevant candidates. Instead, we use a contextual graph random walk method, which can utilize the contextual information through the graph, to select a coarse grained list of candidates given the full form. We only select the top-ranked candidates to reduce the search space. On the other hand, the candidate generation process can only use limited contextual information to give a coarse-grained ranked list of candidates. During generation, can-didate level features cannot be included. There- fore we propose a similarity sensitive re-ranking method to give a fine-grained ranked list. We then select the final result based on the rank of each candidate.</p><p>The contribution of our work is two folds. Firstly we propose an improved method for abbre- viation generation. Compared to previous work, our method can perform well with less training data. This is an advantage in the context of so- cial media. Secondly, we build a new abbreviation corpus and make it publicly available for future re- search on this topic.</p><p>The paper is structured as follows. Section 1 gives the introduction. In section 2 we describe the abbreviation task. In section 3 we describe the candidate generation part and in section 4 we describe the re-ranking part. Experiments are de- scribed in section 5. We also give a detailed anal- ysis of the results in section 5. In section 6 related works are introduced, and the paper is concluded in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Chinese Abbreviation Prediction System</head><p>Chinese Abbreviation Prediction is the task of selecting representative characters from the long full form <ref type="bibr">1</ref> . Previous works mainly use the se- quence labeling strategies, which views the full form as a character sequence and give each char- acter an extra label 'Keep' or 'Skip' to indicate whether the current character should be kept in the abbreviation. An example is shown in <ref type="table">Table  1</ref>. The sequence labeling method assumes that the character context information is crucial to de- cide the keep or skip of a character. However, we can give many counterexamples. An exam- ple is " "(Peking University) and " "(Tsinghua University), whose abbrevia- tions correspond to " " and ' ' respec- tively. Although sharing a similar character con- text, the third character '' is kept in the first case and is skipped in the second case. We believe that a better way is to extract these abbreviation-full pairs from a natural text corpus where the full form and its abbreviation co-exist. Therefore we propose a two stage method. The first stage generates a list of candidates given a large corpus. To reduce the search space, we adopt 1 Details of the difference between English and Chinese abbreviation prediction can be found in <ref type="bibr" target="#b27">Zhang et al. (2012)</ref>. <ref type="table">Table 1</ref>: The abbreviation "" of the full form "" (Hong Kong University) graph random walk to give a coarse-grained rank- ing and select the top-ranked ones as the can- didates. Then we use a similarity sensitive re- ranking method to decide the final result. Detailed description of the two parts is shown in the follow- ing sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full form Status Skip Keep Keep Skip Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Candidate Generation through Graph</head><p>Random Walk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Generation and Graph Representation</head><p>Chinese abbreviations are sub-sequences of the full form. We use a brute force method to select all strings in a given news article that is the sub- sequence of the full form. The brute force method is not time consuming compared to using more complex data structures like trie tree, because in a given news article there are a limited number of sub-strings which meet the sub-sequence criteria for abbreviations. When generating abbreviation candidates for a given full form, we require the full form should appear in the given news article at least once. This is a coarse filter to indicate that the given news article is related to the full form and therefore the candidates generated are potentially meaningful.</p><p>The main motivation of the candidate genera- tion stage in our approach is that the full form and its abbreviation tend to share similar context in a given corpus. To be more detailed, given a word context window w, the words that appear in the context window of the full form tend to be sim- ilar to those words in the context window of the abbreviations.</p><p>We use a bipartite graph G(V word , V context , E) to represent this phenomena. We build bipartite graphs for each full form individually. For a given full form v f ull , we first extract all its candidate abbreviations V C . We have two kinds of nodes in the bipartite graph: the word nodes and the context nodes. We construct the word nodes as V word = V C ∪ {v f ull }, which is the node set of the full form and all the candidates. We construct the context nodes V context as the words that appear in a fixed window of V word . To reduce the size of the graph, we make two extra assumptions: 1) We only consider the nouns and verbs in the context and 2) context words should appear in the vocab- ulary for more than a predefined threshold (i.e. 5 times). Because G is bipartite graph, the edges E only connect word node and context nodes. We use the number of co-occurrence of the candidate and the context word as the weight of each edge and then form the weight matrix W . Details of the bipartite graph construction algorithm are shown in <ref type="table" target="#tab_0">Table 2</ref>. An example bipartite graph is shown in figure 1. <ref type="figure">Figure 1</ref>: An example of the bipartite graph rep- resentation. The full form is ""(Hong Kong University), which is the first node on the left. The three candidates are " ", " ", " ", which are the nodes on the left. The context words in this example are ""(Tsui Lap-chee, the headmaster of Hong Kong Uni- versity), ""(Enrollment), ""(Hold), " "(Enact), ""(Subway), which are the nodes on the right. The edge weight is the co-occurrence of the left word and the right word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Coarse-grained Ranking Using Random Walks</head><p>We perform Markov Random Walk on the con- structed bipartite graph to give a coarse-grained ranked list of all candidates. In random walk, a walker starts from the full form source node S (in later steps, v i ) and randomly walks to another node v j with a transition probability p ij . In ran- dom walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the fi- nal stop. The random walk method we use is sim- ilar to those defined in <ref type="bibr" target="#b15">Norris (1998)</ref> <ref type="bibr">(2013)</ref>; <ref type="bibr" target="#b11">Li et al. (2013)</ref>. The transition probability p ij is calculated us- ing the weights in the weight matrix W and then normalized with respect to the source node v i with the formula</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>; Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes</head><formula xml:id="formula_0">p ij = w ij l w il</formula><p>. When the graph ran- dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candi- dates, which we will describe in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Candidate Re-ranking</head><p>Although the coarse-grained ranked list can serve as a basic reference, it can only use limited in- formation like co-occurrence. We still need a re- ranking process to decide the final result. The rea- son is that we cannot get any candidate-specific features when the candidate is not fully gener- ated. Features such as the length of a candidate are proved to be useful to rank the candidates by pre- vious work. In this section we describe our second stage for abbreviation generation, which we use a similarity sensitive re-ranking method to find the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Similarity Sensitive Re-ranking</head><p>The basic idea behind our similarity sensitive re- ranking model is that we penalize the mistakes based on the similarity of the candidate and the reference. If the model wrongly selects a less sim- ilar candidate as the result, then we will attach a large penalty to this mistake. If the model wrongly chooses a candidate but the candidate is similar to the reference, we slightly penalize this mistake. The similarity between a candidate and the ref- erence is measured through character similarity, which we will describe later.</p><p>Input: the full form v f ull , news corpus U Output: bipartite graph We first give some notation of the re-ranking phase.</p><formula xml:id="formula_1">G(V word , V context , E) Candidate vector V c = ∅, V context = ∅ for each document d in U if d contains v f ull add all words w in the window of v f ull into V context for each n-gram s in d if s is a sub-sequence of v f ull add s into V c add all word w in the window of s into V context end if end for end if end for V word = V c ∪ {v f ull } for each word v i in V word for each word v j in V context calculate edge weight in E based on co-occurrence end for end for Return G(V word , V context , E)</formula><p>1. f (x, y) is a scoring function for a given com- bination of x and y, where x is the original full form and y is an abbreviation candidate. For a given full form x i with K candidates, we assume its corresponding K candidates are y 1 i ,y 2 i ,...,y K i .</p><p>2. evaluation function s(x, y) is used to mea- sure the similarity of the candidate to the refer- ence, where x is the original full form and y is one abbreviation candidate. We require that s(x, y) should be in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and s(x, y) = 1 if and only if y is the reference.</p><p>One choice for s(x, y) may be the indicator function. However, indicator function returns zero for all false candidates. In the abbreviation predic- tion task, some false candidates are much closer to the reference than the rest. Considering this, we use a Longest Common Subsequence(LCS) based criterion to calculate s(x, y). Suppose the length of a candidate is a, the length of the reference is b and the length of their LCS is c, then we can define precision P and recall R as:</p><formula xml:id="formula_2">P = c a , R = c b , F = 2 * P * R P + R<label>(1)</label></formula><p>It is easy to see that F is a suitable s(x, y). Therefore we can use the F-score as the value for s(x, y).</p><p>3. φ(x, y) is a feature function which returns a m dimension feature vector. m is the number of features in the re-ranking.</p><p>4. w is a weight vector with dimension m. w T φ(x, y) is the score after re-ranking. The candi- date with the highest score will be our final result.</p><p>Given these notations, we can now describe our re-ranking algorithm. Suppose we have the train- ing set X = {x 1 , x 2 , ..., x n }. We should find the weight vector w that can minimize the loss func- tion:</p><formula xml:id="formula_3">Loss( w) = n i=1 k j=1 ((s(x i , y 1 i ) − s(x i , y j i )) * I( w T φ(x i , y j i ) ≥ w T φ(x i , y 1 i )))<label>(2)</label></formula><p>I(x) is the indicator function. It equals to 1 if and only if x ≥ 0. I(j) = 1 means that the candidate which is less 'similar' to the reference is ranked higher than the reference. Intuitively, Loss( w) is the weighted sum of all the wrongly ranked candidates.</p><p>It is difficult to optimize Loss( w) because Loss( w) is discontinuous. We make a relaxation here 2 :</p><formula xml:id="formula_4">L( w) = n i=1 k j=1 ((s(x i , y 1 i ) − s(x i , y j i )) * 1 1 + e − w T (φ(x i ,y j i )−φ(x i ,y 1 i )) ) ≤ 1 2 n i=1 k j=1 ((s(x i , y 1 i ) − s(x i , y j i )) * I( w T φ(x i , y j i ) ≥ w T φ(x i , y 1 i ))) = 1 2 Loss( w)<label>(3)</label></formula><p>From the equations above we can see that 2L( w) is the upper bound of our loss function Loss( w). Therefore we can optimize L( w) to ap- proximate Loss( w). We can use optimization methods like gradient descent to get the w that minimize the loss func- tion. Because L is not convex, it may go into a lo- cal minimum. In our experiment we held out 10% data as the develop set and try random initializa- tion to decide the initial w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Features for Re-ranking</head><p>One advantage of the re-ranking phase is that it can now use features related to candidates. There- fore, we can use a variety of features. We list them as follows.</p><p>1. The coarse-grained ranking score from the graph random walk phase. From the de- scription of the previous section we know that this score is the probability a 'walker' 'walk' from the full form node to the current candi- date. This is a coarse-grained score because it can only use the information of words in- side the window. However, it is still informa- tive because in the re-ranking phase we can- not collect this information directly. <ref type="bibr">2</ref> To prove this we need the following two inequalities: 1) when x ≥ 0, I(x) ≤ 2 1+e −x and 2) s(xi,</p><formula xml:id="formula_5">y 1 i ) − s(xi, y j i ) ≥ 0.</formula><p>2. The character uni-grams and bi-grams in the candidate. This kind of feature cannot be used in the traditional character tagging methods.</p><p>3. The language model score of the candi- date. In our experiment, we train a bi-gram language model using Laplace smoothing on the Chinese Gigaword Data 3 .</p><p>4. The length of the candidate. Intuitively, abbreviations tend to be short. Therefore length can be an important feature for the re- ranking.</p><p>5. The degree of ambiguity of the candidate.</p><p>We first define the degree of ambiguity d i of a character c i as the number of identical words that contain the character. We then define the degree of ambiguity of the candidate as the sum of all d i in the candidates. We need a dic- tionary to extract this feature. We collect all words in the PKU data of the second Interna- tional Chinese Word Segmentation Bakeoff 4 .</p><p>6. Whether the candidate is in a word dictio- nary. We use the PKU dictionary in feature 5.</p><p>7. Whether all bi-grams are in a word dictio- nary. We use the PKU dictionary in feature 5.</p><p>8. Adjacent Variety(AV) of the candidate. We define the left AV of the candidate as the probability that in a corpus the character in front of the candidate is a character in the full form. For example if we consider the full form ""(Peking University) and the candidate "", then the left AV of "" is the probability that the character preced- ing "" is '' or '' or '' or '' in a corpus. We can similarly define the right AV, with respect to characters follow the can- didate.</p><p>The AV feature is very useful because in some cases a substring of the full form may have a con- fusingly high frequency. In the example of " "(Peking University), an article in the corpus may mention ""(Peking University) and ""(Tokyo University) at the same time. Then the substring "" may be included in the candidate generation phase for " " with a high frequency. Because the left AV of " " is high, the re-ranker can easily detect this false candidate.</p><p>In practice, all the features need to be scaled in order to speed up training. There are many ways to scale features. We use the most intuitive scal- ing method. For a feature value x, we scale it as (x−mean)/(max−min). Note that for language model score and the score of random walk phase, we scale based on their log value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Evaluation metrics</head><p>For the dataset, we collect 3210 abbreviation pairs from the Chinese Gigaword corpus. The abbre- viation pairs include noun phrases, organization names and some other types. The Chinese Gi- gaword corpus contains news texts from the year 1992 to 2007. We only collect those pairs whose full form and corresponding abbreviation appear in the same article for at least one time. For full forms with more than one reasonable reference, we keep the most frequently used one as its refer- ence. We use 80% abbreviation pairs as the train- ing data and the rest as the testing data.</p><p>We use the top-K accuracy as the evaluation metrics. The top-K accuracy is widely used as the measurement in previous work <ref type="bibr" target="#b23">(Tsuruoka et al., 2005;</ref><ref type="bibr" target="#b21">Sun et al., 2008</ref><ref type="bibr" target="#b20">Sun et al., , 2009</ref><ref type="bibr" target="#b27">Zhang et al., 2012)</ref>. It measures what percentage of the reference abbre- viations are found if we take the top k candidate abbreviations from all the results. In our experi- ment, we compare the top-5 accuracy with base- lines. We choose the top-10 candidates from the graph random walk are considered in re-ranking phase and the measurement used is top-1 accuracy because the final aim of the algorithm is to detect the exact abbreviation, rather than a list of candi- dates. <ref type="table" target="#tab_1">Table 3</ref> shows examples of the candidates. In our algorithm we further reduce the search space to only incorporate 10 candidates from the candidate generation phase. K Top-K Accuracy 1 6.84% 2 19.35% 3 49.01% 4 63.70% 5 73.60% <ref type="table">Table 4</ref>: Top-5 accuracy of the candidate genera- tion phase</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Candidate List</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with baselines</head><p>We first show the top-5 accuracy of the candidate generation phase <ref type="table">Table 4</ref>. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase.</p><p>We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task ( <ref type="bibr" target="#b23">Tsuruoka et al., 2005;</ref><ref type="bibr" target="#b21">Sun et al., 2008</ref><ref type="bibr" target="#b20">Sun et al., , 2009</ref><ref type="bibr" target="#b27">Zhang et al., 2012</ref>). We choose the 'SK' labeling strategy which is used in <ref type="bibr" target="#b20">Sun et al. (2009)</ref>; <ref type="bibr" target="#b27">Zhang et al. (2012)</ref>. The 'SK' labeling strategy gives each character a label in the character sequence, with 'S' represents 'Skip' and 'K' represents 'Keep'. Same with <ref type="bibr" target="#b27">Zhang et al. (2012)</ref>, we use the Con- ditional Random Fields (CRFs) model in the se- quence labeling process.</p><p>The baseline method mainly uses the charac- ter context information to generate the candidate abbreviation. To be fair we use the same fea- ture set in <ref type="bibr" target="#b20">Sun et al. (2009)</ref>; <ref type="bibr" target="#b27">Zhang et al. (2012)</ref>. One drawback of the sequence labeling method is that it relies heavily on the character context in the full form. With the number of new abbrevi- ations grows rapidly (especially in social media like Facebook or twitter), it is impossible to let the model 'remember' all the character contexts. Our method is different from theirs, we use a more in- tuitive way which finds the list of candidates di- rectly from a natural corpus. <ref type="table">Table 5</ref> shows the comparison of the top-5 accu- racy. We can see that our method outperforms the baseline methods. The baseline model performs well when using character features (Column 3). However, it performs poorly without the charac- ter features (Column 2). In contrast, without the character features, our method (Column 4) works much better than the sequence labeling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full form</head><p>Reference Generated Candidates #Enum #Now (Depart- ment of International Politics)  <ref type="table">Table 5</ref>: Comparison of the baseline method and our method. CRF-char ('-' means minus) is the baseline method without character features. CRF is the baseline method. Our-char is our method without character features. We define character features as the features that consider the charac- ters from the original full form as their parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance with less training data</head><p>One advantage of our method is that it only requires weak supervision. The baseline method needs plenty of manually collected full-abbreviation pairs to learn a good model. In our method, the candidate generation and coarse-grained ranking is totally unsupervised. The re-ranking phase needs training instances to decide the parameters. However we can use a very small amount of training data to get a reasonably good model. <ref type="figure">Figure 2</ref> shows the result of using different size of training data. We can see that the performance of the baseline methods drops rapidly when there are less training data. In contrast, when using less training data, our method does not suffer that much.</p><p>Figure 2: Top-1 accuracy when changing the size of training data. For example, "50%" means "us- ing 50% of all the training data".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with previous work</head><p>We compare our method with the method in the previous work DPLVM+GI in <ref type="bibr" target="#b20">Sun et al. (2009)</ref>, which outperforms <ref type="bibr" target="#b23">Tsuruoka et al. (2005)</ref>; <ref type="bibr" target="#b21">Sun et al. (2008)</ref>. We also compare our method with the web-based method CRF+WEB in <ref type="bibr" target="#b27">Zhang et al. (2012)</ref>. Because the comparison is performed on different corpora, we run the two methods on our data. <ref type="table">Table 6</ref> shows the top-1 accuracy. We can see that our method outperforms the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Top-K Accuracy DPLVM+GI 53.29% CRF+WEB 54.02% Our method 55.61% <ref type="table">Table 6</ref>: Comparison with previous work. The search results of CRF+WEB is based on March 9, 2014 version of the Baidu search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Error Analysis</head><p>We perform cross-validation to find the errors and list the two major errors below:</p><p>1. Some full forms may correspond to more than one acceptable abbreviation. In this case, our method may choose the one that is indeed used as the full form's abbreviation in news texts, but not the same as the standard reference abbreviations. The reason for this phenomenon may lie in the fact that the veri- fication data we use is news text, which tends to be formal. Therefore when a reference is often used colloquially, our method may miss it. We can relieve this by changing the corpus we use.</p><p>2. Our method may provide biased information when handling location sensitive phrases. Not only our system, the system of <ref type="bibr" target="#b20">Sun et al. (2009)</ref>; <ref type="bibr" target="#b27">Zhang et al. (2012)</ref> also shows this phenomenon. An example is the case of " " (Democracy League of Hong Kong). Because most of the news is about news in mainland China, it is hard for the model to tell the difference between the ref- erence "" and a false candidate " "(Democracy League of China).</p><p>Another ambiguity is ""(Tsinghua University), which has two abbreviations " " and " ". This happens because the full form itself is ambiguous. Word sense dis- ambiguation can be performed first to handle this kind of problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Abbreviation generation has been studied during recent years. At first, some approaches maintain a database of abbreviations and their correspond- ing "full form" pairs. The major problem of pure database-building approach is obvious. It is im- possible to cover all abbreviations, and the build- ing process is quite laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as "abbreviation generation".</p><p>There is research on using heuristic rules for generating abbreviations <ref type="bibr" target="#b3">Barrett and Grems (1960)</ref>; <ref type="bibr" target="#b4">Bourne and Ford (1961)</ref>; <ref type="bibr" target="#b22">Taghva and Gilbreth (1999)</ref>; <ref type="bibr" target="#b16">Park and Byrd (2001)</ref>; <ref type="bibr" target="#b24">Wren et al. (2002);</ref><ref type="bibr" target="#b8">Hearst (2003)</ref>. Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another.</p><p>Recent studies of abbreviation generation have focused on the use of machine learning tech- niques. <ref type="bibr" target="#b21">Sun et al. (2008)</ref> proposed an SVM ap- proach. <ref type="bibr" target="#b23">Tsuruoka et al. (2005)</ref>; <ref type="bibr" target="#b20">Sun et al. (2009)</ref> formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of method cannot fit the need for abbreviation gen- eration in social media texts where the amount of abbreviations grows fast.</p><p>Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating ab- breviations. <ref type="bibr" target="#b0">Adar (2004)</ref> proposed methods to de- tect such pairs from biomedical documents. <ref type="bibr" target="#b9">Jain et al. (2007)</ref> used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search en- gine backend. The ordinary approaches do not have access to search engine internals. <ref type="bibr" target="#b27">Zhang et al. (2012)</ref> used web search engine information to re- rank the candidate abbreviations generated by sta- tistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of us- ing collective information, which varies from time to time.</p><p>Some of the previous work that relate to ab- breviations focuses on the task of "abbreviation disambiguation", which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used <ref type="bibr" target="#b16">(Park and Byrd, 2001;</ref><ref type="bibr" target="#b5">HaCohen-Kerner et al., 2008;</ref><ref type="bibr" target="#b26">Yu et al., 2006;</ref><ref type="bibr" target="#b1">Ao and Takagi, 2005</ref>). We focus on another aspect. We want to find the abbreviation given the full form. Besides, <ref type="bibr" target="#b19">Sun et al. (2013)</ref> also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work.</p><p>One related research field is text normalization, with many outstanding works ( <ref type="bibr" target="#b17">Sproat et al., 2001;</ref><ref type="bibr" target="#b2">Aw et al., 2006;</ref><ref type="bibr" target="#b7">Hassan and Menezes, 2013;</ref><ref type="bibr" target="#b12">Ling et al., 2013;</ref><ref type="bibr" target="#b25">Yang and Eisenstein, 2013)</ref>. While the two tasks share similarities, abbreviation pre- diction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first gener- ate a list of abbreviation candidates using the co- occurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we get the candidate lists, we can use the features related to the candidates. We use a similarity sensitive re-rank method to get the final abbreviation. Experiments show that our method outperforms the previous systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Algorithm for constructing bipartite graphs</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Generated Candidates. #Enum is the number of candidates generated by enumerating all possi- ble candidates. #Now is the number of candidates generated by our method.</figDesc><table>When we add character features, our method (Col-
umn 5) still outperforms the sequence labeling 
method. 

K CRF-char Our-char CRF 
Our 
1 38.00% 
48.60% 53.27% 55.61% 
2 38.16% 
70.87% 
65.89% 73.10% 
3 39.41% 
81.78% 
72.43% 81.96% 
4 55.30% 
87.54% 
78.97% 87.57% 
5 62.31% 
89.25% 
81.78% 89.27% 

</table></figure>

			<note place="foot" n="3"> http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2003T09 4 http://www.sighan.org/bakeoff2005/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partly supported by Na-tional Natural Science Foundation of <ref type="bibr">China (No.61370117,61333018,61300063</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sarad: A simple and robust abbreviation dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="533" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alice: an algorithm to extract abbreviations from medline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="576" to="586" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A phrase-based statistical model for sms text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abbreviating words systematically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="323" to="324" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A study of methods for systematically abbreviating english words and names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="538" to="552" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combined one sense disambiguation of abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hacohen-Kerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peretz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th</title>
		<meeting>the 46th</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<imprint>
			<biblScope unit="page" from="61" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Social text normalization using contextual graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple algorithm for identifying abbreviation definitions in biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hearst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azzam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acronym-expansion recognition and ranking on the web</title>
	</analytic>
	<monogr>
		<title level="m">Information Reuse and Integration</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="214" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying manipulated offerings on review portals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1933" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Paraphrasing 4 microblog normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automatic acronym acquisition and term variation management within domain-specific texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nenadi´cnenadi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Spasi´cspasi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananiadou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Third International Conference on Language Resources and Evaluation (LREC2002)</title>
		<imprint>
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Norris</surname></persName>
		</author>
		<title level="m">Markov chains. Number</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hybrid text mining for finding abbreviations and their definitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Byrd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 conference on empirical methods in natural language processing</title>
		<meeting>the 2001 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Normalization of non-standard words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="333" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named entity transliteration with comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized abbreviation prediction with negative full forms and its application on improving chinese web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust approach to abbreviating terms: A discriminative latent variable model with global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="905" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="602" to="611" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing acronyms and their definitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taghva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilbreth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="191" to="198" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A machine learning approach to acronym generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics</title>
		<meeting>the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of information in medicine</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="426" to="434" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A log-linear model for unsupervised text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilbur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="380" to="404" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Constructing Chinese abbreviation dictionary: A stacked approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3055" to="3070" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
