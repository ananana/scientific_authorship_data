<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Agreement for Bidirectional Word Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Maosong</roleName><forename type="first">Sun</forename><forename type="middle">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung R&amp;D Institute of China</orgName>
								<address>
									<postCode>100028</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Agreement for Bidirectional Word Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-to-one mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on Chinese-English translation show that joint training with generalized agreement achieves significant improvements over two state-of-the-art alignment methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word alignment is a natural language process- ing task that aims to specify the correspondence between words in two languages ( <ref type="bibr" target="#b0">Brown et al., 1993)</ref>. It plays an important role in statistical machine translation (SMT) as word-aligned bi- lingual corpora serve as the input of translation rule extraction ( <ref type="bibr" target="#b8">Koehn et al., 2003;</ref><ref type="bibr" target="#b3">Chiang, 2007;</ref><ref type="bibr" target="#b4">Galley et al., 2006</ref>; <ref type="bibr" target="#b12">Liu et al., 2006</ref>).</p><p>Although state-of-the-art generative alignment models ( <ref type="bibr" target="#b0">Brown et al., 1993;</ref><ref type="bibr" target="#b16">Vogel et al., 1996)</ref> have been widely used in practical SMT systems, they fail to model the symmetry of word align- ment. While word alignments in real-world bi- lingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, many- to-one, and many-to-many links), these models as- sume that each target word is aligned to exactly * Corresponding author: Yang Liu. one source word. To alleviate this problem, heuris- tic methods (e.g., grow-diag-final) have been pro- posed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments <ref type="bibr" target="#b13">(Och and Ney, 2003;</ref><ref type="bibr" target="#b7">Koehn and Hoang, 2007</ref>).</p><p>Instead of using heuristic symmetrization, <ref type="bibr" target="#b9">Liang et al. (2006)</ref> introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both uni- directional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves align- ment accuracy significantly.</p><p>However, enforcing agreement in joint training faces a major problem: the two models are restrict- ed to one-to-one alignments ( <ref type="bibr" target="#b9">Liang et al., 2006</ref>). This significantly limits the translation accuracy, especially for distantly-related language pairs such as Chinese-English (see Section 5). Although pos- terior decoding can potentially address this prob- lem, <ref type="bibr" target="#b9">Liang et al. (2006)</ref> find that many-to-many alignments occur infrequently because posteriors are sharply peaked around the Viterbi alignments. We believe that this happens because their model imposes a hard constraint on agreement: the two models must share the same alignment when esti- mating the parameters by calculating the products of alignment posteriors (see Section 2).</p><p>In this work, we propose a general framework for imposing agreement constraints in joint train- ing of unidirectional models. The central idea is to use the expectation of a loss function, which mea- sures the disagreement between two models, to replace the original probability of agreement. This allows for many possible ways to quantify agree- ment. Experiments on Chinese-English translation show that our approach outperforms two state-of- the-art baselines significantly.   <ref type="figure">Figure 1</ref>: Comparison of (a) independent training without agreement, (b) joint training with agreement, and (c) joint training with generalized agreement. Bold squares are gold-standard links and solid squares are model predictions. The Chinese and English sentences are segmented into phrases in (c). Joint training with agreement achieves a high precision but generally only produces one-to-one alignments. We propose generalized agreement to account for not only the consensus between asymmetric alignments, but also the conformity of alignments to other latent structures such as phrase segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Asymmetric Alignment Models</head><p>Given a source-language sentence e ≡ e I 1 = e 1 , . . . , e I and a target-language sentence f ≡ f J 1 = f 1 , . . . , f J , a source-to-target translation model ( <ref type="bibr" target="#b0">Brown et al., 1993;</ref><ref type="bibr" target="#b16">Vogel et al., 1996)</ref> can be defined as</p><formula xml:id="formula_0">P (f |e; θ 1 ) = a 1 P (f , a 1 |e; θ 1 )<label>(1)</label></formula><p>where a 1 denotes the source-to-target alignment and θ 1 is the set of source-to-target translation model parameters. Likewise, the target-to-source translation model is given by</p><formula xml:id="formula_1">P (e|f ; θ 2 ) = a 2 P (e, a 2 |f ; θ 2 )<label>(2)</label></formula><p>where a 2 denotes the target-to-source alignment and θ 2 is the set of target-to-source translation model parameters.</p><p>Given a training set D = {{f (s) , e (s) } S s=1 , the two models are trained independently to maximize the log-likelihood of the training data for each direction, respectively:</p><formula xml:id="formula_2">L(θ 1 ) = S s=1 log P (f (s) |e (s) ; θ 1 ) (3) L(θ 2 ) = S s=1 log P (e (s) |f (s) ; θ 2 )<label>(4)</label></formula><p>One key limitation of these generative models is that they are asymmetric: each target word is restricted to be aligned to exactly one source word (including the empty cept) in the source- to-target direction and vice versa. This is un- desirable because most real-world word align- ments are symmetric, in which one-to-one, one- to-many, many-to-one, and many-to-many links are usually mixed. See <ref type="figure">Figure 1</ref>(a) for example. Therefore, a number of heuristic symmetrization methods such as intersection, union, and grow- diag-final have been proposed to combine asym-metric alignments <ref type="bibr" target="#b13">(Och and Ney, 2003;</ref><ref type="bibr" target="#b7">Koehn and Hoang, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Alignment by Agreement</head><p>Rather than using heuristic symmetrization meth- ods, <ref type="bibr" target="#b9">Liang et al. (2006)</ref> propose a principled approach to jointly training of the two models via enforcing agreement:</p><formula xml:id="formula_3">J(θ 1 , θ 2 ) = S s=1 log P (f (s) |e (s) ; θ 1 ) + log P (e (s) |f (s) ; θ 2 ) + log a P (a|f (s) , e (s) ; θ 1 ) × P (a|e (s) , f (s) ; θ 2 )<label>(5)</label></formula><p>Note that the last term in Eq. <ref type="formula" target="#formula_3">(5)</ref> encourages the two models to agree on asymmetric alignments. While this strategy significantly improves align- ment accuracy, the joint model is prone to generate one-to-one alignments because it imposes a hard constraint on agreement: the two models must share the same alignment when estimating the parameters by calculating the products of align- ment posteriors. In <ref type="figure">Figure 1</ref>(b), the two one- to-one alignments are almost identical except for one link. This makes the posteriors to be sharply peaked around the Viterbi alignments ( <ref type="bibr" target="#b9">Liang et al., 2006</ref>). As a result, the lack of many-to-many alignments limits the benefits of joint training to end-to-end machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalized Agreement for Bidirectional Alignment</head><p>Our intuition is that the agreement between two alignments can be defined as a loss function, which enables us to consider various ways of quantification (Section 3.1) and even to incorpo- rate the dependency between alignments and oth- er latent structures such as phrase segmentations (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Agreement between Word Alignments</head><p>The key idea of generalizing agreement is to lever- age loss functions that measure the difference be- tween two unidirectional alignments. For exam- ple, the last term in Eq. <ref type="formula" target="#formula_3">(5)</ref> can be re-written as</p><formula xml:id="formula_4">a P (a|f (s) , e (s) ; θ 1 )P (a|e (s) , f (s) ; θ 2 ) = a 1 a 2 P (a 1 |f (s) , e (s) ; θ 1 ) × P (a 2 |e (s) , f (s) ; θ 2 ) × δ(a 1 , a 2 ) (6)</formula><p>Note that the last term in Eq. <ref type="formula">(6)</ref> is actually the expected value of agreement:</p><formula xml:id="formula_5">E a 1 |f (s) ,e (s) ;θ 1 E a 2 |e (s) ,f (s) ;θ 2 δ(a 1 , a 2 ) (7)</formula><p>Our idea is to replace δ(a 1 , a 2 ) in Eq. <ref type="formula">(6)</ref> with an arbitrary loss function ∆(a 1 , a 2 ) that measures the difference between a 1 and a 2 . This gives the new joint training objective with generalized agreement:</p><formula xml:id="formula_6">J(θ 1 , θ 2 ) = S s=1 log P (f (s) |e (s) ; θ 1 ) + log P (e (s) |f (s) ; θ 2 ) − log a 1 a 2 P (a 1 |f (s) , e (s) ; θ 1 ) × P (a 2 |e (s) , f (s) ; θ 2 ) × ∆(a 1 , a 2 )<label>(8)</label></formula><p>Obviously, <ref type="bibr" target="#b9">Liang et al. (2006)</ref>'s training objec- tive is a special case of our framework. We refer to its loss function as hard matching:</p><formula xml:id="formula_7">∆ HM (a 1 , a 2 ) = 1 − δ(a 1 , a 2 )<label>(9)</label></formula><p>We are interested in developing a soft version of the hard matching loss function because this will help to produce many-to-many symmetric align- ments. For example, in <ref type="figure">Figure 1</ref>(c), the two align- ments share most links but still allow for dis- agreed links to capture one-to-many and many-to- one links. Note that the union of the two asymmet- ric alignments is almost the same with the gold- standard alignment in this example.</p><p>While there are many possible ways to define a soft matching loss function, we choose the dif- ference between disagreed and agreed link counts because it is easy and efficient to calculate during search:  <ref type="figure">Figure 2</ref>: Generalized agreement between word alignments and phrase segmentations. The Chinese and English sentences are segmented into phrases using B (beginning), I (internal), E (ending), S (single) labels. We expect that word alignment does not violate the phrase segmentation. The word "unofficial" in the C → E alignment is labeled with "-" because "unofficial" and "2002" belong to the same English phrase but their counterparts are separated in two Chinese phrases. Words that do not violate the phrase alignment are labeled with "+". See Section 3.2 for details.</p><formula xml:id="formula_8">∆ SM (a 1 , a 2 ) = |a 1 ∪ a 2 | − 2|a 1 ∩ a 2 | (10) China 's head of state will attend the unofficial 2002 APEC summit . 中 国 元 首 将 参 加 2002 亚 太 经 合 组 织 非 正 式 峰 会 。 China 's head of state will attend the unofficial 2002 APEC summit . 中 国 元 首 将 参 加 2002 亚 太 经 合 组 织 非 正 式 峰 会 。 CE EC B E B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Agreement between Word Alignments and Phrase Segmentations</head><p>Our framework is very general and can be extended to include the agreement between word alignment and other latent structures such as phrase segmentations. The words in a Chinese sentence often con- stitute phrases that are translated as units in English and vice versa. Inspired by the alignment consistency constraint widely used in translation rule extraction ( <ref type="bibr" target="#b8">Koehn et al., 2003)</ref>, we make the following assumption to impose a structural agreement constraint between word alignment and phrase segmentation: source words in one source phrase should be aligned to target words belong- ing to the same target phrase and vice versa.</p><p>For example, consider the C → E alignment in <ref type="figure">Figure 2</ref>. We segment Chinese and English sen- tences into phrases, which are sequences of con- secutive words. Since "2002" and "APEC" belong to the same English phrase, their counterparts on the Chinese side should also belong to one phrase.</p><p>While this assumption can potentially improve the correlation between word alignment and phrase-based translation, a question naturally a- rises: how to segment sentences into phrases? Instead of leveraging chunking, we treat phrase segmentation as a latent variable and train the joint alignment and segmentation model from unlabeled data in an unsupervised way.</p><p>Formally, given a target-language sentence f ≡</p><formula xml:id="formula_9">f J 1 = f 1 , . . . , f J , we introduce a latent variable b ≡ b J 1 = b 1 , .</formula><p>. . , b J to denote a phrase segmen- tation. Each label b j ∈ {B, I, E, S}, where B denotes the beginning word of a phrase, I denotes the internal word, E denotes the ending word, and S denotes the one-word phrase. <ref type="figure">Figure 2</ref> shows the label sequences for the sentence pair.</p><p>We use a first-order HMM to model phrase seg- mentation of a target sentence:</p><formula xml:id="formula_10">P (f ; λ 1 ) = b 1 P (f , b 1 ; λ 1 )<label>(11)</label></formula><p>Similarly, the hidden Markov model for the phrase segmentation of the source sentence can be defined as</p><formula xml:id="formula_11">P (e; λ 2 ) = b 2 P (e, b 2 ; λ 2 )<label>(12)</label></formula><p>Then, we can combine word alignment and phrase segmentation and define the joint training objective as</p><formula xml:id="formula_12">J(θ 1 , θ 2 , λ 1 , λ 2 ) = S s=1 log P (f (s) |e (s) ; θ 1 ) + 1: procedure VITERBIEM(D) 2:</formula><p>Initialize Θ (0)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for all k = 1, . . . , K do </p><formula xml:id="formula_13">ˆ H (k) ← SEARCH(D, Θ (k−1) ) 5: Θ (k) ← UPDATE(D, ˆ H (k) ) 6:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>returnˆHreturnˆ returnˆH (K) , Θ (K) <ref type="bibr">k)</ref> is the set of model parameters at the k-th iteration, H (k) is the set of Viterbi latent variables at the k-th iteration.</p><note type="other">8: end procedure Algorithm 1: A Viterbi EM algorithm for learning the joint word alignment and phrase segmentation model from bilingual corpus. D is a bilingual cor- pus, Θ (</note><formula xml:id="formula_14">log P (e (s) |f (s) ; θ 2 ) + log P (f (s) ; λ 1 ) + log P (e (s) ; λ 2 ) − log E(f (s) , e (s) , θ 1 , θ 2 , λ 1 , λ 2 )<label>(13)</label></formula><p>where the expected loss is given by</p><formula xml:id="formula_15">E(f (s) , e (s) , θ 1 , θ 2 , λ 1 , λ 2 ) = a 1 a 2 b 1 b 2 P (a 1 |f (s) , e (s) ; θ 1 ) × P (a 2 |e (s) , f (s) ; θ 2 ) × P (b 1 |f (s) ; λ 1 ) × P (b 2 |e (s) ; λ 2 ) × ∆(a 1 , a 2 , b 1 , b 2 )<label>(14)</label></formula><p>We define a new loss function segmentation violation to measure the degree that an alignment violates phrase segmentations.</p><formula xml:id="formula_16">∆ SV (a 1 , a 2 , b 1 , b 2 ) = J−1 j=1 β(a 1 , j, b 1 , b 2 ) + I−1 i=1 β(a 2 , i, b 2 , b 1 )<label>(15)</label></formula><p>where β(a 1 , j, b 1 , b 2 ) evaluates whether two links l 1 = (j, a j ) and l 2 = (j + 1, a j+1 ) violate the phrase segmentation:</p><p>1. f j and f j+1 belong to one phrase but e a j and e a j+1 belong to two phrases, or 2. f j and f j+1 belong to two phrases but e a j and e a j+1 belong to one phrase.</p><p>The β function returns 1 if there is violation and 0 otherwise. </p><note type="other">procedure SEARCH(D, Θ) 2:</note><formula xml:id="formula_17">ˆ H ← ∅ 3:</formula><p>for all s ∈ {1, . . . , S} do 4:</p><formula xml:id="formula_18">ˆ a 1 ← ALIGN(f (s) , e (s) , θ 1 ) 5: ˆ a 2 ← ALIGN(e (s) , f (s) , θ 2 ) 6: ˆ b 1 ← SEGMENT(f (s) , λ 1 ) 7: ˆ b 2 ← SEGMENT(e (s) , λ 2 ) 8: h 0 ← ˆ a 1 , ˆ a 2 , ˆ b 1 , ˆ b 2 9: ˆ h ←HILLCLIMB(f (s) , e (s) , h 0 , Θ) 10: ˆ H ← ˆ H ∪ { ˆ h} 11:</formula><p>end for 12:</p><p>returnˆHreturnˆ returnˆH 13: end procedure Algorithm 2: A search algorithm for finding the Viterbi latent variables. ˆ a 1 andâandˆandâ 2 denote Viter- bi alignments, ˆ b 1 andˆbandˆ andˆb 2 denote Viterbi seg- mentations. They form a starting point h 0 for the hill climbing algorithm, which keeps chang- ing alignments and segmentations until the model score does not increase. ˆ h is the final set of Viterbi latent variables for one sentence.</p><p>In <ref type="figure">Figure 2</ref>, we use "+" to label words that do not violate the phrase segmentations and "-" to label violations.</p><p>In practice, we combine the two loss functions to enable word alignment and phrase segmentation to benefit each other in a joint search space: <ref type="bibr" target="#b9">Liang et al. (2006)</ref> indicate that it is intractable to train the joint model. For simplicity and efficien- cy, they exploit a simple heuristic procedure that leverages the product of posterior marginal prob- abilities. The intuition behind the heuristic is that links on which two models disagree should be dis- counted because the products of the marginals are small ( <ref type="bibr" target="#b9">Liang et al., 2006</ref>).</p><formula xml:id="formula_19">∆ SM+SV (a 1 , a 2 , b 1 , b 2 ) = ∆ SM (a 1 , a 2 ) + ∆ SV (a 1 , a 2 , b 1 , b 2 ) (16)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>Unfortunately, it is hard to develop a similar heuristic for our model that allows for arbitrary loss functions. Alternatively, we resort to a Viterbi EM algorithm, as shown in Algorithm 1. The algorithm takes the training data D = {{f (s) , e (s) } S s=1 as input (line 1). We use</p><formula xml:id="formula_20">Θ (k) = θ (k) 1 , θ (k) 2 , λ (k) 1 , λ (k)</formula><p>2 to denote the set of model parameters at the k-th iteration. After initializing the model parameters (line 2), the algorithm alter- nates between searching for the Viterbi alignments and segmentationsˆHsegmentationsˆ segmentationsˆH (k) using the SEARCH proce- dure (line 4) and updating model parameters using the UPDATE procedure (line 5). The algorithm ter- minates after running for K iterations.</p><p>It is challenging to search for the Viterbi align- ments and segmentations because of complicat- ed structural dependencies. As shown in Al- gorithm 2, our strategy is first to find Viter- bi alignments and segmentations independently using the ALIGN and SEGMENT procedures (lines 4-7), which then serve as a starting point for the HILLCLIMB procedure (lines 8-9). <ref type="figure" target="#fig_3">Figure 3</ref> shows three operators we use in the HILLCLIMB procedure. The MOVE operator moves a link in an alignment, the MERGE oper- ator merges two phrases into one phrase, and the SPLIT operator splits one phrase into two small- er phrases. Note that each operator can be further divided into two variants: one for the source side and another for the target side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We evaluate our approach on Chinese-English alignment and translation tasks.</p><p>The training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit <ref type="bibr" target="#b15">(Stolcke, 2002</ref>) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD cor- pus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. <ref type="bibr">1</ref> The evalu- ation metric is alignment error rate (AER) <ref type="bibr" target="#b13">(Och and Ney, 2003)</ref>. For translation evaluation, we used the NIST 2006 dataset as the development set and the <ref type="bibr">NIST 2002</ref><ref type="bibr">NIST , 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref><ref type="bibr">NIST , and 2008</ref> datasets as the test sets. The evaluation metric is case-insensitive BLEU ( <ref type="bibr" target="#b14">Papineni et al., 2002</ref>).</p><p>We used both phrase-based ( <ref type="bibr" target="#b8">Koehn et al., 2003)</ref> and hierarchical phrase-based <ref type="bibr" target="#b3">(Chiang, 2007)</ref> translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses ( <ref type="bibr" target="#b7">Koehn and Hoang, 2007)</ref>. For the hierarchical phrase-based model, we used an in- house re-implementation on par with state-of-the- art open-source decoders.</p><p>We compared our approach with two state-of- the-art generative alignment models:</p><p>1. GIZA++ <ref type="bibr" target="#b13">(Och and Ney, 2003)</ref>: unsupervised training of IBM models <ref type="bibr" target="#b0">(Brown et al., 1993</ref>) and the HMM model ( <ref type="bibr" target="#b16">Vogel et al., 1996</ref>) us- ing EM, 2. BERKELEY ( <ref type="bibr" target="#b9">Liang et al., 2006</ref>): unsuper- vised training of joint HMMs using EM.</p><p>For GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments. For BERKELEY, we trained joint HMMs using the default setting. The hyper- parameter of posterior decoding was optimized on the development set.</p><p>We used first-order HMMs for both word alignment and phrase segmentation. Our joint alignment and segmentation model were trained using the Viterbi EM algorithm for five iterations. Note that the Chinese-to-English and English-to- Chinese alignments are generally non-identical but share many links (see <ref type="figure">Figure 1(c)</ref>). Then, we used the grow-diag-final heuristic to generate symmetric alignments.  <ref type="table" target="#tab_2">Table 1</ref>: Comparison with GIZA++ and BERKELEY. "word-word" denotes the agreement between Chinese-to-English and English-to-Chinese word alignments. "word-phrase" denotes the agreement be- tween word alignments and phrase segmentations. "HM" denotes the hard matching loss function, "SM" denotes soft matching, and "SV" denotes segmentation violation. "GDF" denotes grow-diag-final. "PD" denotes posterior decoding.  <ref type="table">Table 2</ref>: Results on (hierarchical) phrase-based translation. The evaluation metric is case-insensitive BLEU. "HM" denotes the hard matching loss function, "SM" denotes soft matching, and "SV" denotes segmentation violation. "*": significantly better than GIZA++ (p &lt; 0.05). "**": significantly better than GIZA++ (p &lt; 0.01). "+": significantly better than BERKELEY (p &lt; 0.05). "++": significantly better than BERKELEY (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with GIZA++ and BERKELEY</head><p>trains two models jointly with the hard-matching (i.e., HM) loss function and uses posterior decod- ing for symmetrization. For our approach, we distinguish between two variants:</p><p>1. Imposing agreement between word align- ments (i.e., word-word) that uses the soft matching loss function (i.e., SM) (see Section 3.1);</p><p>2. Imposing agreement between word align- ments and phrase segmentations (i.e., word- word, word-phrase) that uses both the soft matching and segmentation violation loss functions (i.e., SM+SV) (see Section 3.2).</p><p>We used the grow-diag-final heuristic for symmetrization.</p><p>For the alignment evaluation, we find that our approach achieves higher AER scores than the two baseline systems. One possible reason is that links in the intersection of two symmetric alignments or two symmetric models agree usually correspond to sure links in the gold-standard annotation. Our approach loosens the hard constraint on agreement and makes the posteriors less peaked around the Viterbi alignments.</p><p>For the translation evaluation, we used the phrase-based system Moses to report BLEU s- cores on the NIST 2008 test set. We find that both the two variants of our approach significantly out- performs the two baselines (p &lt; 0.01). <ref type="table">Table 2</ref> shows the results on phrase-based and hierarchical phrase-based translation systems. We find that our approach systematically outperforms GIZA++ and BERKELEY on all NIST datasets. In particular, generalizing the agreement to model the discrepancy between word alignment and phrase segmentation is consistently beneficial for improving translation quality, suggesting that it is important to introduce structural constraints into word alignment to increase the correlation between alignment and translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on (Hierarchical) Phrase-based Translation</head><p>While "SM+SV" improves over "SM" signifi- cantly on phrase-based translation, the margins on the hierarchical phrase-based system are relative- ly smaller. One possible reason is that the "SV" system loss 20.54M 69.00 <ref type="table">Table 3</ref>: Agreement evaluation of GIZA++, BERKELEY and our approach. The F1 score reflects how well two asymmetric alignments agree with each other.</p><formula xml:id="formula_21">|A C→E | |A E→C | |A C→E ∩ A E→C | F1 GIZA++ N/A</formula><p>loss function can better account for phrase-based rather than hierarchical phrase-based translation. It is possible to design new loss functions tailored to hierarchical phrase-based translation. We also find that the BLEU scores of BERKE- LEY on hierarchical phrase-based translation are much lower than those on phrase-based transla- tion. This might result from the fact that BERKE- LEY is prone to produce one-to-one alignments, which are not optimal for hierarchical phrase- based translation. <ref type="table">Table 3</ref> compares how well two asymmetric models agree with each other among GIZA++, BERKELEY and our approach. We use F1 score to measure the degree of agreement:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Agreement Evaluation</head><formula xml:id="formula_22">2|A C→E ∩ A E→C | |A C→E | + |A E→C | (17)</formula><p>where A C→E is the set of Chinese-to-English alignments on the training data and A E→C is the set of English-to-Chinese alignments. It is clear that independent training leads to low agreement and joint training results in high agree- ment. BERKELEY achieves the highest value of agreement because of the hard constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>This work is inspired by two lines of research: (1) agreement-based learning and (2) joint modeling of multiple NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Agreement-based Learning</head><p>The key idea of agreement-based learning is to train a set of models jointly by encouraging them to agree on the hidden variables ( <ref type="bibr" target="#b9">Liang et al., 2006;</ref><ref type="bibr" target="#b10">Liang et al., 2008)</ref>. This can also be seen as a particular form of posterior constraint or poste- rior regularization <ref type="bibr" target="#b6">(Graça et al., 2007;</ref><ref type="bibr" target="#b5">Ganchev et al., 2010</ref>). The agreement is prior knowledge and indirect supervision, which helps to train a more reasonable model with biased guidance.</p><p>While agreement-based learning provides a principled approach to training a generative mod- el, it constrains that the sub-models must share the same output space. Our work extends ( <ref type="bibr" target="#b9">Liang et al., 2006</ref>) to introduce arbitrary loss functions that can encode prior knowledge. As a result, <ref type="bibr" target="#b9">Liang et al. (2006)</ref>'s model is a special case of our frame- work. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase seg- mentations and parse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Joint Modeling of Multiple NLP Tasks</head><p>It is well accepted that different NLP tasks can help each other by providing additional informa- tion for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received in- tensive attention in recent years, including phrase segmentation and alignment ( <ref type="bibr" target="#b19">Zhang et al., 2003)</ref>, alignment and parsing <ref type="bibr" target="#b1">(Burkett et al., 2010)</ref>, tok- enization and translation <ref type="bibr" target="#b18">(Xiao et al., 2010)</ref>, pars- ing and translation ( , alignment and named entity recognition <ref type="bibr" target="#b2">(Chen et al., 2010;</ref><ref type="bibr" target="#b17">Wang et al., 2013</ref>).</p><p>Among them, <ref type="bibr" target="#b19">Zhang et al. (2003)</ref>'s integrat- ed search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possi- ble phrase pairs. The major difference is we train models jointly instead of integrated decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented generalized agreement for bidi- rectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. We develop a Viter- bi EM algorithm to train the joint model. Exper- iments on Chinese-English translation show that joint training with generalized agreement achieves significant improvements over two baselines for (hierarchical) phrase-based MT systems. In the fu- ture, we plan to investigate more loss functions to account for syntactic constraints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>China</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Operators used in the HILLCLIMB procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>the comparison of our approach 
with GIZA++ and BERKELEY in terms of AER 
and BLEU. GIZA++ trains two asymmetric 
models independently and uses the grow-diag-
final (i.e., GDF) for symmetrization. BERKELEY </table></figure>

			<note place="foot" n="1"> http://nlp.csai.tsinghua.edu.cn/˜ly/systems/TsinghuaAlig ner/TsinghuaAligner.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yang Liu and Maosong Sun are supported by the 863 Program (2015AA011808), the National Nat-ural Science Foundation of China <ref type="bibr">(No. 61331013 and No. 61432013)</ref>, and Samsung R&amp;D Institute of China. Huanbo Luan is supported by the Na-tional Natural Science Foundation of China (No. 61303075). This research is also supported by the Singapore National Research Foundation un-der its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme. We sincerely thank the reviewers for their valuable suggestions. We also thank Yue Zhang, Meng Zhang and Shiqi Shen for their in-sightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint parsing and alignment with weakly synchronized grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2010</title>
		<meeting>NAACL-HLT 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On jointly recognizing and aligning bilingual named entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL 2006</title>
		<meeting>COLING-ACL 2006<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzmann</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Expectation maximization and posterior constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Joao V Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factored translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="868" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2006</title>
		<meeting>HLT-NAACL 2006<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Agreement-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint parsing and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2010</title>
		<meeting>COLING 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING/ACL</title>
		<meeting>COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bleu: a methof for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hmm-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint word alignment and bilingual named entity recognition using dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2013</title>
		<meeting>ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint tokenization and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Sook</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2010</title>
		<meeting>COLING 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integrated phrase segmentation and alignment algorithm for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Knowledge Engineering</title>
		<meeting>Natural Language Processing and Knowledge Engineering</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
