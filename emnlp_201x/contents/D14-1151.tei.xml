<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Mined Coreference Chains as a Resource for a Semantic Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
							<email>heike.adel@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Mined Coreference Chains as a Resource for a Semantic Task</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1447" to="1452"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks. We extract three million coreference chains and train word embeddings on them. Then, we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F 1 on the task of antonym classification by up to .09.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>After more than a decade of work on coreference resolution, coreference resolution systems have reached a certain level of maturity (e.g., <ref type="bibr" target="#b10">Recasens et al. (2010)</ref>). While accuracy is far from perfect and many phenomena such as bridging still pose difficult research problems, the quality of the out- put of these systems is high enough to be useful for many applications.</p><p>In this paper, we propose to run coreference res- olution systems on large corpora, to collect the coreference chains found and to use them as a re- source for solving semantic tasks. This amounts to using mined coreference chains as an automat- ically compiled resource similar to the way cooc- currence statistics, dependency pairs and aligned parallel corpora are used in many applications in NLP. Coreference chains have interesting comple- mentary properties compared to these other re- sources. For example, it is difficult to distinguish true semantic similarity (e.g., "cows" -"cattle") from mere associational relatedness (e.g., "cows" -"milk") based on cooccurrence statistics. In con- trast, coreference chains should be able to make that distinction since only "cows" and "cattle" can occur in the same coreference chain, not "cows" and "milk".</p><p>As a proof of concept we compile a resource of mined coreference chains from the Gigaword corpus and apply it to the task of identifying antonyms. We induce distributed representations for words based on (i) cooccurrence statistics and (ii) mined coreference chains and show that a com- bination of both outperforms cooccurrence statis- tics on antonym identification.</p><p>In summary, we make two contributions. First, we propose to use coreference chains mined from large corpora as a resource in NLP and publish the first such resource. Second, in a proof of concept study, we show that they can be used to solve a se- mantic task -antonym identification -better than is possible with existing resources. We focus on the task of finding antonyms in this paper since antonyms usually are distributionally similar but semantically dissimilar words. Hence, it is often not possible to distinguish them from synonyms with distributional models only. In con- trast, we expect that the coreference-based repre- sentations can provide useful complementary in- formation to this task. In general, coreference- based similarity can however be used as an addi- tional feature for any task that distributional simi- larity is useful for. Thus, our coreference resource can be applied to a variety of NLP tasks, e.g. find- ing alternative names for entities (in a way similar to Wikipedia anchors) for tasks in the context of knowledge base population.</p><p>The remainder of the paper is organized as fol- lows. In Section 2, we describe how we create word embeddings and how our antonym classi- fier works. The word embeddings are then eval- uated qualitatively, quantitatively and for the task of antonym detection (Section 3). Section 4 dis- cusses related work and Section 5 concludes.</p><p>2 System description 2.1 Coreference-based embeddings Standard word embeddings derived from text data may not be able to distinguish between semantic text-based coref.-based his my, their, her, your, our he, him, himself, zechariah, ancestor woman man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehia <ref type="table">Table 1</ref>: Nearest neighbors of "his" / "woman" for text-based &amp; coreference-based embeddings association and true synonymy. As a result, syn- onyms and antonyms may be mapped to similar word vectors ( <ref type="bibr" target="#b19">Yih et al., 2012</ref>). For many NLP tasks, however, information about true synonymy or antonymy may be important.</p><p>In this paper, we develop two different word embeddings: embeddings calculated on raw text data and embeddings derived from automatically extracted coreference chains. For the calcula- tion of the vector representations, the word2vec toolkit 1 by <ref type="bibr" target="#b8">Mikolov et al. (2013)</ref> is applied. We use the skip-gram model for our experiments be- cause its results for semantic similarity are better according to <ref type="bibr" target="#b8">Mikolov et al. (2013)</ref>. We train a first model on a subset of English Gigaword data. <ref type="bibr">2</ref> In the following sections, we call the resulting embeddings text-based. To improve the seman- tic similarities of the vectors, we prepare another training text consisting of coreference chains. We use CoreNLP ( <ref type="bibr" target="#b5">Lee et al., 2011</ref>) to extract coref- erence chains from the Gigaword corpus. Then we build a skip-gram model on these coreference chains. The extracted coreference chains are pro- vided as an additional resource to this paper 3 . Al- though they have been developed using only a publicly available toolkit, we expect this resource to be helpful for other researchers since the pro- cess to extract the coreference chains of such a large text corpus takes several weeks on multi-core machines. In total, we extracted 3.1M coreference chains. 2.7M of them consist of at least two differ- ent markables. The median (mean) length of the chains is 3 (4.0) and the median (mean) length of a markable is 1 (2.7). To train word embeddings, the markables of each coreference chain are con- catenated to one text line. These lines are used as input sentences for word2vec. We refer to the re- sulting embeddings as coreference-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Antonym detection</head><p>In the following experiments, we use word em- beddings to discriminate antonyms from non- antonyms. We formalize this as a supervised clas- The following features are used to represent a pair of two words w and v:</p><p>1. cosine similarity of the text-based embed- dings of w and v;</p><p>2. inverse rank of v in the nearest text-based neighbors of w;</p><p>3. cosine similarity of the coreference-based embeddings of w and v;</p><p>4. inverse rank of v in the nearest coreference- based neighbors of w;</p><p>5. difference of <ref type="formula">(1)</ref> and <ref type="formula">(3);</ref> 6. difference of <ref type="formula">(2)</ref> and (4).</p><p>We experiment with three different subsets of these features: text-based (1 and 2), coreference- based (3 and 4) and all features. <ref type="table">Table 1</ref> lists the five nearest neighbors based on cosine similarity of text-based and coreference- based word vectors for "his" and "woman".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Qualitative analysis of word vectors</head><p>We see that the two types of embeddings cap- ture different notions of similarity. Unlike the text- based neighbors, the coreference-based neighbors have the same gender. The text-based neighbors are mutually substitutable words, but substitution seems to change the meaning more than for the coreference-based neighbors.</p><p>In <ref type="figure" target="#fig_1">Figure 1</ref>, we illustrate the vectors for some antonyms (connected by lines).</p><p>For reducing the dimensionality of the vector space to 2D, we applied the t-SNE toolkit <ref type="bibr">4</ref> . It uses stochastic neighbor embedding with a Student's t-distribution to map high dimensional vectors into a lower dimensional space (Van der Maaten and Hinton, 2008). The Figure shows that the coreference-based word embeddings are able to enlarge the distance between antonyms (especially for guilt vs. innocence and toughness vs. frailty) compared to text-based word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative analysis of word vectors</head><p>To verify that coreference-based embeddings bet- ter represent semantic components relevant to coreference, we split our coreference resource into two parts (about 85% and 15% of the data), trained embeddings on the first part and computed the co- sine similarity -both text-based and coreference- based -for each pair of words occurring in the same coreference chain in the second part. The statistics in <ref type="table" target="#tab_1">Table 2</ref> confirm that coreference-based vectors have higher similarity within chains than text-based vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental setup</head><p>We formalize antonym detection as a binary classi- fication task. Given a target word w and one of its nearest neighbors v, the classifier decides whether v is an antonym of w. Our data set is a set of pairs, each consisting of a target word w and a candi- date v. For all word types of our vocabulary, we search for antonyms using the online dictionary Merriam Webster. <ref type="bibr">5</ref> The resulting list is provided as an additional resource <ref type="bibr">6</ref> . It contains 6225 words with antonyms. Positive training examples are col- lected by checking if the 500 nearest text-based neighbors of w contain one of the antonyms listed by Webster. Negative training examples are cre- ated by replacing the antonym with a random word from the 500 nearest neighbors that is not listed as <ref type="bibr">5</ref> http://www.merriam-webster.com 6 https://code.google.com/p/cistern an antonym. By selecting both the positive and the negative examples from the nearest neighbors of the word vectors, we intend to develop a task which is hard to solve: The classifier has to find the small portion of semantically dissimilar words (i.e., antonyms) among distributionally very simi- lar words. The total number of positive and nega- tive examples is 2337 each. The data are split into training (80%), development (10%) and test (10%) sets.</p><p>In initial experiments, we found only a small difference in antonym classification performance between text-based and coreference-based fea- tures. When analyzing the errors, we realized that our rationale for using coreference-based embed- dings only applies to nouns, not to other parts of speech. This will be discussed in detail below. We therefore run our experiments in two modes: all word classification (all pairs are considered) and noun classification (only pairs are considered for which the target word is a noun). We use the Stan- ford part-of-speech tagger ( <ref type="bibr" target="#b14">Toutanova et al., 2003)</ref> to determine whether a word is a noun or not.</p><p>Our classifier is a radial basis function (rbf) sup- port vector machine (SVM). The rbf kernel per- formed better than a linear kernel in initial exper- iments. The SVM parameters C and γ are opti- mized on the development set. The representation of target-candidate pairs consists of the features described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental results and discussion</head><p>We perform the experiments with the three differ- ent feature sets described in Section 2: text-based, coreference-based and all features. <ref type="table" target="#tab_0">Table 3</ref> shows all word classification noun classification development set test set development set test set feature set</p><formula xml:id="formula_0">P R F 1 P R F 1 P R F 1 P R F 1 text-based</formula><p>. <ref type="bibr">83 .66 .74 .74 .55 .63 .91 .61 .73 .74 .51 .60 coreference-based .67 .42 .51 .65 .43 .52 .86 .47 .61 .77 .45 .57 text+coref .79 .65 .72 .75 .58 .66 .88 .70 .78 .79</ref> .61 .69  For all word classification, coreference-based features do not improve performance on the de- velopment set (e.g., F 1 is .74 for text-based vs .72 for text+coref). On the test set, however, the com- bination of all features (text+coref) has better per- formance than text-based alone: .66 vs .63.</p><p>For noun classification, using coreference- based features in addition to text-based features improves results on development set (F 1 is .78 vs .73) and test set (.69 vs .60).</p><p>These results show that mined coreference chains are a useful resource and provide infor- mation that is complementary to other methods. Even though adding coreference-based embed- dings improves performance on antonym classi- fication, the experiments also show that using only coreference-based embeddings is almost al- ways worse than using only text-based embed- dings. This is not surprising given that the amount of training data for the word embeddings is differ- ent in the two cases. Coreference chains provide only a small subset of the word-word relations that are given to the word2vec skip-gram model when applied to raw text. If the sizes of the training data sets were similar in the two cases, we would ex- pect performance to be comparable.</p><p>In the beginning, our hypothesis was that coref- erence information should be helpful for antonym classification in general. When we performed an error analysis for our initial results, we realized that this hypothesis only holds for nouns. Other types of words cooccurring in coreference chains are not more likely to be synonyms than words cooccurring in text windows. Two contexts that illustrate this point are "bright sides, but also dif- ficult and dark ones" and "a series of black and white shots" (elements of coreference chains in italics). Thus, adjectives with opposite meanings can cooccur in coreference chains just as they can cooccur in window-based contexts. For nouns, it is much less likely that the same coreference chain will contain both a noun and its antonym since - by definition -markables in a coreference chain refer to the same identical entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Traditionally, words have been represented by vectors of the size of the vocabulary with a one at the word index and zeros otherwise (one-hot vec- tors). However, this approach cannot handle un- known words <ref type="bibr" target="#b15">(Turian et al., 2010</ref>) and similari- ties among words cannot be represented ( <ref type="bibr" target="#b8">Mikolov et al., 2013)</ref>. Therefore, distributed word repre- sentations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. <ref type="bibr" target="#b8">Mikolov et al. (2013)</ref> have published word2vec, a toolkit that provides different possi- bilities to estimate word embeddings (cbow model and skip-gram model). They show that the re- sulting word vectors capture semantic and syntac- tic relationships of words. <ref type="bibr" target="#b1">Baroni et al. (2014)</ref> show that word embeddings are able to outper- form count based word vectors on a variety of NLP tasks. Recently, <ref type="bibr" target="#b6">Levy and Goldberg (2014)</ref> have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. <ref type="bibr" target="#b0">Andreas and Klein (2014)</ref> investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In ( <ref type="bibr" target="#b19">Yih et al., 2012)</ref>, the vector space representation of words is modified so that high positive similarities are as- signed to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is applied to a matrix of thesaurus entries. The val-ues representing antonyms are negated.</p><p>There has been a great deal of work on apply- ing the vector space model and cosine similarity to find synonyms or antonyms. <ref type="bibr" target="#b3">Hagiwara et al. (2006)</ref> represent each word as a vector with cooc- currence frequencies of words and contexts as el- ements, normalized by the inverse document fre- quency. The authors investigate three types of con- textual information (dependency, sentence cooc- currence and proximity) and find that a combi- nation of them leads to the most stable results. Schulte im <ref type="bibr" target="#b12">Walde and Köper (2013)</ref> build a vector space model on lexico-syntactic patterns and ap- ply a Rocchio classifier to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in different languages to build context vectors. Based on these vectors, they detect synonyms.</p><p>In contrast, there are also studies using linguis- tic knowledge from external resources: <ref type="bibr" target="#b13">Senellart and Blondel (2008)</ref> propose a method for syn- onym detection based on graph similarity in a graph generated using the definitions of a mono- lingual dictionary. <ref type="bibr" target="#b4">Harabagiu et al. (2006)</ref> rec- ognize antonymy by generating antonymy chains based on WordNet relations. <ref type="bibr" target="#b9">Mohammad et al. (2008)</ref> look for the word with the highest degree of antonymy to a given target word among five candi- dates. For this task, they use thesaurus information and the similarity of the contexts of two contrast- ing words. <ref type="bibr" target="#b7">Lin et al. (2003)</ref> use Hearst patterns to distiguish synonyms from antonyms. Work by <ref type="bibr" target="#b16">Turney (2008)</ref> is similar except that the patterns are learned.</p><p>Except for the publicly available coreference resolution system, our approach does not need ex- ternal resources such as dictionaries or bilingual corpora and no human labor is required. Thus, it can be easily applied to any corpus in any lan- guage as long as there exists a coreference resolu- tion system in this language. The pattern-based approach ( <ref type="bibr" target="#b7">Lin et al., 2003;</ref><ref type="bibr" target="#b16">Turney, 2008</ref>) dis- cussed above also needs few resources. In contrast to our work, it relies on patterns and might there- fore restrict the number of recognizable synonyms and antonyms to those appearing in the context of the pre-defined patterns. On the other hand, pat- terns could explicitely distinguish contexts typical for synonyms from contexts for antonyms. Hence, we plan to combine our coreference-based method with pattern-based methods in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we showed that mined corefer- ence chains can be used for creating word em- beddings that capture a type of semantic sim- ilarity that is different from the one captured by standard text-based embeddings. We showed that coreference-based embeddings improve per- formance of antonym classification by .09 F 1 compared to using only text-based embeddings. We achieved precision values of up to .79, recall values of up to .61 and F 1 scores of up to .69.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://code.google.com/p/word2vec 2 LDC2012T21, Agence France-Presse 2010 3 https://code.google.com/p/cistern sification task and apply SVMs (Chang and Lin, 2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 2D-positions of words in the text-based (top) and coreference-based embeddings (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for different feature sets. Best result in each column in bold. 

minimum maximum median 
text-based vectors 
-0.350 
0.998 
0.156 
coref.-based vectors 
-0.318 
0.999 
0.161 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Cosine similarity of words in the same coreference chain results for development and test sets.</figDesc><table></table></figure>

			<note place="foot" n="4"> http://homepage.tudelft.nl/19j49/t-SNE.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by DFG (grant SCHU 2246/4-2).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How much do word embeddings encode about syntax? In ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="822" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selection of effective contextual information for automatic synonym acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Negation, contrast and contradiction in text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finley</forename><surname>Lacatusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stanford&apos;s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL: Shared Task</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying synonyms among distributionally similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1492" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Workshop at ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing word-pair antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emili</forename><surname>Sapena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Semeval-2010 task 1: Coreference resolution in multiple languages</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>5th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Processing and Knowledge in the Web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="184" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic discovery of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Survey of Text Mining II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A uniform approach to analogies, synonyms, antonyms, and associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding synonyms using automatic word alignment and measures of distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="866" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polarity inducing latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1212" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
