<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured prediction models for RNN based sequence labeling in clinical text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhyuday</forename><forename type="middle">N</forename><surname>Jagannatha</surname></persName>
							<email>abhyuday@cs.umass.edu , hong.yu@umassmed.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Bedford VAMC and CHOIR</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured prediction models for RNN based sequence labeling in clinical text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="856" to="865"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication , indication, and side-effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF-LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Patient data collected by hospitals falls into two cat- egories, structured data and unstructured natural lan- guage texts. It has been shown that natural text clinical documents such as discharge summaries, progress notes, etc are rich sources of medically rel- evant information like adverse drug events, medica- tion prescriptions, diagnosis information etc. Infor- mation extracted from these natural text documents can be useful for a multitude of purposes ranging <ref type="bibr">1</ref> Code is available at https://github.com/abhyudaynj/LSTM- CRF-models from drug efficacy analysis to adverse effect surveil- lance.</p><p>A widely used method for Information Extrac- tion from natural text documents involves treating the text as a sequence of tokens. This format al- lows sequence labeling algorithms to label the rel- evant information that should be extracted. Sev- eral sequence labeling algorithms such as Condi- tional Random Fields (CRFs), Hidden Markov Mod- els (HMMs), Neural Networks have been used for information extraction from unstructured text. CRFs and HMMs are probabilistic graphical models that have a rich history of Natural Language Process- ing (NLP) related applications. These methods try to jointly infer the most likely label sequence for a given sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recently, Recurrent (RNN) or Convolutional</head><p>Neural Network (CNN) models have increasingly been used for various NLP related tasks. These Neu- ral Networks by themselves however, do not treat sequence labeling as a structured prediction prob- lem. Different Neural Network models use dif- ferent methods to synthesize a context vector for each word. This context vector contains informa- tion about the current word and its neighboring con- tent. In the case of CNN, the neighbors comprise of words in the same filter size window, while in Bidirectional-RNNs (Bi-RNN) they contain the en- tire sentence.</p><p>Graphical models and Neural Networks have their own strengths and weaknesses. While graphical models predict the entire label sequence jointly, they usually rely on special hand crafted features to pro- vide good results. Neural Networks (especially Re-current Neural Networks), on the other hand, have been shown to be extremely good at identifying pat- terns from noisy text data, but they still predict each word label in isolation and not as a part of a se- quence. In simpler terms, RNNs benefit from rec- ognizing patterns in the surrounding input features, while structured learning models like CRF benefit from the knowledge about neighboring label predic- tions. Recent work on Named Entity Recognition by  and others have combined the benefits of Neural Networks(NN) with CRF by modeling the unary potential functions of a CRF as NN models. They model the pairwise potentials as a paramater matrix <ref type="bibr">[A]</ref> where the entry A i,j corre- sponds to the transition probability from the label i to label j. Incorporating CRF inference in Neural Network models helps in labeling exact boundaries of various named entities by enforcing pairwise con- straints.</p><p>This work focuses on labeling clinical events (medication, indication, and adverse drug events) and event related attributes (medication dosage, route, etc) in unstructured clinical notes from Elec- tronic Health Records. Later on in the Section 4, we explicitly define the clinical events and attributes that we evaluate on. In the interest of brevity, for the rest of the paper, we use the broad term "Clinical En- tities" to refer to all medically relevant information that we are interested in labeling.</p><p>Detecting entities in clinical documents such as Electronic Health Record notes composed by hospi- tal staff presents a somewhat different set of chal- lenges than similar sequence labeling applications in the open domain. This difference is partly due to the critical nature of medical domain, and partly due to the nature of clinical texts and entities therein. Firstly, in the medical domain, extraction of exact clinical phrase is extremely important. The names of clinical entities often follow polynomial nomen- clature. Disease names such as Uveal melanoma or hairy cell leukemia need to be identified exactly, since partial names ( hairy cell or melanoma) might have significantly different meanings. Addition- ally, important clinical entities can be relatively rare events in Electronic Health Records. For example, mentions of Adverse Drug Events occur once ev- ery six hundred words in our corpus. CRF inference with NN models cited previously do improve exact phrase labeling. However, better ways of modeling the pairwise potential functions of CRFs might lead to improvements in labeling rare entities and detect- ing exact phrase boundaries.</p><p>Another important challenge in this domain is a need to model long term label dependencies. For ex- ample, in the sentence "the patient exhibited A sec- ondary to B", the label for A is strongly related to the label prediction of B. A can either be labeled as an adverse drug reaction or a symptom if B is a Med- ication or Diagnosis respectively. Traditional linear chain CRF approaches that only enforce local pair- wise constraints might not be able to model these dependencies. It can be argued that RNNs may im- plicitly model label dependencies through patterns in input features of neighboring words. While this is true, explicitly modeling the long term label depen- dencies can be expected to perform better.</p><p>In this work, we explore various methods of struc- tured learning using RNN based feature extractors. We use LSTM as our RNN model. Specifically, we model the CRF pairwise potentials using Neural Networks. We also model an approximate version of skip chain CRF to capture the aforementioned long term label dependencies. We compare the proposed models with two baselines. The first baseline is a standard Bi-LSTM model with softmax output. The second baseline is a CRF model using handcrafted feature vectors. We show that our frameworks im- prove the performance when compared to the base- lines or previously used CRF-LSTM models. To the best of our knowledge, this is the only work fo- cused on usage and analysis of RNN based struc- tured learning techniques on extraction of clinical entities from EHR notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>As mentioned in the previous sections, both Neural Networks and Conditional Random Fields have been widely used for sequence labeling tasks in NLP. Specially, CRFs ( <ref type="bibr" target="#b9">Lafferty et al., 2001</ref>) have a long history of being used for various sequence labeling tasks in general and named entity recognition in par- ticular. Some early notable works include <ref type="bibr" target="#b13">McCallum et. al. (2003)</ref>, <ref type="bibr" target="#b17">Sarawagi et al. (2004)</ref> and <ref type="bibr" target="#b19">Sha et. al. (2003)</ref>. <ref type="bibr" target="#b4">Hammerton et. al. (2003)</ref> and <ref type="bibr" target="#b0">Chiu et. al. (2015)</ref> used Long Short Term Memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) for named en- tity recognition.</p><p>Several recent works on both image and text based domains have used structured inference to improve the performance of Neural Network based mod- els. In NLP, Collobert et al (2011) used Convolu- tional Neural Networks to model the unary poten- tials. Specifically for Recurrent Neural Networks, <ref type="bibr" target="#b10">Lample et al. (2016) and</ref> used LSTMs to model the unary potentials of a CRF.</p><p>In biomedial named entity recognition, several approaches use a biological corpus annotated with entities such as protein or gene name. <ref type="bibr" target="#b18">Settles (2004)</ref> used Conditional Random Fields to extract occur- rences of protein, DNA and similar biological en- tity classes. <ref type="bibr" target="#b11">Li et. al. (2015)</ref> recently used LSTM for named entity recognition of protein/gene names from BioCreative corpus. <ref type="bibr" target="#b3">Gurulingappa et. al. (2010)</ref> evaluated various existing biomedical dictio- naries on extraction of adverse effects and diseases from a corpus of Medline abstracts.</p><p>This work uses a real world clinical corpus of Electronic Health Records annotated with various clinical entities. <ref type="bibr" target="#b8">Jagannatha et. al. (2016)</ref> recently showed that RNN based models outperform CRF models on the task of Medical event detection on clinical documents. Other works using a real world clinical corpus include <ref type="bibr" target="#b16">Rochefort et al. (2015)</ref>, who worked on narrative radiology reports. They used a SVM-based classifier with bag of words feature vec- tor to predict deep vein thrombosis and pulmonary embolism. <ref type="bibr" target="#b15">Miotto et. al. (2016)</ref> used a denoising autoencoder to build an unsupervised representation of Electronic Health Records which could be used for predictive modeling of patient's health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We evaluate the performance of three different Bi- LSTM based structured prediction models described in section 3.2, 3.3 and 3.4. We compare this perfor- mance with two baseline methods of Bi-LSTM(3.1) and CRF(3.5) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bi-LSTM (baseline)</head><p>This model is a standard bidirectional LSTM neu- ral network with word embedding input and a Soft- max Output layer. The raw natural language input sentence is processed with a regular expression to- kenizer into sequence of tokens x = [x t ] T 1 . The to- ken sequence is fed into the embedding layer, which produces dense vector representation of words. The word vectors are then fed into a bidirectional RNN layer. This bidirectional RNN along with the em- bedding layer is the main machinery responsible for learning a good feature representation of the data. The output of the bidirectional RNN produces a feature vector sequence ω(x) = [ω(x)] T 1 with the same length as the input sequence x. In this base- line model, we do not use any structured inference. Therefore this model alone can be used to predict the label sequence, by scaling and normalizing [ω(x)] T</p><p>1 . This is done by using a softmax output layer, which scales the output for a label l where l ∈ {1, 2, ..., L} as follows:</p><formula xml:id="formula_0">P (˜ y t = j|x) = exp(ω(x) t W j ) L l=1 exp(ω(x) t W l )<label>(1)</label></formula><p>The entire model is trained end-to-end using cate- gorical cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bi-LSTM CRF</head><p>This model is adapted from the Bi-LSTM CRF model described in . It combines the framework of bidirectional RNN layer[ω(x)] T 1 described above, with linear chain CRF inference. For a general linear chain CRF the probability of a label sequence˜ysequence˜ sequence˜y for a given sentence x can be written as :</p><formula xml:id="formula_1">P (˜ y|x) = 1 Z N t=1 exp{φ(˜ y t ) + ψ(˜ y t , ˜ y t+1 )} (2)</formula><p>Where φ(y t ) is the unary potential for the label po- sition t and ψ(y t , y t+1 ) is the pairwise potential be- tween the positions t,t+1. Similar to , the outputs of the bidirectional RNN layer ω(x) are used to model the unary potentials of a lin- ear chain CRF. In particular, the NN based unary po- tential φ nn (y t ) is obtained by passing ω(x) t through a standard feed-forward tanh layer. The binary po- tentials or transition scores are modeled as a matrix</p><formula xml:id="formula_2">[A] L×L .</formula><p>Here L equals the number of possible la- bels including the Outside label. Each element A i,j represents the transition score from label i to j. The probability for a given sequence˜ysequence˜ sequence˜y can then be cal- culated as :</p><formula xml:id="formula_3">P (˜ y|x; θ) = 1 Z T t=1 exp{φ nn (˜ y t ) + A ˜ yt,˜ y t+1 } (3)</formula><p>The network is trained end-to-end by minimizing the negative log-likelihood of the ground truth label sequencê y for a sentence x as follows:</p><formula xml:id="formula_4">L(x, ˆ y; θ) = − t yt δ(y t = ˆ y t ) log P (y t |x; θ)}<label>(4)</label></formula><p>The negative log likelihood of given label se- quence for an input sequence is calculated by sum- product message passing. Sum-product message passing is an efficient method for exact inference in Markov chains or trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bi-LSTM CRF with pairwise modeling</head><p>In the previous section, the pairwise potential is cal- culated through a transition probability matrix <ref type="bibr">[A]</ref> irrespective of the current context or word. For rea- sons mentioned in section 1, this might not be an effective strategy. Some clinical entities are rela- tively rare. Therefore transition from an Outside la- bel to a clinical label might not be effectively mod- eled by a fixed parameter matrix. In this method, the pairwise potentials are modeled through a non- linear Neural Network which is dependent on the current word and context. Specifically, the pairwise potential ψ(y t , y t+1 ) in equation 2 is computed by using a one dimensional CNN with 1-D filter size 2 and tanh non-linearity. At every label position t, it takes [ω(x) t ; ω(x) t+1 ] as input and produces a L×L pairwise potential output ψ nn (y t , y t+1 ). This CNN layer effectively acts as a non-linear feed-forward neuron layer, which is repeatedly applied on con- secutive pairs of label positions. It uses the output of the bidirectional LSTM layer at positions t and t + 1 to prepare the pairwise potential scores.</p><p>The unary potential calculation is kept the same as in Bi-LSTM-CRF. Substituting the neural network based pairwise potential ψ nn (y t , y t+1 ) into equation 2 we can reformulate the probability of the label se- quence˜yquence˜ quence˜y given the word sequence x as : The neural network is trained end-to-end with the objective of minimizing the negative log likelihood in equation 4. The negative log-likelihood scores are obtained by sum-product message passing.</p><formula xml:id="formula_5">P (˜ y|x; θ) = 1 Z N t=1 exp{φ nn (˜ y t ) + ψ nn (˜ y t , ˜ y t+1 )}<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Approximate Skip-chain CRF</head><p>Skip chain models are modifications to linear chain CRFs that allow long term label dependencies through the use of skip edges. These are basically edges between label positions that are not adjacent to each other. Due to these skip edges, the skip chain CRF model <ref type="bibr" target="#b21">(Sutton and McCallum, 2006</ref>) explicitly models dependencies between labels which might be more than one position apart. The joint inference over these dependencies are taken into account while decoding the best label sequence. However, unlike the two models explained in the preceding section, the skip-chain CRF contains loops between label variables. As a result we cannot use the sum-product message passing method to calculate the negative log-likelihood.The loopy structure of the graph in skip chain CRF renders exact message passing in- ference intractable. Approximate solutions for these models include loopy belief propagation(BP) which requires multiple iterations of message passing.</p><p>However, an approach like loopy BP is pro- hibitively expensive in our model with large Neural Net based potential functions. The reason for this is that each gradient descent iteration for a combined RNN-CRF model requires a fresh calculation of the marginals. In one approach to mitigate this, <ref type="bibr" target="#b12">Lin et. al. (2015)</ref> directly model the messages in the mes- sage passing inference of a 2-D grid CRF for image segmentation. This bypasses the need for modeling the potential function, as well as calculating the ap-proximate messages on the graph using loopy BP. Approximate CRF message passing inference: <ref type="bibr" target="#b12">Lin et. al. (2015)</ref> directly estimate the factor to variable message using a Neural Network that uses input image features. Their underlying reasoning is that the factor-to-variable message from factor F to label variable y t for any iteration of loopy BP can be approximated as a function of all the input vari- ables and previous messages that are a part of that factor. They only model one iteration of loopy BP, and empirically show that it leads to an apprecia- ble increase in performance. This allows them to model the messages as a function of only the input variables, since the messages for the first iteration of message passing are computed using the potential functions alone.</p><p>We follow a similar approach for calculation of variable marginals in our skip chain model. However, instead of estimating individual factor-to- variable messages, we exploit the sequence struc- ture in our problem and estimate groups of factor- to-variable messages. For any label node y t , the first group contains factors that involve nodes which oc- cur before y t in the sentence (from left). The second group of factor-to-variable messages corresponds to factors involving nodes occurring later in the sen- tence. We use recurrent computational units like LSTM to estimate the sum of log factor-to-variable messages within a group. Essentially, we use bidi- rectional recurrent computation to estimate all the incoming factors from left and right separately.</p><p>To formulate this, let us assume for now that we are using skip edges to connect the current node t to m preceding and m following nodes. Each edge, skip or otherwise, is denoted by a factor which con- tains the binary potential of the edge and the unary potential of the connected node. As mentioned ear- lier, we will divide the factors associated with node t into two sets, F L (t) and F R (t). Here F L (t) , con- tains all factors formed between the variables from the group {y t−m , ..., y t−1 } and y t . So we can for- mulate the combined message from factors in F L (t) as</p><formula xml:id="formula_6">β L (y t ) = [ F ∈F L (t) β F →t (y t )]<label>(6)</label></formula><p>The combined messages from factors in F R (t) which contains variables from y t+1 to y t+m can be formulated as :</p><formula xml:id="formula_7">β R (y t ) = [ F ∈F R (t) β F →t (y t )]<label>(7)</label></formula><p>We also need the unary potential of the label vari- able t to compose its marginal. The unary po- tentials of each variable from {y t−m , ..., y t−1 } and {y t+1 , ..., y t+m } should already be included in their respective factors. The log of the unnormalized marginal ¯ P (y t |x) for the variable y t , can therefore be calculated by</p><formula xml:id="formula_8">log ¯ P (y t |x) = β R (y t ) + β L (y t ) + φ(y t )<label>(8)</label></formula><p>Similar to <ref type="bibr" target="#b12">Lin et. al. (2015)</ref>, in the interest of limited network complexity, we use only one mes- sage passing iteration. In our setup, this means that a variable-to-factor message from a neighboring vari- able y i to the current variable y t contains only the unary potentials of y i and binary potential between y i , y t . As a consequence of this, we can see that β L (y t ) can be written as :</p><formula xml:id="formula_9">β L (y t ) = t−m i=t−1 log y i [exp ψ(y t , y i ) + φ(y i )]<label>(9)</label></formula><p>Similarly, we can formulate a function for β R (y t ) in a similar way :</p><formula xml:id="formula_10">β R (y t ) = t+m i=t+1 log y i [exp ψ(y t , y i ) + φ(y i )]<label>(10)</label></formula><p>Modeling the messages using RNN: As mentioned previously in equation 8, we only need to estimate β L (y t ), β R (y t ) and φ(y t ) to calculate the marginal of variable y t . We can use φ nn (y t ) framework intro- duced in section 3.2 to estimate the unary potential for y t . We use different directions of a bidirectional LSTM to estimate β R (y t ) and β L (y t ). This elim- inates the need to explicitly model and learn pair- wise potentials for variables that are not immediate neighbors. The input to this layer at position t is [φ nn (y t ); ψ nn (y t , y t+1 )] (composed of potential functions described in section 3.3). This can be viewed as an LSTM layer aggregating beliefs about y t from the unary and binary potentials of <ref type="bibr">[y]</ref>   to approximate the sum of messages from left side β L (y t ). Similarly, β R (y t ) can be approximated from the LSTM aggregating information from the oppo- site direction. Formally, β L (y t ) is approximated as a function of neural network based unary and binary potentials as follows:</p><note type="other">t−1 1 Strict Evaluation ( Exact Match) Relaxed Evaluation (Word based)</note><formula xml:id="formula_11">β L (y t ) ≈ f ([φ nn (y i ); ψ nn (y i , y i+1 )] t−1 1 ) (11)</formula><p>Using LSTM as a choice for recurrent compu- tation here is advantageous, because LSTMs are able to learn long term dependencies. In our framework, this allows them to learn to prioritize more relevant potential functions from the sequence [[φ nn (y i ); ψ nn (y i , y i+1 )] t−1</p><p>1 . Another advantage of this method is that we can approximate skip edges between all preceding and following nodes, instead of modeling just m surrounding ones. This is be- cause LSTM states are maintained throughout the sentence.</p><p>The partition function for y t can be easily ob- tained by using logsumexp over all label entries of the unnormalized log marginal shown in equation 8 as follows:</p><formula xml:id="formula_12">Z t = yt exp[β R (y t ) + β L (y t ) + φ(y t )] (12)</formula><p>Here the partition function Z is a different for differ- ent positions of t. Due to our approximations, it is not guaranteed that the partition function calculated from different marginals of the same sentence are equal. The normalized marginal can be now calcu- lated by normalizing log ¯ P (y t |x) in equation 8 using Z t .</p><formula xml:id="formula_13">L(x, ˆ y; θ) = − t yt δ(y t = ˆ y t )(β R (y t ; θ) +β L (y t ; θ) + φ(y t ; θ) − log Z t (θ))<label>(13)</label></formula><p>The model is optimized using cross entropy loss between the true marginal and the predicted marginal. The loss for a sentence x with a ground truth label sequencê y is provided in equation 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CRF (baseline)</head><p>We use the linear chain CRF, which is a widely used model in extraction of clinical named entities. As mentioned previously, Conditional Random Fields explicitly model dependencies between output vari- ables conditioned on a given input sequence.</p><p>The main inputs to CRF in this model are not RNN outputs, but word inputs and their correspond- ing word vector representations. We add additional sentence features consisting of four vectors. Two of them are bag of words representation of the sentence sections before and after the word respectively. The remaining two vectors are dense vector representa- tions of the same sentence sections. The dense vec- tors are calculated by taking the mean of all indi- vidual word vectors in the sentence section. We add these features to explicitly mimic information pro- vided by the bidirectional chains of the LSTM mod- els.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>We use an annotated corpus of 1154 English Elec- tronic Health Records from cancer patients. Each note was annotated 2 by two annotators who label clinical entities into several categories. These cate- gories can be broadly divided into two groups, Clin- ical Events and Attributes. Clinical events include any specific event that causes or might contribute to a change in a patient's medical status. Attributes are phrases that describe certain important proper- ties about the events. <ref type="bibr">2</ref> The annotation guidelines can be found at https://github.com/abhyudaynj/LSTM-CRF- models/blob/master/annotation.md Clinical Event categories in this corpus are Ad- verse Drug Event (ADE), Drugname , Indication and Other Sign Symptom and Diseases (Other SSD). ADE, Indication and Other SSD are events having a common vocabulary of Sign, Symptoms and Dis- eases (SSD). They can be differentiated based on the context that they are used in. A certain SSD should be labeled as ADE if it can be manually identified as a side effect of a drug based on the evidence in the clinical note. It is an Indication if it is an affliction that a doctor is actively treating with a medication. Any other SSD that does not fall into the above two categories ( for e.g. an SSD in patients history) is labeled as Other SSD. Drugname event labels any medication or procedure that a physician prescribes.</p><p>The attribute categories contain the following properties, Severity , Route, Frequency, Duration and Dosage. Severity is an attribute of the SSD event types , used to label the severity a disease or symp- tom. Route, Frequency, Duration and Dosage are attributes of Drugname. They are used to label the medication method, frequency of dosage, duration of dosage, and the dosage quantity respectively. The annotation statistics of the corpus are provided in the <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Each document is split into separate sentences and the sentences are tokenized into individual word and special character tokens. The models operate on the tokenized sentences. In order to accelerate the training procedure, all LSTM models use batch-wise training using a batch of 64 sentences. In order to do this, we restricted the sentence length to 50 tokens. All sentences longer than 50 tokens were split into shorter size samples, and shorter sentences were pre- padded with masks. The CRF baseline model(3.5) does not use batch training and so the sentences were used unaltered.</p><p>The first layer for all LSTM models was a 200 dimensional word embedding layer. In order to im- prove performance, we initialized embedding layer values in these models with a skip-gram word em- bedding ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>). The skip-gram em- bedding was calculated using a combined corpus of PubMed open access articles, English Wikipedia and an unlabeled corpus of around hundred thousand Electronic Health Records. The EHRs used in the annotated corpus are not in this unlabeled EHR cor- pus. This embedding is also used to provide word vector representation to the CRF baseline model.</p><p>The bidirectional LSTM layer which outputs ω(x) contains LSTM neurons with a hidden size ranging from 200 to 250. This hidden size is kept variable in order to control for the number of trainable parameters between different LSTM based models. This helps ensure that the improved perfor- mance in these models is only because of the modi- fied model structure, and not an increase in trainable parameters. The hidden size is varied in such a way that the number of trainable parameters are close to 3.55 million parameters. Therefore, the Approx skip chain CRF has 200 hidden layer size, while stan- dard Bi-LSTM model has 250 hidden layer. Since the ω(x) layer is bidirectional, this effectively means that the Bi-LSTM model has 500 hidden layer size, while Approx skip chain CRF model has 400 dimen- sional hidden layer.</p><p>We use dropout ( <ref type="bibr" target="#b20">Srivastava et al., 2014</ref>) with a probability of 0.50 in all LSTM models in order to improve regularization performance. We also use batch norm <ref type="bibr" target="#b7">(Ioffe and Szegedy, 2015</ref>) between lay- ers wherever possible in order to accelerate training. All RNN models are trained in an end-to-end fashion using Adagrad <ref type="bibr" target="#b2">(Duchi et al., 2011</ref>) with momentum. The CRF model was trained using L-BFGS with L2 regularization. We use Begin Inside Outside (BIO) label modifiers for models that use CRF objective.</p><p>We use ten-fold cross validation for our results. The documents are divided into training and test documents. From each training set fold, 20% of the sentences form the validation set which is used for model evaluation during training and for early stop- ping.</p><p>We report the word based and exact phrase match based micro-averaged recall, precision and F-score. Exact phrase match based evaluation is calculated on a per phrase basis, and considers a phrase as pos- itively labeled only if the phrase exactly matches the true boundary and label of the reference phrase. Word based evaluation metric is calculated on labels of individual words. A word's predicted label is con- sidered as correct if it matches the reference label, irrespective of whether the remaining words in its phrase are labeled correctly. Word based evaluation is a more relaxed metric than phrase based evalua- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The micro-averaged Precision, Recall and F-score for all five models are shown in <ref type="table" target="#tab_2">Table 2</ref>. We report both strict (exact match) and relaxed (word based) evaluation results. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the best per- formance is obtained by Skip-Chain CRF (0.8210 for strict and 0.8632 for relaxed evaluation). All LSTM based models outperform the CRF baseline. Bi-LSTM-CRF and Bi-LSTM-CRF-pair models us- ing exact CRF inference improve the precision of strict evaluation by 2 to 5 percentage points. Bi- LSTM CRF-pair achieved the highest precision for exact-match. However, the recall (both strict and re- laxed) for exact CRF-LSTM models is less than Bi- LSTM. This reduction in recall is much less in the Bi-LSTM-pair model. In relaxed evaluation, only the Skip Chain model has a better F-score than the baseline LSTM. Overall, Bi-LSTM-CRF-pair and Approx-Skip-Chain models lead to performance im- provements. However, the standard Bi-LSTM-CRF model does not provide an appreciable increase over the baseline. <ref type="figure" target="#fig_0">Figure 1</ref> shows the breakdown of performance for each RNN model with respect to individual clinical entity labels. CRF baseline model perfor- mance is not shown in <ref type="figure" target="#fig_0">Figure 1</ref>, because its per- formance is consistently lower than Bi-LSTM-CRF model across all label categories. We use pairwise t-test on strict evaluation F-score for each fold in cross validation, to calculate the statistical signifi- cance of our scores. The improvement in F-score for Bi-LSTM-CRF-pair and Approx-Skip Chain as compared to Bi-LSTM baseline is statistically sig- nificant (p &lt; 0.01). The difference in Bi-LSTM- CRF and Bi-LSTM baseline, does not appear to be statistically significant (p &gt; 0.05). However, the im- provements over CRF baseline for all LSTM models are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Overall, Approx-Skip-Chain CRF model achieved better F-scores than CRF,Bi-LSTM and Bi-LSTM-CRF in both strict and relaxed evaluations. The re- sults of strict evaluation, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, are our main focus of discussion due to their impor- tance in the clinical domain. As expected, two ex- act inference-based CRF-LSTM models <ref type="bibr">(Bi-LSTMCRF and Bi-LSTM-CRF-pair)</ref> show the highest pre- cision for all labels. Approx-Skip-Chain CRF's pre- cision is lower(due to approximate inference) but it still mostly outperforms Bi-LSTM. The recall for Skip Chain CRF is almost equal or better than all other models due to its robustness in modeling de- pendencies between distant labels. The variations in recall contribute to the major differences in F-scores. These variations can be due to several factors includ- ing the rarity of that label in the dataset, the com- plexity of phrases of a particular label, etc.</p><p>We believe, exact CRF-LSTM models described here require more training samples than the baseline Bi-LSTM to achieve a comparable recall for labels that are complex or "difficult to detect". For exam- ple, as shown in table 1, we can divide the labels into frequent ( Other SSD, Indication, Severity, Drug- name, Dosage, and Frequency) and rare or sparse (Duration, ADE, Route). We can make a broad gen- eralization, that exact CRF models (especially Bi- LSTM-CRF) have somewhat lower recall for rare labels. This is true for most labels except for Route, Indication, and Severity. The CRF models have very close recall (0.780,0.782) to the baseline Bi-LSTM (0.803) for Route even though its number of inci- dences are lower (2,862 incidences) than Indication (3,724 incidences) and Severity (3,628 incidences), both of which have lower recall even though their incidences are much higher.</p><p>Complexity of each label can explain the afore- mentioned phenomenon. Route for instance, fre- quently contains unique phrases such as "by mouth" or "p.o.," and is therefore easier to detect. In con- trast, Indication is ambiguous. Its vocabulary is close to two other labels: ADE (1,807 incidences) and the most populous Other SSD (40,984 inci- dences). As a consequence, it is harder to sepa- rate the three labels. Models need to learn cues from surrounding context, which is more difficult and requires more samples. This is why the re- call for Indication is lower for CRF-LSTM models, even though its number of incidences is higher than Route. To further support our explanation, our re- sults show that the exact CRF-LSTM models mis- labeled around 40% of Indication words as Other SSD, as opposed to just 20 % in case of the Bi- LSTM baseline. The label Severity is a similar case. It contains non-label-specific phrases such as "not terribly", "very rare" and "small area," which may explain why almost 35% of Severity words are mis- labeled as Outside by the bi-LSTM-CRF as opposed to around 20% by the baseline.</p><p>It is worthwhile to note that among exact CRF- LSTM models, the recall for Bi-LSTM-CRF-pair is much better than Bi-LSTM-CRF even for sparse la- bels. This validates our initial hypothesis that Neu- ral Net based pairwise modeling may lead to better detection of rare labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have shown that modeling pairwise potentials and using an approximate version of Skip-chain in- ference increase the performance of the LSTM-CRF models. We also show that these models perform much better than baseline LSTM and CRF models. These results suggest that the structured prediction models are good directions for improving the exact phrase extraction for clinical entities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plots of Recall, Precision and F-score for RNN based methods. The metrics with prefix Strict are using phrase based evaluation. Relaxed metrics use word based evaluation.Bar-plots are in order with Bi-LSTM on top and Approx-skip-chain-CRF at the bottom.</figDesc><graphic url="image-1.png" coords="7,83.70,57.83,444.59,269.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Cross validated micro-average of Precision, Recall and F-score for all clinical tags 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the UMassMed annotation team: Elaine Freund, Wiesong Liu, Steve Belknap, Nadya Frid, Alex Granillo, Heather Keating, and Victoria Wang for creating the gold standard evaluation set used in this work. We also thank the anonymous reviewers for their comments and suggestions.</p><p>This work was supported in part by the grant HL125089 from the National Institutes of Health (NIH). We also acknowledge the support from the United States Department of Veterans Affairs (VA) through Award 1I01HX001457. This work was also supported in part by the Center for Intelligent Infor-mation Retrieval. The contents of this paper do not represent the views of CIIR, NIH, VA or the United States Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional lstm-cnns</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical evaluation of resources for the identification of diseases and adverse effects in biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmannapitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Workshop on Building and evaluating resources for biomedical text mining</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>7th edition of the Language Resources and Evaluation Conference</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hammerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLTNAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="172" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional rnn for medical event detection in electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhyuday</forename><surname>Jagannatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition based on extended recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lishuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuke</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingxin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="652" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply learning the messages in message passing inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep patient: An unsupervised representation to predict the future of patients from the electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">A</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26094</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel method of adverse event detection can accurately identify venous thromboembolisms (vtes) from narrative electronic health record data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christian M Rochefort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tewodros</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eguale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buckeridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition using conditional random fields and rich feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="107" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields for relational learning. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="93" to="128" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
