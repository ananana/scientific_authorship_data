<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Operation-guided Neural Networks for High Fidelity Data-To-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Nie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Operation-guided Neural Networks for High Fidelity Data-To-Text Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3879" to="3889"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3879</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require inference or calculations over raw data. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-to-text generation is a classic language gen- eration task that takes structured data (e.g., a ta- ble of statistics or a set of event records) as in- put, aiming at automatically producing texts that informatively, correctly and fluently describe the data <ref type="bibr" target="#b16">(Kukich, 1983;</ref><ref type="bibr" target="#b26">Reiter and Dale, 1997;</ref><ref type="bibr" target="#b0">Angeli et al., 2010;</ref><ref type="bibr" target="#b15">Konstas and Lapata, 2012;</ref><ref type="bibr">PerezBeltrachini and Gardent, 2017)</ref>. Traditionally, a data-to-text generation system should pay at- tention to the problem of content selection (i.e., what to say) and surface realization (i.e., how to say) <ref type="bibr" target="#b26">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b9">Gatt and Krahmer, 2018)</ref>. Modern neural generation systems avoid the distinction of these aspects by building over a standard encoder-decoder architecture <ref type="bibr" target="#b31">(Sutskever et al., 2014</ref>) with the attention mechanism over in- put content ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> and train the * Contribution during internship at Microsoft. edges the Heat with 95 -94 <ref type="table">Table 1</ref>: An example of generated texts from structured data. In this example, the wining team is not indicated explicitly, but can be inferred from the scores for hte two teams. The words with underlining and ::::: wave :::: lines are based on the facts from the input data and the results of inferring, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Data</head><p>whole system in an end-to-end fashion. As a re- sult, end-to-end neural text generation has drawn increasing attention from the natural language re- search community ( <ref type="bibr" target="#b22">Mei et al., 2016;</ref><ref type="bibr" target="#b17">Lebret et al., 2016;</ref><ref type="bibr" target="#b32">Wiseman et al., 2017;</ref><ref type="bibr" target="#b14">Kiddon et al., 2016</ref>).</p><p>However, a critical issue for neural text gen- eration has been largely overlooked. In domains such as sports, finance or medical care, language generation should adhere to facts which are sup- ported by or can be derived from the input data through analysis or inference. For instance, the sentence "Hawks edges the Heat with 95-94" de- scribing the result of a basketball game should al- ways conform to the original data in team names and the scoreline. More importantly, the word "edges" in the description is an inferred fact that the scores between the two competing teams are rather close, while "Hawks" is the winner that scores the slightly higher point total of "95". Since current neural models do not have special treat- ment for such data analytics, they are likely to generate spurious and incorrect statements. This problem has already been pointed out in recent studies <ref type="bibr" target="#b32">(Wiseman et al., 2017)</ref>. Related studies on neural program induction have shown that cur-rent neural models have difficulties in learning arithmetic operations such as addition and com- parisons ( <ref type="bibr" target="#b13">Joulin and Mikolov, 2015;</ref><ref type="bibr" target="#b24">Neelakantan et al., 2016)</ref>.</p><p>A straightforward way to improve the fidelity of neural text generation is to separate symbolic op- erations out from the neural models. More specifi- cally, it is viable to pre-execute a few symbolic op- erations before generation, and then use the results of execution to guide the whole generation pro- cess. However, there are two major challenges for incorporating pre-defined operations: (1) if we ap- ply operations exhaustively on all fields with com- patible value types in the table, it would create a huge search space in which mention worthy results are rare events and (2) it is difficult to establish the correspondences between specific spans of nu- meric results and lexical choices. For example, the word "edges" corresponds to the slight difference in score, i.e. 1, in <ref type="table">Table.</ref> 1.</p><p>Inspired by recent work that separates neural representations and symbolic operations ( <ref type="bibr" target="#b18">Liang et al., 2017)</ref>, we propose a framework for neural data-to-text generation that is able to utilize infor- mation from pre-computed operations on raw data. Based on a standard sequence-to-sequence model with an attention and copying mechanism, we de- sign a gating mechanism for the neural model to decide which part of the execution results should be used for generation. To address the second challenge, we also design a quantization layer to map numerical execution results into bins to guide different lexical choices according to differ- ent quantities of values.</p><p>To examine the effectiveness of our proposed model, we collect a large dataset of sports headline generation for NBA basketball games 1 . We also evaluate the models on the ROTOWIRE dataset released by <ref type="bibr" target="#b32">Wiseman et al. (2017)</ref> which targets at generating short paragraphs. Experiments show that our model outperforms current state-of-the- art neural methods in terms of both fluency and fidelity. In summary, we make the following con- tributions in this paper:</p><p>• We propose a neural data-to-text framework that generate texts by additional processing over input data. Based on a basic sequence- to-sequence model with attention and copy- ing, we design a gating mechanism to enable the model to decide which part of the exe- cuted results should be utilized. We also pro- pose a novel quantization layer to map spe- cific numerical values onto different spans to affect lexical choices under different condi- tions.</p><p>• To focus our study on correct text generation, we collect a challenging dataset for NBA headline generation.</p><p>• We conduct experiments on the NBA head- line dataset as well as the ROTOWIRE dataset from previous work. Results show improvements on both correctness and flu- ency from our proposed framework over baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Attention-Based Neural Sequence-to-Sequence Model</head><p>In this section, we briefly introduce the architec- ture of the attention-based sequence-to-sequence (Seq2Seq) ( <ref type="bibr" target="#b6">Cho et al., 2014b;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) model with a copy mechanism ( <ref type="bibr" target="#b28">See et al., 2017)</ref>, which is the basis of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RNN Encoder-Decoder</head><p>The goal of data-to-text generation is to generate a natural language description for a given set of data records S = {r j } K j=1 . Usually, a Seq2Seq model consists of an encoder and a decoder with recurrent neural networks (RNN). First, each input record r j is encoded into a hidden vector h j with j ∈ {1, ..., K} using a bidirectional RNN. The de- coder generates the description word by word us- ing another RNN.</p><p>In the training phase, given a record set and its corresponding natural language description (S, y), the Seq2Seq model maximizes the conditional probability as follows:</p><formula xml:id="formula_0">P (y|S) = T t=1 P (y t |y &lt;t , S)<label>(1)</label></formula><p>where y t is the t-th word in the description and T is the length of the description. The conditional probability P (y t |y &lt;t , S) is computed as:</p><formula xml:id="formula_1">P (y t |y &lt;t , S) = softmax(f (d t , y t−1 , c t )) (2)</formula><p>where f (·) is a non-linear function and d t is the hidden state of the decoder at step t:  <ref type="figure">Figure 1</ref>: A diagram of the operation guided neural data-to-text generation. The input record table is converted from the first 3 columns of Table1. First, a set of operations are applied to the input records. Then, the records, operations and pre-excuted operation results are encoded. Finally, an attention-equipped GRU decoder with a gating mechanism decides which part of the execution results and context should be used for generation.</p><formula xml:id="formula_2">d t = g(d t−1 , y t−1 , c t−1 )<label>(3</label></formula><p>where g(·) is a non-linear function. We adopt the Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b5">Cho et al., 2014a</ref>) as the recurrent unit for the encoder and decoder. c t in Eq. 2 is the context vector at timestep t, com- puted as a weighted hidden vectors h j :</p><formula xml:id="formula_3">c t = K j=1 α t,j h j (4)</formula><p>where α t,j is computed by an attention scheme, typically implemented as a softmax distribution over scores calculated with a multi-layer percep- tron ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Copy Mechanism</head><p>Recent work augments Seq2Seq models to copy words directly from the source information on which they are conditioned ( <ref type="bibr" target="#b12">Gu et al., 2016;</ref><ref type="bibr" target="#b28">See et al., 2017</ref>). These models usually introduce an additional binary variable z t into per-timestep tar- get word distribution, which indicates whether the target word y t is copied from the source or is gen- erated from the recurrent hidden states. We use the pointer-generator network (See et al., 2017) for the copy mechanism. Specifically, the binary variable z t is calculated from the context vector c t , the de- coder state d t and the decoder input y t−1 :</p><formula xml:id="formula_4">p gen = σ(w c c t + w d d t + w y y t−1 + b ptr ) (5)</formula><p>where vectors w c , w d , w y and the scalar b ptr are learnable parameters, and σ is the sigmoid func- tion. The joint probability for generating y t is for- mulated as follows:</p><p>P copy (y t |y &lt;t , S) = p gen P (y t |y &lt;t , S) (6)</p><formula xml:id="formula_5">+(1 − p gen ) i:r i =yt α t,i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Model</head><p>In this paper, we propose to utilize information from pre-executed operations on the input data to guide the generation. As shown in <ref type="figure">Fig. 1</ref>, our model consists of a record encoder, an oper- ation encoder and an operation result encoder, and an attention-equipped GRU decoder with a gat- ing mechanism. First, a set of operations are ap- plied to all valid records in the input data, yielding their corresponding pre-executed results. The pre- executed results act as facts inferred from input data to guide the generation. Then, the records, operation and pre-executed operation results are encoded into corresponding representation. Fi- nally, we design a gating mechanism for the GRU decoder to decide which part of the inferred facts should be used for generation. Moreover, to address the challenge in establishing correspon- dences between specific numeric results and lex- ical choices, a quantization layer maps the re- sults into several segmentations to guide the lex- ical choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Given the input data and description pair (S, y), where each target description y = y 1 , ..., y T con-sists of T words, and each input data is stored in a table (e.g., <ref type="table">Table 1</ref>), where each row is an entity and each column is a field of this entity. The input data can be transferred into K records</p><formula xml:id="formula_6">S = {r i } K i=1</formula><p>, where each record r is a triple (r.idx, r.f, r.v). For r 4 in the table of <ref type="figure">Fig. 1</ref>, r.idx r.f and r.v refer to the row index (e.g., row 2), the field name (e.g., column Points) and value (e.g., cell value 95), respectively. We also define a set of operations {op i }, and the operations are applied to the input records S to produce corre- sponding results at the preprocessing stage. The results of operations can be categorized into two types: op scl i denotes results with a type of scalar value and op idx i denotes results with a type of in- dexing value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Records</head><p>We map each record r ∈ S into a vector r by con- catenating the embedding of r.idx (e.g., row 2), r.f (e.g., column Points) and r.v (e.g., cell value 95), denoted as r</p><formula xml:id="formula_7">= [e idx , e f , e v ]</formula><p>, where e idx , e f , e v are trainable word embeddings of r.idx, r.f and r.v respectively, similar to ( <ref type="bibr" target="#b34">Yang et al., 2017</ref>). We feed a set of record vectors r 1 , ..., r K to a bidirec- tional GRU and yield the final record representa- tions h ctx 1 , ..., h ctx K as introduced in Section 2. We leave the exploring of different encoding methods as future work, as it would affect the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Operations</head><p>As shown in <ref type="figure">Fig. 1</ref>, each operation op i consists of: a) the name of the operation op i .t (e.g., minus); b) the column op i .c to which the operation applies (e.g., Points); and c) the row to which the oper- ation applies, denoted as op i .arg = {r i .idx} A i=1 , where A is the count of arguments. We then en- code each operation op i by concatenating the rep- resentation of these three components and feed them into a nonlinear layer to represent each op- eration as follows:</p><formula xml:id="formula_8">h op i = tanh(W op [op t i , op c i , op arg i ] i + b op ),<label>(7)</label></formula><p>where op t i is the embedding of op i .t; op c i is the embedding of column op i .c which shares the same parameters of embedding with record column r.f . For op i .arg, it may contain multiple arguments, so we apply a nonlinear layer to get a fixed length representation as follows:</p><formula xml:id="formula_9">op arg i = tanh( k∈arg i W arg k e idx k + b arg ),<label>(8)</label></formula><p>where e idx k is the same embedding as used to en- code the row index r.idx, and W arg k and b arg are learnable parameters. For operations which are applied in the entire column (e.g., argmax) with- out specific rows, the representation of arguments is a special vector which stands for ALL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Encoding Operation Results</head><p>The operations produce two types of results, one is scalar results (e.g., the minus operation returns -1), the other is indexing results (e.g., the argmax oper- ation returns the row number 2), and two encoders are designed to encode these results respectively.</p><p>Scalar Results Representation In <ref type="table">Table.</ref> 1, the word "edges" is generated based on the fact that the points gap of the two teams is -1. In fact, other value likes -2 or -3 is close to -1, and the word "edges" is also applicable to them. However, di- rectly establishing the lexical choices on various sparse numeric values is not easy ( <ref type="bibr" target="#b27">Reiter et al., 2005;</ref><ref type="bibr" target="#b29">Smiley et al., 2016;</ref><ref type="bibr" target="#b35">Zarrieß and Schlangen, 2016)</ref>. <ref type="bibr" target="#b27">Reiter et al. (2005)</ref> use consistent data-to- word rules for time-series weather forecast sum- mary generation. In this paper, we aim to capture the data-to-word mapping automatically by a sim- ple quantization unit. A quantization layer is de- signed to map the scalar values into several bins, namely quantization units. Specifically, we feed each scalar value op scl i to a softmax layer, and its representation h res i is computed as the weighted sum of all quantization embeddings:</p><formula xml:id="formula_10">q i = W q op scl i + b q ,<label>(9)</label></formula><formula xml:id="formula_11">µ i,l = exp(q i,l ) L j=1 exp(q i,j ) ,<label>(10)</label></formula><formula xml:id="formula_12">h res i = L l=1 µ i,l e scl l<label>(11)</label></formula><p>where W q and b q are trainable parameters, e scl is the quantization embedding and L is the size of quantization units. Note that L is much smaller than the unique number of scalar results. We set L to 5 in this paper.</p><p>Indexing Results Representation Some opera- tions produce the row number of records (denoted as idx i ) as a result. For instance, the argmax op- eration in <ref type="figure" target="#fig_0">Fig. 1 returns row 2</ref>. We then look up the row embedding of the selected record defined in Section 3.2 to represent the result. Defined as</p><formula xml:id="formula_13">h res i = e idx i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decoder</head><p>Comparing with the Seq2Seq model described in Section 2 and our model, the main difference is in the context vector c t . Different from Eq. 4, our model has both records and operations as input.</p><p>We design two attention layers to summarize in- formation from both parts respectively, the overall context vector c t is balanced by a dynamic gate λ t .</p><formula xml:id="formula_14">c t = (1 − λ t )c op t + λ t c ctx t ,<label>(12)</label></formula><formula xml:id="formula_15">λ t = σ(W g d t + b g ),<label>(13)</label></formula><p>where c op t and c ctx t are the context vector of oper- ation results and records, respectively.</p><p>As there are two types of operation results which have quite different meanings, their con- text vectors are calculated separately and then put together by a nonlinear layer. The context vec- tors c scl t of operation results with scalar value at timestep t are constructed as ( <ref type="bibr" target="#b21">Luong et al., 2015)</ref>:</p><formula xml:id="formula_16">c scl t = N j=1 α scl t,j * h res j<label>(14)</label></formula><formula xml:id="formula_17">β scl t,j = MLP(d t−1 , h op j ),<label>(15)</label></formula><formula xml:id="formula_18">α scl t,j = exp(β scl t,j ) k exp(β scl t,k )<label>(16)</label></formula><p>where MLP stands for standard 1-layer perceptron (with tanh nonlinearity), and α scl t,j refers to the im- portance of j-th operations at the current timestep t. Eq. 14 is based on the attention mechanism which can be treated as mapping a query and a set of key-value pairs to an output. The output c scl t is computed as a weighted sum of the values h res j , where the weight assigned to each value is computed by a compatibility function of the query d t−1 with the corresponding key h op j . In this way, we also construct c idx t . Then the context vector of operation results at time step t is computed by putting these two context vectors together:</p><formula xml:id="formula_19">c op t = MLP([c scl t , c idx t ] )<label>(17)</label></formula><p>The  we need to update the attention weights for Eq. 6 based on the newly computed context vector c t and decoding state d t :</p><formula xml:id="formula_20">β new t,j = MLP(h ctx j , [d t−1 , c t ] )<label>(18)</label></formula><formula xml:id="formula_21">α new t,j = exp(β new t,j ) k exp(β new t,k )<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training</head><p>As the results of operations are pre-computed in an offline stage, our proposed model is fully dif- ferentiable and can be optimized in an end-to-end manner using back propagation. Given the batches of records {S} N and the standard natural language descriptions {Y } N , the objective function is to minimize the negative log-likelihood:</p><formula xml:id="formula_22">L = − 1 N N k=1 T k t=1 log p(y k t |y k &lt;t , S k )<label>(20)</label></formula><p>where the superscript k indicates the index of the records-description pair, and T k is the length of the k-th description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Several benchmark datasets have been used in recent years for data-to-text generation ( <ref type="bibr" target="#b19">Liang et al., 2009;</ref><ref type="bibr" target="#b3">Chen and Mooney, 2008;</ref><ref type="bibr" target="#b17">Lebret et al., 2016</ref>). For instance, <ref type="bibr" target="#b17">Lebret et al. (2016)</ref> have built a biography generation dataset from Wikipedia. However, a recent study by <ref type="bibr" target="#b25">Perez-Beltrachini and Gardent (2017)</ref> shows that existing datasets have a few missing properties such as lacking syntactic and semantic diversity. To check whether the facts mentioned in the descriptions are based on input data, we identify the text spans which contain facts (e.g., in table 1, "Hawks" is a span contain fact) from the descriptions and divide each span into three categories: a) input facts (facts that can be directly found from the input), b) inferred facts (facts that can not be directly found from the in- put but can be derived), c) unsupported facts (facts that can not be found or derived from input data). <ref type="table" target="#tab_3">Table 2</ref> shows that WikiBio dataset requires infer- ence on only 5.4% of its data. To better demon- strate the effectiveness of our approach, we adopt the following datasets which require substantially more inference based on the input data: ROTOWIRE We use the dataset and its stan- dard splits released by <ref type="bibr" target="#b32">Wiseman et al. (2017)</ref>, which consists of 4,853 human written NBA bas- ketball game summaries aligned with their cor- responding game statistics. <ref type="table" target="#tab_3">Table 2</ref> shows that 11.7% of facts in the game summaries can be in- ferred based on the input data. However, this dataset focuses on generating long text and 27.1% of facts are unsupported 2 , which brings difficulties to the analysis of fidelity for the generated text.</p><p>ESPN We collect 15,054 NBA game result headlines during 2006-2017 from the ESPN web- site, paired with their corresponding game statis- tics. These headlines are professional and concise, e.g., the description in <ref type="figure">Fig. 1</ref>. The percentage of inferred facts is 29.1% while unsupportive facts is only 8%, so we can focus on generation for the inferred facts. We split the dataset into 12,043 (80%) for training, 1,505 (10%) for development and 1,506 (10%) for testing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instantiation</head><p>In the following experiments, we define two op- erations, the minus operation which returns the scalar result and the argmax operation which re- turns a id of a row. These operations are applied to all columns and rows whose record values are numeric numbers. The number of pre-executed results increases with the number of operations, arguments and the size of input data, which will impact the efficiency of our model. The unneces- sary operation arguments can be pruned, e.g., only apply operations to the arguments co-mentioned in descriptions on the training set. We will leave this part of research for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Setup</head><p>In the main experiments, we compare our model with the following methods: (a) Template: a problem-specific template-based generator which fills structured data into corresponding place- holders to generate texts 3 , (b) Seq2Seq+copy: Seq2Seq model with pointer network copy mech- anism introduced in Section 2. It is one of the state-of-the-art methods, (c) Seq2Seq+op: Seq2Seq+copy plus the results of operations, where results are directly treated as extra records and fed to the record encoder introduced in Sec- tion 3.2 with the original input together, (d) Seq2Seq+op+quanti: We apply the quantization layer Eq. 9-11 to the results of minus operation on the basis of Seq2Seq+op. For completeness, we also report the results of <ref type="bibr" target="#b32">Wiseman et al. (2017)</ref> on the ROTOWIRE dataset. The difference between this baseline and Seq2Seq+copy is that the former uses an LSTM rather than GRU for decoding and an additional copying loss. All the experiments use a beam size of 5 in decoding 4 .</p><p>For model training, we use the stochastic gra- dient descent algorithm and the AdaDelta opti- mizer <ref type="bibr" target="#b36">(Zeiler, 2012)</ref>. The dimension of trainable word embeddings are set to 256 except for the di- mension of input record row embedding, which is set to 32; and the dimension of hidden units in GRUs are all set to 512. All the parameters are ini- tialized using a normal distribution with zero mean and a variance of</p><formula xml:id="formula_23">6/(d in + d out )</formula><p>, where d in is the dimension of the input layer and d out is the di- mension of the output layer ( <ref type="bibr" target="#b10">Glorot and Bengio, 2010)</ref>. Training converges after 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>We adopt both automatic evaluation and hu- man evaluation to evaluate the proposed model. Automatic Evaluation We employ BLEU-4 as the metric for automatic evaluation. <ref type="table" target="#tab_6">Table 4</ref> gives the automatic evaluation results for gen- eration on two datasets. Our proposed model OpAtt outperforms neural network baselines ( <ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr" target="#b32">Wiseman et al., 2017)</ref>.</p><p>The results show that our method which incorpo- rates the operations enables generating texts that are fidelity to facts and therefore yields the best performance. Seq2Seq+op+quant outper-   forms the baseline method Seq2Seq+copy, but is not as good as our method. The result con- firms that our proposed method with special- ized operation encoder and gating mechanism utilizes the information of operations more ef- fectively. Moreover, Seq2Seq+op+quant outper- forms Seq2Seq+op showing the effectiveness of the quantization layer. Human Evaluation Because of the approximate nature of the automated metric BLEU, we also conduct human evaluation to examine the fidelity of the generated texts. We randomly select some games from testing set, and entrust a professional crowdsourcing company to annotate the generated texts <ref type="bibr">5</ref> . Specifically, three native English workers who are familiar with NBA games are hired. They are first required to identify the text spans which contain facts from the generated texts, then cate- gorize the text spans into one of three facts listed in <ref type="table" target="#tab_3">Table 2</ref>, and finally judge whether the span is supported or contradicted by the input data. <ref type="table" target="#tab_5">Table 3</ref> shows the annotation results. Our method talks more about the inferred facts in the generated texts while includes less contradictions. In addition, all methods produce some unsup- <ref type="bibr">5</ref> The Fleiss' kappa score of the annotation is 0.782 for ESPN and 0.761 for ROTOWIRE respectively. For the ESPN dataset, we select 50 games and each with one generated sen- tence. For ROTOWIRE, by following ( <ref type="bibr" target="#b32">Wiseman et al., 2017)</ref>, we select 15 games and each with 3 randomly selected sen- tences.   ported facts which affect the fidelity of the gen- erated texts. We leave this issue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>As discussed in Section 4.1, the ESPN dataset is rich in inferred facts. Therefore, the model analy- sis is based on this dataset, and all case studies are made on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Effect of Operations</head><p>We examine the necessity and the benefit of intro- ducing operations by removing the argmax oper- ation (see "OpAtt w/o argmax op" in <ref type="table" target="#tab_8">Table 5</ref>). Comparing to Seq2Seq+copy, the results show that our full model and "OpAtt w/o argmax op" which incorporates results of operations both work well in terms of BLEU, and the improvements in- crease with the number of operations.</p><p>To better illustrate that our model can generate factually correct text, we show the texts gener- ated by different models in <ref type="table" target="#tab_9">Table 6</ref>. The game results mentioned in the text generated by the Seq2Seq+copy model are wrong, which shows the inability for existing neural models on inferring facts from the structured data. After adding the minus operation, "OpAtt w/o argmax op" is able to infer the game result by applying the minus op- eration on the points of the two competing teams, therefore its generated text conforms to the game results. The results confirm the necessity of intro- ducing operations to ensure factually correct gen- eration. Furthermore, our full model generates text with the correct point leader and game result based on the results of operation argmax and op- eration minus respectively.  <ref type="table" target="#tab_8">Table 5</ref> which removes the quan- tization layer decreases the BLEU performance, which shows the effectiveness of the quantization layer in the lexical choices during generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effect of Quantization</head><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, we visualize the weights of quantiza- tion softmax layer µ i,l produced by Eq. 10 when mapping the points gap of two competing teams to five bins. We can see that the points gaps with close numerical values are mapped to the same bin, so the decoder can choose similar words for them in generation. When the absolute value of the points gap is small, the weights distribution over the points gap is dispersive. At this time, the decoder tends to generate general words. This distribution becomes more centralized with the in- crease of the absolute value of the points gap, to generate more unique words. Moreover, we show the distribution of words that describes the win- ning relationship of games over different intervals of game points gap. As shown in <ref type="table">Table 7</ref>, we can clearly see that apart from three most common</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Points Gap</head><p>Words describes winning relationship <ref type="bibr">[0,</ref><ref type="bibr">5)</ref> beat, past, win over, edge, hold off, survive <ref type="bibr">[5,</ref><ref type="bibr">10)</ref> beat, past, win over, out last, hold off <ref type="bibr">[10,</ref><ref type="bibr">20)</ref> beat, past, win over, blow out, top, pull away, rout &gt;= 20 beat, past, win over, power, rout, easy win over, roll past <ref type="table">Table 7</ref>: The words that describing the winning re- lationship of games over different intervals of game points gap.</p><p>word "beat", "past", "win over", our proposed quantization layer can choose specific words ac- cording to the points gap. The word "edge" and "hold off" will only be chosen when the points gap is small, while the word "rout" and "blow out" will appear when the points gap is larger than 10. We design a gating mechanism to decide when to incorporate the results of operations to guide the process of generation. From <ref type="table" target="#tab_8">Table 5</ref>, "OpAtt w/o gate" stands for the method which replaces the balancing weight λ in Eq. 12 to 0.5, which is a spe- cial case of our proposed gating mechanism. The performance of this ablation is worse than our full model, which demonstrates that the gating mech- anism is an essential component. <ref type="figure" target="#fig_1">Fig. 3</ref> shows an example of the gating weights at each time step in generation, where a darker cell means the incorpo- ration of more information from operation results for decoding corresponding word. We can see that the gate weights are reasonable, as the gate values are large when deciding the team leader "horford" and the winner of the game "hawks".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Effect of Gating Mechanism</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Data-to-text generation is a task of natural lan- guage generation (NLG) ( <ref type="bibr" target="#b9">Gatt and Krahmer, 2018)</ref>. Previous research has focused on indi- vidual content selection <ref type="bibr" target="#b16">(Kukich, 1983;</ref><ref type="bibr" target="#b26">Reiter and Dale, 1997;</ref><ref type="bibr" target="#b8">Duboué and McKeown, 2003;</ref><ref type="bibr" target="#b2">Barzilay and Lapata, 2005</ref>) and surface realization ( <ref type="bibr" target="#b11">Goldberg et al., 1994;</ref><ref type="bibr" target="#b30">Soricut and Marcu, 2006;</ref><ref type="bibr" target="#b33">Wong and Mooney, 2007)</ref>.</p><p>Recent work avoids the distinction of the con- tent selection and sentence realization. <ref type="bibr" target="#b3">Chen and Mooney (2008)</ref> use an SMT based approach to learn alignments between comments and their cor- responding event records. <ref type="bibr" target="#b0">Angeli et al. (2010)</ref> transform the problem into a sequence of local de- cisions using a log-linear model. <ref type="bibr" target="#b15">Konstas and Lapata (2012)</ref> employ a PCFG to simultaneously op- timize the content selection and surface realization problem.</p><p>In the field of neural text generation, <ref type="bibr" target="#b22">Mei et al. (2016)</ref> uses a neural encoder-decoder approach for end-to-end training. Some have focused on condi- tional language generation based on tables ( <ref type="bibr" target="#b34">Yang et al., 2017</ref>), short biographies generation from Wikipedia tables ( <ref type="bibr" target="#b17">Lebret et al., 2016;</ref><ref type="bibr" target="#b4">Chisholm et al., 2017)</ref> and comments generation based on stock prices ( <ref type="bibr" target="#b23">Murakami et al., 2017)</ref>. However, none of these methods consider incorporating the facts that can be inferred from the input data to guide the process of generation. <ref type="bibr" target="#b23">Murakami et al. (2017)</ref> post-process the price by extending the copy mechanism and replacing numerical values with defined arithmetic operations after genera- tion. While our model, OpAtt utilizes informa- tion from pre-computed operations on raw data to guide the generation.</p><p>Our work is related to research areas on deep learning models for program induction and ques- tion answering from a knowledge base <ref type="bibr" target="#b24">(Neelakantan et al., 2016;</ref><ref type="bibr" target="#b18">Liang et al., 2017;</ref>. <ref type="bibr" target="#b24">Neelakantan et al. (2016)</ref> solve the prob- lem of semantic parsing from structured data and generate programs using pre-defined arithmetic operations. <ref type="bibr" target="#b18">Liang et al. (2017)</ref> design a set of ex- ecutable operators and obtain the answers by the generated logic forms.  design a set of operators to generate the latent program for math problem solving. However, data-to-text is a different task. The operations for these methods are designed to find the answers, while we use the operations to guide the process of generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work, we address the problem of generat- ing consistent text from structured data in a neural data-to-text generation framework, where we ex- tract facts that can be inferred in the given data by applying several executable symbolic operations to guide the generation. Moreover, we design a special quantization layer to operations whose re- sult type is numeric value and establish the corre- spondence between the numeric values and lexi- cal choice in generation. Experiments show that our method, OpAtt, outperforms existing state-of- the-art neural methods, in both fluency and fidelity evaluations.</p><p>As applying operations on a large number of records greatly increases the search space for the attention mechanism, we will extend our model to automatically detect the relevant operations to re- duce computing complexity. We will also extend the set of operations to accommodate historical data, graph data and detect the unsupported facts in the generation within the single framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Weights of the quantization softmax layer when mapping the points gap of two competing teams to five bins. X-axis is points gap and Y-axis is quantization bin. The quantization layer maps the numerical execution results into several bins to enable different lexical choices according to different quantities of values. Compared to our full model, "OpAtt w/o quantization" in Table 5 which removes the quantization layer decreases the BLEU performance, which shows the effectiveness of the quantization layer in the lexical choices during generation. In Fig. 2, we visualize the weights of quantization softmax layer µ i,l produced by Eq. 10 when mapping the points gap of two competing teams to five bins. We can see that the points gaps with close numerical values are mapped to the same bin, so the decoder can choose similar words for them in generation. When the absolute value of the points gap is small, the weights distribution over the points gap is dispersive. At this time, the decoder tends to generate general words. This distribution becomes more centralized with the increase of the absolute value of the points gap, to generate more unique words. Moreover, we show the distribution of words that describes the winning relationship of games over different intervals of game points gap. As shown in Table 7, we can clearly see that apart from three most common</figDesc><graphic url="image-1.png" coords="8,114.10,286.81,134.06,69.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The gating weights at different time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset statistics. For each dataset, we also 
manually label the source for the facts mentioned in 20 
descriptions, and report the percentage of facts based 
on the input data, inferred facts and unsupported facts. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average annotators judgment for the count of facts contradicting (#Cont.) and supporting (#Supp.) on 
facts based on input data, inferred facts and unsupported facts respectively. 

ESPN 
ROTOWIRE 
Dev 
Test 
Dev 
Test 
Template 
13.75 
14.27 
8.97 
8.93 
Wiseman's 
-
-
13.57 
13.62 
Seq2Seq+copy 
15.63 
15.30 
13.72 
13.47 
Seq2Seq+op 
14.07 
13.74 
13.52 
13.44 
Seq2Seq+op+quant 15.68 
15.49 
14.05 
13.88 
OpAtt 
17.19* 18.00* 14.96* 14.74* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>BLEU scores (%) over two datasets. Statistical 
significant is indicated with *(p &lt; 0.05) with respect to 
Seq2Seq+copy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 5 : BLEU scores (%) of model ablation.</head><label>5</label><figDesc></figDesc><table>Reference 

::::: 

horford 's . . . . . . 
dunk helps hawks :::: edge nets 
, 114 -111 
Seq2Seq 
+copy 
nets rally from . . . 
17 . . . . . . 
down to ::: top nets 
111 -111 
OpAtt w/o 
argmax op 
hawks rally from . . . 
17 . . . . . . 
down to ::: 
beat nets 
114 -111 
OpAtt 

::::: 

horford scores 24 as hawks ::: 
beat nets 
114 -111 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The generated texts by introducing different 
operations. The words with underline, ::::: wavy ::: line and 
. . . 
dot . . . . . 
line are input facts, inferred facts and unsupported 
fact, respectively. And the bold words are contradicted 
facts. 

</table></figure>

			<note place="foot" n="1"> Available at https://github.com/janenie/ espn-nba-data</note>

			<note place="foot" n="2"> e.g., injuries, rankings in the league, team schedule, etc.</note>

			<note place="foot" n="3"> For the ROTOWIRE dataset, we adopt Wiseman et al. (2017)&apos;s templates. For the ESPN dataset, we use Dou et al. (2018)&apos;s system to extract templates. The template is constructed by emitting teams and players information in a sentence: &lt;team1&gt; beats &lt;team2&gt; with &lt;point1&gt;&lt;point2&gt;. 4 The authors have updated the dataset to fix some mistakes recently, so we cannot use the result which is reported in their paper and rerun this baseline with the authors&apos; code.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>We thank the anonymous reviewers for their help-ful comments. We also thank Zhirui Zhang, Shuangzhi Wu and Yu Wu for helpful conversa-tions and comments on the work. The contact au-thor of this paper, according to the meaning given to this role by Sun Yat-Sen University, is Rong Pan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="502" to="512" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-30" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to sportscast: a test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-05" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to generate one-sentence biographies from wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<idno>abs/1702.06235</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data2text studio: Automated text generation from structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longxu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical acquisition of content selection rules for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Duboué</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using natural-language processing to produce weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Driedger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Kittredge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Expert</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Concept-totext generation via discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012-07-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design of a knowledge-based report generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Annual Meeting of the Association for Computational Linguistics, Massachusetts Institute of Technology</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983-06-15" />
			<biblScope unit="page" from="145" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-02-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to generate market comments from stock prices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soichiro</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiichi</forename><surname>Goshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1374" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysing data-to-text generation benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-04" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="238" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Choosing words in computergenerated weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayajulu</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When to plummet and when to soar: Corpus based verb selection for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charese</forename><surname>Smiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schilder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroko</forename><surname>Bretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation conference</title>
		<meeting>the 9th International Natural Language Generation conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic language generation using widl-expressions and its application in machine translation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Rochester, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04-22" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reference-aware language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards generating colour terms for referents in photographs: Prefer the expected or the unexpected?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation conference</title>
		<meeting>the 9th International Natural Language Generation conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
