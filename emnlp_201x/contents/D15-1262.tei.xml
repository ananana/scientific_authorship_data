<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparing Word Representations for Implicit Discourse Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
							<email>chloe.braud@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ALPAGE</orgName>
								<orgName type="institution" key="instit2">Univ Paris Diderot &amp; INRIA Paris</orgName>
								<address>
									<postCode>75013</postCode>
									<settlement>Rocquencourt, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<email>pascal.denis@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">INRIA Lille Nord-Europe</orgName>
								<address>
									<addrLine>59650 Villeneuve d&apos;Ascq</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparing Word Representations for Implicit Discourse Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a detailed comparative framework for assessing the usefulness of unsupervised word representations for identifying so-called implicit discourse relations. Specifically, we compare standard one-hot word pair representations against low-dimensional ones based on Brown clusters and word embeddings. We also consider various word vector combination schemes for deriving discourse segment representations from word vectors, and compare representations based either on all words or limited to head words. Our main finding is that denser representations systematically outperform sparser ones and give state-of-the-art performance or above without the need for additional hand-crafted features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying discourse relations is an important task, either to build a discourse parser or to help other NLP systems such as text summarization or question-answering. This task is relatively straightforward when a discourse connective, such as but or because, is used . The identification becomes much more challenging when such an overt marker is lacking, and the relation needs to be inferred through other means. In (1), the presence of the pair of verbs (rose,tumbled) triggers a Contrast relation. Such relations are extremely pervasive in real text cor- pora: they account for about 50% of all relations in the Penn Discourse Treebank ( <ref type="bibr" target="#b12">Prasad et al., 2008</ref> Automatically classifying implicit relations is dif- ficult in large part because it relies on numerous factors, ranging from syntax, and tense and as- pect, to lexical semantics and even world knowl- edge <ref type="bibr" target="#b0">(Asher and Lascarides, 2003)</ref>. Consequently, a lot of previous work on this problem have at- tempted to incorporate some of these information into their systems. These assume the existence of syntactic parsers and lexical databases of var- ious kinds, which are available but for a few lan- guages, and they often involve heavy feature en- gineering ( <ref type="bibr">Park and Cardie, 2012)</ref>. While acknowledging this knowledge bot- tleneck, this paper focuses on trying to predict im- plicit relations based on easily accessible lexical features, targeting in particular simple word-based features, such as pairs like (rose,tumbled) in (1). Most previous studies on implicit relations, go- ing back to ( <ref type="bibr">Marcu and Echihabi, 2002</ref>), in- corporate word-based information in the form of word pair features defined across the pair of text segments to be related. Such word pairs are often encoded in a one-hot represen- tation, in which each possible word pair corre- sponds to a single component of a very high- dimensional vector. From a machine learning perspective, this type of sparse representation makes parameter estimation extremely difficult and prone to overfitting. It also makes it diffi- cult to achieve any interesting semantic general- ization. To see this, consider the distance (e.g., Euclidean or cosine) induced by such representa- tion. Assuming for simplicity that one character- izes each pair of discourse segments via their main verbs, the corresponding one-hot encoding for the pair (rose,tumbled) would be at equal distance from the synonymic pair (went up,lost) and the antonymic pair (went down,gained), as all three vectors are orthogonal to each others.</p><p>Various attempts have been made at reducing sparsity of lexical features. Recently, Ruther-ford and <ref type="bibr" target="#b13">Xue (2014)</ref> proposed to use Brown clus- ters ( <ref type="bibr" target="#b4">Brown et al., 1992)</ref> for this task, in effect replacing each token by its cluster binary code. These authors conclude that these denser, cluster- derived representations significantly improve the identification of implicit discourse relations and report the best performance to date using also additional features. Unfortunately, their claim is somewhat weakened by the fact that they fail to compare the use of their cluster word pairs against other types of word representations, in- cluding one-hot encodings of word pairs or other low-dimensional word representations. This work also leaves other important questions open. In par- ticular, it is unclear whether all word pairs con- structed over the two discourse segments are truly informative and should be included in the model. Given that word embeddings capture latent syn- tactic and semantic information, yet another im- portant question is to which extent the use of these representations dispenses us from using additional hand-crafted syntactic and semantic features.</p><p>This paper fills these gaps and significantly ex- tends the work of <ref type="bibr" target="#b13">(Rutherford and Xue, 2014)</ref> by explicitly comparing various types of word representations and vector composition meth- ods. Specifically, we investigate three well-known word embeddings, namely Collobert and We- ston <ref type="bibr" target="#b5">(Collobert and Weston, 2008)</ref>, hierarchical log-bilinear model <ref type="bibr">(Mnih and Hinton, 2007)</ref> and Hellinger Principal Component Analysis ( <ref type="bibr">Lebret and Collobert, 2014)</ref>, in addition to Brown cluster- based and standard one-hot representations. All these word representations are publicly available for English and can be easily acquired for other languages just using raw text data, thus alleviat- ing the need for hand-crafted lexical databases. This makes our approach easily extendable to resource-poor languages. In addition, we also in- vestigate the issue of which specific words need to be fed to the model, by comparing using just pairs of verbs against all pairs of words, and how word representations should be combined over discourse segments, comparing component-wise product against simple vector concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word Representations</head><p>A word representation associates a word to a math- ematical object, typically a high-dimensional vec- tor in {0, 1} |V| or R |V| , where V is a base vocabu- lary. Each dimension of this vector corresponds to a feature which might have a syntactic or semantic interpretation. In the following, we review differ- ent types of word representations used in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-hot Word Representations</head><p>Given a particular NLP problem, the crudest and yet most common type of word representation consists in mapping each word into a one-hot vec- tor, wherein each observed word corresponds to a distinct vector component. More formally, let V denote the set of all words found in the texts and w a particular word in V. The one-hot represen- tation of w is the d-dimensional indicator vector, noted 1 w , such that d = |V|: that is, all of this vector's components are 0's but for one 1 compo- nent corresponding to the word's index in V. It is easy to see that this representation is extremely sparse, and makes learning difficult as it mechani- cally blows up the parameter space of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clustering-based Word Representations</head><p>An alternative to these very sparse representations consists in learning word representations in an un- supervised fashion using clustering. An example of this approach are the so-called Brown clusters induced using the Brown hierarchical clustering algorithm ( <ref type="bibr" target="#b4">Brown et al., 1992</ref>) with the goal of maximizing the mutual information of bigrams. As a result, each word is associated to a binary code corresponding to the cluster it belongs to. Given the hierarchical nature of the algorithm, one can create word classes of different levels of granularity, corresponding to bit codes of different sizes. The less clusters, the less fine-grained the distinctions between words but the less sparsity. Note that this kind of representations also yields one-hot encodings but on a much smaller vocab- ulary size (i.e., the number of clusters). Brown clusters have been used for several NLP tasks, in- cluding NER, chunking <ref type="bibr" target="#b16">(Turian et al., 2010)</ref>, pars- ing ( <ref type="bibr" target="#b7">Koo et al., 2008</ref>) and implicit discourse rela- tion classification <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dense Real-Valued Representations</head><p>Another approach to induce word representations from raw text is to learn distributed word represen- tations (aka word embeddings), which are dense, low-dimensional, and real-valued vectors. These are typically learned using neural language models ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref>. Each dimension correspond- ing to a latent feature of the word that captures paradigmatic information. An example of such embeddings are the so-called Collobert and We- ston embeddings <ref type="bibr" target="#b5">(Collobert and Weston, 2008)</ref>. The embeddings are learned discriminatively by minimizing a loss between the current n-gram and a corrupted n-gram whose last word comes from the same vocabulary but is different from the last word of the original n-gram. Another example are the Hierarchical log-bilinear embeddings <ref type="bibr">(Mnih and Hinton, 2007</ref>) induced using a probabilistic and linear neural model, with a hierarchical prin- ciple used to speed up the model evaluation. The embeddings are obtained by concatenating the em- beddings of the n−1 words of a n-gram and learn- ing the embedding of the last word.</p><p>A final approach is based on the assumption that words occurring in similar contexts tend to have similar meanings. Building word distribu- tional representations is done by computing the raw cooccurrence frequencies between each word and the |D| words that serve as context, with D generally smaller than the overall vocabulary, then applying some transformation (e.g. TF-IDF). As |D| is generally too large to form a tractable rep- resentation, a dimensionality reduction algorithm is used to end up with p |D| dimensions. Like for distributed representations, we end up with a dense low-dimensional real-valued vector for each word. A recent example of such approach is the Hellinger PCA embeddings of <ref type="bibr">(Lebret and Collobert, 2014</ref>) which were built using Principal Component Analysis based on Hellinger distance as dimensionality reduction algorithm. An impor- tant appeal of these representations is that they are much less time-consuming to train than the ones based on neural language models while allowing similar performance <ref type="bibr">(Lebret and Collobert, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Segment Pair Representations</head><p>We now turn to the issue of combining the word representations as described in section 2 into com- posite vectors corresponding to implicit discourse classification instances. Schematically, the rep- resentations employed for pairs of discourse seg- ments differ along three main dimensions. First, we compare the use of a single word per segment (roughly, the two main verbs) against that of all the words contained in the two segments. Second, we compare the use of sparse (i.e., one-hot) vs. dense representations for words. As discussed, Brown cluster bit representations are a special (i.e., low- dimensional) version of one-hot encoding. Third, we use two different types of combinations of word vectors to yield segment vector representa- tions: concatenation and Kronecker product. The proposed framework is therefore much more gen- eral than the one given in previous work such as <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Our classification inputs are pairs of text seg- ments, the two arguments of the relation to be pre- dicted. Let S 1 = {w 1 1 , . . . , w 1n } denote the n words that make up the first segment and S 2 = {w 2 1 , . . . , w 2m } the m words in the second seg- ment. That is, we regard segments as bags of words. Let V again denote the word vocabulary, that is the set of all words found in the segments. Sometimes, we will find it useful to refer to a par- ticular subset of V. Let head(·) refer to the func- tion that extracts the head word of segment S, 1 and V h ⊆ V the set of head words. As our goal is to compare different feature representations, we de- fine Φ as a generic feature function mapping pairs of segments to a d-dimensional real vector:</p><formula xml:id="formula_0">Φ : V n × V m → R d (S 1 , S 2 ) → Φ(S 1 , S 2 )</formula><p>The goal of learning is to acquire for each relation a linear classification function f w (·), parametrized by w ∈ R d , mapping Φ(S 1 , S 2 ) into {−1, +1}.</p><p>Recall that 1 w refers to the d-dimensional one- hot encoding for word w ∈ V. Let us also denote by ⊕ and ⊗ the vector concatenation operator and the Kronecker product, respectively. Note that do- ing a Kronecker product on two vectors u ∈ R m and v ∈ R n is equivalent to doing the outer prod- uct uv ∈ R m×n . Finally, the vec(·) operator converts a m × n matrix into an mn × 1 column vector by stacking its columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Representations based on head words</head><p>One of the simplest representation one can con- struct for a pair of segments (S 1 , S 2 ) is to con- sider only their head words: h 1 = head(S 1 ) and h 2 = head(S 2 ). In this simple scenario, two main questions that remain are: (i) which vector representations do we use for h 1 and h 2 , and (ii) how do we combine these representations. An im- portant criterion for word vector combination is that they retain text ordering information between text segments which really matters for this task.</p><p>Thus, inverting the order between two main verbs (e.g., push and fall) will often lead to distinct dis- course relation being inferred, as some relations are asymmetric (e.g., Result or Explanation).</p><p>One-hot representations Starting again with the simplest case, one can use the one-hot encod- ings corresponding to the two head words, 1 h 1 and 1 h 2 respectively, and combine them using either concatenation or product, leading to our two first feature mappings:</p><formula xml:id="formula_1">Φ h,1,⊕ (S 1 , S 2 ) = 1 h 1 ⊕ 1 h 2 Φ h,1,⊗ (S 1 , S 2 ) = vec(1 h 1 ⊗ 1 h 2 ) Note that Φ h,1,⊕ (S 1 , S 2 ) lives in {0, 1} 2|V h | and Φ h,1,⊗ in {0, 1} |V h | 2 .</formula><p>The latter representation amounts to assigning one 1 component for each pair of words in V h × V h , and is the sparsest representation one can construct from head words alone. In some sense, it is also the most expressive in that we learn one parameter for each word pair, hence capturing interaction between words across segments. By contrast, Φ h,1,⊕ (S 1 , S 2 ) doesn't ex- plicitly model word interaction across discourse segments, treating each word in a given segment (left or right) as a separate dimension.</p><p>Dense representations Alternatively, one can represent head words through their real low- dimensional embeddings. Let M denote a n × p real matrix, wherein the i th row corresponds to the p-dimensional embedding of the i th word of V h , with p |V h |. <ref type="bibr">2</ref> Using this notation, one can derive the word embeddings of the head words h 1 and h 2 from their one-hot representations us- ing simple matrix multiplication: M 1 h 1 and M 1 h 2 , respectively. Concatenation and product yield two new feature mappings, respectively:</p><formula xml:id="formula_2">Φ h,M ,⊕ (S 1 , S 2 ) = M 1 h 1 ⊕ M 1 h 2 Φ h,M ,⊗ (S 1 , S 2 ) = vec(M 1 h 1 ⊗ M 1 h 2 )</formula><p>These new representations live in a much lower dimensional real spaces: Φ h,M ,⊕ (S 1 , S 2 ) lives in R 2p and Φ h,M ,⊗ (S 1 , S 2 ) in R p 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Representations based on all words</head><p>The various segment-pair representations that we derived from pairs of head words can be general- ized to the case in which we keep all the words in each segment. The additional issue in this context is in the combination of the different word vec- tor representations within and across the two seg- ments, and that of normalizing the segment vec- tors thus obtained. For simplicity, we assume that the representation for each segment is computed by summing over the pairs of words vectors com- posing the segments.</p><p>One-hot representations Following this ap- proach and recalling that S 1 contains n words, while S 2 has m words, one can construct one-hot encodings for segment pairs as follows:</p><formula xml:id="formula_3">Φ all,1,⊕ (S 1 , S 2 ) = n i m j 1 w 1 i ⊕ 1 w 2 j Φ all,1,⊗ (S 1 , S 2 ) = n i m j vec(1 w 1 i ⊗ 1 w 2 j )</formula><p>If used without any type of frequency thresh- olding, these mappings result in very high- dimensional feature representations living in Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2|V| ≥0</head><p>and Z |V| 2 ≥0 , respectively. Interestingly, note that the feature mapping Φ all,1,⊗ (S 1 , S 2 ) corresponds to the standard segment-pair representation used in many previous work, as <ref type="bibr">(Marcu and Echihabi, 2002;</ref><ref type="bibr">Park and Cardie, 2012</ref>).</p><p>Dense representations We can apply the same composition operations to denser representations, yielding two new mappings:</p><formula xml:id="formula_4">Φ all,M ,⊕ (S 1 , S 2 ) = n,m i,j M 1 w 1 i ⊕ M 1 w 2 j Φ all,M ,⊗ (S 1 , S 2 )= n,m i,j vec(M 1 w 1 i ⊗M 1 w 2 j )</formula><p>Like their head word versions, these vectors live in R 2p and R p 2 , respectively.</p><p>Vector Normalization Normalization is impor- tant as unnormalized composite vectors are sen- sitive to the number of words present in the seg- ments. The first type of normalization we consider is to simply convert our vector representation into vectors on the unit hypersphere: this is achieved by dividing each vector by its L 2 norm.</p><p>Another type of normalization is obtained by in- verting the order of summation and concatenation in the construction of composite vectors. Instead of summing over concatenated pairs of word vec- tors across the two segments, one can first sum in- dividual word vectors within each segment, then concatenate the two segment vectors. One can thus use mapping Φ all,1,⊕ in lieu of Φ all,1,⊕ :</p><formula xml:id="formula_5">Φ all,1,⊕ (S 1 , S 2 ) = n i 1 w 1 i ⊕ m j 1 w 2 j</formula><p>It should be clear that Φ all,1,⊕ provides a nor- malized version of Φ all,1,⊕ as this latter mapping amounts to weighted versions of the former:</p><formula xml:id="formula_6">Φ all,1,⊕ (S 1 , S 2 ) = m n i 1 w 1 i ⊕ n m j 1 w 2 j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Settings</head><p>Through the comparative framework described in section 3, our objective is to assess the useful- ness of different vectorial representations for pairs of discourse segments. Specifically, we want to establish whether dense representations are better than sparse ones, and whether certain word pairs are more relevant than others, which resource and which combination schemes are more adapted to the task, and, finally, whether standard features de- rived from external databases are still relevant in the presence of dense representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Set</head><p>Our main features are primarily lexical in nature and based on surface word forms. These are de- fined either on all words used in the relation argu- ments or only on their heads.</p><p>Head Extraction Heads of discourse segments are first extracted using Collins syntactic head rules <ref type="bibr">3</ref> . In order to retrieve the semantic predi- cate, we define a heuristic which looks for the past participle of auxiliaries, the adjectival or nominal attribute of copula, the infinitive complementing "have to" forms and the first head of coordination conjunctions. In case of multiple subtrees, we look for the head of the first independent clause, or, fail- ing that, of the first phrase.  When presenting our results, we distinguish be- tween systems based on one-hot encoding built from raw tokens (one-hot) or Brown clusters (Brown). We group the systems that use embed- dings under Embed. When relevant, we indicate the number of dimensions (e.g. Brown 3,200 is the system using Brown clusters with 3, 200 clusters). We use the symbols defined in section 3 to repre- sent the operation linking the arguments represen- tations (e.g. one-hot ⊕ corresponds to the transfor- mation defined by Φ h,1,⊕ when using heads and by Φ all,1,⊕ when using all words).</p><note type="other">(HLBL) embeddings correspond to the versions implemented in (Turian et al., 2010) 4 . They have been built on Reuters English newswire with case left intact. We test versions with 100, 320, 1000 and 3, 200 clusters for Brown, with 25, 50, 100 and 200 dimensions for CnW and with 50 and 100 dimensions for HLBL. The Hellinger PCA (H- PCA) embeddings come from (Lebret and Col- lobert, 2014) 5 and have been built over the en- tire English Wikipedia, the Reuters corpus and the Wall Street Journal with all words in lower case. The vocabulary corresponds to the words that ap- pear at least 100 times and normalized frequency is computed with the 10, 000 most frequent words as context words. We test versions with 50, 100 and 200 dimensions for H</note><p>Vocabulary Sizes For one-hot encoding, the case is left intact. We ignore the unknown words when using the Brown clusters following <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>). For the word embeddings, we use the mean of the vectors of all words.</p><p>In order to give an idea of the sparsity of the one-hot encodings, note that we have |V| = 33, 649 different tokens considering all implicit examples without filtering. The Brown clusters merge these tokens into 3, 190 codes (for 3, 200 clusters), 393 (1, 000 clusters), 59 (320 clusters) or 16 (100 clusters). For heads, we count 5, 615 dif- ferent tokens which correspond to 1, 988 codes for 3, 200 clusters and roughly the same number for the others. For the dense representations, the vo- cabulary size is twice the number of dimensions of the embedding, thus from 50 to 400, or the square of this number, thus from 625 to 40, 000.</p><p>Other Features We experiment with additional features commonly used for this task: produc- tions rules, average verb phrases length, Levin verb classes, modality, polarity, General Inquirer tags, number, first last and first three words. These feature templates are well described in <ref type="bibr">Park and Cardie, 2012)</ref>. They all corre- spond to a one-hot encoding, except average verb phrases length which is continuous. We thus con- catenate these features to the lexical ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model</head><p>We use the same classification algorithm for com- paring all the described feature configurations. Specifically, we train a Maximum Entropy (ME) classifier (aka, logistic regression). <ref type="bibr">6</ref> As in previ- ous studies, we build one binary classifier for each relation. In order to deal with class imbalance, we use a sample weighting scheme: each sample re- ceives a weight inversely proportional to the fre- quency of its class in the training set. We optimize the hyper-parameters of the algorithm (i.e., the regularization norm: L 1 or L 2 , and its strength) and a filter on the features on the development set, based on the F1 score. Note that filtering is point- less for purely dense representations. We test sta- tistical significancy of the results using t-test and Wilcoxon test on a split of the test set in 20 folds.</p><p>Previous studies have tested several algorithms generally concluding that Naive Bayes (NB) gives the best performance ( <ref type="bibr" target="#b13">Rutherford and Xue, 2014</ref>). We found that, when the hyper-parameters of ME are well tuned, the per- formance are comparable to NB if not better. Note that NB cannot be used with word embed- dings representations as it does not handle neg- ative value. Concerning the class imbalance is- sue, the downsampling scheme is the most spread since (  but it has been shown <ref type="bibr">6</ref> We use the implementation provided in Scikit-Learn <ref type="bibr" target="#b9">(Pedregosa et al., 2011</ref>), available at: http://scikit-learn.</p><p>org/dev/index.html. that oversampling and instance weighting lead to better performance ( <ref type="bibr">Li and Nenkova, 2014a</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Penn Discourse Treebank</head><p>We use the Penn Discourse Treebank ( <ref type="bibr" target="#b12">Prasad et al., 2008</ref>), a corpus annotated at the discourse level upon the Penn Treebank, giving access to a gold syntactic annotation, and composed of arti- cles from the Wall Street Journal. Five types of examples are distinguished: implicit, explicit, al- ternative lexicalizations, entity relations, and no relation. Each example could carry multiple rela- tions, up to four for implicit ones, and the relations are organized into a three-level hierarchy. We keep only true implicit examples and only the first annotation. We focus on the top level re- lations which correspond to general categories in- cluded in most discursive frameworks. Finally, in order to make comparison easier, we choose the most spread split of the data, used in ( <ref type="bibr">Park and Cardie, 2012;</ref><ref type="bibr" target="#b13">Rutherford and Xue, 2014</ref>) among others. The amount of data for train- ing (sections 2 − 21), development (00, 01, 23, 24) and evaluation <ref type="bibr">(21,</ref><ref type="bibr">22</ref>) is summarized in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We first discuss the models that use only lexical features, defined either over all the words that ap- pear in the arguments or only the head words. We then compare our best performing lexical configu- rations with the ones that also integrate additional standard features, and to state-of-the-art systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Pairs over the Arguments</head><p>Our first finding in this setting is that feature con- figurations that employ unsupervised word repre- sentations almost systematically outperform those that use raw tokens. This is shown in the left part of table 3. Although the optimal word rep- resentation differs from one relation to another, it is always a dense representation that achieves the  <ref type="table">Table 3</ref>: F1 score for systems using all words and only heads for Temporal (Temp.), Contingency (Cont.), Comparison (Compa.) and Expansion (Exp.). * p ≤ 0.1, * * p ≤ 0.05 compared to One-hot ⊗ with t-test and Wilcoxon ; for head words, all the improvements observed against One-hot ⊗ are significant.</p><p>best F1 score. Our baselines correspond to mul- tiplicative and additive one-hot encodings, noted One-hot ⊗ and One-hot ⊕, the former being the most commonly used in previous work. These are strong baselines in the sense they have been ob- tained after optimizing a frequency cut-off. Our best systems based on dense representations corre- spond to significant improvements in terms of F1 of about 8 points for Expansion, 7 points for Tem- poral and 3.5 for Contingency. The gains for Com- parison are not statistically significant. All these results are obtained using the normalization to unit vectors possibly combined to the concatenation- specific normalization described in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing Dense Representations</head><p>The best results are obtained using the Brown clusters <ref type="bibr">(Brown)</ref> showing that this resource merges words in a way that is relevant to the task. Strikingly, the Brown configuration used in <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>) (One-hot Brown 3, 200 ⊗) does not do better than the raw word pair baselines, except for Expansion. Recall that these authors did not explicitly provide this comparison. While doing a little worse, word embeddings (Embed.) also yield significant improvements for Temporal and Contingency, and smaller improvements for the others. This suggests that, even if they were not built based on discursive criteria, the latent dimen- sions encode word properties that are relevant to their rhetorical function. The superiority of Brown clusters over word embeddings is in line with the conclusions in (Turian et al., 2010) for two rather different NLP tasks (i.e., NER and chunking). <ref type="bibr" target="#b16">Turian et al. (2010)</ref> showed that the optimal word embedding is task dependent. Our exper- iments suggest that it is relation dependent: the best scores are obtained with HLBL for Tempo- ral, CnW for Contingency, H-PCA for Expansion and CnW (Best Embed. ⊗) and HPCA (Best Em- bed. ⊕) for Comparison. This again demonstrates that these four relations have to be considered as four distinct tasks. Identifying temporal or causal links is indeed sensitive to very different factors, the former relying more on temporal expressions and temporal ordering of events whereas the lat- ter relies on lexical and encyclopaedic knowledge on events. We think that this also explains that the behavior of the F1 against the optimal number of clusters for Expansion really differs from what we observed for the other relations: only 100 clus- ters for the best concatenated system and 320 for the best multiplicative one. Expansion is the least semantically marked relation and thus takes less advantage of fine-grained semantic groupings.</p><p>Comparing Word Combinations Comparing concatenated configurations (⊕ systems) against multiplicative ones (⊗ systems), we first note that for raw tokens the concatenated form (one-hot ⊕) yields results that are comparable, and some- times better, than the standard multiplicative sys- tem (one-hot ⊗), while failing to explicitly model word pair interactions. With Brown clusters, the concatenated form Best Brown ⊕ lead to better F1 scores than Best Brown ⊗ except for Contingency.</p><p>When comparing performance on dev set, we found that the differences between concatenated and multiplicative forms for Brown (excluding Ex- pansion for now) depend on the number of clusters used. <ref type="bibr" target="#b16">Turian et al. (2010)</ref> found that the more clus- ters, the better the performance. This is also the case here with concatenated forms, but not with multiplicative forms. In that case, F1 increases un-til 1, 000 clusters and then decreases. There is in- deed a trade-off between expressivity and sparsity: having too few clusters means that we loose im- portant distinctions, but having too many clusters leads to a loss in generalization. A similar trend is also found with word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Head Words Only</head><p>Considering the right part of table 3, the first find- ing is that performance of systems that use only head words decrease compared to those using all words, but much more so with the baseline One- hot ⊗ than with other representations. One-hot ⊗ has very poor performance for most relations, losing between 7 and 17 points in F 1 score. The performance loss is much less striking with One- hot ⊕ and with denser representations, which are again the best performing. The only exception is Expansion whose precision however increases. As said, this relation is the less semantically marked, making it less likely to take advantage of the use of word representations. The best performance in this setting are obtained with word embeddings (not Brown) with significant gain from 8 to 13 points in F1 for most relations. Moreover, the best systems are all based on the multiplicative form confirming that this is a better way of representing pairs than simple concatenation when the number of initial dimensions is not too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adding Other Features</head><p>Finally, we would like to assess how much im- provement can still be obtained by adding other standard features, such as those in §4.1, to word- based features. Conversely, we want to evaluate how far we are from state-of-the-art performance by just using word representations. We compare our results with those presented in <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>) and in <ref type="bibr" target="#b6">(Ji and Eisenstein, 2014</ref>), both systems deal with sparsity either by using Brown clusters or by learning task-dependent representa- tions. To make comparison easier we reproduce the experiments in <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>) with Naive Bayes (NB) <ref type="bibr">7</ref> and Maximum Entropy (ME) but without their coreference features and using gold syntactic parse. These correspond to the "repr." lines in table 4. We attribute the small differences observed with NB by the lack of coref- erence features and/or the use of different filter- ing thresholds. Concerning the difference between NB and ME, the only obvious issue is the low F1 score for Expansion: the system built using NB predicts all examples as positive thus leading to a high F1 score whereas the other one produces more balanced predictions, meaning neither sys- tems is truly satisfactory. Finally, we give results using the traditional one-encoding based on word pairs plus additional features (One-hot ⊗ + addi- tional features). These results are summarized in table 4, also including the best results of our ex- periments without additional features ("only").</p><p>Our first finding is that the addition of extra features to our previous word-based only config- uration appears to outperform state-of-the art re- sults for Temporal and Contingency, thus giving the best performance to date on these relations. These improvements are significant compared to our reproduced systems. Note that we also out- perform the task-dependent embeddings of <ref type="bibr" target="#b6">Ji and Eisenstein (2014)</ref>, except for Expansion. Our ten- tative explanation for this is that these authors in- cluded Entity relations and coreference features. Note that our system corresponding to a reproduc- tion of <ref type="bibr" target="#b13">(Rutherford and Xue, 2014)</ref> gives results similar to the baseline using raw word pairs (One- hot ⊗ + additional features) showing that their im- provements were due to other factors, the opti- mized filter threshold and the coreference features.</p><p>Overall, the addition of these hand-crafted fea- tures to our best systems do not provide improve- ments as high as one might have hoped. While improvements are significant compared to our re- produced systems, they are not with respect to the best systems given in table 3. When using all words, we only have a tendency toward significant improvement for Contingency 8 . These very small differences demonstrate that semantic and syntac- tic properties encoded in these features are already taken into account into the unsupervised word rep- resentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Automatically identifying implicit relations is challenging due to the complex nature of the pre- dictors. Previous studies have thus used many fea- tures relying on several external resources <ref type="bibr">Park and Cardie, 2012;</ref><ref type="bibr" target="#b2">Biran and McKeown, 2013)</ref> as the MPQA lexicon ( <ref type="bibr" target="#b17">Wilson et al., 2005</ref>) or the General Inquirer lexicon <ref type="bibr" target="#b14">(Stone and Hunt, 1963)</ref>, or on constituent or dependency <ref type="table" target="#tab_3">System  F1  F1  F1  F1  (Ji and Eisenstein, 2014)</ref> 26.91 51.39 <ref type="bibr">35.84 79.91 (Rutherford and Xue, 2014)</ref> 28  <ref type="table">Table 4</ref>: Systems using additional features ("+ add.features"), state-of-the art results either reported or reproduced ("repr.") using Naive Bayes ("NB") or logistic regression ("ME") and best system from previous table ("only").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Contingency Comparison Expansion</head><p>parsers ( <ref type="bibr">Li and Nenkova, 2014b;</ref><ref type="bibr">Lin et al., 2009)</ref>. Feature selection methods have been proved nec- essary to handle all of these features <ref type="bibr">(Park and Cardie, 2012;</ref><ref type="bibr">Lin et al., 2009</ref>). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syn- tactic and semantic information.</p><p>The major problem of standard word pair repre- sentations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explicit examples ( <ref type="bibr">Marcu and Echihabi, 2002</ref>), possibly using some kind of filtering or adaptation methods <ref type="bibr" target="#b2">Biran and McKeown, 2013;</ref><ref type="bibr" target="#b3">Braud and Denis, 2014</ref>). An- other line of work propose to make use of dense representations as Brown clusters in <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>). These authors claim that this re- source provides word representations that are rele- vant to the task, a conclusion that we considerably refined. <ref type="bibr" target="#b6">Ji and Eisenstein (2014)</ref> propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB rela- tions are mostly below those reported by <ref type="bibr" target="#b13">(Rutherford and Xue, 2014</ref>), possibly because their repre- sentations are learned on a rather small corpus, the PDTB itself, whereas building this kind of repre- sentation requires massive amount of data.</p><p>Our work also relates to studies comparing un- supervised representations for other NLP tasks such as name entity recognition, chunking <ref type="bibr" target="#b16">(Turian et al., 2010)</ref>, sentiment analysis ( <ref type="bibr">Lebret and Collobert, 2014</ref>) or POS tagging <ref type="bibr" target="#b15">(Stratos and Collins, 2005</ref>). In particular, we found some similarities between our conclusions and those in <ref type="bibr" target="#b16">(Turian et al., 2010)</ref>. Our comparison is slightly richer in that it includes different methods of vector com- positions and add an extra distributional represen- tation to our comparison (namely, H-PCA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we show that one can reach state-of- the-art results for implicit discourse relation iden- tification using only shallow lexical features and existing unsupervised word representations thus contradicting previous conclusions on the worth- lessness of these features. We carefully assess the usefulness of word representations for dis- course by comparing various formulations and combination schemes, demonstrating the inade- quacy of the previously proposed strategy based on Brown clusters and the distinctive relevance of head words, and by establishing that the created dense representations already provide most of the semantic and syntactic information relevant to the task thus alleviating the need for traditional exter- nal resources.</p><p>In future work, we first plan to extend our com- parative framework to a larger set of relations and to other languages. We also want to explore meth- ods for learning embeddings that are directly re- lated to the task of discourse relation classifica- tion, potentially using existing embeddings as ini- tialization ( <ref type="bibr" target="#b8">Labutov and Lipson, 2013)</ref>. It is also clear that seeing discourse segments as bag of words is too simplistic, we would like to investi- gate ways of learning adequate segment-wide em- beddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Word Representations We use either one-hot encodings or use word embeddings to build denser representations as described in section 3. The Brown clusters (Brown), Collobert-Weston (CnW) representations, and the hierarchical log-bilinear 3 https://github.com/jkkummerfeld/nlp-util</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>-PCA. The coverage of each resource is presented in table 1.</figDesc><table># words 
# missing words 
All words Head words 
HLBL 
246, 122 
5, 439 
171 
CnW 
268, 810 
5, 638 
171 
Brown 
247, 339 
5, 413 
171 
H-PCA 178, 080 
7, 042 
190 

Table 1: Word embeddings and Brown clusters 
lexicon coverage. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Relation 
Train 
Dev 
Test 
Temporal 
665 
93 
68 
Contingency 
3, 281 
628 
276 
Comparison 
1, 894 
401 
146 
Expansion 
6, 792 1, 253 
556 
Total 
12, 632 2, 375 1, 046 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number of examples in train, dev, test. 

</table></figure>

			<note place="foot" n="1"> Head extraction will be detailed in section 4.1.</note>

			<note place="foot" n="2"> For now, we assume that n = V h which is unrealistic. See section 4.1 for a discussion of unknown words.</note>

			<note place="foot" n="4"> http://metaoptimize.com/projects/wordreprs/ 5 http://lebret.ch/words/</note>

			<note place="foot" n="7"> Implemented in Scikit-Learn, we optimized the hyperparameter corresponding to the smoothing.</note>

			<note place="foot" n="8"> p = 0.135 with ttest and p = 0.061 with Wilcoxon.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Logics of Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lascarides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregated word pair features for implicit discourse relation disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining natural and artificial examples to improve implicit discourse relation identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One vector is not enough: Entity-augmented distributional semantics for discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using syntax to disambiguate explicit discourse connectives in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discovering implicit discourse relations through brown cluster pair representation and coreference patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
		<meeting>the 14th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A computer approach to content analysis: Studies using the general inquirer system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">B</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spring Joint Computer Conference</title>
		<meeting>the Spring Joint Computer Conference</meeting>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple semisupervised pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL Workshop on Vector Space Modeling for NLP</title>
		<meeting>NAACL Workshop on Vector Space Modeling for NLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
