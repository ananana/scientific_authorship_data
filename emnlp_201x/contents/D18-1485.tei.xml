<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Foreign Languages</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4554" to="4564"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4554</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel model for multi-label text classification, which is based on sequence-to-sequence learning. The model generates higher-level semantic unit representations with multi-level dilated convolution as well as a corresponding hybrid attention mechanism that extracts both the information at the word-level and the level of the semantic unit. Our designed dilated convolution effectively reduces dimension and supports an exponential expansion of receptive fields without loss of local information, and the attention-over-attention mechanism is able to capture more summary relevant information from the source context. Results of our experiments show that the proposed model has significant advantages over the baseline models on the dataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is competitive to the de-terministic hierarchical models and it is more robust to classifying low-frequency labels 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-label text classification refers to assigning multiple labels for a given text, which can be ap- plied to a number of important real-world appli- cations. One typical example is that news on the website often requires labels with the purpose of the improved quality of search and recommenda- tion so that the users can find the preferred infor- mation with high efficiency with less disturbance of the irrelevant information. As a significant task of natural language processing, a number of meth- ods have been applied and have gradually achieved satisfactory performance. For instance, a series of methods based on machine learning have been extensively utilized in the industries, such as Bi- nary Relevance ( <ref type="bibr" target="#b2">Boutell et al., 2004</ref>). BR treats the task as multiple single-label classifications and can achieve satisfactory performance. With the de- velopment of Deep Learning, neural methods are applied to this task and achieved improvements ( <ref type="bibr" target="#b30">Zhang and Zhou, 2006;</ref><ref type="bibr" target="#b19">Nam et al., 2013;</ref><ref type="bibr" target="#b1">Benites and Sapozhnikova, 2015)</ref>.</p><p>However, these methods cannot model the in- ternal correlations among labels. To capture such correlations, the following work, including ML- DT <ref type="bibr" target="#b4">(Clare and King, 2001</ref>), Rank-SVM ( <ref type="bibr" target="#b5">Elisseeff and Weston, 2002</ref>), LP ( <ref type="bibr" target="#b26">Tsoumakas and Katakis, 2006</ref>), ML-KNN ( <ref type="bibr" target="#b31">Zhang and Zhou, 2007</ref>), CC <ref type="bibr" target="#b22">(Read et al., 2011</ref>), attempt to capture the re- lationship, which though demonstrated improve- ments yet simply captured low-order correlations. A milestone in this field is the application of sequence-to-sequence learning to multi-label text classification ( <ref type="bibr" target="#b20">Nam et al., 2017)</ref>. Sequence- to-sequence learning is about the transformation from one type of sequence to another type of se- quence, whose most common architecture is the attention-based sequence-to-sequence (Seq2Seq) model. The attention-based <ref type="bibr">Seq2Seq (Sutskever et al., 2014</ref>) model is initially designed for neu- ral machine translation (NMT) ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b18">Luong et al., 2015</ref>). Seq2Seq is able to en- code a given source text and decode the represen- tation for a new sequence to approximate the tar- get text, and with the attention mechanism, the decoder is competent in extracting vital source- side information to improve the quality of de- coding. Multi-label text classification can be re- garded as the prediction of the target label se- quence given a source text, which can be modeled by the Seq2Seq. Moreover, it is able to model the high-order correlations among the source text as well as those among the label sequence with deep recurrent neural networks (RNN).</p><p>Nevertheless, we study the attention-based Seq2Seq model for multi-label text classification <ref type="bibr" target="#b20">(Nam et al., 2017)</ref> and find that the attention mech- anism does not play a significant role in this task as it does in other NLP tasks, such as NMT and ab- stractive summarization. In Section 3, we demon- strate the results of our ablation study, which show that the attention mechanism cannot improve the performance of the Seq2Seq model. We hypoth- esize that compared with neural machine transla- tion, the requirements for neural multi-label text classification are different. The conventional at- tention mechanism extracts the word-level infor- mation from the source context, which makes lit- tle contribution to a classification task. For text classification, human does not assign texts labels simply based on the word-level information but usually based on their understanding of the salient meanings in the source text.</p><p>For example, regarding the text "The young boys are playing basketball with great excitement and apparently they enjoy the fun of competition", it can be found that there are two salient ideas, which are "game of the young" and "happiness of basketball game", which we call "semantic units" of the text. The semantic units, instead of word- level information, mainly determine that the text can be classified into the target categories "youth" and "sports".</p><p>Semantic units construct the semantic meaning of the whole text. To assign proper labels for text, the model should capture the core semantic units of the source text, the higher-level informa- tion compared with word-level information, and then assign the text labels based on its understand- ing of the semantic units. However, it is diffi- cult to extract information from semantic units as the conventional attention mechanism focuses on extracting word-level information, which contains redundancy and irrelevant details.</p><p>In order to capture semantic units in the source text, we analyze the texts and find that the seman- tic units are often wrapped in phrases or sentences, connecting with other units with the help of con- texts. Inspired by the idea of global encoding for summarization ( <ref type="bibr" target="#b17">Lin et al., 2018)</ref>, we utilize the power of convolutional neural networks (CNN) to capture local interaction among words and gener- ate representations of information of higher levels than the word, such as phrase or sentence. More- over, to tackle the problem of long-term depen- dency, we design a multi-level dilated convolu- tion for text to capture local correlation and long- term dependency without loss of coverage as we do not apply any form of pooling or strided con- volution. Based on the annotations generated by our designed module and those by the original re- current neural networks, we implement our hy- brid attention mechanism with the purpose of cap- turing information at different levels, and further- more, it can extract word-level information from the source context based on its attention on the se- mantic units.</p><p>In brief, our contributions are illustrated below:</p><p>• We analyze that the conventional attention mechanism is not useful for multi-label text classification, and we propose a novel model with multi-level dilated convolution to cap- ture semantic units in the source text.</p><p>• Experimental results demonstrate that our model outperforms the baseline models and achieves the state-of-the-art performance on the dataset RCV1-v2 and Ren-CECps, and our model is competitive with the hierarchi- cal models with the deterministic setting of sentence or phrase.</p><p>• Our analysis shows that compared with the conventional Seq2Seq model, our model with effective information extracted from the source context can better predict the labels of low frequency, and it is less influenced by the prior distribution of the label sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention-based Seq2Seq for Multi-label Text Classification</head><p>As illustrated below, multi-label text classification has the potential to be regarded as a task of se- quence prediction, as long as there are certain cor- relation patterns in the label data. Owing to the correlations among labels, it is possible to improve the performance of the model in this task by as- signing certain label permutations for the label se- quence and maximizing subset accuracy, which means that the label permutation and the cor- responding attention-based Seq2Seq method are competent in learning the label classification and the label correlations. By maximizing the sub- set accuracy, the model can improve the perfor- mance of classification with the assistance of the information about the label correlations. Regard- ing label permutation, a straightforward method is to reorder the label data in accordance with the de- scending order by frequency, which shows satis- factory effects <ref type="bibr" target="#b3">(Chen et al., 2017</ref>).</p><p>Multi-label text classification can be regarded as a Seq2Seq learning task, which is formu- lated as below. Given a source text x = {x 1 , ..., x i , ..., x n } and a target label sequence y = {y 1 , ..., y i , ..., y m }, the Seq2Seq model learns to approximate the probability P (y|x) = m t=1 P (y t |y &lt;t , x), where P (y t |y &lt;t , x) is com- puted by the Seq2Seq model, which is commonly based on recurrent neural network (RNN).</p><p>The encoder, which is bidirectional Long Short- Term Memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1996)</ref>, encodes the source text x from both directions and generates the source annotations h, where the annotations from both directions at each time step are concatenated (</p><formula xml:id="formula_0">h i = [ − → h i ; ← − h i ])</formula><p>. To be specific, the computations of − → h i and ← − h i are illus- trated below:</p><formula xml:id="formula_1">− → h i = LST M (x i , − − → h i−1 , C i−1 ) (1) ← − h i = LST M (x i , ← − − h i−1 , C i−1 )<label>(2)</label></formula><p>We implement a unidirectional LSTM decoder to generate labels sequentially. At each time step t, the decoder generates a label y t by sampling from a distribution of the target label set P vocab until sampling the token representing the end of sentence, where:</p><formula xml:id="formula_2">P vocab = g(y t−1 , c t , s t−1 )<label>(3)</label></formula><p>where g(·) refers to non-linear functions includ- ing the LSTM decoder, the attention mechanism as well as the softmax function for prediction. The attention mechanism generates c t as shown in the following:</p><formula xml:id="formula_3">c t = n i=1 α t,i h i (4) α t,i = exp(e t,i ) n j=1 exp(e t,j )<label>(5)</label></formula><formula xml:id="formula_4">e t,i = s t−1 W a h i<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem</head><p>As we analyze the effects of the attention mech- anism in multi-label text classification, we find that it contributes little to the improvement of the model's performance. To verify the effects of the attention mechanism, we conduct an ablation test to compare the performance of the Seq2Seq model without the attention mechanism and the attention- based SeqSeq model on the multi-label text classi- fication dataset RCV1-v2, which is introduced in detail in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models HL(-) P(+) R(+) F1(+) w/o attention 0.0082 0.883 0.849 0.866 +attention</head><p>0.0081 0.889 0.848 0.868 <ref type="table">Table 1</ref>: Performances of the Seq2Seq models with and without attention on the RCV1-v2 test set, where HL, P, R, and F1 refer to hamming loss, micro-precision, micro-recall and micro-F 1 . The symbol "+" indi- cates that the higher the better, while the symbol "-" indicates that the lower the better.</p><p>As is shown in <ref type="table">Table 1</ref>, the Seq2Seq models with and without the attention mechanism demon- strate similar performances on the RCV1-v2 ac- cording to their scores of micro-F 1 , a significant evaluation metric for multi-label text classifica- tion. This can be a proof that the conventional at- tention mechanism does not play a significant role in the improvement of the Seq2Seq model's per- formance. We hypothesize that the conventional attention mechanism does not meet the require- ments of multi-label text classification. A com- mon sense for such a classification task is that the classification should be based on the salient ideas of the source text. The semantic units, in- stead of word-level information, mainly determine that the text can be classified into the target cate- gories "youth" and "sports". For each of a variety of texts, there are always semantic units that con- struct the semantic meaning of the whole text. Re- garding an automatic system for multi-label text classification, the system should be able to extract the semantic units in the source text for better per- formance in classification. Therefore, we propose our model to tackle this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>In the following, we introduce our proposed mod- ules to improve the conventional Seq2Seq model for multi-label text classification. In general, it contains two components: multi-level dilated con- volution (MDC) as well as hybrid attention mech- anism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-level Dilated Convolution</head><p>On top of the representations generated by the original encoder, which is an LSTM in our model, we apply the multi-layer convolutional neural net- works to generate representations of semantic units by capturing local correlations and long-term dependencies among words. To be specific, our CNN is a three-layer one-dimensional CNN. Fol- lowing the previous work <ref type="bibr" target="#b10">(Kalchbrenner et al., 2014</ref>) on CNN for NLP, we use one-dimensional convolution with the number of channels equal to the number of units of the hidden layer, so that the information at each dimension of a representation vector will not be disconnected as 2-dimension convolution does. Besides, as we are to capture semantic units in the source text instead of higher- level word representations, there is no need to use padding for the convolution.</p><p>A special design for the CNN is the implemen- tation of dilated convolution. Dilation has be- come popular in semantic segmentation in com- puter vision in recent years ( <ref type="bibr" target="#b29">Yu and Koltun, 2015;</ref>, and it has been introduced to the fields of NLP ( <ref type="bibr" target="#b21">Kalchbrenner et al., 2016)</ref> and speech processing (van den <ref type="bibr" target="#b21">Oord et al., 2016)</ref>. Dilated convolution refers to convolution inserted with "holes" so that it is able to remove the neg- ative effects such as information loss caused by common down-sampling methods, such as max- pooling and strided convolution. Besides, it is able to expand the receptive fields at the exponential level without increasing the number of parame- ters. Thus, it becomes possible for dilated con- volution to capture longer-term dependency. Fur- thermore, with the purpose of avoiding gridding effects caused by dilation (e.g., the dilated seg- ments of the convolutional kernel can cause miss- ing of vital local correlation and break the continu- ity between word representations), we implement a multi-level dilated convolution with different di- lation rates at different levels, where the dilation rates are hyperparameters in our model.</p><p>Instead of using the same dilation rate or di- lation rates with the common factor, which can cause gridding effects, we apply multi-level di- lated convolution with different dilation rates, such as <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref>. Following , for N layers of 1-dimension convolution with kernel size K with dilation rates [r 1 , ..., r N ], the max- imum distance between two nonzero values is</p><formula xml:id="formula_5">max(M i+1 − 2r i , M i+1 − 2(M i+1 − r i )</formula><p>, r i ) with M N = r N , and the goal is M 2 ≤ K. In our ex- periments, we set the dilation rates to <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref> and K to 3, and we have M 2 = 2. The implemen- tations can avoid the gridding effects and allows the top layer to access information between longer distance without loss of coverage. Moreover, as there may be irrelevant information to the seman- tic units at a long distance, we carefully design the dilation rates to <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref> based on the performance in validation, instead of others such as <ref type="bibr">[2,</ref><ref type="bibr">5,</ref><ref type="bibr">9]</ref>, so that the top layer will not process the informa- tion among overlong distance and reduce the in- fluence of unrelated information. Therefore, our model can generate semantic unit representations from the information at phrase level with small di- lation rates and those at sentence level with large dilation rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hybrid Attention</head><p>As we have annotations from the RNN encoder and semantic unit representations from the MDC, we design two types of attention mechanism to evaluate the effects of information of different lev- els. One is the common attention mechanism, which attends to the semantic unit representations instead of the source word annotations as the con- ventional does, the other is our designed hybrid attention mechanism to incorporate information of the two levels.</p><p>The idea of hybrid attention is motivated by memory networks <ref type="bibr" target="#b24">(Sukhbaatar et al., 2015</ref>) and multi-step attention <ref type="bibr" target="#b7">(Gehring et al., 2017)</ref>. It can be regarded as the attention mechanism with multiple "hops", with the first hop attending to the higher-level semantic unit information and the second hop attending to the lower-level word unit information based on the decoding and the first at- tention to the semantic units. Details are presented below.</p><p>For the output of the decoder at each time step, it not only attends to the source annotations from the RNN encoder as it usually does but also at- tends to the semantic unit representations from the MDC. In our model, the decoder output first pays attention to the semantic unit representations from the MDC to figure out the most relevant semantic units and generates a new representation based on the attention. Next, the new representation with both the information from the decoding process as well as the attention to the semantic units at- tends to the source annotations from the LSTM encoder, so it can extract word-level information from the source text with the guidance of the se- mantic units, mitigating the problem of irrelevance and redundancy.</p><p>To be specific, for the source annotations from the LSTM encoder h = {h 1 , ..., h i , ..., h n } and the semantic unit representations g = {g 1 , ..., g i , ..., g m }, the decoder output s t first at- tends to the semantic unit representations g and generates a new representation s t . Then the new representation s t attends to the source annota- tions h and generates another representatioñ s t fol- lowing the identical attention mechanism as men- tioned above. In the final step, the model generates o t for the prediction of y t , where:</p><formula xml:id="formula_6">o t = s t ⊕ ˜ s t<label>(7)</label></formula><p>For comparison, we also propose another type of attention called "additive attention", whose ex- perimental results are in the ablation test. In this mechanism, instead of paying attention to the two types of representations step by step as mentioned above, the output of the LSTM decoder s t at- tends to the semantic unit representations g and the source annotations h respectively to generate s t and˜sand˜ and˜s t , which are finally added element-wisely for the final output o t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Setup</head><p>In the following, we introduce the datasets and our experiment settings as well as the baseline models that we compare with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Preprocessing</head><p>Reuters Corpus Volume I (RCV1-v2) 2 : The dataset ( <ref type="bibr" target="#b14">Lewis et al., 2004</ref>) consists of more than 800k manually categorized newswire stories by Reuters Ltd. for research purpose, where each story is assigned with multiple topics. The total number of topics is 103. To be specific, the train- ing set contains around 802414 samples, while the development set and test set contain 1000 sam- ples respectively. We filter the samples whose lengths are over 500 words in the dataset, which removes about 0.5% of the samples in the train- ing, development and test sets. The vocabulary size is set to 50k words. Numbers as well as out- of-vocabulary words are masked by special tokens "#" and "UNK". For label permutation, we apply the descending order by frequency following <ref type="bibr" target="#b11">Kim (2014)</ref>. Ren-CECps: The dataset is a sentence corpus col- lected from Chinese blogs, annotated with 8 emo- tion tags anger, anxiety, expect, hate, joy, love, sorrow and surprise as well as 3 polarity tags pos- itive, negative and neutral. The dataset contains 35096 sentences for multi-label text classification. We apply preprocessing for the data similar to that for the RCV1-v2, which are filtering samples of over 500 words, setting the vocabulary size to 50k and applying the descending order by frequency for label permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Settings</head><p>We implement our experiments in PyTorch on an NVIDIA 1080Ti GPU. In the experiments, the batch size is set to 64, and the embedding size and the number of units of hidden layers are 512. We use Adam optimizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with the default setting β 1 = 0.9, β 2 = 0.999 and = 1 × 10 −8 . The learning rate is initial- ized to 0.0003 based on the performance on the development set, and it is halved after every epoch  <ref type="table">Table 2</ref>: Performance on the RCV1-V2 test set. HL, P, R, and F1 denote hamming loss, micro-precision, micro-recall and micro-F 1 , respectively (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models HL(-) P(+) R(+) F1(+)</head><p>of training. Gradient clipping is applied with the range <ref type="bibr">[-10, 10]</ref>.</p><p>Following the previous studies <ref type="bibr" target="#b31">(Zhang and Zhou, 2007;</ref><ref type="bibr" target="#b3">Chen et al., 2017)</ref>, we choose ham- ming loss and micro-F 1 score to evaluate the per- formance of our model. Hamming loss refers to the fraction of incorrect prediction <ref type="bibr" target="#b23">(Schapire and Singer, 1999)</ref>, and micro-F 1 score refers to the weighted average F 1 score. For reference, the micro-precision as well as micro-recall scores are also reported. To be specific, the computations of Hamming Loss (HL) micro-F 1 score are illus- trated below:</p><formula xml:id="formula_7">HL = 1 L I(y = ˆ y)<label>(8)</label></formula><formula xml:id="formula_8">microF 1 = L j=1 2tp j L j=1 2tp j + f p j + f n j (9)</formula><p>where tp j , f p j and f n j refer to the number of true positive examples, false positive examples and false negative examples respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Models</head><p>In the following, we introduce the baseline models for comparison for both datasets.</p><p>• Binary Relevance (BR) ( <ref type="bibr" target="#b2">Boutell et al., 2004</ref>) transforms the MLC task into multiple single-label classification problems.</p><p>• Classifier Chains (CC) (Read et al., 2011) transforms the MLC task into a chain of bi- nary classification problems to model the cor- relations between labels.</p><p>• Label Powerset (LP) <ref type="bibr" target="#b26">(Tsoumakas and Katakis, 2006</ref>) creates one binary classifier for every label combination attested in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models HL(-) P(+) R(+) F1(+) BR</head><p>0.1663 0.649 0.472 0.546 CC 0.1828 0.572 0.551 0.561 LP 0.1902 0.556 0.517 0.536 CNN 0.1726 0.628 0.512 0.565 CNN-RNN 0.1876 0.576 0.538 0.556 S2S 0.1814 0.587 0.571 0.579 S2S+Attn 0.1793 0.589 0.573 0.581 Our Model 0.1782 0.593 0.585 0.590 <ref type="table">Table 3</ref>: Performance of the models on the Ren-CECps test set. HL, P, R, and F1 denote hamming loss, micro-precision, micro-recall and micro-F 1 , respec- tively (p &lt; 0.05).</p><p>• CNN <ref type="bibr" target="#b11">(Kim, 2014</ref>) uses multiple convolution kernels to extract text feature, which is then input to the linear transformation layer fol- lowed by a sigmoid function to output the probability distribution over the label space.</p><p>• CNN-RNN ( <ref type="bibr" target="#b3">Chen et al., 2017</ref>) utilizes CNN and RNN to capture both global and local tex- tual semantics and model label correlations.</p><p>• <ref type="bibr">S2S and S2S+Attn (Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) are our implementa- tion of the RNN-based sequence-to-sequence models without and with the attention mech- anism respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>In the following sections, we report the results of our experiments on the RCV1-v2 and Ren-CECps. Moreover, we conduct an ablation test and the comparison with models with hierarchical mod- els with the deterministic setting of sentence or phrase, to illustrate that our model with learnable semantic units possesses a clear advantage over the baseline models. Furthermore, we demonstrate that the higher-level representations are useful for the prediction of labels of low frequency in the dataset so that it can ensure that the model is not strictly learning the prior distribution of the label sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results</head><p>We present the results of our implementations of our model as well as the baselines on the RCV1-v2 on <ref type="table">Table 2</ref>. From the results of the conventional baselines, it can be found that the classical methods for multi-label text classifica- tion still own competitiveness compared with the machine-learning-based and even deep-learning- based methods, instead of the Seq2Seq-based models. Regarding the Seq2Seq model, both the S2S and the S2S+Attn achieve improvements on the dataset, compared with the baselines above. However, as mentioned previously, the attention mechanism does not play a significant role in the Seq2Seq model for multi-label text classification. By contrast, our proposed mechanism, which is label-classification-oriented, can take both the in- formation of semantic units and that of word units into consideration. Our proposed model achieves the best performance in the evaluation of Ham- ming loss and micro-F 1 score, which reduces 9.8% of Hamming loss and improves 1.3% of micro-F 1 score, in comparison with the S2S+Attn. We also present the results of our experiments on Ren-CECps. Similar to the models' perfor- mance on the RCV1-v2, the conventional base- lines except for Seq2Seq models achieve lower performance on the evaluation of micro-F 1 score compared with the Seq2Seq models. Moreover, the S2S and the S2S+Attn still achieve similar per- formance on micro-F 1 on this dataset, and our pro- posed model achieves the best performance with the improvement of 0.009 micro-F 1 score. An in- teresting finding is that the Seq2Seq models do not possess an advantage over the conventional base- lines on the evaluation of Hamming Loss. We observe that there are fewer labels in the Ren- CECps than in the RCV1-v2 (11 and 103). As our label data are reordered according to the de- scending order of label frequency, the Seq2Seq model is inclined to learn the frequency distri- bution, which is similar to a long-tailed distribu- tion. However, regarding the low-frequency labels with only a few samples, their amounts are similar, whose distributions are much more uniform than that of the whole label data. It is more difficult for the Seq2Seq model to classify them correctly while the model is approximating the long-tailed distribution compared with the conventional base- lines. As Hamming loss reflects the average incor- rect prediction, the errors in classifying into low- frequency labels will lead to a sharper increase in Hamming Loss, in comparison with micro-F 1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Test</head><p>To evaluate the effects of our proposed modules, we present an ablation test for our model. We re- Models HL(-) P(+) R(+) F1(+) w/o attention 0.0086 0.904 0.816 0.871 attention 0.0087 0.887 0.828 0.869 MDC 0.0074 0.889 0.871 0.880 additive 0.0073 0.888 0.871 0.879 hybrid 0.0072 0.891 0.873 0.882 <ref type="table">Table 4</ref>: Performance of the models with different at- tention mechanisms on the RCV1-V2 test set. HL, P, R, and F1 denote hamming loss, micro-precision, micro- recall and micro-F 1 , respectively (p &lt; 0.05).</p><p>move certain modules to control variables so that their effects can be fairly compared. To be spe- cific, besides the evaluation of the conventional attention mechanism mentioned in Section 3, we evaluate the effects of hybrid attention and its modules. We demonstrate the performance of five models with different attention implementation for comparison, which are model without attention, one with only attention to the source annotations from LSTM, one with only attention to the seman- tic unit representations from the MDC, one with the attention to both the source annotations and semantic unit representations (additive) and hy- brid attention, respectively. Therefore, the effects of each of our proposed modules, including MDC and hybrid attention, can be evaluated individually without the influence of the other modules. Results in <ref type="table">Table 4</ref> reflect that our model still performs the best in comparison with models with the other types of attention mechanism. Except for the insignificant effect of the conventional at- tention mechanism mentioned above, it can be found that the high-level representations gener- ated by the MDC contribute much to the perfor- mance of the Seq2Seq model for multi-label text classification, which improves about 0.9 micro-F 1 score. Moreover, simple additive attention mecha- nism, which is equivalent to the element-wise ad- dition of the representations of MDC and those of the conventional mechanism, achieves similar per- formance to the single MDC, which also demon- strates that conventional attention mechanism in this task makes little contribution. As to our pro- posed hybrid attention, which is a relatively com- plex combination of the two mechanisms, can im- prove the performance of MDC. This shows that although conventional attention mechanism for word-level information does not influence the per- formance of the SeqSeq model significantly, the hybrid attention which extracts word-level infor-Models HL(-) P(+) R(+) F1(+) Hier-5 0.0075 0.887 0.869 0.878 Hier-10 0.0077 0.883 0.873 0.878 Hier-15 0.0076 0.879 0.879 0.879 Hier-20 0.0076 0.876 0.881 0.878 Our model 0.0072 0.891 0.873 0.882 <ref type="table">Table 5</ref>: Performance of the hierarchical model and our model on the RCV1-V2 test set. Hier refers to hierar- chical model, and the subsequent number refers to the length of sentence (word) for sentence-level represen- tations (p &lt; 0.05).</p><p>mation based on the generated high-level semantic information can provide some information about important details that are relevant to the most con- tributing semantic units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with the Hierarchical Models</head><p>Another method that can extract high-level rep- resentations is a heuristic method that manually annotates sentences or phrases first and applies a hierarchical model for high-level representations.</p><p>To be specific, the method does not only apply an RNN encoder to the word representations but also to sentence representations. In our reimplementa- tion, we regard the representation from the LSTM encoder at the time step of the end of each sentence as the sentence representation, and we implement another LSTM on top of the original encoder that receives sentence representations as input so that the whole encoder can be hierarchical. We imple- ment the experiment on the dataset RCV1-v2. As there is no sentence marker in the dataset RCV1- v2, we set a sentence boundary for the source text and we apply a hierarchical model to generate sen- tence representations. Compared with our proposed MDC, the hier- archical model for the high-level representations is relatively deterministic since the sentences or phrases are predefined manually. However, our proposed MDC learns the high-level representa- tions through dilated convolution, which is not restricted by the manually-annotated boundaries. Through the evaluation, we expect to see if our model with multi-level dilated convolution as well as hybrid attention can achieve similar or better performance than the hierarchical model. More- over, we note that the number of parameters of the hierarchical model is more than that of our model, which are 47.24M and 45.13M respec- tively. Therefore, it is obvious that our model does not possess the advantage of parameter number in the comparison.</p><p>We present the results of the evaluation on Ta- ble 5, where it can be found that our model with fewer parameters still outperforms the hierarchical model with the deterministic setting of sentence or phrase. Moreover, in order to alleviate the influ- ence of the deterministic sentence boundary, we compare the performance of different hierarchical models with different boundaries, which sets the boundaries at the end of every 5, 10, 15 and 20 words respectively. The results in <ref type="table">Table 5</ref> show that the hierarchical models achieve similar per- formances, which are all higher than the perfor- mances of the baselines. This shows that high- level representations can contribute to the perfor- mance of the Seq2Seq model on the multi-label text classification task. Furthermore, as these per- formances are no better than that of our proposed model, it can reflect that the learnable high-level representations can contribute more than deter- ministic sentence-level representations, as it can be more flexible to represent information of di- verse levels, instead of fixed phrase or sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Error Analysis</head><p>Another finding in our experiments is that the model's performance on low-frequency label clas- sification is lower than that on high-frequency la- bel classification. This problem is also reflected in our report of the experimental results on the Ren-CECps. The decrease in performance is rea- sonable since the model is sensitive to the amount of data, especially on small datasets such as Ren- CECps. We also hypothesize that this error comes from the essence of the Seq2Seq model. As the frequency of our label data is similar to a long- tailed distribution and the data are organized by descending order of label frequency, the Seq2Seq model is inclined to model the distribution. As the frequency distribution of the low-frequency labels is relatively uniform, it is much harder for it to model the distribution.</p><p>In contrast, as our model is capable of cap- turing deeper semantic information for the label classification, we believe that it is more robust to the classification of low-frequency labels with the help of the information from multiple levels. We remove the top 10, 20, 30, 40, 50 and 60 most frequent labels subsequently, and we eval- uate the performance of our model and the base- line Seq2Seq model on the classification of these labels. <ref type="figure" target="#fig_2">Figure 3</ref> shows the results of the models on label data of different frequency. It is obvious that although the performances of both models de- crease with the decrease of the label frequency, our model continues to perform better than the base- line on all levels of label frequency. In addition, the gap between the performances of the two mod- els continues to increase with the decrease of label frequency, demonstrating our model's advantage over the baseline on classifying low-frequency la- bels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The current models for the multi-label classifica- tion task can be classified into three categories: problem transformation methods, algorithm adap- tation methods, and neural network models.</p><p>Problem transformation methods decompose the multi-label classification task into multiple single-label learning tasks. The BR algorithm ( <ref type="bibr" target="#b2">Boutell et al., 2004</ref>) builds a separate classifier for each label, causing the label correlations to be ignored. In order to model label correlations, La- bel Powerset (LP) ( <ref type="bibr" target="#b26">Tsoumakas and Katakis, 2006</ref>) creates one binary classifier for every label com- bination attested in the training set and Classifier Chains (CC) <ref type="bibr" target="#b22">(Read et al., 2011</ref>) connects all clas- sifiers in a chain through feature space.</p><p>Algorithm adaptation methods adopt specific learning algorithms to the multi-label classifica- tion task without requiring problem transforma- tions. <ref type="bibr" target="#b4">Clare and King (2001)</ref> constructed decision tree based on multi-label entropy to perform clas- sification. <ref type="bibr" target="#b5">Elisseeff and Weston (2002)</ref> adopted a Support Vector Machine (SVM) like learning sys- tem to handle multi-label problem. <ref type="bibr" target="#b31">Zhang and Zhou (2007)</ref> utilized the k-nearest neighbor algo- rithm and maximum a posteriori principle to de- termine the label set of each sample. <ref type="bibr">Fürnkranz et al. (2008)</ref> made ranking among labels by uti- lizing pairwise comparison. <ref type="bibr" target="#b15">Li et al. (2015)</ref> used joint learning predictions as features. Recent studies of multi-label text classification have turned to the application of neural networks, which have achieved great success in natural lan- guage processing. <ref type="bibr" target="#b30">Zhang and Zhou (2006)</ref> imple- mented the fully-connected neural networks with pairwise ranking loss function. <ref type="bibr" target="#b19">Nam et al. (2013)</ref> changed the ranking loss function to the cross- entropy loss to better the training. <ref type="bibr" target="#b13">Kurata et al. (2016)</ref> proposed a novel neural network initializa- tion method to treat some neurons as dedicated neurons to model label correlations. <ref type="bibr" target="#b3">Chen et al. (2017)</ref> incorporated CNN and RNN so as to cap- ture both global and local semantic information and model high-order label correlations. ( <ref type="bibr" target="#b20">Nam et al., 2017)</ref> proposed to generate labels sequen- tially, and <ref type="bibr" target="#b28">Yang et al. (2018)</ref>;  both adopted the Seq2Seq, one with a novel decoder and one with a soft loss function respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this study, we propose our model based on the multi-level dilated convolution and the hybrid at- tention mechanism, which can extract both the semantic-unit-level information and word-level in- formation. Experimental results demonstrate that our proposed model can significantly outperform the baseline models. Moreover, the analyses re- flect that our model is competitive with the de- terministic hierarchical models and it is more ro- bust to classifying the low-frequency labels than the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of Multi-level Dilated Convolution (MDC). A example of MDC with kernel size k = 2 and dilation rates [1, 2, 3]. To avoid gridding effects, the dilation rates do not share a common factor other than 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structure of Hybrid Attention. The blue circles at the left bottom represent the source annotations generated by the LSTM encoder, the yellow circles at the right bottom represent the semantic unit representations generated by MDC, and the blue circles at the top represent the LSTM decoder outputs. At each decoding time step, the output of the LSTM attends to the semantic unit representations first, and then the new representation incorporated with high-level information attends to the source annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Micro-F 1 scores of our model and the baseline on the evaluation of labels of different frequency. The x-axis refers to the ranking of the most frequent label in the labels for classification, and the y-axis refers to the micro-F 1 score performance.</figDesc></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ lancopku/SU4MLC</note>

			<note place="foot" n="2"> http://www.ai.mit.edu/projects/jmlr/ papers/volume5/lewis04a/lyrl2004_rcv1v2_ README.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Nat-ural Science Foundation of China (No. 61673028) and the National Thousand Young Talents Pro-gram. Xu Sun is the corresponding author of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Haram: a hierarchical aram neural network for large-scale text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Benites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Sapozhnikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="847" to="854" />
		</imprint>
	</monogr>
	<note>Data Mining Workshop</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning multilabel scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ensemble application of convolutional and recurrent neural networks for multi-label text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2377" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge discovery in multi-label phenotype data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A kernel method for multi-labelled classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multilabel classification via calibrated label ranking. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="133" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LSTM can solve hard long time lag problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="473" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved neural network-based multi-label classification with better initialization leveraging label cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-label text categorization with joint learning predictions-as-features method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="835" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sememe prediction: Learning semantic knowledge from unstructured textual wiki descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1808.05437</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large-scale multi-label text classification-revisiting neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<idno>abs/1312.5419</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximizing subset accuracy with recurrent neural networks in multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyunwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5419" to="5429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">333</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SGM: sequence generation model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3915" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ml-knn: A lazy learning approach to multi-label learning. Pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
