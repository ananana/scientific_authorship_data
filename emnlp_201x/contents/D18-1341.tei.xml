<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy-Trang</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Information Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Austrlia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Austrlia</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3048" to="3053"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3048</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automated Post-Editing (PE) is the task of automatically correcting common and repetitive errors found in machine translation (MT) output. In this paper, we present a neural programmer-interpreter approach to this task, resembling the way that humans perform post-editing using discrete edit operations, which we refer to as programs. Our model out-performs previous neural models for inducing PE programs on the WMT17 APE task for German-English up to +1 BLEU score and-0.7 TER scores.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic post-editing (APE) is the automated task that aims to correct common systematic and repetitive errors found in machine translation (MT) output. APE systems can also be used to adapt general-purpose MT output to specific do- mains without re-training MT models, or to incor- porate information which is not available or ex- pensive to compute at MT decoding stage. Post- editing is considered as the modification process of a machine translated text with a minimum labor effort rather than re-translation from scratch.</p><p>Previous studies in neural APE have primarily concentrated on formalizing APE as a monolin- gual MT problem in the target language, with or without conditioning on the source sentence ( <ref type="bibr" target="#b9">Pal et al., 2016;</ref>. MT approach has suffered from over-correction where APE sys- tem performs unnecessary correction leading to paraphasing and the degradation of the output quality ( .</p><p>Recent works <ref type="bibr" target="#b7">(Libovick´yLibovick´y et al., 2016;</ref><ref type="bibr" target="#b1">Berard et al., 2017</ref>) have attempted to learn the predict of sequence of post-editing operations, e.g. inser- tion and deletion, to induce APE programs to turn the machine translated text into the desired out- put. Previous program induction approaches suf- fer from over-cautiousness, where the APE system tends to keep the machine translated text without any modification ( .</p><p>In this paper, we propose a programmer- interpreter approach to the APE task to address the over-cautiousness problem. Our architecture in- cludes an interpreter module, which executes the previous editing action before generating the next one. This is in contrast to the previous work, where the full program is induced before it is executed. The ability of execution immediately at every time step provides a proper condition- ing context based on the actual partial edited sen- tence to assist better prediction of the next op- eration. Moreover, the execution module can be pre-trained on monolingual target text, enabling our architecture to benefit from monolingual data in addition to PE data, which is hard to obtain. Our model is jointly trained on translation task and APE program induction task. The multi-task architecture allows the model to reconstruct the source-target alignment of the black-box MT sys- tem and inject it into post-editing task.</p><p>We compare our programmer-interpreter archi- tecture against previous works on the English- German APE based on the data for this task in WMT16 and WMT17. Compared to the previous work on APE program induction, our architecture achieves improvements up to +1 BLEU and -0.7 TER scores. Our analysis also shows that APE programs generated by our model are not only bet- ter at correcting errors but also attempt to perform more editing actions. <ref type="bibr" target="#b9">Pal et al. (2016)</ref> has applied the SEQ2SEQ model to APE. Their monolingual MT learned to post- edit English-Italian Google Translation output and was able to reduce the preposition related er- rors. Blindly performing edition over MT out- put, the monolingual APE has difficulty to cor- rect missing word or information in the source sentence. Neural multi-source MT architectures are applied to better capture the connection be- tween the source sentence/machine translated text and the PE output <ref type="bibr" target="#b7">(Libovick´yLibovick´y et al., 2016;</ref><ref type="bibr" target="#b11">Varis and Bojar, 2017;</ref><ref type="bibr" target="#b6">Junczys-Dowmunt and Grundkiewicz, 2017</ref>). Our work is motivated by <ref type="bibr" target="#b8">Ling et al. (2017)</ref> on learning to indirectly solve an algebraic word problem by inducing a program which generates the answer together with an explanation. It further builds up on recent work on neural programmer- interpreter <ref type="bibr" target="#b10">(Reed and De Freitas, 2016)</ref>, where a neural network programmer learns to program an interpreter. The architecture is then trained using expert action trajectories as programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The NPI-APE Approach</head><p>Given a source sentence s s s and a machine trans- lated sentence m m m, the goal is to find a post- edited sentence t t t = arg max t t t P ape (t t t |m m m, s s s) where P ape (.) is our probabilistic APE model. In our pro- posed approach, we aim to find an editing action sequence z z z to execute in order to generate the de- sired post-edited sentence, P ape (t t t|m m m, s s s) = z z z∈Z P ape (t t t, z z z|m m m, s s s).</p><p>We decompose the joint probability of a program and an output as:</p><formula xml:id="formula_0">P ape (z z z, t t t|m m m, s s s) = |z z z| i=1 P prog (z i |t t t ≤j i−1 , m m m, s s s) (1) ×P intp (t j i |m k i , z i ) (2)</formula><p>where P prog (z i |t t t ≤j i−1 , m m m, s s s) is the programmer's probability in producing the next edit operation z i given the post edited output t t t ≤j i−1 generated from the operations so far z z z ≤i−1 , and P intp (t j i |m k i , z i ) is the interpreter's probability of outputing t j i given the edit operation z i and the MT word m k i .</p><p>Following <ref type="bibr" target="#b1">Berard et al.(2017)</ref>, our action se- quence is performed on the MT sentence from left to right. At each position, we can take one of the following editing operations: (i) KEEP to keep the word and go to the next word, (ii) DELETE to delete the word and go to the next word, (iii) IN- SERT(WORD) to insert a new WORD and stay in that position, or (iv) STOP to terminate the pro- cess. In other words, the size of the operation set equals the size of the target vocabulary plus three, where we add the symbols KEEP, DELETE, and STOP as new tokens. Furthermore, j i is the num- ber of KEEP and INSERT(WORD) operations, and k i is the number of KEEP and DELETE operations in the sequence of operations z z z ≤i . This hard atten- tion mechanism is the outcome of the semantics of the operations, and injects task knowledge into the model. Moreover, P intp (t|m, z) is 1 if the output word t is consistent with performing the operation z on m, and zero otherwise.</p><p>Our decomposition of the joint probability of a program and post-edited output is distinguished from that proposed in ( <ref type="bibr" target="#b1">Berard et al., 2017)</ref>, P ape (t t t, z z z|m m m, s s s) = P intp (t t t|z z z, m m m)P prog (z z z|m m m, s s s). Cru- cially, in our decomposition (eqns 1 and 2), the programming and interpreting are interleaved at each position, whereas in ( <ref type="bibr" target="#b1">Berard et al., 2017</ref>) the programming is fully done before the interpreta- tion phase and they are independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Architecture and Joint Training</head><p>The architecture consists of three components (i) A SEQ2SEQ model to translate the source sen- tence to the target in the forced-decoding mode (MT), (ii) A SEQ2SEQ model to incrementally generate the sequence of edit operations (Action Generator), and (iii) An RNN to summarize the post edited sequence of words produced from the execution of actions generated so far (Interpreter). í µí±í µí±í µí±í µí± í µí±í µí±í µí°¸í µí°¸ í µí± §í µí± § 1 í µí±í µí±í µí±í µí±í µí± ℎí µí± <ref type="bibr">1</ref> ℎí µí± í µí±í µí±í µí±í µí± ℎí µí± í µí±í µí± í µí± §í µí± § í µí±í µí±−1 í µí± §í µí± § í µí±í µí± í µí±¡í µí±¡ <ref type="bibr">1</ref> í µí±¡í µí±¡ í µí±í µí±í µí±í µí±−1 í µí±£í µí±£ <ref type="bibr">1</ref> í µí±£í µí±£ í µí±í µí±í µí±í µí±−1 Interpreter The encoder and decoder of the MT compo- nent are a bidirectional and unidirectional LSTM, whose states are denoted by h h h l and g g g k , respec- tively. Similarly, the encoder and decoder of the AG (Action Generator) component are a bidirec- tional and unidirectional LSTM, whose states are denoted by h h h k and g g g i , respectively. The states of the unidirectional RNN in the interpreter are de- noted by v v v j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Generator</head><p>The next edit operation z i is generated from the decoder state of the AG g g g i (see <ref type="figure">Figure 2</ref>), which is computed from the previous state g g g i−1 and a con- text including the following: (i) h h h k i where k i is the index of the MT output word currently pro- cessed, (ii) c c c k i which is the context vector from the MT component when generating the current target word, and (iii) v v v j i−1 which is the last hidden state of the interpreter RNN encoding the post edited sentence generated so far.</p><p>The model is trained jointly for the translation task (SRC→TGT), and for the post editing task (SRC,TGT→OP,PE). For the training data, we compute the lowest-cost sequence of editing op- erations (OP) using dynamic programming, where the cost of insertion and deletion are 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset. We evaluate the proposed approach on the English-to-German (En-De) post-editing task in the IT domain using the data from WMT16 1 and WMT17. <ref type="bibr">2</ref> The official WMT'16 and WMT'17 1 http://www.statmt.org/wmt16/ape-task.html 2 http://www.statmt.org/wmt17/ape-task.html dataset contains 12K and 11K post-editing triplets <ref type="bibr">(English, translated German, post-edited German)</ref> respectively in IT domain. We concatenated them to an 23K triplets. A synthetic corpus of 500K triplets <ref type="bibr" target="#b5">(Junczys-Dowmunt and Grundkiewicz, 2016</ref>) is also available as additional train- ing data. We performed our experiment in two dif- ferent settings with and without synthetic data for comparison with <ref type="bibr" target="#b1">Berard et al. (2017)</ref>.</p><p>The RNN in the interpreter component can be thought of as a language model. This paves the way to pre-train it using monolingual text. We collect in-domain IT text from OPUS 3 from the following sections: GNOME, KDE, KDEdoc, OpenOffice, OpenOffice3, PHP and Ubuntu. Af- ter tokenizing, filtering out sentences containing special characters, and removing duplications, we obtain around 170K sentences. Furthermore, we compare against monolin- gual SEQ2SEQ (TGT→PE) as well as the multi- source SEQ2SEQ (SRC+TGT→PE) <ref type="bibr" target="#b11">(Varis and Bojar, 2017)</ref>. Monolingual SEQ2SEQ (TGT→PE) model is an attentional SEQ2SEQ model <ref type="bibr" target="#b0">(Bahdanau et al., 2015</ref>) that takes target sentence as input and outputs desired PE sentence. In multi- source SEQ2SEQ (SRC+TGT→PE), we use two encoders for source and target sentences and con- catenate their context vectors. In both models, the encoder and decoder contain a single layer of bidirectional and unidirectional LSTM respec- tively. The size of the LSTM hidden dimensions and word embedding in these models is set to 256 and 128, respectively. This ensures almost the same number of parameters (∼13M) in all archi- tectures.</p><p>Training. We use a multi-task scenario to jointly train the parameters of the components in MT+AG as well as MT+AG+LM models. For the latter, we warm start the embedding of the target words with  All models are trained with SGD, where the learning rate is initialised to 1 and decays after each epoch. The learning rate is decayed 0.8 after every epoch for model trained with official post- editing data, and 0.5 every half epoch for model with synthetic data. All models use the same vo- cabulary on the same data condition. The Vocabu- lary size is 30K for large dataset experiments, and 27K/19K for the 23K/12K data conditions. In all experiments, the best model is selected based on TER on the validation set. For decoding, we use beam search with the beam size of 10. <ref type="table" target="#tab_1">Table 1</ref> shows the result on different training datasets to compare our model against the base- lines. Original MT is the strong standard do- nothing baseline, i.e. copying the MT trans- lation as the PE output. In all settings, our MT+AG+LM models outperforms the MT+AG and monolingual/multi-source SEQ2SEQ models. Specifically, our model outperform MT+AG in 500K+12K training condition by almost 1 BLEU score on test2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>As expected, the models trained on 23K data perform better than those trained on 12K; further gains are obtained by adding 500K synthetic data. Interestingly, training MT+AG and MT+AG+LM models on 23K data lead to better TER/BLEU than those trained on 500K+12K. This implies the importance of in-domain training data, as the synthetic corpus is created using general domain Common-Crawl corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>We perform fine-grained analysis of the changes made by our model vs MT+AG in order to un- derstand the sources of improvements. For dif- ferent data conditions, <ref type="table" target="#tab_2">Table 2</ref> shows the number of modified sentences by each model as well as the sentence level precision defined as the fraction of sentences with improved TER. Moreover, it re- ports the total number of actions generated by the model on sentences with improved TER, as well as the precision of such actions, i.e. the fraction of those observed in the ground truth action trajecto-ries.</p><p>As reported in <ref type="bibr" target="#b1">Berard et al. (2017)</ref>, one ma- jor challenge of predicting action sequences is the class imbalance. The model is often too conserva- tive about its edits, and tends to use the KEEP far more than the INSERT and DELETE actions. Our Programmer-Interpreter model tackles this prob- lem, as evidenced by its comparable number of modified sentences, but with higher sentence and action level precision in almost all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented a neural programmer-interpreter approach to automated post-editing of MT output. Our approach inter- leaves generating the sequence of edit actions by a programmer component, and executing those ac- tions with an interpreter component. This leads to better capturing the history of the past gener- ated actions when generating the next action. Our approach achieves up to +1 BLEU and -.7 TER improvement compared to a variant in which pro- gramming is not interleaved with execution. Fu- ture work includes inducing macro-actions com- posed of simpler building block actions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of PE program when executing on MT would return the PE reference. The color denotes the alignment between MT, reference PE and the APE program. The number subscript shows the edit position in original MT sentence.</figDesc><graphic url="image-1.png" coords="2,72.00,62.81,226.77,66.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Chatterjee et al. (2017) ensemble several different models including monolingual MT (TGT→PE), bilingual MT (SRC→PE), and multi-source (SRC,TGT→PE). Libovick´yLibovick´y et al. (2016); Berard et al. (2017) have proposed learn- ing to predict the sequence of edit operations, aka the program, to produce the post-editing sentence (c.f. §3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>í</head><label></label><figDesc>Figure 2: Our proposed NPI-APE model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Setup.</head><label></label><figDesc>There are three components in our archi- tecture: machine translation (MT), action genera- tor (AG), and interpreter (LM). We compare our MT+AG+LM architecture against MT+AG 4 Be- rard et al. (2017) which does not have the LM component. The size of the hidden dimensions (LSTMs in the MT and AG, and simple RNN in the LM component) as well as word embedding in these models is set to 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>TER and BLEU scores of our model (MT+AG+LM) v.s. the rest on various data conditions for the EN-DE 
post-editing task. bold: Best results within a data condition; : Best results across data conditions 

;  † : Statistically significant compared to MT+AG with pvalue ≤ 0.05. 

those obtained from training the LM component 
on the monolingual text 5 . 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Sentence precision and action precision of models trained on 23K and 500K+23K dataset.</figDesc><table></table></figure>

			<note place="foot" n="3"> http://opus.nlpl.eu/ 4 The AG decoder in MT+AG conditions a state on the last generated action as well.</note>

			<note place="foot" n="5"> The MT, AG, and LM components share the target word embedding table in our model. Similarly, MT and AG components share the target embedding in the MT+AG model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to the anonymous review-ers for their helpful comments and corrections. This work was supported by the Multi-modal Aus-tralian ScienceS Imaging and Visualisation Envi-ronment (MASSIVE) (www. massive.org.au), and partially supported by a Google Faculty Award to GH and the Australian Research Council through DP160102686.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>References Dzmitry Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LIG-CRIStAL submission for the WMT 2017 automatic post-editing task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="623" to="629" />
		</imprint>
	</monogr>
	<note>Shared Task Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (WMT17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
	<note>Shared Task Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-source neural automatic post-editing: FBK&apos;s participation in the WMT 2017 APE shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajen Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Amin Farajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="630" to="638" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="751" to="758" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An exploration of neural sequence-tosequence architectures for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CUNI system for WMT16 automatic post-editing and multimodal translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Tlust´ytlust´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural network based approach to automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santanu</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Sudip Kumar Naskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Vela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="281" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural programmer-interpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CUNI system for WMT17 automatic post-editing task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusan</forename><surname>Varis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="661" to="666" />
		</imprint>
	</monogr>
	<note>Shared Task Papers. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
