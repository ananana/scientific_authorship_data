<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2044" to="2054"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper, we model the task as a machine comprehension problem where pseudo question-answer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspect-specific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information , and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document-level sentiment classification is one of the pragmatical sentiment analysis tasks <ref type="bibr" target="#b25">(Pang and Lee, 2007;</ref><ref type="bibr" target="#b19">Liu, 2010)</ref>. There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc. Most of reviews are very compre- hensive and thus long documents. Analyzing these documents to predict ratings of products or ser- vices is an important complementary way for bet- ter customer relationship management. Recently, neural network based approaches have been de- veloped and become state-of-the-arts for long- document sentiment classification ( <ref type="bibr">Tang et al., 2015a,b;</ref>. However, predict- ing an overall score for each long document is not enough, because the document can mention dif- "The situation is good, it's very clean, but there is nothing special. Breakfast at downstairs is directly from grocery store. Water pressure is good! A decent choice for sleeping. New York is expensive place!" Cleanliness:5 Room:: 4 Value:: 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating</head><p>Figure 1: Example: hotel review with aspects.</p><p>ferent aspects of the corresponding product or ser- vice. For example, in <ref type="figure">Figure 1</ref>, there could be dif- ferent aspects for a review of hotel. These aspects help customer service better understand what are the major pros and cons of the product or ser- vice. Compared to the overall rating, users are less motivated to give aspect ratings. Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predict- ing different ratings for each aspect rather than an overall rating.</p><p>One straightforward approach for document- level multi-aspect sentiment classification is multi-task learning <ref type="bibr" target="#b3">(Caruana, 1997)</ref>. For neural networks, we can simply treat each aspect (e.g., rating from one to five) as a classification task, and let different tasks use softmax classifier to extract task-specific representations at the top layer while share the input and hidden layers to mutually en- hance the prediction results <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b22">Luong et al., 2016)</ref>. However, such approach ig- nores the fact that the aspects themselves have semantic meanings. For example, as human be- ings, if we were asked to evaluate the aspect rat- ing of a document, we simply read the review, and find aspect-related keywords, and see around com- ments. Then, we aggregate all the related snippets to make a decision.</p><p>In this paper, we propose a novel approach to treat document-level multi-aspect sentiment clas-  <ref type="figure">Figure 2</ref>: The architecture of our model. Left: multi-task learning. Right: hierarchical attention module which includes input encoders and iterative attention modules. sification as a machine comprehension ( <ref type="bibr" target="#b16">Kumar et al., 2016;</ref><ref type="bibr" target="#b33">Sordoni et al., 2016)</ref> problem. To mimic human's evaluation of aspect classification, we create a list of keywords for each aspect. For example, when we work on the Room aspect, we generate some keywords such as "room," "bed," "view," etc. Then we can ask pseudo questions: "How is the room?" "How is the bed?" "How is the view?" and provide an answer "Rating 5." In this case, we can train a machine comprehen- sion model to automatically attend corresponding text snippets in the review document to predict the aspect rating. Specifically, we introduce a hier- archical and iterative attention model to construct aspect-specific representations. We use a hierar- chical architecture to build up different representa- tions at both word and sentence levels interacting with aspect questions. At each level, the model consists of input encoders and iterative attention modules. The input encoder learns memories 1 of documents and questions with Bi-directional LSTM (Bi-LSTM) model and non-linear mapping respectively. The iterative attention module takes into memories as input and attends them sequen- tially with a multiple hop mechanism, performing effective interactions between documents and as- pect questions.</p><p>To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines. We also analyze the effects of num-bers of the hop and aspect words on performances. Moreover, a case study for attention results is per- formed at both word and sentence levels.</p><p>The contributions of this paper are two-fold. First, we study the document-level multi-aspect sentiment classification as a machine comprehen- sion problem and introduce a hierarchical itera- tive attention model for it. Second, we demon- strate the effectiveness of proposed model on two datasets, showing that our model outperforms classical baselines. The code and data for this paper are available at https://github.com/ HKUST-KnowComp/DMSCMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we introduce our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition and Hierarchical Framework</head><p>We first briefly introduce the problem we work on. Given a piece of review, our task is to predict the ratings of different aspects. For example, in <ref type="figure">Fig- ure 1</ref>, we predict the ratings of Cleanliness, Room, and Value. To achieve this, we assume that there are existing reviews with aspect ratings for ma- chines to learn. Formally, we denote the review document as d containing a set of T d sentences {s 1 , s 2 , . . . s T d }. For the t-th sentence s t , we use a set of words w 1 , w 2 , . . . w |st| to represent it, and use w i , w w i and w p i as the one-hot encoding, word embedding, and phrase embedding for w i respec- tively. The phrase embedding encodes the seman- tics of phrases where the current word w i is the center (e.g., hidden vectors learned by Bi-LSTM shown in Section 2.2). For each q k of K aspects {q 1 , q 2 , . . . , q K }, we use N k aspect-related key- words, q k 1 , q k 2 . . . q k N k , to represent it. Simi- larly, we use q k i , q w k i as the one-hot encoding and word embedding for q k i respectively.</p><p>There are several sophisticated methods for choosing aspect keywords (e.g., topic model).</p><p>Here, we consider a simple way where five seeds were first manually selected for each aspect and then more words were obtained based on their co- sine similarities with seeds <ref type="bibr">2</ref> As shown in <ref type="figure">Figure 2</ref> (left), our framework fol- lows the idea of multi-task learning, which learns different aspects simultaneously. In this case, all these tasks share the representations of words and architecture of semantic model for the final clas- sifiers. Different from straightforward neural net- work based multi-task learning <ref type="bibr" target="#b7">(Collobert et al., 2011</ref>), for each document d and an aspect q k , our model uses both the content of d and all the related keywords q k 1 , q k 2 . . . q k N k as input. Since the keywords can cover most of the semantic mean- ings of the aspect, and we do not know which document mentions which semantic meaning, we build an attention model to automatically decide it (introduced in Section 2.3). Assuming that the keywords have been decided, we use a hierarchi- cal attention model to select useful information from the review documents. As shown in <ref type="figure">Figure 2</ref> (right), the hierarchical attention of keywords is applied to both sentence level (to select meaning- ful words) and document level (to select mean- ingful sentence). Thus, our model builds aspect- specific representations in a bottom-up manner.</p><p>Specifically, we obtain sentence representa- tions</p><formula xml:id="formula_0">s k 1 , s k 2 , . . . s k T</formula><p>using the input encoder (Sec- tion 2.2) and iterative attention module (Sec- tion 2.3) at the word level. Then we take sen- tence representations and k-th aspect as input and apply the sentence-level input encoder and atten- tion model to generate the document representa- tion d k for final classification. As shown in Fig- ure 2 (right), the attention model is applied twice at different levels of the representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Input Encoder</head><p>The input module builds memory vectors for the iterative attention module and is performed both at word and sentence levels. For a document, it con- 2 For example, the words "value," "price," "worth," "cost," and "$" are selected as seeds for aspect Price. The informa- tion for seeds can be found in our released resource. verts word sequence into word level memory M d w and sentence sequence into sentence level mem- ory M d s respectively. For an aspect question q k , it takes a set of aspect-specific words {q k i } 1≤i≤N k as input and derives word level memory M q w and sentence level memory M q s . To construct M d w , we obtain word embeddings w w 1 , w w 2 , . . . w w |st| from an embedding matrix E A applied to all words shown in the corpus. Then, LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> model is used as the encoder to produce hidden vectors of words based on the word embeddings. At each step, LSTM takes input w w t and derives a new hidden vector by h t = LSTM(w w t , h t−1 ). To preserve the subsequent context information for words, another LSTM is ran over word se- quence in a reverse order simultaneously. Then the forward hidden vector − → h t and backward hidden vector ← − h t are concatenated as phrase embedding w p t . We stack these phrase embeddings together as word level memory M d w . Similarly, we feed sentence representations into another Bi-LSTM to derive the sentence level memory M d s . Note that, the sentence representations are obtained using the iterative attention module which is described as Eq. <ref type="formula" target="#formula_6">(5)</ref> in Section 2.3.</p><p>Since we have question keywords as input, to allow the interactions between questions and doc- uments, we also build question memory in follow- ing way. We obtain</p><formula xml:id="formula_1">Q k = q w k i 1≤i≤N k</formula><p>by look- ing up an embedding matrix 3 E B applied to all question keywords. Then a non-linear mapping is applied to obtain the question memory at word level:</p><formula xml:id="formula_2">M q k w = tanh(Q k W q w ),<label>(1)</label></formula><p>where W q w is the parameter matrix to adapt q k at word level. Similarly, we use another mapping to obtain the sentence level memory:</p><formula xml:id="formula_3">M q k s = tanh(Q k W q s ),<label>(2)</label></formula><p>where W q s is the parameter matrix to adapt q k at sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Iterative Attention Module</head><p>The iterative attention module (IAM) attends and reads memories of questions and documents al- ternatively with a multi-hop mechanism, deriving  aspect-specific sentence and document represen- tations. As we discussed in the introduction, the set of selected question keywords may not best characterize the aspect for different documents. Thus, the IAM module introduces a backward at- tention to use document information (word or sen- tence) to select useful keywords of each aspect as the document-specific question to build attention model.</p><p>The illustration of IAM is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. To obtain sentence representations, it takes M d w and M q w as the input and performs m iterations (hops). For each iteration, IAM conducts four operations: (1) attends the question memory by the selective vector p and summarizes question memory vec- tors into a single vectorˆqvectorˆ vectorˆq; (2) updates the selec- tive vector by the previous one andˆqandˆ andˆq; (3) attends document (content) memory based on the updated selective vector and summarizes memory vectors in to a single vectorˆcvectorˆvectorˆc; (4) updates the selective vector by the previous one andˆcandˆandˆc.</p><p>We unify operations (1) and <ref type="formula" target="#formula_4">(3)</ref>  </p><formula xml:id="formula_4">H = tanh(MW a (1p)) a = softmax(Hv T a ) ˆ x = a i M i ,<label>(3)</label></formula><p>where 1 is a vector with all elements are 1, which copies the selective vector to meet the dimension requirement. The W a and v a are parameters, a is attention weights for memory vectors, and M i means i-th row in M.</p><p>Operations <ref type="formula" target="#formula_3">(2)</ref> and <ref type="formula" target="#formula_5">(4)</ref> are formulated as an up- date function p 2i−{l} = U(ˆ x, p 2i−{l}−1 ), where i is the hop index, l can be 0 or 1 which corresponds tô x = ˆ c orˆxorˆorˆx = ˆ q respectively. We initialize p 0 by a zero vector. The update function U can be a recurrent neural network <ref type="figure">(Xiong et al., 2017)</ref> or other heuristic weighting functions. In this paper, we introduce a simple strategy:</p><formula xml:id="formula_5">p 2i−{l} = ˆ x,<label>(4)</label></formula><p>which ignores the previous selective vector but succeeds to obtain comparable results with other more complicated function in the initial experi- ments. Multi-hop mechanism attends different mem- ory locations in different hops ( <ref type="bibr" target="#b34">Sukhbaatar et al., 2015)</ref>, capturing different interactions between documents and questions. In order to preserve the information of various kinds of interactions, we concatenate alî c's in each hop as the final repre- sentations of sentences:</p><formula xml:id="formula_6">s = [ˆ c 1 ; ˆ c 2 ; · · · ˆ c m ].<label>(5)</label></formula><p>After obtaining sentence representations, we feed them into the sentence-level input encoder, deriving the memories M d s and M q s . Then, the aspect-specific document representation d k is ob- tained by the sentence-level IAM in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Objective Function</head><p>For each aspect, we obtain aspect-specific docu- ment representations {d k } 1≤k≤K . All these repre- sentations are fed into classifiers, each of which in- cludes a softmax layer. The softmax layer outputs the probability distribution over |Y| categories for the distributed representation, which is defined as:</p><formula xml:id="formula_7">p (d, k) = softmax(W class k d k ),<label>(6)</label></formula><p>where W class k is the parameter matrix. We define the cross-entropy objective function between gold sentiment distribution p(d, k) and predicted sentiment distribution p (d, k) as the classification loss function:</p><formula xml:id="formula_8">− d∈D K k=1 |Y| i=1 p(d, k)log(p (d, k)),<label>(7)</label></formula><p>where p(d, k) is a one-hot vector, which has the same dimension as the number of classes, and only the dimension associated with the ground truth la- bel is one, with others being zeros. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>In this section, we show experimental results to demonstrate our proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct our experiments on TripAdvi- sor ( <ref type="bibr" target="#b40">Wang et al., 2010</ref>) and BeerAdvocate ( <ref type="bibr" target="#b23">McAuley et al., 2012;</ref><ref type="bibr" target="#b18">Lei et al., 2016</ref>) datasets, which contain seven aspects (value, room, lo- cation, cleanliness, check in/front desk, service, and business service) and four aspects (feel, look, smell, and taste) respectively. We follow the pro- cessing step ( <ref type="bibr" target="#b18">Lei et al., 2016</ref>) by choosing the re- views with different aspect ratings and the new datasets are described in <ref type="table">Table 1</ref>. We tokenize the datasets by Stanford corenlp <ref type="bibr">4</ref> and randomly split them into training, development, and testing sets with 80/10/10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline Methods</head><p>To demonstrate the effectiveness of the proposed method, we compare our model with following baselines: Majority uses the majority sentiment label in development sets as the predicted label.</p><p>SVM uses unigram and bigram as text features and uses <ref type="bibr">Liblinear (Fan et al., 2008</ref>) for learning.</p><p>SLDA refers to supervised latent Dirichlet allo- cation ( <ref type="bibr" target="#b2">Blei and Mcauliffe, 2010)</ref> which is a sta- tistical model of labeled documents.</p><p>NBoW is a neural bag-of-words model averag- ing embeddings of all words in a document and feeds the resulted embeddings into SVM classifier.</p><p>DAN is a deep averaging network model which consists of several fully connected layers with av- eraged word embeddings as input. One novel word dropout strategy is employed to boost model performances ( <ref type="bibr" target="#b14">Iyyer et al., 2015)</ref>.</p><p>CNN continuously performs a convolution op- eration over a sentence to extract words neighbor- ing features, then gets a fixed-sized representation by a pooling layer <ref type="bibr" target="#b15">(Kim, 2014</ref>).</p><p>LSTM is one variant of recurrent neural net- work and has been proved to be one of state-of- the-art models for document-level sentiment clas- sification ( <ref type="bibr" target="#b35">Tang et al., 2015a</ref>). We use LSTM to refer Bi-LSTM which captures both forward and backward semantic information.</p><p>HAN means the hierarchical attention network which is proposed in <ref type="figure">(Yang et al., 2016)</ref> for doc- ument classification. Note that, the original HAN depends GRU as the encoder. In our experiments, LSTM-based HAN obtains slightly better results. Thus, we report the results of HAN with LSTM as the encoder.</p><p>We extend DAN, CNN, LSTM with the hierar- chical architecture and multi-task framework, the corresponding models are MHDAN, MHCNN and MHLSTM respectively. Besides, MHAN is also evaluated as one baseline, which is HAN with the multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>We implement all neural models using Theano <ref type="bibr">(Theano Development Team, 2016)</ref>. The model parameters are tuned based on the de- velopment sets. We learn 200-dimensional word embeddings with Skip-gram model ( <ref type="bibr" target="#b24">Mikolov et al., 2013</ref>) on in-domain corpus, which fol- lows ( <ref type="bibr" target="#b35">Tang et al., 2015a</ref>). The pre-trained word embeddings are used to initialize the embedding matrices E A and E B . The dimensions of all hidden vectors are set to 200. For TripAdvisor dataset, the hop numbers of word-level and sentence-level iterative attention modules are set to 4 and 2 respectively. For BeerAdvocate dataset, the hop numbers are set to 6 and 2. The number of selected keywords N k = N is set to 20. To avoid model over-fitting, we use dropout and regularization as follows: (1) the regularization parameter is set to 1e-5; (2) the dropout rate is set to 0.3, which is applied to both sentence and document vectors. All parameters are trained by ADADELTA <ref type="bibr" target="#b45">(Zeiler, 2012</ref>) without needing to set the initial learning rate. To ensure fair comparisons, we make baselines have same settings as the proposed model, such as word embeddings, dimensions of hidden vectors and optimization details and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results and Analyses</head><p>We use accuracy and mean squared error (MSE) as the evaluation metrics and the results are shown in <ref type="table" target="#tab_4">Table 2</ref>.    Compared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effec- tive than traditional ngram features on these two datasets. All neural network models outperform NBoW. It shows the advantages of neural net- works in the document sentiment classification.</p><p>From the results of neural networks, we can observe that DAN performs worse than LSTM and CNN, and LSTM achieves slightly higher re- sults than CNN. It can be explained that the sim- ple composition method averaging embeddings of words in a document but ignoring word order, may not be as effective as other flexible compo- sition models, such as LSTM and CNN, on as- pect classification. Additionally, we observe that the multi-task learning and hierarchical architec- ture are beneficial for neural networks. Among all baselines, MHAN and MHLSTM achieve compa- rable results and outperform others.</p><p>Compared with MHAN and MHLSTM, our method achieves improvements of 1.5% (3% rel- ative improvement) and 1.0% (2.5% relative im- provement) on TripAdvisor and BeerAdvocate re- spectively, which shows that the incorporation of iterative attention mechanism helps the deep neu- ral network based model build up more discrim- inative aspect-aware representation. Note that BeerAdvocate is relatively more difficult since the predicted ratings are from 1 to 10 while TripAd- visor is 1 to 5. Moreover, t-test is conducted by randomly splitting datasets into train/dev/test sets and random initialization. The results on test sets are described in <ref type="table" target="#tab_5">Table 3</ref> which show performance of our model is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study for Attention Results</head><p>In this section, we sample two sentences from TripAdvisor to show the visualization of atten- tion results for case study. Both word-level and sentence-level attention visualizations are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.</p><p>From the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect. For example, in the first sentence, the words comfortable and bed are assigned higher weights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness. In the second sentence, the word internet is assigned a high attention value for Business. Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable. Specif- ically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops. In the second sentence, we observe that the value of word internet increases with the number of hop. Thus, we can see that the more sensible weights are obtained for words through the pro- posed iterative attention mechanism. Similarly, the figures (c) and <ref type="bibr">(d)</ref> show that the conclusion from words is also suitable for sentences. For the first sentence, the sentence weight regarding the aspect Room is lower than Cleanliness in the first hop, but surpasses Cleanliness in the second hop. For the second sentence, the weight for Business becomes higher in the second hop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Effects of Hop and Aspect Keywords</head><p>In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances. All the experiments are conducted on the development set. Due to lack of space, we only present the results of TripAdvisor and the re- sults of BeerAdvocate have a similar behavior as TripAdvisor.</p><p>For the hop number, we vary m from 1 to 7 and the results are shown in <ref type="figure">Figure 5</ref> (left). We can see that: (1) at the word level, the performance in- creases when m ≤ 4, but shows no improvement after m &gt; 4; (2) at the sentence level, model per- forms best when m = 2. Moreover, we can see that the hop number of word level leads to larger variation than the hop number of sentence level.</p><p>For the size of aspect keywords, we vary N from 0 to 35, incremented by 5. Note that, we set a learnable vector to represent question mem- ory when N = 0. The results are shown in <ref type="figure">Fig- ure 5 (right)</ref>. We observe that the performance in- creases when N ≤ 20, and has no improvement after N &gt; 20. This indicates that a small number of keywords can help the proposed model achieve competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Multi-Aspect Sentiment Classification. Multi- aspect sentiment classification has been studied extensively in literature. <ref type="bibr" target="#b20">Lu et al. (2011)</ref>   <ref type="bibr">Donald, 2008;</ref><ref type="bibr" target="#b40">Wang et al., 2010;</ref><ref type="bibr" target="#b9">Diao et al., 2014;</ref><ref type="bibr" target="#b26">Pappas and Popescu-Belis, 2014</ref>). How- ever, these approaches often rely on strict assump- tions about words and sentences, for example, us- ing the word syntax to determine if a word is an aspect or a sentiment word, or relating a sen- tence with an specific aspect. Another related problem is called aspect-based sentiment classi- fication ( <ref type="bibr" target="#b28">Pontiki et al., 2014</ref><ref type="bibr" target="#b27">Pontiki et al., , 2016</ref><ref type="bibr" target="#b30">Poria et al., 2016)</ref>, which first extracts aspect expressions from sentences ( <ref type="bibr" target="#b29">Poria et al., 2014;</ref><ref type="bibr" target="#b1">Balahur and Montoyo, 2008;</ref>, and then determines their sentiments. With the develop- ments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engi- neering work. <ref type="bibr" target="#b37">Tang et al. (2016)</ref> employed a deep memory network for aspect-based sentiment clas- sification given the aspect location and <ref type="bibr" target="#b17">Lakkaraju et al. (2014)</ref> employed recurrent neural networks and its variants for the task of extraction of aspect- sentiment pair. However, these tasks are sentence- level. Another related research field is document- level sentiment classification because we can treat single aspect sentiment classification as an indi- vidual document classification task. This line of research includes ( <ref type="bibr" target="#b36">Tang et al., 2015b;</ref><ref type="bibr" target="#b4">Chen et al., 2016;</ref><ref type="bibr" target="#b37">Tang et al., 2016;</ref> which are based on neural networks in a hierarchical structure. However, they did not work on multi- ple aspects.</p><p>Machine Comprehension. Recently, neural network based machine comprehension (or read- ing) has been studied extensively in NLP, with the releases of large-scale evaluation datasets <ref type="bibr" target="#b11">(Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Hill et al., 2016;</ref><ref type="bibr" target="#b31">Rajpurkar et al., 2016</ref>). Most of the related studies focus on attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref> which is firstly proposed in machine translating and aims to solve the long-distance dependency between words. <ref type="bibr" target="#b11">Hermann et al. (2015)</ref> used Bi- LSTM to encode document and query, and pro- posed Attentive Reader and Impatient Reader. The first one attends document based on the query rep- resentation, and the second one attends document by the representation of each token in query with an incremental manner. Memory Networks <ref type="bibr" target="#b34">Sukhbaatar et al., 2015</ref>) attend and reason document representation in a multi- hop fashion, enriching interactions between doc- uments and questions. Dynamic Memory Net- work ( <ref type="bibr" target="#b16">Kumar et al., 2016</ref>) updates memories of documents by re-running GRU models based on derived attention weights. Meanwhile, the query representation is refined by another GRU model. Gated-Attention Reader ( <ref type="bibr" target="#b8">Dhingra et al., 2016)</ref> proposes a novel attention mechanism, which is based on multiplicative interactions between the query embeddings and the intermediate states of a recurrent neural network document reader. Bi- Directional Attention Model ( <ref type="bibr" target="#b43">Xiong et al., 2017;</ref><ref type="bibr" target="#b32">Seo et al., 2017</ref>) fuses co-dependent representa- tions of queries and documents in order to fo- cus on relevant parts of both. Iterative Atten- tion model ( <ref type="bibr" target="#b33">Sordoni et al., 2016</ref>) attends question and document sequentially, which is related to our model. Different from Iterative Attention model, our model focuses on the document-level multi- aspect sentiment classification, which is proposed in a hierarchical architecture and has different pro- cedures in the iterative attention module. Another related research problem is visual question an- swering which uses an image as question context rather than a set of keywords as question. Neu- ral network based visual question answering ( <ref type="bibr" target="#b21">Lu et al., 2016;</ref><ref type="bibr" target="#b42">Xiong et al., 2016</ref>) is similar as the proposed models in text comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we model the document-level multi- aspect sentiment classification as a text compre- hension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model. Ex- tensive experiments show that our model outper- forms the other neural models with multi-task framework and hierarchical architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61472006 and 91646202) as well as the National Basic Research <ref type="bibr">Program (973 Program No. 2014CB340405)</ref>. This work was also supported by NVIDIA Corporation with the do- nation of the Titan X GPU, Hong Kong CERG Project 26206717, <ref type="bibr">China 973 Fundamental R&amp;D Program (No.2014CB340304)</ref>, and the LORELEI Contract HR0011-15-2-0025 with DARPA. The views expressed are those of the authors and do not reflect the official policy or position of the De- partment of Defense or the U.S. Government. We also thank the anonymous reviewers for their valu- able comments and suggestions that help improve the quality of this manuscript.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The iterative attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>by an attention functionˆxfunctionˆfunctionˆx = A(p, M), where M could be M d w or M q w which correspondsˆxcorrespondsˆcorrespondsˆx = ˆ c orˆxorˆorˆx = ˆ q. The attention function A is decomposed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The attention visualization of words and sentences. Darker color means higher weight. (a) and (b) are the visualization of word weights; (c) and (d) are the visualization of sentence weights. The top figures in (a) and (b) show the word weights of fourth hop for each aspect. The bottom figures in (a) and (b) visualize the word weights of different hops for the aspects Room and Business respectively.</figDesc><graphic url="image-2.png" coords="7,72.00,62.81,498.90,244.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 : Comparison of our model and other baseline methods.</head><label>2</label><figDesc></figDesc><table>Model 
TripAdvisor 
BeerAdvocate 
Accuracy 
MSE 
Accuracy 
MSE 
MHLSTM 44.75 (0.24) 1.256 (0.05) 37.28 (0.43) 1.802 (0.17) 
MHAN 
45.02 (0.33) 1.221 (0.12) 37.02 (0.22) 1.810 (0.15) 
Our 
46.65  † (0.29) 1.084  *  (0.06) 38.25  † (0.35) 1.749  *  (0.18) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The results of average accuracy/MSE and standard deviation of models on test sets. We choose 
MHAN and MHLSTM as comparison baselines for TripAdvisor and BeerAdvocate respectively. In t-
tests, the marker  *  refers to p-value &lt; 0.05 and the marker  † refers to p-value &lt; 0.01. 

</table></figure>

			<note place="foot" n="1"> Following the work (Weston et al., 2015; Sukhbaatar et al., 2015), we refer the memory to a set vectors which are stacked together and could be attended.</note>

			<note place="foot" n="3"> E A and E B are initialized by the same pre-trained embeddings but are different embedding matrices with different updates.</note>

			<note place="foot" n="4"> http://nlp.stanford.edu/software/corenlp.shtml</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A feature dependent method for opinion mining and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="327" to="332" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aspect extraction with automated prior knowledge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="347" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting domain knowledge in aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1655" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD. ACM</title>
		<meeting>KDD. ACM</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boydgraber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect specific sentiment analysis using hierarchical deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentiment analysis and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="627" to="666" />
		</imprint>
	</monogr>
	<note>Second Edition.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-aspect sentiment analysis with topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multiaspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM. IEEE</title>
		<meeting>ICDM. IEEE</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explaining the stars: Weighted multiple-instance learning for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Salud María Jiménez-Zafra, and GülsGüls¸en Eryi˘ git</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronique</forename><surname>Orphee De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Núria</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Semeval-2016 task 5: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A rule-based approach to aspect extraction from product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second workshop on natural language processing for social media (SocialNLP)</title>
		<meeting>the second workshop on natural language processing for social media (SocialNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sentic lda: Improving on lda with semantic similarity for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bisio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4465" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02245</idno>
		<title level="m">Iterative alternating neural attention for machine reading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A joint model of text and aspect ratings for sentiment summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Citeseer</title>
		<meeting>ACL. Citeseer</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis on review text data: a rating regression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD. ACM</title>
		<meeting>KDD. ACM</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
