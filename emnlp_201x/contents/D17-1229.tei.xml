<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Preserving Distributional Information in Dialogue Act Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><forename type="middle">Hung</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Zukerman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Preserving Distributional Information in Dialogue Act Classification</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2151" to="2156"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a novel train-ing/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neu-ral network model is relatively simple, it outperforms more complex neural models , achieving state-of-the-art results on the MapTask and Switchboard corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue Act (DA) classification is a sequence- labeling task, where a sequence of utterances is mapped into a sequence of DAs. The DAs are se- mantic classifications of the utterances, and differ- ent corpora usually have their own DA labels.</p><p>Two of the most popular DA classification datasets are Switchboard ( <ref type="bibr" target="#b3">Godfrey et al., 1992</ref>; Ju- rafsky et al., 1997) and MapTask ( <ref type="bibr" target="#b0">Anderson et al., 1991)</ref>. There have been many works on DA classi- fication applied to these two datasets; some focus on textual data <ref type="bibr" target="#b9">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr">Stolcke et al., 2000</ref>), while others explore speech data ( <ref type="bibr" target="#b7">Julia et al., 2010</ref>). The classification meth- ods used can be broadly divided into instance- based methods <ref type="bibr" target="#b7">(Julia et al., 2010;</ref><ref type="bibr" target="#b2">Gambäck et al., 2011</ref>) and sequence-labeling methods ( <ref type="bibr">Stolcke et al., 2000;</ref><ref type="bibr" target="#b9">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Ji et al., 2016;</ref><ref type="bibr">Shen and Lee, 2016;</ref><ref type="bibr">Tran et al., 2017)</ref>. Instance-based methods treat each utter- ance as an independent data point, which allows the application of general machine learning mod- els, such as Support Vector Machines. Sequence- labeling methods include methods based on Hid- den Markov Models (HMMs) ( <ref type="bibr">Stolcke et al., 2000</ref>) and neural networks <ref type="bibr" target="#b9">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Ji et al., 2016;</ref><ref type="bibr">Shen and Lee, 2016;</ref><ref type="bibr">Tran et al., 2017</ref>).</p><p>Stolcke et al. employed an HMM, using a Language Model to produce emission probabili- ties. The neural models are particularly success- ful, posting a higher accuracy on Switchboard than the HMM. Specifically, Kalchbrenner and Blun- som (2013) model a DA sequence with a recur- rent neural network (RNN) where sentence repre- sentations are constructed by means of a convolu- tional neural network (CNN); <ref type="bibr" target="#b6">Ji et al. (2016)</ref> treat the labels as latent variables in a generative RNN; Shen and Lee (2016) employ attentional RNNs for the independent prediction of <ref type="bibr">DAs;</ref><ref type="bibr">and Tran et al. (2017)</ref> model the DAs in a conversation by means of a hierarchical RNN. In this paper, we also rely on RNNs, but our architecture is much simpler than the above neural models, while post- ing competitive results.</p><p>Most neural network models for DA classifica- tion employ greedy decoding ( <ref type="bibr">Tran et al., 2017;</ref><ref type="bibr" target="#b6">Ji et al., 2016)</ref>, as its speed and simplicity support an on-line decoding process (i.e., producing a label immediately after receiving an utterance). For se- quential labeling, the DA label in the current time step is very important ( <ref type="bibr">Tran et al., 2017)</ref>. How- ever, using a greedy approach to connect the cur- rent label directly to the next label may degrade performance, because the current predicted label may be noisy, which in turn leads to the propa- gation of errors through the sequence ( <ref type="bibr">Tran et al., 2017;</ref><ref type="bibr">Ranzato et al., 2015)</ref>.</p><p>Recently, <ref type="bibr" target="#b1">Bengio et al. (2015)</ref> proposed a tech- nique called Scheduled Sampling that tries to solve the label-bias problem by alternating between the predicted label and the correct label during train- ing. This makes the model gradually adapt to the noisiness of the predicted label. However, this method still relies upon a single current label, and, by omitting the distribution over the possible la- bels, this model loses information about the cur- rent stage. In contrast, we propose to condition the next label on a predicted distribution of the current label. Specifically, we introduce two variants of this idea: the Uncertainty Propagation model and the Average Embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sequential DA Prediction</head><p>We are interested in predicting DAs {z 1 , . . . , z t } in a conversation as we receive textual utterances {x x x 1 , . . . , x x x t } sequentially. Importantly, we do not have access to future utterances when predicting a DA at time t.</p><p>Model. We propose a discriminative model, where the probability of DAs conditioned on ut- terances is decomposed as follows <ref type="figure" target="#fig_0">(Figure 1</ref>):</p><formula xml:id="formula_0">p(z z z 1:t |x x x 1:t ) = t i=1 p θ θ θ (z i |z i−1 , x x x i )<label>(1)</label></formula><p>where z z z 1:t and x x x 1:t respectively denote the se- quence of DAs and utterances up to time step t. Our model resembles a maximum entropy Markov model, as it conditions the label of the next time step on the label of the current step and the next received utterance. The conditional distribution term p θ θ θ (z i |z i−1 , x x x i ) is realised by neural models as follows:</p><formula xml:id="formula_1">z i |z i−1 , x x x i ∼ softmax(W W W (z i−1 ) c c c(x x x i ) + b b b (z i−1 ) ) (2) where c c c(x x x i ) is the distributed representa- tion of utterance x x x i (discussed below), and {W W W (z i−1 ) , b b b (z i−1 ) } are DA-specific parameters gated on the current DA z i−1 .</formula><p>The encoding function for an utterance is an RNN with long-short term memory (LSTM) units <ref type="bibr" target="#b4">(Graves, 2013;</ref><ref type="bibr" target="#b5">Hochreiter and Schmidhuber, 1997)</ref>, where the final hidden state of the RNN is taken as the representation of the whole sequence of text:</p><formula xml:id="formula_2">h h h t,n = f f f φ φ φ (h h h t,n−1 , e e e(x t,n )) , c c c(x x x t ) = h h h t,Nt (3)</formula><p>where x t,n is the n-th token in the t-th utterance, and N t is the length of the utterance.</p><p>The parameter set of our model θ θ θ includes</p><formula xml:id="formula_3">{W W W () , b b b () } L =1</formula><p>for the gating component (where L is the number of DAs), as well as the LSTM parameters φ φ φ and the word-embedding ta- ble {e e e(w)} w∈W , where W is the dictionary.</p><p>Uncertainty Propagation. In this model, the distribution over the labels at the current time step is passed to the next time step. Specifically, the quantity of interest is the posterior probability of the DA of the next time step given all the utter- ances observed so far. This posterior probability can be rewritten as</p><formula xml:id="formula_4">p θ θ θ (z t |x x x 1:t ) = z 1 ,...,z t−1 p θ θ θ (z z z 1:t |x x x 1:t ) = z t−1 p θ θ θ (z t |z t−1 , x x x t )p θ θ θ (z t−1 |x x x 1:t−1 ) (4)</formula><p>According to Equation 4, the label uncertainty at the next time step t can be computed by a dy- namic programming algorithm based on the label uncertainty of the current time step combined with the local potentials p θ θ θ (z t |z t−1 , x x x t ).</p><p>The use of posterior probability for prediction is also motivated by the minimum Bayes risk decod- ing (MBR). In the sequential setting, we are inter- ested in predicting the next DA that minimizes the expected loss arg minˆzt minˆ minˆzt</p><formula xml:id="formula_5">z 1 ,...,z t−1 p θ θ θ (z z z 1:t |x x x 1:t )loss(z t , ˆ z t ) = arg minˆzt minˆ minˆzt p θ θ θ (z t |x x x 1:t )loss(z t , ˆ z t )<label>(5)</label></formula><p>wherê z t is the predicted label, z t is the actual la- bel, and loss(z t , ˆ z t ) = 1 zt = ˆ zt . In addition to decoding, we use posterior prob- ability when training the model. That is, our train- ing objective is</p><formula xml:id="formula_6">(x x x 1:T ,z z z 1:T )∈D T t=1 log p θ θ θ (z t |x x x 1:t ) (6)</formula><p>where D is the set of conversations in the train- ing set, each consisting of a sequence of utterances x x x 1:T annotated with its gold sequence of DAs z z z 1:T .</p><p>Average Embedding. This model offers a new perspective where a neural net combines an infer- ence machine and a model (rather than simply en- coding a model). Specifically, this model repre- sents in its architecture, through a weighted sum of embeddings, the inference procedure encoded in Equation 4 for the Uncertainty Propagation model:</p><formula xml:id="formula_7">softmax(E q(z t−1 ) [W W W (z t−1 ) ]c c c(x x x t )+E q(z t−1 ) [b b b (z t−1 ) ])<label>(7)</label></formula><p>where q(z t ) is an embedding that represents the uncertainty at time step t. q(z t ) is computed se- quentially as new utterances are received, and used in both decoding and training.</p><p>This formulation contrasts with Uncertainty Propagation, where the expectation is over the dis- tributions:</p><formula xml:id="formula_8">E p θ θ θ (z t−1 |x x x 1:t−1 ) [softmax(W W W (z t−1 ) c c c(x x x t ) + b b b (z t−1 ) )]<label>(8)</label></formula><p>It is worth noting that Equations 7 and 8 yield the same result if the distributions involved in cal- culating the expectations are point-mass distribu- tions and they are equal.</p><p>Although we could have used a more elab- orate neural architecture as the inference ma- chine for the Average Embedding model, we em- ployed a simple softmax architecture to make this model comparable with the principled inference algorithm for our Uncertainty Propagation model, which is based on Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to traditional graphical models</head><p>Our models have several similarities with the tra- ditional HMM model and inference algorithms, such as Forward-Backward decoding and the Viterbi algorithm. However, there are some key differences. Firstly, our model is discrimina- tive, whereas HMM is generative. Secondly, our method is designed for online decoding (the fu- ture inputs to a specific classification decision are unknown), whereas both Forward-Backward de- coding and Viterbi require access to the whole se- quence. Thirdly, Viterbi's objective is to decode for the most probable sequence of labels, whereas our decoding algorithm's objective is to find the sequence of most probable labels (conditioned on the inputs observed so far). Lastly, our Uncer- tainty Propagation model is not only a basis for decoding, but also for training (the training ob- jective in Equation 6 requires the calculation of the posterior probability in Equation 4). Overall, the best analogue of our Uncertainty Propagation model to methods used in HMMs and other graph- ical models is the forward message calculation in the Forward-Backward algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data sets</head><p>For our experiments, we use the MapTask and Switchboard corpora.</p><p>The MapTask Dialog Act corpus ( <ref type="bibr" target="#b0">Anderson et al., 1991)</ref> consists of 128 conversations tagged with 13 DAs. The MapTask conversations focus on instructions and clarifications -in the Map- Task experiment, there is one instruction giver and one instruction follower. The task of the instruc- tion giver is to guide the instruction follower to follow a pre-determined path, and the instruction follower must draw this path on his/her map. We use 12 conversations for validation, 13 for testing, and the rest for training.</p><p>The Switchboard Dialog Act corpus (Godfrey et al., 1992; Jurafsky et al., 1997) consists of 1155 transcribed telephone conversations about general topics, encoded into 42 DAs. We use the exper- imental setup proposed by <ref type="bibr">Stolcke et al. (2000)</ref>: 1115 conversations for training and 19 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>Our first baseline is the model without any current label information. Next, we compare our models with other strategies for incorporating the current  train- ing model uses the current correct label with prob- ability p and the predicted label with probability 1 − p, and p is scheduled to decrease over time. This strategy tries to solve the label-bias problem by making the model gradually adapt to the noisy predicted current label.</p><p>Finally, we consider the results obtained by corpus-specific baselines, viz <ref type="bibr" target="#b7">(Julia et al., 2010;</ref><ref type="bibr">Surendran and Levow, 2006;</ref><ref type="bibr">Tran et al., 2017)</ref> for MapTask, and ( <ref type="bibr">Stolcke et al., 2000;</ref><ref type="bibr" target="#b9">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Ji et al., 2016;</ref><ref type="bibr">Shen and Lee, 2016;</ref><ref type="bibr">Tran et al., 2017</ref>) for Switchboard. <ref type="table">Table 1</ref> compares our results with those obtained by the baselines. Our two models, Uncertainty Propagation and Average Embedding, outperform all the baselines. Among these two models, Un- certainty Propagation, which is more analytically grounded, outperforms the Average Embedding model. Using the true current label during train- ing seems to degrade performance compared to us- ing the predicted label, which is expected, since the true label is not available during testing. The Scheduled Sampling method performs similarly to the predicted-label method for the MapTask cor- pus, and outperforms this method for the Switch- board corpus. <ref type="table" target="#tab_1">Tables 2 and 3</ref> compare our models' perfor- mance on the MapTask and Switchboard corpora respectively with that of several strong baselines. On MapTask, we achieved the best results for Baseline models Accuracy <ref type="bibr" target="#b7">Julia et al. (2010)</ref> 55.4 % Surendran and <ref type="bibr">Levow (2006)</ref> 59.1% <ref type="bibr">Tran et al. (2017)</ref> 61.6% Our models:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Average Embedding 62.6% Uncertainty Propagation 62.9% <ref type="table">Table 2</ref>: Results on MapTask data. <ref type="bibr">Stolcke et al. (2000)</ref> 71.0% Shen and Lee <ref type="formula" target="#formula_0">(2016)</ref> 72.6% <ref type="bibr" target="#b9">Kalchbrenner and Blunsom (2013)</ref> 73.9% <ref type="bibr">Tran et al. (2017)</ref> 74.5% <ref type="bibr" target="#b6">Ji et al. (2016)</ref> (77.0%) 72.5% Our models:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline models Accuracy</head><p>Average Embedding 75.0% Uncertainty Propagation 75.6% textual input, using the four-fold cross-validation setup used by <ref type="bibr">Surendran and Levow (2006)</ref> and <ref type="bibr" target="#b7">Julia et al. (2010)</ref>. On Switchboard, we also ob- tained the best results among the systems with the same experimental setting. It is worth noting that <ref type="bibr" target="#b6">Ji et al. (2016)</ref> reported a higher accuracy of 77.0%, but the paper does not provide enough in- formation about the experimental setup to repli- cate this result, and we only got 72.5% accuracy using the paper's publicly available code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>To quantify the effectiveness of the different mod- els on reducing the label-bias problem, we calcu- late the probability of the models making a cor- rect prediction after they have made a sequence of n mistakes. We expect our models, Uncertainty Propagation and Average Embedding, to be more robust than the label-sensitive baselines in recov- ering from errors.</p><p>The results in <ref type="table">Table 4</ref> confirm our expectations. The simple model with no current label, while performing worse than all other models in accu- racy, does not suffer from the label-bias problem. Among the models with current label information, Uncertainty Propagation suffers the least from la- bel bias. It even outperforms the model with no current label on Switchboard for all values of n, and on MapTask for n = 2. Interestingly, Aver- age Embedding performs quite well for n = 1, but 2154 MapTask Switchboard n = 1 n = 2 n = 3 n = 1 n = 2 n = 3 Not affected by label bias:</p><p>No Previous label 60.29% 56.90% 55.67% 66.99% 63.10% 56.52% Affected by label bias:</p><p>True current label 53.12% 50.38% 47.89% 61.74% 60.93% 60.71% Predicted current label 55.89% 53.65% 49.10% 64.38% 62.21% 62.59% Scheduled Sampling 54.28% 53.32% 50.00% 64.67% 63.49% 60.87% Average Embedding 56.50% 53.76% 52.56% 66.51% 61.71% 55.22% Uncertainty Propagation 57.13% 57.37% 53.93% 67.78% 66.57% 66.36% <ref type="table">Table 4</ref>: Probability that the models recover from a sequence of n prediction mistakes. its ability to recover from errors drops quickly as the length of the erroneous conditioning sequence increases, especially on Switchboard, where the number of labels is higher. This may explain its slightly lower accuracy compared to the Uncer- tainty Propagation model. However, in general, the difference in accuracy between these two mod- els is small, because they are rather unlikely to make several consecutive errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed two strategies to encode current label uncertainty in sequence- labeling RNN models. The experimental results show that our models achieve a very strong perfor- mance on the MapTask and Switchboard corpora using a simple underlying RNN architecture.</p><p>Although we experimented with DA classifica- tion, the idea presented in this paper is general, and can be applied to many sequence-labeling tasks. Our approach is particularly suitable for tasks in- volving streaming data where the model only has access to current and previous observations.</p><p>In the future, we plan to combine our strategies with more complex neural architectures, and ex- plore their application to other sequence-labeling problems. <ref type="bibr">Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture.</figDesc><graphic url="image-1.png" coords="2,114.52,62.81,368.50,88.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Accuracy</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 : Results on Switchboard data.</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The HCRC MapTask corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Anne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">Gurman</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwyneth</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and speech</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="366" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active learning for dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Gambäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2011-Proceedings of the International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1329" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
<note type="report_type">ICASSP-92</note>
	<note>IEEE International Conference on. IEEE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A latent variable recurrent neural network for discourse-driven language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="332" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dialog act classification using acoustic and discourse information of MapTask data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atiq U Islam</forename><surname>Khan M Iftekharuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="289" to="311" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual, Draft 13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debra</forename><surname>Biasca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Colorado</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.3584</idno>
		<title level="m">Recurrent convolutional neural networks for discourse compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
