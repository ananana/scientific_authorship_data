<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why is unsupervised alignment of English embeddings from different algorithms so hard?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mareike</forename><surname>Hartmann</surname></persName>
							<email>hartmann@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dep. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dep. of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dep. of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<email>soegaard@di.ku.dk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Why is unsupervised alignment of English embeddings from different algorithms so hard?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="582" to="586"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>582</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a challenge to the community: Generative adversarial networks (GANs) can perfectly align independent English word embeddings induced using the same algorithm , based on distributional information alone; but fails to do so, for two different em-beddings algorithms. Why is that? We believe understanding why, is key to understand both modern word embedding algorithms and the limitations and instability dynamics of GANs. This paper shows that (a) in all these cases, where alignment fails, there exists a linear transform between the two embeddings (so algorithm biases do not lead to non-linear differences), and (b) similar effects can not easily be obtained by varying hyper-parameters. One plausible suggestion based on our initial experiments is that the differences in the induc-tive biases of the embedding algorithms lead to an optimization landscape that is riddled with local optima, leading to a very small basin of convergence, but we present this more as a challenge paper than a technical contribution.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper brings together two fascinating re- search topics in natural language processing (NLP), namely understanding the properties of word embeddings <ref type="bibr" target="#b6">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b9">Mitchell and Steedman, 2015;</ref><ref type="bibr" target="#b8">Mimno and Thompson, 2017)</ref> and unsupervised bilingual dictionary in- duction ( <ref type="bibr" target="#b3">Conneau et al., 2018;</ref><ref type="bibr" target="#b14">Zhang et al., 2017;</ref><ref type="bibr" target="#b13">Søgaard et al., 2018)</ref>. In an effort to better under- stand when unsupervised bilingual dictionary in- duction is possible, we factored out linguistic dif- ferences between languages, and studied English- English alignability (by learning to align English embeddings trained on different samples of the English Wikipedia), when we came across a puz- zling phenomena: English-English can be aligned with almost 100% precision, if you use the same embedding algorithms for the two samples, but not at all (0% precision), if you use different em- bedding algorithms. This results suggest that the properties of word embeddings induced by differ- ent algorithms challenge unsupervised bilingual dictionary algorithms. Understanding why will enable us to develop more stable adversarial learn- ing algorithms and give us a better understanding of how embedding algorithms differ.</p><p>Contributions We are, to the best of our knowl- edge, the first to study unsupervised alignability of pairs of English word embeddings. We show that unsupervised alignment -specifically the MUSE system ( <ref type="bibr" target="#b3">Conneau et al., 2018</ref>) -fails when the al- gorithms used to induce the two embeddings dif- fer, and that this is not because there is no linear transformation between the two spaces. We fur- ther show that poor initialization, as a result of MUSE initially applying an identity transform to two word embeddings far apart in space, is not the sole reason the discriminator suffers from local optima. Finally, we present an experiment show- ing what the minimal corpus size is for unsuper- vised alignment to succeed, in the absence of lin- guistic differences.</p><p>2 Aligning embeddings 2.1 Unsupervised alignment using generative adversarial networks MUSE ( <ref type="bibr" target="#b3">Conneau et al., 2018</ref>) uses a vanilla gen- erative adversarial network (GAN) with a linear generator to learn alignments between embedding spaces without supervision. In a two-player game, a discriminator D aims to tell the two language spaces apart, while a generator G aims to map the source language into the target language space, fooling the discriminator. While MUSE achieves impressive results at times, MUSE is highly un- stable, e.g., with different initializations precision scores vary between 0% and 45% for English- Greek ( <ref type="bibr">Søgaard et al., 2018)</ref>. The parameters of a GAN with a linear gener- ator are (Ω, w). They are obtained by solving the following min-max problem:</p><formula xml:id="formula_0">min Ω max w E[log(D w (X)) + log(1 − D w (g Ω (Z)))]</formula><p>(1) which reduces to</p><formula xml:id="formula_1">min Ω JS (P X | P Ω )<label>(2)</label></formula><p>Ω is initialized as the identity matrix I.</p><p>If G wins the game against an ideal discrimi- nator on a very large number of samples, then F (the source vector space) and ΩE (with E being the target vector space) can be shown to be close in Jensen-Shannon divergence, and thus the model has learned the true distribution. This result, re- ferring to the distributions of the data, p data , and the distribution, p g , G is sampling from, is from <ref type="bibr" target="#b4">Goodfellow et al. (2014)</ref>: If G and D have enough capacity, and at each step of training, the discrim- inator is allowed to reach its optimum given G, and p g is updated so as to improve the criterion</p><formula xml:id="formula_2">E x∼p data [log D * G (x)] + E x∼pg [log(1 − D * G (x))]</formula><p>then p g converges to p data . This result relies on a number of assumptions that do not hold in practice. Our generator, which learns a linear transform Ω, has very limited ca- pacity, for example, and we are updating Ω rather than p g . In practice, therefore, during training, we alternate between k steps of optimizing the dis- criminator and one step of optimizing the genera- tor. If the GAN-based alignment is not success- ful, this can thus be a result of two things: Ei- ther that G does not have enough capacity, or that D is stuck in a local optimum. Our results in §3 show that the inability to align English-English in the case of different word embedding algorithms is not a result of limited capacity, but a result of the GAN being trapped in one of the many local optima of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supervised alignment using Procrustes Analysis</head><p>Procrustes Analysis ( <ref type="bibr" target="#b11">Schönemann, 1966</ref>) has been commonly used for supervised alignment of word embeddings ( <ref type="bibr" target="#b12">Smith et al., 2017;</ref><ref type="bibr" target="#b1">Artetxe et al., 2018)</ref>. Here, the optimal alignment between two embedding spaces is computed using singular value decomposition of the aligned embeddings in a seed dictionary. <ref type="bibr" target="#b3">Conneau et al. (2018)</ref> use Procrustes Analysis to refine an initial seed dic- tionary learned by the generative adversarial net- work without supervision. In our supervised ex- periments, we use 5000 seed words as supervision for learning the alignment between embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Geometry of embeddings</head><p>Below we summarize some previous findings about the geometry of monolingual embeddings <ref type="bibr" target="#b8">(Mimno and Thompson, 2017)</ref>, and add some new observations. We discuss five embed- ding algorithms: SVD on positive PMI matrices (Hyperwords-SVD) ( <ref type="bibr" target="#b5">Levy et al., 2015)</ref>, skip-gram negative sampling applied to co-occurrence matri- ces (Hyperwords-SGNS) ( <ref type="bibr" target="#b5">Levy et al., 2015)</ref>, con- tinuous bag-of-words (CBOW) ( <ref type="bibr" target="#b7">Mikolov et al., 2013a</ref>), GloVe ( <ref type="bibr" target="#b10">Pennington et al., 2014)</ref>, and Fast- Text ( <ref type="bibr" target="#b2">Bojanowski et al., 2017)</ref>. To analyze the ge- ometry of our monolingual embeddings in space, we report average inner product to mean vector; see <ref type="bibr" target="#b8">Mimno and Thompson (2017)</ref> for details.</p><p>Hyperwords-SVD have a small average in- ner product (0.0032), suggesting they are well- dispersed through space; like Hyperwords-SGNS and standard SGNS ( <ref type="bibr" target="#b8">Mimno and Thompson, 2017)</ref>, they do not exhibit a clear word frequency bias. Hyperwords-SGNS vectors also have a small average inner product (0.0002), in contrast with standard SGNS vectors, which are narrowly clustered in a single orthant <ref type="bibr" target="#b8">(Mimno and Thompson, 2017</ref>). In line with standard SGNS vectors, the frequency of words has relatively little effect on their inner product, with the exception of the rare words, which have slightly less positive in- ner products. CBOW vectors have a relatively large average inner product (4.2985). The vectors trained by GloVe show a clear relationship with word frequency, with low-frequency words oppos- ing the frequency-balanced mean vector. The em- beddings are well-dispersed, with an average in- ner product of 0.0002. Finally, FastText vectors have a large, positive inner product with the mean (0.2988), indicating that they are not evenly dis- persed through the space, but pointing in roughly the same direction. The FastText vectors exhibit a frequency bias, much like what has been previ- ously observed with GloVe vectors. The differ- ences are the results of the inductive biases of the different embedding algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>This section presents our data, the hyper- parameters of our embeddings, our experimental protocols, and our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>In the following experiments we learn word embeddings on samples of a publicly available Wikipedia dump from March 2018. <ref type="bibr">1</ref> The data is preprocessed using a publicly available pre- processing script 2 , extracting text, removing non- alphanumeric characters, converting digits to text, and lowercasing the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyper-parameters</head><p>We train 300-dimensional word embeddings using the algorithms' recommended hyperparameter set- tings, listed in the following: 3 For Hyperwords- SGNS, the window size is set to 2 and the subsam- pling of frequent words and smoothing of the con- text distribution are disabled. The minimal word count for being in the vocabulary is 100. The same applies for Hyperwords-SVD, and the ex- ponent for weighting the eigenvalue matrix is 0.5. For CBOW, the window size is set to 8, the num- ber of negative samples is 25, and the subsampling threshold for frequent words is 1e-4. For GloVe, the window size is set to 15 and the cutoff param- eter x max to 10. Finally, for FastText, the window size is 5, the number of negatives samples is 5 and the sampling threshold is 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main experiments</head><p>We train word embeddings using the differ- ent embedding algorithms listed in §3.2 on two non-overlapping 10% samples of the English Wikipedia dump (the samples contain 463,576 and 528,556 distinct words, with an overlap in vocab- ulary of 351,858 words). We learn unsupervised and supervised alignments for embeddings (as de- scribed in §2) trained by different algorithms on the same datasplits, and for embeddings trained by the same algorithm on the two different datas- plits. For the unsupervised alignments, we use the default parameters of the MUSE system for the adversarial training, i.e. a discriminator with 2 fully connected layers of 2048 units trained over 5 epochs, 1,000,000 iterations per epoch with 5 discriminator steps per iteration and a batch size of 32.</p><p>We evaluate the alignments in terms of Preci- sion@1 in the word translation retrieval task for the 1500 test words used by <ref type="bibr" target="#b2">Bojanowski et al. (2017)</ref>. The results are shown in <ref type="table">Table 1</ref>  <ref type="bibr">4</ref> . Our main observations are: (a) MUSE learns perfect alignments for embeddings learned by the same al- gorithm on different data splits. (b) MUSE cannot learn alignments for embeddings learned by differ- ent algorithms on the same data splits, even if there exists a linear transformation aligning both sets of embeddings (the supervised algorithm learns per- fect alignments). We also verify that MUSE can- not learn to align embeddings from different algo- rithms even when induced from the same sample. As already mentioned, we also ran experiments to check that the failure of MUSE to learn good alignments was not a result of the differences in hyper-parameter settings. §3.4 presents additional experiments with normalization, for control; §3.5 addresses how much data is needed to align inde- pendently induced embeddings from the same al- gorithm. §4 discusses potential answers to why MUSE fails when embeddings are induced using different algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiments with normalization</head><p>The embeddings in the main experiments differ in several ways; see §2. One possible explana- tion for the inability of MUSE to align embed- dings from different algorithms could be that the two embeddings are so far apart in space that the discriminator learns to discriminate between them too quickly. Recall that Ω is initialized as the iden- tity matrix I, which means that the generator ini- tially presents the discriminator with the source embedding as is. This is an effect that has of- ten been observed with GANs (Arjovsky and Bot- tou, 2017); could this also be the explanation for our results? At a first glance, this seems a pos- sible explanation. The inner products with the mean differ significantly for the five embedding  algorithms (see §2). The only embeddings that have roughly the same directionality are Hyper- words and GloVe, and their centroids are very far apart in cosine space. The cosine similarity of the centroids of the two versions of Hyperwords is - 0.006, and the cosine similarity for Hyperwords- SVD and GloVe is 0.019. However, poor initial- ization as a result of applying the identity trans- form to very distant word embeddings is not the explanation for the poor performance of MUSE in this set-up: Both sets of Hyperwords embed- dings were normalized, but alignment still failed.</p><p>To verify this holds in general, i.e., that results are not affected by normalization in general, we also ran experiments with the remaining 14 embedding pairs, normalizing and/or centering both embed- dings. Results stayed the same: Precision at 1 scores of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning curve</head><p>MUSE perfectly aligns independently induced word embeddings induced by the same algorithm. For FastText, it correctly aligns 99.7% of all words in the evaluation lexicon with itself. Our samples are 10% of a publicly available Wikipedia dump, amounting to more than 400 million tokens per sample. English-English alignment is an interest- ing control experiment for unsupervised bilingual dictionary induction, abstracting away from lin- guistic differences, and we ran a series of exper- iments to see how small samples MUSE can align in the absence of linguistic differences. The learn- ing curve is presented in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We have shown that the fact that MUSE cannot align two embedding spaces for English induced by different algorithms (even if using the same corpus), is not a result of there not being a linear transformation, and not a result of (lack of) nor- malization or trivial differences in model hyper- parameters. The only explanation left seems to be that the inductive biases of the different algorithms lead to a loss landscape so riddled with local op- tima that MUSE cannot possible escape them.</p><p>To support this hypothesis, compare the loss curves for the MUSE runs aligning embeddings induced with the same algorithms (black curves) to the runs aligning embeddings induced with dif-  <ref type="figure">Figure 2</ref>: Discriminator losses using the same algo- rithm for source and target (black curves) or using dif- ferent algorithms (grey curves).</p><p>ferent algorithms, in <ref type="figure">Figure 2</ref>. When the em- beddings are induced by the same algorithm, we clearly see the contours of a min-max game, sug- gesting that the generator and discriminator chal- lenge each other, both contributing to a good alignment. When the embeddings are induced by different algorithms, however, the discriminator quickly drops, with the generator unable to push the discriminator out of a local optimum. Under- standing when biases induce highly non-convex landscapes, and how to make adversarial training less sensitive to such scenarios, remains an open problem, which we think will be key to the success of unsupervised machine translation and related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Hyperwords</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Unsupervised alignment quality for FastText embeddings trained on samples of different sizes, evaluated on 878 words covered by all of the embeddings. The x-axis is log-scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>-SGNS Hyperwords-SVD CBOW GloVe FastText</head><label></label><figDesc></figDesc><table>UNSUPERVISED 

Hyperwords-SGNS 
0.997 

Hyperwords-SVD 
0.000 
0.992 

CBOW 
0.000 
0.000 
0.997 

GloVe 
0.000 
0.000 
0.000 
0.997 

FastText 
0.000 
0.000 
0.000 
0.000 
0.997 

SUPERVISED 

Hyperwords-SVD 
0.967 

CBOW 
0.990 
0.989 

GloVe 
0.985 
0.992 
0.999 

FastText 
0.994 
0.994 
0.999 
0.997 

Table 1: Precision at 1 (P@1) for unsupervised GAN alignment with Procrustes refinement (top) and supervised 
Procrustes analysis for the cases in which unsupervised alignment fails (bottom). Results clearly show that GANs 
can align two independent embeddings induced by the same algorithm; but not embeddings aligned by different 
ones. Supervised Procrustes analysis, on the other hand, perfectly aligns the embeddings in both cases. 

</table></figure>

			<note place="foot" n="1"> https://dumps.wikimedia.org/enwiki/ 2 http://mattmahoney.net/dc/textdata. html 3 We also ran experiments with one of the embedding algorithms (FastText) to check if our results were robust across hyper-parameter settings</note>

			<note place="foot" n="4"> We report Precision at 1 scores but find that the pattern is the same for Precision at 10, with perfect alignments for embeddings from the same algorithm and 0 scores for alignments between embeddings from different algorithms in the unsupervised experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word Translation Without Parallel Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wardefarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregroy</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The strange geometry of skip-gram with negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orthogonality of syntax and semantics within distributional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Conference Track)</title>
		<meeting>ICLR (Conference Track)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the limitations of unsupervised bilingual dictionary induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
