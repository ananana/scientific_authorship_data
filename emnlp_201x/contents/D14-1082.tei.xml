<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="740" to="750"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser. Because this classifier learns and uses just a small number of dense features , it can work very fast, while achieving an about 2% improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Concretely , our parser is able to parse more than 1000 sentences per second at 92.2% unlabeled attachment score on the English Penn Treebank.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, enormous parsing success has been achieved by the use of feature-based discrim- inative dependency parsers <ref type="bibr">(Kübler et al., 2009</ref>). In particular, for practical applications, the speed of the subclass of transition-based dependency parsers has been very appealing.</p><p>However, these parsers are not perfect. First, from a statistical perspective, these parsers suffer from the use of millions of mainly poorly esti- mated feature weights. While in aggregate both lexicalized features and higher-order interaction term features are very important in improving the performance of these systems, nevertheless, there is insufficient data to correctly weight most such features. For this reason, techniques for introduc- ing higher-support features such as word class fea- tures have also been very successful in improving parsing performance ( <ref type="bibr" target="#b14">Koo et al., 2008)</ref>. Second, almost all existing parsers rely on a manually de- signed set of feature templates, which require a lot of expertise and are usually incomplete. Third, the use of many feature templates cause a less stud- ied problem: in modern dependency parsers, most of the runtime is consumed not by the core pars- ing algorithm but in the feature extraction step <ref type="bibr" target="#b8">(He et al., 2013</ref>). For instance, <ref type="bibr" target="#b0">Bohnet (2010)</ref> reports that his baseline parser spends 99% of its time do- ing feature extraction, despite that being done in standard efficient ways.</p><p>In this work, we address all of these problems by using dense features in place of the sparse indi- cator features. This is inspired by the recent suc- cess of distributed word representations in many NLP tasks, e.g., POS tagging ), machine translation ( <ref type="bibr" target="#b4">Devlin et al., 2014)</ref>, and constituency parsing <ref type="bibr" target="#b21">(Socher et al., 2013)</ref>. Low-dimensional, dense word embeddings can ef- fectively alleviate sparsity by sharing statistical strength between similar words, and can provide us a good starting point to construct features of words and their interactions.</p><p>Nevertheless, there remain challenging prob- lems of how to encode all the available infor- mation from the configuration and how to model higher-order features based on the dense repre- sentations. In this paper, we train a neural net- work classifier to make parsing decisions within a transition-based dependency parser. The neu- ral network learns compact dense vector represen- tations of words, part-of-speech (POS) tags, and dependency labels. This results in a fast, com- pact classifier, which uses only 200 learned dense features while yielding good gains in parsing ac- curacy and speed on two languages (English and Chinese) and two different dependency represen- tations (CoNLL and Stanford dependencies). The main contributions of this work are: (i) showing the usefulness of dense representations that are learned within the parsing task, (ii) developing a neural network architecture that gives good accu- racy and speed, and (iii) introducing a novel acti-vation function for the neural network that better captures higher-order interaction features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transition-based Dependency Parsing</head><p>Transition-based dependency parsing aims to pre- dict a transition sequence from an initial configu- ration to some terminal configuration, which de- rives a target dependency parse tree, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In this paper, we examine only greedy parsing, which uses a classifier to predict the cor- rect transition based on features extracted from the configuration. This class of parsers is of great in- terest because of their efficiency, although they tend to perform slightly worse than the search- based parsers because of subsequent error prop- agation. However, our greedy parser can achieve comparable accuracy with a very good speed. <ref type="bibr">1</ref> As the basis of our parser, we employ the arc-standard system <ref type="bibr">(Nivre, 2004</ref>), one of the most popular transition systems. In the arc- standard system, a configuration c = (s, b, A) consists of a stack s, a buffer b, and a set of dependency arcs A. The initial configuration for a sentence</p><formula xml:id="formula_0">w 1 , . . . , w n is s = [ROOT], b = [w 1 , . . . , w n ], A = ∅.</formula><p>A configuration c is termi- nal if the buffer is empty and the stack contains the single node ROOT, and the parse tree is given by A c . Denoting s i (i = 1, 2, . . .) as the i th top element on the stack, and b i (i = 1, 2, . . .) as the i th element on the buffer, the arc-standard system defines three types of transitions:</p><p>• LEFT-ARC(l): adds an arc s 1 → s 2 with label l and removes s 2 from the stack. Pre- condition: |s| ≥ 2.</p><p>• RIGHT-ARC(l): adds an arc s 2 → s 1 with label l and removes s 1 from the stack. Pre- condition: |s| ≥ 2.</p><p>• SHIFT: moves b 1 from the buffer to the stack. Precondition: |b| ≥ 1.</p><p>In the labeled version of parsing, there are in total |T | = 2N l + 1 transitions, where N l is number of different arc labels. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an ex- ample of one transition sequence from the initial configuration to a terminal one.</p><p>The essential goal of a greedy parser is to pre- dict a correct transition from T , based on one Single-word features (9) s 1 .w; s 1 .t; s 1 .wt; s 2 .w; s 2 .t; s 2 .wt; b 1 .w; b 1 .t; b 1 .wt Word-pair features (8) s 1 .wt • s 2 .wt; s 1 .wt • s 2 .w; s 1 .wts 2 .t; <ref type="table">Table 1</ref>: The feature templates used for analysis. lc 1 (s i ) and rc 1 (s i ) denote the leftmost and right- most children of s i , w denotes word, t denotes POS tag.</p><formula xml:id="formula_1">s 1 .w • s 2 .wt; s 1 .t • s 2 .wt; s 1 .w • s 2 .w s 1 .t • s 2 .t; s 1 .t • b 1 .t Three-word feaures (8) s 2 .t • s 1 .t • b 1 .t; s 2 .t • s 1 .t • lc 1 (s 1 ).t; s 2 .t • s 1 .t • rc 1 (s 1 ).t; s 2 .t • s 1 .t • lc 1 (s 2 ).t; s 2 .t • s 1 .t • rc 1 (s 2 ).t; s 2 .t • s 1 .w • rc 1 (s 2 ).t; s 2 .t • s 1 .w • lc 1 (s 1 ).t; s 2 .t • s 1 .w • b 1 .t</formula><p>given configuration. Information that can be ob- tained from one configuration includes: (1) all the words and their corresponding POS tags (e.g., has / VBZ); (2) the head of a word and its label (e.g., nsubj, dobj) if applicable; (3) the posi- tion of a word on the stack/buffer or whether it has already been removed from the stack.</p><p>Conventional approaches extract indicator fea- tures such as the conjunction of 1 ∼ 3 elements from the stack/buffer using their words, POS tags or arc labels. <ref type="table">Table 1</ref> lists a typical set of feature templates chosen from the ones of ( <ref type="bibr" target="#b11">Huang et al., 2009;</ref><ref type="bibr">Zhang and Nivre, 2011</ref>). <ref type="bibr">2</ref> These features suffer from the following problems:</p><p>• Sparsity. The features, especially lexicalized features are highly sparse, and this is a com- mon problem in many NLP tasks. The sit- uation is severe in dependency parsing, be- cause it depends critically on word-to-word interactions and thus the high-order features.</p><p>To give a better understanding, we perform a feature analysis using the features in <ref type="table">Table 1</ref> on the English Penn Treebank (CoNLL rep- resentations). The results given in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stack Bu↵er</head><p>Correct transition: Features UAS All features in <ref type="table">Table 1</ref> 88.0 single-word &amp; word-pair features 82.7 only single-word features 76.9 excluding all lexicalized features 81.5 <ref type="table" target="#tab_0">Table 2</ref>: Performance of different feature sets. UAS: unlabeled attachment score.</p><formula xml:id="formula_2">SHIFT 1 Transition Stack Buffer A [ROOT] [He has good control .] ∅ SHIFT [ROOT He] [has good control .] SHIFT [ROOT He has] [good control .] LEFT-ARC(nsubj) [ROOT has] [good control .] A∪ nsubj(has,He) SHIFT [ROOT has good] [control .] SHIFT [ROOT has good control] [.] LEFT-ARC(amod) [ROOT has control] [.] A∪amod(control,good) RIGHT-ARC(dobj) [ROOT has] [.] A∪ dobj(has,control) . . . . . . . . . . . . RIGHT-ARC(root) [ROOT] [] A∪ root(</formula><p>• Incompleteness. Incompleteness is an un- avoidable issue in all existing feature tem- plates. Because even with expertise and man- ual handling involved, they still do not in- clude the conjunction of every useful word combination. For example, the conjunc- tion of s 1 and b 2 is omitted in almost all commonly used feature templates, however it could indicate that we cannot perform a RIGHT-ARC action if there is an arc from s 1 to b 2 .</p><p>• Expensive feature computation. The fea- ture generation of indicator features is gen- erally expensive -we have to concatenate some words, POS tags, or arc labels for gen- erating feature strings, and look them up in a huge table containing several millions of fea- tures. In our experiments, more than 95% of the time is consumed by feature computation during the parsing process.</p><p>So far, we have discussed preliminaries of transition-based dependency parsing and existing problems of sparse indicator features. In the fol- lowing sections, we will elaborate our neural net- work model for learning dense features along with experimental evaluations that prove its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Network Based Parser</head><p>In this section, we first present our neural network model and its main components. Later, we give details of training and speedup of parsing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Figure 2 describes our neural network architec- ture. First, as usual word embeddings, we repre- sent each word as a d-dimensional vector e w i ∈ R d and the full embedding matrix is E w ∈ R d×Nw where N w is the dictionary size. Meanwhile, we also map POS tags and arc labels to a d- dimensional vector space, where e t i , e l j ∈ R d are the representations of i th POS tag and j th arc la- bel. Correspondingly, the POS and label embed- ding matrices are E t ∈ R d×Nt and E l ∈ R d×N l where N t and N l are the number of distinct POS tags and arc labels.</p><p>We choose a set of elements based on the stack / buffer positions for each type of in- formation (word, POS or label), which might be useful for our predictions. We denote the sets as S w , S t , S l respectively. For example, given the configuration in <ref type="figure">Figure 2</ref> and</p><formula xml:id="formula_3">S t = · · · · · · · · · · · · Input layer: [x w , x t , x l ]</formula><p>Hidden layer:</p><formula xml:id="formula_4">h = (W w 1 x w + W t 1 x t + W l 1 x l + b 1 ) 3</formula><p>Softmax layer:</p><formula xml:id="formula_5">p = softmax(W 2 h)</formula><p>words POS tags arc labels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROOT has VBZ</head><p>He PRP nsubj has VBZ good JJ control NN . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stack Buffer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration</head><p>Figure 2: Our neural network architecture.</p><formula xml:id="formula_6">{lc 1 (s 2 ).t, s 2 .t, rc 1 (s 2 ).t, s 1 .</formula><p>t}, we will extract PRP, VBZ, NULL, JJ in order. Here we use a spe- cial token NULL to represent a non-existent ele- ment.</p><p>We build a standard neural network with one hidden layer, where the corresponding embed- dings of our chosen elements from S w , S t , S l will be added to the input layer. Denoting n w , n t , n l as the number of chosen elements of each type, we add</p><formula xml:id="formula_7">x w = [e w w 1 ; e w w 2 ; . . . e w wn w</formula><p>] to the input layer, where S w = {w 1 , . . . , w nw }. Similarly, we add the POS tag features x t and arc label features x l to the input layer.</p><p>We map the input layer to a hidden layer with d h nodes through a cube activation function:</p><formula xml:id="formula_8">h = (W w 1 x w + W t 1 x t + W l 1 x l + b 1 ) 3 where W w 1 ∈ R d h ×(d·nw) , W t 1 ∈ R d h ×(d·nt) , W l 1 ∈ R d h ×(d·n l )</formula><p>, and b 1 ∈ R d h is the bias. A softmax layer is finally added on the top of the hidden layer for modeling multi-class prob- abilities p = softmax(W 2 h), where W 2 ∈ R |T |×d h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS and label embeddings</head><p>To our best knowledge, this is the first attempt to introduce POS tag and arc label embeddings in- stead of discrete representations.</p><p>Although the POS tags P = {NN, NNP, NNS, DT, JJ, . . .} (for English) and arc labels L = {amod, tmod, nsubj, csubj, dobj, . . .} (for Stanford Dependencies on English) are rela- tively small discrete sets, they still exhibit many semantical similarities like words. For example, NN (singular noun) should be closer to NNS (plural noun) than DT (determiner), and amod (adjective modifier) should be closer to num (numeric mod- ifier) than nsubj (nominal subject). We expect these semantic meanings to be effectively captured by the dense representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cube activation function</head><p>As stated above, we introduce a novel activation function: cube g(x) = x 3 in our model instead of the commonly used tanh or sigmoid functions <ref type="figure" target="#fig_1">(Figure 3)</ref>.</p><p>Intuitively, every hidden unit is computed by a (non-linear) mapping on a weighted sum of input units plus a bias. Using g(x) = x 3 can model the product terms of x i x j x k for any three different elements at the input layer directly:</p><formula xml:id="formula_9">g(w 1 x 1 + . . . + w m x m + b) = i,j,k (w i w j w k )x i x j x k + i,j b(w i w j )x i x j . . .</formula><p>In our case, x i , x j , x k could come from different dimensions of three embeddings. We believe that this better captures the interaction of three ele-ments, which is a very desired property of depen- dency parsing.</p><p>Experimental results also verify the success of the cube activation function empirically (see more comparisons in Section 4). However, the expres- sive power of this activation function is still open to investigate theoretically.</p><p>The choice of S w , S t , S l Following (Zhang and Nivre, 2011), we pick a rich set of elements for our final parser. In de- tail, S w contains n w = 18 elements: (1) The top 3 words on the stack and buffer: s 1 , s 2 , s 3 , b 1 , b 2 , b 3 ; (2) The first and second leftmost / rightmost children of the top two words on the stack:</p><formula xml:id="formula_10">lc 1 (s i ), rc 1 (s i ), lc 2 (s i ), rc 2 (s i ), i = 1, 2. (3)</formula><p>The leftmost of leftmost / rightmost of right- most children of the top two words on the stack:</p><formula xml:id="formula_11">lc 1 (lc 1 (s i )), rc 1 (rc 1 (s i )), i = 1, 2.</formula><p>We use the corresponding POS tags for S t (n t = 18), and the corresponding arc labels of words excluding those 6 words on the stack/buffer for S l (n l = 12). A good advantage of our parser is that we can add a rich set of elements cheaply, instead of hand-crafting many more indicator fea- tures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We first generate training examples {(c i , t i )} m i=1 from the training sentences and their gold parse trees using a "shortest stack" oracle which always prefers LEFT-ARC l over SHIFT, where c i is a configuration, t i ∈ T is the oracle transition.</p><p>The final training objective is to minimize the cross-entropy loss, plus a l 2 -regularization term:</p><formula xml:id="formula_12">L(θ) = − i log p t i + λ 2 θ 2</formula><p>where θ is the set of all parameters</p><formula xml:id="formula_13">{W w 1 , W t 1 , W l 1 , b 1 , W 2 , E w , E t , E l }.</formula><p>A slight variation is that we compute the softmax prob- abilities only among the feasible transitions in practice.</p><p>For initialization of parameters, we use pre- trained word embeddings to initialize E w and use random initialization within (−0.01, 0.01) for E t and E l . Concretely, we use the pre-trained word embeddings from (Collobert et al., 2011) for En- glish (#dictionary = 130,000, coverage = 72.7%), and our trained 50-dimensional word2vec em- beddings ( <ref type="bibr" target="#b19">Mikolov et al., 2013</ref>) on Wikipedia and Gigaword corpus for Chinese (#dictionary = 285,791, coverage = 79.0%). We will also com- pare with random initialization of E w in Section 4. The training error derivatives will be back- propagated to these embeddings during the train- ing process.</p><p>We use mini-batched AdaGrad (Duchi et al., 2011) for optimization and also apply a dropout ( <ref type="bibr" target="#b10">Hinton et al., 2012</ref>) with 0.5 rate. The parame- ters which achieve the best unlabeled attachment score on the development set will be chosen for final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parsing</head><p>We perform greedy decoding in parsing. At each step, we extract all the corresponding word, POS and label embeddings from the current configu- ration c, compute the hidden layer h(c) ∈ R d h , and pick the transition with the highest score: t = arg max t is feasible W 2 (t, ·)h(c), and then ex- ecute c → t(c).</p><p>Comparing with indicator features, our parser does not need to compute conjunction features and look them up in a huge feature table, and thus greatly reduces feature generation time. Instead, it involves many matrix addition and multiplica- tion operations. To further speed up the parsing time, we apply a pre-computation trick, similar to ( <ref type="bibr" target="#b4">Devlin et al., 2014</ref>). For each position cho- sen from S w , we pre-compute matrix multiplica- tions for most top frequent 10, 000 words. Thus, computing the hidden layer only requires looking up the table for these frequent words, and adding the d h -dimensional vector. Similarly, we also pre- compute matrix computations for all positions and all POS tags and arc labels. We only use this opti- mization in the neural network parser, but it is only feasible for a parser like the neural network parser which uses a small number of features. In prac- tice, this pre-computation step increases the speed of our parser 8 ∼ 10 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets.</p><p>For English, we follow the standard splits of PTB3, using sections 2-21 for training, section 22 as development set and 23 as test set. We adopt two different dependency representations: CoNLL Syntactic Dependencies (CD) (Johansson Dataset #Train #Dev #Test #words (N w ) #POS (N t ) #labels (N l ) projective <ref type="table" target="#tab_0">(%)  PTB: CD 39,832 1,700 2,416  44,352  45  17  99.4  PTB: SD 39,832 1,700 2,416  44,389  45  45  99.9  CTB  16,091 803 1,910  34,577  35  12</ref> 100.0 For Chinese, we adopt the same split of CTB5 as described in <ref type="bibr">(Zhang and Clark, 2008)</ref>. Depen- dencies are converted using the Penn2Malt tool <ref type="bibr">5</ref> with the head-finding rules of ( <ref type="bibr">Zhang and Clark, 2008)</ref>. And following <ref type="bibr">(Zhang and Clark, 2008;</ref><ref type="bibr">Zhang and Nivre, 2011)</ref>, we use gold segmenta- tion and POS tags for the input. <ref type="table" target="#tab_1">Table 3</ref> gives statistics of the three datasets. <ref type="bibr">6</ref> In particular, over 99% of the trees are projective in all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The following hyper-parameters are used in all ex- periments: embedding size d = 50, hidden layer size h = 200, regularization parameter λ = 10 −8 , initial learning rate of Adagrad α = 0.01.</p><p>To situate the performance of our parser, we first make a comparison with our own implementa- tion of greedy arc-eager and arc-standard parsers. These parsers are trained with structured averaged perceptron using the "early-update" strategy. The feature templates of (Zhang and Nivre, 2011) are used for the arc-eager system, and they are also adapted to the arc-standard system. <ref type="bibr">7</ref> Furthermore, we also compare our parser with two popular, off-the-shelf parsers: Malt- Parser -a greedy transition-based dependency parser ( <ref type="bibr" target="#b20">Nivre et al., 2006</ref>), <ref type="bibr">8</ref> and MSTParser -a first-order graph-based parser <ref type="bibr" target="#b18">(McDonald and Pereira, 2006</ref>). <ref type="bibr">9</ref> In this comparison, for Malt- Parser, we select stackproj (arc-standard) and nivreeager (arc-eager) as parsing algorithms, and liblinear <ref type="bibr" target="#b6">(Fan et al., 2008</ref>) for optimization. <ref type="bibr">10</ref> For MSTParser, we use default options.</p><p>On all datasets, we report unlabeled attach- ment scores (UAS) and labeled attachment scores (LAS) and punctuation is excluded in all evalua- tion metrics. 11 Our parser and the baseline arc- standard and arc-eager parsers are all implemented in Java. The parsing speeds are measured on an Intel Core i7 2.7GHz CPU with 16GB RAM and the runtime does not include pre-computation or parameter loading time. <ref type="table" target="#tab_3">Table 4</ref>, <ref type="table" target="#tab_4">Table 5</ref> and <ref type="table" target="#tab_5">Table 6</ref> show the com- parison of accuracy and parsing speed on PTB (CoNLL dependencies), PTB (Stanford dependen- cies) and CTB respectively.    parser even surpasses MaltParser using liblinear, which is known to be highly optimized, while our parser achieves much better accuracy. Also, despite the fact that the graph-based MST- Parser achieves a similar result to ours on PTB (CoNLL dependencies), our parser is nearly 100 times faster. In particular, our transition-based parser has a great advantage in LAS, especially for the fine-grained label set of Stanford depen- dencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parser</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of Parser Components</head><p>Herein, we examine components that account for the performance of our parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cube activation function</head><p>We compare our cube activation function (x 3 ) with two widely used non-linear functions: tanh</p><formula xml:id="formula_14">( e x −e −x e x +e −x ), sigmoid ( 1 1+e −x )</formula><p>, and also the identity function (x), as shown in <ref type="figure">Figure 4</ref> (left).</p><p>In short, cube outperforms all other activation functions significantly and identity works the worst. Concretely, cube can achieve 0.8% ∼ 1.2% improvement in UAS over tanh and other functions, thus verifying the effectiveness of the cube activation function empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization of pre-trained word embeddings</head><p>We further analyze the influence of using pre- trained word embeddings for initialization. <ref type="figure">Fig- ure 4 (middle)</ref> shows that using pre-trained word embeddings can obtain around 0.7% improve- ment on PTB and 1.7% improvement on CTB, compared with using random initialization within (−0.01, 0.01). On the one hand, the pre-trained word embeddings of Chinese appear more use- ful than those of English; on the other hand, our model is still able to achieve comparable accuracy without the help of pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS tag and arc label embeddings</head><p>As shown in <ref type="figure">Figure 4 (right)</ref>, POS embeddings yield around 1.7% improvement on PTB and nearly 10% improvement on CTB and the label embeddings yield a much smaller 0.3% and 1.4% improvement respectively.</p><p>However, we can obtain little gain from la- bel embeddings when the POS embeddings are present. This may be because the POS tags of two tokens already capture most of the label informa- tion between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis</head><p>Last but not least, we will examine the parame- ters we have learned, and hope to investigate what these dense features capture. We use the weights learned from the English Penn Treebank using Stanford dependencies for analysis.</p><formula xml:id="formula_15">What do E t , E l capture?</formula><p>We first introduced E t and E l as the dense rep- resentations of all POS tags and arc labels, and we wonder whether these embeddings could carry some semantic information. <ref type="figure">Figure 5</ref> presents t-SNE visualizations <ref type="bibr">(van der Maaten and Hinton, 2008</ref>) of these embeddings. It clearly shows that these embeddings effectively exhibit the similarities between POS tags or arc labels. For instance, the three adjective POS tags JJ, JJR, JJS have very close embeddings, and also the three labels representing clausal comple- ments acomp, ccomp, xcomp are grouped to- gether.</p><p>Since these embeddings can effectively encode the semantic regularities, we believe that they can be also used as alternative features of POS tags (or arc labels) in other NLP tasks, and help boost the performance.</p><formula xml:id="formula_16">What do W w 1 , W t 1 , W l 1 capture?</formula><p>Knowing that E t and E l (as well as the word em- beddings E w ) can capture semantic information very well, next we hope to investigate what each feature in the hidden layer has really learned.</p><p>Since we currently only have h = 200 learned dense features, we wonder if it is sufficient to learn the word conjunctions as sparse indicator features, or even more. We examine the weights</p><formula xml:id="formula_17">W w 1 (k, ·) ∈ R d·nw , W t 1 (k, ·) ∈ R d·nt , W l 1 (k, ·) ∈ R d·n l</formula><p>for each hidden unit k, and reshape them to d × n t , d × n w , d × n l matrices, such that the weights of each column corresponds to the embed- dings of one specific element (e.g., s 1 .t).</p><p>We pick the weights with absolute value &gt; 0.2, and visualize them for each feature. <ref type="figure" target="#fig_3">Figure 6</ref> gives the visualization of three sampled features, and it exhibits many interesting phenomena:</p><p>• Different features have varied distributions of the weights. However, most of the discrim- inative weights come from W t 1 (the middle zone in <ref type="figure" target="#fig_3">Figure 6</ref>), and this further justifies the importance of POS tags in dependency pars- ing.</p><p>• We carefully examine many of the h = 200</p><p>features, and find that they actually encode very different views of information. For the three sampled features in <ref type="figure" target="#fig_3">Figure 6</ref>, the largest weights are dominated by:</p><formula xml:id="formula_18">-Feature 1: s 1 .t, s 2 .t, lc(s 1 ).t. -Feautre 2: rc(s 1 ).t, s 1 .t, b 1 .t. -Feature 3: s 1 .t, s 1 .w, lc(s 1 ).t, lc(s 1 ).l.</formula><p>These features all seem very plausible, as ob- served in the experiments on indicator feature systems. Thus our model is able to automati- cally identify the most useful information for predictions, instead of hand-crafting them as indicator features.</p><p>• More importantly, we can extract features re- garding the conjunctions of more than 3 ele- ments easily, and also those not presented in the indicator feature systems. For example, the 3rd feature above captures the conjunc- tion of words and POS tags of s 1 , the tag of its leftmost child, and also the label between them, while this information is not encoded in the original feature templates of (Zhang and Nivre, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There have been several lines of earlier work in us- ing neural networks for parsing which have points of overlap but also major differences from our work here. One big difference is that much early work uses localist one-hot word representations rather than the distributed representations of mod- ern work. <ref type="bibr" target="#b16">(Mayberry III and Miikkulainen, 1999</ref>) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in <ref type="bibr" target="#b17">(Mayberry III and Miikkulainen, 2005)</ref>. <ref type="bibr" target="#b9">(Henderson, 2004</ref>) was the first to attempt to use neural networks in a broad-coverage Penn Tree- bank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, <ref type="bibr" target="#b24">(Titov and Henderson, 2007)</ref> ap- plied Incremental Sigmoid Belief Networks to constituency parsing and then <ref type="bibr" target="#b7">(Garg and Henderson, 2011</ref>) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neu- ral network architectures, and are much less scal- able and in practice a restricted vocabulary was used to make the architecture practical.</p><p>There have been a number of recent uses of deep learning for constituency parsing <ref type="bibr" target="#b2">(Collobert, 2011;</ref><ref type="bibr" target="#b21">Socher et al., 2013</ref><ref type="bibr" target="#b22">). (Socher et al., 2014</ref>) has also built models over dependency representa- tions but this work has not attempted to learn neu- ral networks for dependency parsing.</p><p>Most recently, <ref type="bibr" target="#b23">(Stenetorp, 2013)</ref> attempted to build recursive neural networks for transition- based dependency parsing, however the empirical performance of his model is still unsatisfactory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a novel dependency parser us- ing neural networks. Experimental evaluations show that our parser outperforms other greedy parsers using sparse indicator features in both ac- curacy and speed. This is achieved by represent- ing all words, POS tags and arc labels as dense vectors, and modeling their interactions through a novel cube activation function. Our model only relies on dense features, and is able to automat- ically learn the most useful feature conjunctions for making predictions.</p><p>An interesting line of future work is to combine our neural network based classifier with search- based models to further improve accuracy. Also,  In each feature, each row denotes a dimension of embeddings and each column denotes a chosen element, e.g., s 1 .t or lc(s 1 ).w, and the parameters are divided into 3 zones, corresponding to W w 1 (k, :) (left), W t 1 (k, :) (middle) and W l 1 (k, :) (right). White and black dots denote the most positive weights and most negative weights respectively. there is still room for improvement in our architec- ture, such as better capturing word conjunctions, or adding richer features (e.g., distance, valency). <ref type="bibr">Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003</ref>. Feature-rich part-of- speech tagging with a cyclic dependency network. In NAACL.</p><p>Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. The Journal of Ma- chine Learning Research.</p><p>Yue <ref type="bibr">Zhang and Stephen Clark. 2008</ref>. A tale of two parsers: Investigating and combining graph- based and transition-based dependency parsing us- ing beam-search. In EMNLP.</p><p>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In ACL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of transition-based dependency parsing. Above left: a desired dependency tree, above right: an intermediate configuration, bottom: a transition sequence of the arc-standard system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different activation functions used in neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Effects of different parser components. Left: comparison of different activation functions. Middle: comparison of pre-trained word vectors and random initialization. Right: effects of POS and label embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Three sampled features. In each feature, each row denotes a dimension of embeddings and each column denotes a chosen element, e.g., s 1 .t or lc(s 1 ).w, and the parameters are divided into 3 zones, corresponding to W w 1 (k, :) (left), W t 1 (k, :) (middle) and W l 1 (k, :) (right). White and black dots denote the most positive weights and most negative weights respectively.</figDesc><graphic url="image-1.png" coords="9,19.53,520.61,558.31,167.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 demonstrate</head><label>2</label><figDesc></figDesc><table>that: (1) lexicalized features are 
indispensable; (2) Not only are the word-pair 
features (especially s 1 and s 2 ) vital for pre-
dictions, the three-word conjunctions (e.g., 
{s 2 , s 1 , b 1 }, {s 2 , lc 1 (s 1 ), s 1 }) are also very 
important. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 : Data Statistics. "Projective" is the percentage of projective trees on the training set.</head><label>3</label><figDesc></figDesc><table>and Nugues, 2007) using the LTH Constituent-to-
Dependency Conversion Tool 3 and Stanford Basic 
Dependencies (SD) (de Marneffe et al., 2006) us-
ing the Stanford parser v3.3.0. 4 The POS tags are 
assigned using Stanford POS tagger (Toutanova et 
al., 2003) with ten-way jackknifing of the training 
data (accuracy ≈ 97.3%). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Accuracy and parsing speed on PTB + CoNLL dependencies. Clearly, our parser is superior in terms of both accuracy and speed. Comparing with the base- lines of arc-eager and arc-standard parsers, our parser achieves around 2% improvement in UAS and LAS on all datasets, while running about 20 times faster. It is worth noting that the efficiency of our</figDesc><table>Parser 

Dev 
Test 
Speed 
UAS LAS UAS LAS (sent/s) 
standard 
90.2 87.8 89.4 87.3 
26 
eager 
89.8 87.4 89.6 87.4 
34 
Malt:sp 
89.8 87.2 89.3 86.9 
469 
Malt:eager 89.6 86.9 89.4 86.8 
448 
MSTParser 91.4 88.1 90.7 87.6 
10 
Our parser 92.0 89.7 91.8 89.6 
654 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy and parsing speed on PTB + 
Stanford dependencies. 

Parser 
Dev 
Test 
Speed 
UAS LAS UAS LAS (sent/s) 
standard 
82.4 80.9 82.7 81.2 
72 
eager 
81.1 79.7 80.3 78.7 
80 
Malt:sp 
82.4 80.5 82.4 80.6 
420 
Malt:eager 81.2 79.3 80.2 78.4 
393 
MSTParser 84.0 82.1 83.0 81.2 
6 
Our parser 84.0 82.4 83.9 82.4 
936 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 : Accuracy and parsing speed on CTB.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Additionally, our parser can be naturally incorporated with beam search, but we leave this to future work.</note>

			<note place="foot" n="2"> We exclude sophisticated features using labels, distance, valency and third-order features in this analysis, but we will include all of them in the final evaluation.</note>

			<note place="foot" n="3"> http://nlp.cs.lth.se/software/treebank converter/ 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 6 Pennconverter and Stanford dependencies generate slightly different tokenization, e.g., Pennconverter splits the token WCRS\/Boston NNP into three tokens WCRS NNP / CC Boston NNP. 7 Since arc-standard is bottom-up, we remove all features using the head of stack elements, and also add the right child features of the first stack element. 8 http://www.maltparser.org/</note>

			<note place="foot" n="9"> http://www.seas.upenn.edu/ strctlrn/MSTParser/ MSTParser.html 10 We do not compare with libsvm optimization, which is known to be sightly more accurate, but orders of magnitude slower (Kong and Smith, 2014). 11 A token is a punctuation if its gold POS tag is {&quot; &quot; : , .} for English and PU for Chinese.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040 and the Defense Threat Re-duction Agency (DTRA) under Air Force Re-search Laboratory (AFRL) contract no. FA8650-10-C-7020. Any opinions, findings, and conclu-sion or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very high accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In Coling</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal restricted boltzmann machines for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilingually-constrained (monolingual) shift-reduce parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NODALIDA</title>
		<meeting>NODALIDA<address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An empirical comparison of parsing methods for Stanford dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1404.4314</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dependency Parsing. Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sardsrn: A neural network shift-reduce parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Mayberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Broad-coverage parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Mayberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Neural Processing Letters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In EACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and robust multilingual dependency parsing with a generative latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
