<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Godin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Demuynck</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>De Neve</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3275" to="3284"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3275</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neu-ral networks learn and if those patterns coincide with manually-defined word segmenta-tions and annotations. To that end, we extend the contextual decomposition (Murdoch et al., 2018) technique to convolutional neu-ral networks which allows us to compare con-volutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Character-level features are an essential part of many Natural Language Processing (NLP) tasks. These features are for instance used for language modeling <ref type="bibr" target="#b7">(Kim et al., 2016)</ref>, part-of-speech tag- ging (  and machine translation <ref type="bibr" target="#b14">(Luong and Manning, 2016)</ref>. They are especially useful in the context of part-of-speech and mor- phological tagging, where for example the suffix -s can easily differentiate plural words from sin- gular words in English or Spanish.</p><p>The use of character-level features is not new. Rule-based taggers were amongst the earliest sys- tems that used character-level features/rules for grammatical tagging <ref type="bibr" target="#b9">(Klein and Simmons, 1963)</ref>. Other approaches rely on fixed lists of affixes <ref type="bibr" target="#b19">(Ratnaparkhi, 1996;</ref><ref type="bibr" target="#b27">Toutanova et al., 2003)</ref>. Next, these features are used by a tagging model, such as a rule-based model or statistical model. Rule- based taggers are transparent models that allow us to easily trace back why the tagger made a certain decision (e.g., <ref type="bibr" target="#b3">Brill (1994)</ref>). Similarly, statistical models are merely a weighted sum of features.</p><p>For example, <ref type="bibr" target="#b3">Brill (1994)</ref>'s transformation- based error-driven tagger uses a set of templates to derive rules by fixing errors. The following rule template:</p><p>"Change the most-likely tag X to Y if the last (1,2,3,4) characters of the word are x", resulted in the rule:</p><p>"Change the tag common noun to plural com- mon noun if the word has suffix -s".</p><p>Subsequently, whenever the tagger makes a tag- ging mistake, it is easy to trace back why this hap- pened. Following the above rule, the word mis- tress will mistakingly be tagged as a plural com- mon noun while it actually is a common noun <ref type="bibr">1</ref> . This is in stark contrast with the most recent generation of part-of-speech and morphological taggers which mainly rely on neural networks.</p><p>Words are split into individual characters and are in general either aggregated using a Bidirec- tional Long Short-Term Memory network (BiL- STM)  or Convolutional Neural Network (CNN) <ref type="bibr" target="#b20">(dos Santos and Zadrozny, 2014)</ref>. However, it is currently unknown which character- level patterns these neural network models learn and whether these patterns coincide with our lin- guistic knowledge. Moreover, different neural net- work architectures are currently only compared quantitatively and lack a qualitative analysis.</p><p>In this paper, we investigate which character patterns neural networks learn and to what ex- tent those patterns comprise any known linguistic rules. We do this for three morphologically dif- ferent languages: Finnish, Spanish and Swedish. A Spanish example is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. By vi- sualizing the contributions of each character, we observe that the model indeed uses the suffix -s to correctly predict that the word is plural.</p><p>Our main contributions are as follows:</p><p>• We show how word-level tagging decisions can be traced back to specific sets of charac- ters and interactions between them.</p><p>• We extend the contextual decomposition method ( <ref type="bibr" target="#b15">Murdoch et al., 2018)</ref> to CNNs.</p><p>• We quantitatively compare CNN and BiL- STM models in the context of morphologi- cal tagging by performing an evaluation on three manually segmented and morphologi- cally annotated corpora.</p><p>• We found out that the studied neural models are able to implicitly discover character pat- terns that coincide with the same rules lin- guists use to indicate the morphological func- tion of subword segments.</p><p>Our implementation is available online 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural network-based taggers currently outper- form statistical taggers in morphological tagging ( <ref type="bibr" target="#b5">Heigold et al., 2017)</ref> and part-of-speech tagging ( ) for a wide variety of lan- guages. Character-level features form a crucial part of many of these systems. Generally, two neu- ral network architectures are considered for aggre- gating the individual characters: a BiLSTM ( <ref type="bibr" target="#b12">Ling et al., 2015;</ref>) or a CNN (dos <ref type="bibr" target="#b20">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b2">Bjerva et al., 2016;</ref><ref type="bibr" target="#b5">Heigold et al., 2017</ref>). These architectures outper- form similar models that use manually defined fea- tures ( <ref type="bibr" target="#b12">Ling et al., 2015;</ref><ref type="bibr" target="#b20">dos Santos and Zadrozny, 2014)</ref>. However, it is still unclear which useful character-level features they have learned. Ar- chitectures are compared quantitatively but lack insight into learned patterns. Moreover, <ref type="bibr" target="#b28">Vania and Lopez (2017)</ref> showed in the context of lan- guage modeling that training a BiLSTM on ground truth morphological features still yields better re- sults than eight other character-based neural net- work architectures. Hence, this raises the question which patterns neural networks learn and whether these patterns coincide with manually-defined lin- guistic rules. While a number of interpretation techniques have been proposed for images <ref type="bibr" target="#b24">(Springenberg et al., 2014;</ref><ref type="bibr" target="#b21">Selvaraju et al., 2017;</ref><ref type="bibr" target="#b22">Shrikumar et al., 2017)</ref>, these are generally not applicable in the context of NLP where LSTMs are mainly used. Moreover, gradient-based techniques are not trustworthy when strongly saturating activa- tion functions such as tanh and sigmoid are used (e.g., <ref type="bibr" target="#b10">Li et al. (2016a)</ref>). Hence, current interpre- tations in NLP are limited to visualizing the mag- nitude of the LSTM hidden states of each word ( <ref type="bibr" target="#b13">Linzen et al., 2016;</ref><ref type="bibr" target="#b18">Radford et al., 2017;</ref><ref type="bibr" target="#b25">Strobelt et al., 2018)</ref>, removing words ( <ref type="bibr" target="#b11">Li et al., 2016b;</ref><ref type="bibr" target="#b6">Kádár et al., 2017</ref>) or changing words ( <ref type="bibr" target="#b13">Linzen et al., 2016)</ref> and measuring the impact, or training surrogate tasks ( <ref type="bibr" target="#b0">Adi et al., 2017;</ref>. These techniques only provide limited local interpretations and do not model fine-grained interactions of groups of inputs or intermediate representations. In con- trast, <ref type="bibr" target="#b15">Murdoch et al. (2018)</ref> recently introduced an LSTM interpretation technique called Contextual Decomposition (CD), providing a solution to the aforementioned issues. We will build upon this in- terpretation technique and introduce an extension for CNNs, making it possible to compare different neural network architectures within a single inter- pretation framework.</p><p>CNNs. First, we introduce the concept of CD, fol- lowed by the extension for CNNs. For details on CD for LSTMs, we refer the reader to the afore- mentioned paper. Finally, we explain how the CD of the final classification layer is done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contextual decomposition</head><p>The idea behind CD is that, in the context of character-level decomposition, we can decompose the output value of the network for a certain class into two distinct groups of contributions: (1) con- tributions originating from a specific character or set of characters within a word and (2) contri- butions originating from all the other characters within the same word.</p><p>More generally, we can decompose every out- put value z of every neural network component into a relevant contribution β and an irrelevant contribution γ:</p><formula xml:id="formula_0">z = β + γ (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decomposing CNN layers</head><p>A CNN typically consist of three components: the convolution itself, an activation function and an optional max-pooling operation. We will discuss each component in the next paragraphs.</p><p>Decomposing the convolution Given a se- quence of character embeddings x 1 , ..., x T ∈ R d 1 of length T , we can calculate the convolution of size n of a single filter over the sequence x 1:T by applying the following equation to each n-length subsequence {x t+i , i = 0, .., n − 1}, denoted as x t:t+n−1 :</p><formula xml:id="formula_1">z t = n−1 i=0 W i · x t+i + b,<label>(2)</label></formula><p>with z t ∈ R and where W ∈ R d 1 ×n and b ∈ R are the weight matrix and bias of the convolutional filter. W i denotes the i-th column of the weight matrix W . When we want to calculate the contribution of a subset of characters, where S is the set of cor- responding character position indexes and S ⊆ {1, ..., T }, we should decompose the output of the filter z t into three parts:</p><formula xml:id="formula_2">z t = β t + γ t + b.<label>(3)</label></formula><p>That is, the relevant contribution β t originating from the selected subset of characters with in- dexes S, the irrelevant contribution γ t originating from the remaining characters in the sequence, and a bias which is deemed neutral ( <ref type="bibr" target="#b15">Murdoch et al., 2018)</ref>. This can be achieved by decomposing the con- volution itself as follows:</p><formula xml:id="formula_3">β t = n−1 i=0 W i · x t+i (t + i) ∈ S,<label>(4)</label></formula><formula xml:id="formula_4">γ t = n−1 i=0 W i · x t+i (t + i) / ∈ S,<label>(5)</label></formula><p>Linearizing the activation function After ap- plying a linear transformation to the input, a non- linearity is typically applied. In CNNs, the ReLU activation function is often used. In Murdoch et al. <ref type="formula" target="#formula_1">(2018)</ref>, a linearization method for the non-linear activation function f is pro- posed, based on the differences of partial sums of all N components y i involved in the pre- activation sum z t . In other words, we want to split</p><formula xml:id="formula_5">f ReLU (z t ) = f ReLU ( N i=1 y i ) into a sum of individual linearized contributions L f ReLU (y i ), namely f ReLU ( N i=1 y i ) = N i=1 L f ReLU (y i ).</formula><p>To that end, we compute L f ReLU (y k ), the lin- earized contribution of y k as the average differ- ence of partial sums over all possible permutations π 1 , ..., π M N of all N components y i involved:</p><formula xml:id="formula_6">L f (y k ) = 1 M N M N i=1 [f ( π −1 i (k) l=1 y π i (l) ) − f ( π −1 i (k)−1 l=1 y π i (l) )]<label>(6)</label></formula><p>Consequently, we can decompose the output c t after the activation function as follows:</p><formula xml:id="formula_7">c t =f ReLU (z t ) (7) =f ReLU (β z,t + γ z,t + b) (8) =L ReLU (β z,t ) + [L ReLU (γ z,t ) + L ReLU (b)] (9) =β c,t + γ c,t<label>(10)</label></formula><p>Following <ref type="bibr" target="#b15">Murdoch et al. (2018)</ref>, β c,t contains the contributions that can be directly attributed to the specific set of input indexes S. Hence, the bias b is part of γ c,t . Note that, while the decomposition in Eq. <ref type="formula" target="#formula_7">(10)</ref> is exact in terms of the total sum, the in- dividual attribution to relevant (β c,t ) and irrelevant (γ c,t ) is an approximation, due to the linearization.</p><p>Max-pooling over time When applying a fixed- size convolution over a variable-length sequence, the output is again of variable size. Hence, a max- pooling operation is executed over the time dimen- sion, resulting in a fixed-size representation that is independent of the sequence length:</p><formula xml:id="formula_8">c = max t (c t ).<label>(11)</label></formula><p>Instead of applying a max operation over the β c,t and γ c,t contributions separately, we first deter- mine the position t of the highest c t value and propagate the corresponding β c,t and γ c,t values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Calculating the final contribution scores</head><p>The final layer is a classification layer, which is the same for a CNN-or LSTM-based architecture.</p><p>The probability p j of predicting class j is defined as follows:</p><formula xml:id="formula_9">p j = e W j ·x+b j C i=1 e W i ·x+b i ,<label>(12)</label></formula><p>in which W ∈ R d 2 ×C is a weight matrix and W i the i-th column, x ∈ R d 2 the input, b ∈ R d 2 the bias vector and b i the i-th element, d 2 the input vector size and C the total number of classes.</p><p>The input x is either the output c of a CNN or h of a LSTM. Consequently, we can decompose x into β and γ contributions. In practice, we only consider the preactivation and decompose it as fol- lows:</p><formula xml:id="formula_10">W j · x + b j = W j · β + W j · γ + b j .<label>(13)</label></formula><p>Finally, the contribution of a set of characters with indexes S to the final score of class j is equal to W j · β. The latter score is used throughout the paper for visualizing contributions of sets of char- acters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We execute experiments on morphological tagging in three different languages: Finnish, Spanish and Swedish. We describe the dataset in Section 4.1, whereas model and training details can be found in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>For our experiments, we use the Universal De- pendencies 1.4 (UD) dataset <ref type="bibr" target="#b16">(Nivre et al., 2016)</ref>, which contains morphological features for a large number of sentences. Additionally, we acquired  <ref type="bibr" target="#b23">Silfverberg and Hulden (2017)</ref> selected the first non-unique 300 words from the UD test set and manually segmented each word according to the associated lemma and mor- phological features in the dataset. Whenever pos- sible, they assigned each feature to a specific sub- set of characters. For example, the Spanish word "económicas" is segmented as follows:</p><p>• económic : lemma=económico</p><p>• a : gender=feminine</p><p>• s : number=plural</p><p>For our experiments, we are only interested in word/feature pairs for which a feature can be as- signed to a specific subset of characters. Hence, we filter the test set on those specific word/feature pairs. In the above example, we have two word/feature pairs. This resulted in 278, 340 and 137 word/feature pairs for Finnish, Spanish and Swedish, respectively. Using the same procedure, we selected relevant feature classes, resulting in 12, 6 and 9 feature classes for Finnish, Spanish and Swedish, respectively. <ref type="bibr">4</ref> For each class, when a feature was not available, we introduced an ad- ditional Not Applicable (NA) label.</p><p>We always train and validate on the full UD dataset for which we have filtered out all dupli- cate words. After that, we perform our analysis on either the UD test set or the annotated subset of manually segmented and annotated words. An overview can be found in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model</head><p>We experiment with both a CNN and BiLSTM ar- chitecture for character-level modeling of words.</p><p>At the input, we split every word into charac- ters and add a start-of-word (ˆ) and an end-of-word ($) character. With every character, we associate a character embedding of size 50.</p><p>Our CNN architecture is inspired by <ref type="bibr" target="#b7">Kim et al. (2016)</ref> </p><note type="other">and consists of a set of filters of varying width, followed by a ReLU activation function and a max-over-time pooling operation. We adopt their small-CNN parameter choices and have 25, 50, 75, 100, 125 and 150 convolutional filters of size 1, 2, 3, 4, 5 and 6, respectively. We do not add an additional highway layer. For the character-level BiLSTM architecture, we follow the variant used in Plank et al. (2016). That is, we simply run a BiLSTM over all the char- acters and concatenate the final forward and back- ward hidden state. To obtain a similar number of parameters as the CNN model, we set the hidden state size to 100 units for each LSTM.</note><p>Finally, the word-level representation gener- ated by either the CNN or BiLSTM architecture is classified by a multinomial logistic regression layer. Each morphological class type has a differ- ent layer. We do not take into account context to rule out any influence originating from somewhere other than the characters of the word itself.</p><p>Training details For morphological tagging, we train a single model for all classes at once. We minimize the joint loss by summing the cross-entropy losses of each class. We orthogo- nally initialize all weight matrices, except for the embeddings, which are uniformly initialized ([- 0.01;0.01]). All models are trained using Adam ( <ref type="bibr" target="#b8">Kingma and Ba, 2015</ref>) with minibatches of size 20 and learning rate 0.001. No specific regulariza- tion is used. We select our final model based on early stopping on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>First, we verify that the CD algorithm works cor- rectly by executing a controlled experiment with a synthetic token. Next, we quantitatively and qual- itatively evaluate on the full test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Validation of contextual decomposition for convolutional neural networks</head><p>To verify that the contextual decomposition of CNNs works correctly, we devise an experiment  in which we add a synthetic token to a word of a certain class, testing whether this token gets a high attribution score with respect to that specific class.</p><p>Given a word w and a corresponding binary la- bel t, we add a synthetic character c to the be- ginning of word w with probability p syn if that word belongs to the class t = 1 and with prob- ability 1 − p syn if that word belongs to the class t = 0. Consequently, if p syn = 1, the model should predict the label with a 100% accuracy, thus attributing this to the synthetic character c. When p syn = 0.5, the synthetic character does not provide any additional information about the label t, and c should thus have a small contribution.</p><p>Experimental setup We train a CNN model on the Spanish dataset and only use words having the morphological label number. This label has two classes plur and sing, and assign those classes to the binary labels zero and one, respectively. Fur- thermore, we add a synthetic character to each word with probability p syn , varying p syn from 1 to 0.5 with steps of 0.1. We selected 112 unique word/feature pairs from our test set with label sing or plur. While plurality is marked by the suf- fix s, a variety of suffixes are used for the singu- lar form. Therefore, we focus on the latter class (t = 1). The corresponding suffix is called the Ground Truth (GT) character.</p><p>To measure the impact of p syn , we add a syn- thetic character to each word of the class t = 1 and calculate the contribution of each character by us- ing the CD algorithm. We run the experiment five times with a different random seed and report the average correct attribution. The attribution is cor- rect if the contribution of the synthetic/GT charac- ter is the highest contribution of all character con- tributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results of our evaluation are de- picted in <ref type="figure" target="#fig_3">Figure 2</ref>. When p syn = 1, all words of the class t = 1 contain the synthetic character, and consequently, the accuracy for predicting t = 1 is indeed 100%. Moreover, the correct predic- tion is effectively attributed to the synthetic char- acter ('syn. char attr.' in <ref type="figure" target="#fig_3">Figure 2</ref> at 100%), with the GT character being deemed irrelevant. When the synthetic character probability p syn is lowered, the synthetic character is less trustworthy and the GT character becomes more important (increas- ing 'GT char attr.' in <ref type="figure" target="#fig_3">Figure 2)</ref>. Finally, when p syn = 0.5, the synthetic character is equally plau- sible in both classes. Hence, the contribution of the synthetic character becomes irrelevant and the model attributes the prediction to other characters.</p><p>Consequently, we can conclude that whenever there is a clear character-level pattern, the model learns the pattern and the CD algorithm is able to accurately attribute it to the correct character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of character-level attribution</head><p>In this section, we measure and analyze (1) which characters contribute most to the final prediction of a certain label and (2) whether those contribu- tions coincide with our linguistic knowledge about a language. To that end, we train a model to predict morphological features, given a particular word. The model does not have prior word seg- mentation information and thus needs to discover useful character patterns by itself. After training, we calculate the attribution scores of each charac- ter pattern within a word with respect to the correct feature class using CD, and evaluate whether this coincides with the ground truth attribution.</p><p>Model We train CNN and BiLSTM models on Finnish, Spanish and Swedish. The average accu- racies on the full test set are reported in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="bibr">5</ref> As a reference for the trained models' ability to predict morphological feature classes, we provide a naive baseline, constructed from the majority vote for each feature type.</p><p>Overall, our neural models yield substantially higher average accuracies than the baseline and perform very similar. Consequently, both the CNN and LSTM models learned useful character patterns for predicting the correct morphological feature classes. Hence, this raises the question whether these patterns coincide with our linguis- tic knowledge.</p><p>Evaluation For each annotated word/feature pair, we measure if the ground truth character se-  quence corresponds to the set or sequence of char- acters with the same length within the considered word that has the highest contribution for predict- ing the correct label for that word.</p><formula xml:id="formula_11">3281ô 3281ô l i v a t $ BiLSTM CNN -3.2</formula><p>In the first setup, we only compare with charac- ter sequences having a consecutive set of charac- ters (denoted cons). In the second setup, we com- pare with any set of characters (denoted all). We rank the contributions of each character set and re- port top one, two, and three scores. Because start- of-word and end-of-word characters are not anno- tated in the dataset, we do not consider them part of the candidate character sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The aggregated results for all classes and character sequence lengths are shown in <ref type="figure">Fig-  ure</ref> 3. In general, we observe that for almost all models and setups, the contextual decomposition attribution coincides with the manually-defined segmentations for at least half of the word/feature pairs. When we only consider the top two con- secutive sequences (marked as cons), accuracies range from 76% up to 93% for all three languages. For Spanish and Swedish, the top two accuracies for character sets (marked as all) are still above 67%, despite the large space of possible character sets, whereas all ground truth patterns are consec- utive sequences. While the accuracy for Finnish is lower, the top two accuracy is still above 50%.</p><p>Examples for Finnish, Spanish and Swedish are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. For Finnish, the character with the highest contribution i coincides with the ground truth character for the CNN model. This is not the case for the BiLSTM model which focuses on the character v, even though the correct label is predicted. For Spanish, both models strongly focus on the ground truth character a for predict- ing the feminine gender. For Swedish, the ground truth character sequence is the suffix or which de- notes plurality. Given that or consists of two char- acters, all contributions of character sets of two characters are visualized. As can be seen, the most important set of two characters is {o,r} for the CNN and {k,r} for the BiLSTM model. However, {o,r} is the second most important character set for the BiLSTM model. Consequently, the BiLSTM model deemed the interaction between a root and suffix character more important than between two suffix characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of learned patterns</head><p>In the previous section, we showed that there is a strong relationship between the manually-defined morphological segmentation and the patterns a neural network learns. However, there is still an accuracy gap between the results obtained using consecutive sequences only and results obtained using all possible character sets. Hence, this leads to the question which patterns the neural network focuses on, other than the manually defined pat- terns we evaluated before. To that end, for each of the three languages, we selected a morphological class of interest and evaluated for all words in the full UD test set that were assigned to that class what the most important character set of length one, two and three was. In other words, we eval- uated for each word for which the class was cor- The most frequent character sets used by a model for predicting a specific class. The frequency of occurrence is shown between brackets. An underscore denotes an unknown character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One character</head><p>Two characters Three characters Examples</p><formula xml:id="formula_12">Finnish Tense=Past BiL. i (69%), t (22%), v (4%), a (2%) ti (13%), t_i (12%), v_t (9%), ui (6%) tti (8%), iv_t (5%), t__ti (3%), sti (3%) olivat, näyttikään CNN i (71%), t (8%), s (6%), o (5%) ui (12%), si (11%), ti (11%), oi (9%) a__ui (3%), tii (3%), iv__$ (2%), ui__t (2%) tiesi, meidät Spanish Gend=Fem BiL. a (69%), i (16%), d (6%), e (4%) as (23%), a$ (13%), ad (7%), ia (5%) ia$ (4%), ad$ (3%), da$ (3%), ca$ (2%) tolerancia, ciudad CNN a (77%), ó (14%), n (4%), d (3%) a$ (34%), as (20%), da (8%), ió (7%) dad (5%), da$ (4%), a_ió (4%), sió (2%) firmas, precisión Swedish Numb=Plur BiL. n (25%), r (19%), a (14%), g (7%)</formula><p>na (13%), a__r (4%), or (3%), n__r (3%) iga (5%), rna (3%), ner (1%), der (1%) kronor, perioder CNN n (21%), a (18%), r (15%), d (5%) rn (8%), na (5%), or (4%), er (3%) rna (7%), arn (3%), iga (2%), n_ar (2%) krafterna, saker rectly predicted, which character set had the high- est positive contribution towards predicting that class. The results can be found in <ref type="table" target="#tab_2">Table 3</ref>. Spanish While there is no single clear-cut rule for the Spanish gender, in general the suffix a de- notes the feminine gender in adjectives. However, there exist many nouns that are feminine but do not have the suffix a. <ref type="bibr" target="#b26">Teschner and Russell (1984)</ref> identify d, and ión as typical endings of feminine nouns, which our models identified too as for ex- ample ad$ or ió/sió.</p><p>Swedish In Swedish, there exist four suffixes for creating a plural form: or, ar, (e)r and n. Both models identified the suffix or. However, similar to Finnish, multiple suffixes are merged. In Swedish, the suffix na only occurs together with one of the first three plural suffixes. Hence, both models correctly identified this pattern as an important pattern for predicting the class num- ber=plural, rather than the linguistically-defined pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Interactions of learned patterns</head><p>In the previous section, the pattern a$ showed to be the most important pattern in 34% of the correctly-predicted feminine Spanish words in our dataset. However, there exist many words that end with the character a that are not feminine. For ex- ample the third person singular form of the verb gustar is gusta. Hence, this raises the question if the model will classify gusta wrongly as feminine or correctly as NA. As an illustration of the appli- cability of CD for morphological analysis, we will study this case in more detail. From the full UD test set, we selected all words that end with the character a and that do not be- long to the class gender=feminine. Using the Spanish CNN model, we predicted the gender class for each word and divided the words into two groups: predicted as feminine and predicted as not-feminine (_NA_ or masculine). The re- sulted in 44 and 199 words. Next, for each word in both groups we calculated the most pos- itively and negatively contributing character set out of all possible character sets of any length within the considered word, using the CD algo- rithm. We compared the contribution scores in both groups using a Kruskal-Wallis significance test. <ref type="bibr">6</ref> While no significant (p &lt; 0.05) difference could be found between the positive contributions of both groups (p=1.000), a borderline significant difference could be found between the negative contributions of words predicted as feminine and words predicted as not-feminine (p=0.070).</p><p>Consequently, the CNN model's classification decision is based on finding enough negative evi- dence to counteract the positive evidence found in the pattern a$, which CD was able to uncover.</p><p>A visualization of this interaction is shown in <ref type="figure" target="#fig_8">Figure 5</ref> for the word gusta. While the positive ev- idence is the strongest for the class feminine, the model identifies the verb stem gust as negative ev- idence which ultimately leads to the correct final prediction NA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>While neural network-based models are part of many NLP systems, little is understood on how they handle the input data. We investigated how specific character sequences at the input of a neu- ral network model contribute to word-level tag- ging decisions at the output, and if those contri- butions follow linguistically interpretable rules.</p><p>First, we presented an analysis and visualization technique to decompose the output of CNN mod- els into separate input contributions, based on the principles outlined by <ref type="bibr" target="#b15">Murdoch et al. (2018)</ref> for LSTMs. This allowed us then to quantitatively and qualitatively compare the character-level patterns the CNNs and BiLSTMs learned for the task of morphological tagging. We showed that these pat- terns generally coincide with the morphological segments as defined by linguists for three morpho- logically different languages, but that sometimes other linguistically plausible patterns are learned. Finally, we showed that our CD algorithm for CNNs is able to explain why the model made a wrong or correct prediction.</p><p>By visualizing the contributions of each input unit or combinations thereof, we believe that much can be learned on how a neural network handles the input data, why it makes certain decisions, or even for debugging neural network models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Individual character contributions of the Spanish adjective económicas. The character a has the highest positive (red) contribution for predicting the label Gender=Fem, and the character s for predicting the label Number=Plur. This coincides with our linguistic knowledge of Spanish.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3279</head><label>3279</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the contribution of the synthetic character versus the Ground Truth (GT) character for the class t = 1. The prediction curve denotes the classification accuracy for class t = 1, and consequently, the prediction curve denotes the upper bound for the attributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evaluation of the attributions of CNN and BiLSTM models on the three different languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>c) Example of Swedish. Word (noun): kronor (Swedish valuta as in dollars), target: Number=Plur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Character-level contributions for predicting a particular class. Positive contributions are highlighted in red and negative contributions in blue. The ground truth character sequence is highlighted in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Finnish</head><label></label><figDesc>In Finnish, adding the suffix i to a verb, transforms it in the past tense. Sometimes the character s is added, resulting in the suffix si. The latter is a frequently used bigram pattern by the CNN but less by the BiLSTM. The BiLSTM com- bines the suffix i with another suffix vat which de- notes third person plural in the character pattern iv_t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the most positively and negatively contributing character set for each class of the morphological feature class gender for the Spanish verb gusta (likes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Overview of the training, validation and test set used.</head><label>1</label><figDesc></figDesc><table>Finnish Spanish Swedish 

Train words 
53547 
62556 
16295 
Valid words 
2317 
4984 
1731 
Test words 
2246 
956 
3538 

Annotated 
Test pairs 
278 
340 
137 

manually-annotated character-level morphologi-
cal segmentations and labels for a subset of the test 
set for three morphological different languages: 
Finnish, Spanish and Swedish. 3 
For each language, </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Average accuracy of all models trained on Finnish, Spanish and Swedish for the task of mor- phological feature prediction for all unique words in the full UD test set.</figDesc><table>Finnish Spanish Swedish 

Maj. Vote 82.20% 72.39% 69.79% 
CNN 
94.81% 88.93% 90.09% 
BiLSTM 95.13% 89.33% 89.45% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> In Brill (1994), an additional rule encodes an exception to this rule to correctly tag the word mistress.</note>

			<note place="foot" n="2"> https://github.com/FredericGodin/ ContextualDecomposition-NLP</note>

			<note place="foot" n="3"> Method For visualizing the contributions of character sets, we use the recently introduced Contextual Decomposition (CD) framework, as originally developed for LSTMs (Murdoch et al., 2018), and extend it to</note>

			<note place="foot" n="3"> Available online: http://github.com/mpsilfve/ud-segmen ter/commit/5959214d494cbc13e53e1b26650813ff950d2ee3 4 Full list available as supplementary material</note>

			<note place="foot" n="5"> The results of the individual classes are provided as supplementary material.</note>

			<note place="foot" n="6"> The full statistical analysis is provided as supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous re-viewers and members of IDLab for their valuable feedback. FG would like to thank Kim Bettens for helping out with the statistical analysis.</p><p>The research activities as described in this pa-per were funded by Ghent University, imec, Flan-ders Innovation &amp; Entrepreneurship (VLAIO), the Fund for Scientific Research-Flanders (FWO-Flanders), and the European Union.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What do neural machine translation models learn about morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic tagging with deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A report of recent progress in transformation-based error-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology (HLT)</title>
		<meeting>the Workshop on Human Language Technology (HLT)</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representations of language in a model of visually grounded speech signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieke</forename><surname>Gelderloos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An extensive empirical evaluation of character-based morphological tagging for 14 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guenter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ákos</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A computational approach to grammatical coding of english words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheldon</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">F</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Computing Machinery</title>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>abs/1612.08220</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
		<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Beyond word importance: Contextual decomposition to extract interactions from LSTMs. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC)<address><addrLine>Natalia Silveira, Reut Tsarfaty, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Confernce of Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic morpheme segmentation and labeling in universal dependencies resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miikka</forename><surname>Silfverberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mans</forename><surname>Hulden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies</title>
		<meeting>the NoDaLiDa 2017 Workshop on Universal Dependencies</meeting>
		<imprint>
			<publisher>UDW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The gender patterns of spanish nouns: an inverse dictionarybased analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Teschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Hispanic Linguistics 1</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From characters to words to in between: Do we capture morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
