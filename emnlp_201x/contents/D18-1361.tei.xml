<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Playing 20 Question Game with Policy-Based Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianchao</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Development Co</orgName>
								<address>
									<settlement>Ltd, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Development Co</orgName>
								<address>
									<settlement>Ltd, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Playing 20 Question Game with Policy-Based Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3233" to="3242"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3233</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly out-performs an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The 20 Question Game (Q20 Game) is a classic game that requires deductive reasoning and cre- ativity. At the beginning of the game, the an- swerer thinks of a target object and keeps it con- cealed. Then the questioner tries to figure out the target object by asking questions about it, and the answerer answers each question with a simple "Yes", "No" or "Unknown", honestly. The ques- tioner wins the game if the target object is found within 20 questions. In a Q20 game system, the * The work was done when the first author was an intern in Microsoft XiaoIce team.</p><p>user is considered as the answerer while the sys- tem itself acts as the questioner which requires a good question selection strategy to win the game.</p><p>As a game with the hype read your mind, Q20 has been played since the 19th century, and was brought to screen in the 1950s by the TV show Twenty Questions. Burgener's program <ref type="bibr" target="#b1">(Burgener, 2006</ref>) further popularized Q20 as an electronic game in 1988, and modern virtual assistants like Microsoft XiaoIce and Amazon Alexa also incor- porate this game into their system to demonstrate their intelligence.</p><p>However, it is not easy to design the algorithm to construct a Q20 game system. Although the de- cision tree based method seems like a natural fit to the Q20 game, it typically require a well de- fined Knowledge Base (KB) that contains enough information about each object, which is usually not available in practice. <ref type="bibr" target="#b1">Burgener (2006)</ref> instead uses a object-question relevance table as the pivot for question and object selection, which does not depend on an existing KB. <ref type="bibr" target="#b11">Wu et al. (2018)</ref> further improve the relevance table with a lot of engineer- ing tricks. Since these table-based methods greed- ily select questions and the model parameters are only updated by rules, their models are very sen- sitive to noisy answers from users, which is com- mon in the real-world Q20 games. <ref type="bibr" target="#b12">Zhao and Maxine (2016)</ref> utilizes a value-based Reinforcement Learning (RL) model to improve the generaliza- tion ability but still relies on the existing KB.</p><p>In this paper, we formulate the process of ques- tion selction in the game as a Markov Deci- sion Process (MDP), and further propose a novel policy-based RL framework to learn the optimal policy of question selection in the Q20 game. Our questioner agent maintains a probability distribu- tion over all objects to model the confidence of the target object, and updates the confidence based on answers from the user. At each time-step. the agent uses a policy network π θ (a|s) to take in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q20 Game Environment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next State</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RewardNet</head><p>Take Action</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-Batch Samples</head><p>Episodes st, at, rt+1, st+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replay Memory</head><p>Policy-based Agent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-Batch Samples with Estimated Reward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ask Question</head><p>Answer the Question <ref type="figure">Figure 1</ref>: The overview of our RL framework. the confidence vector and output a question dis- tribution for selecting the next question. To solve the problem that there is no immediate reward for each selected question, we also propose to employ a RewardNet to estimate the appropriate immedi- ate reward at each time-step, which is further used to calculate the long-term return to train our RL model. Our RL framework makes the agent robust to noisy answers since the model parameters are fully learnable and the question distribution from π θ (a|s) provides us with a principled way to sam- ple questions, which enables the agent to jump out of the local optimum caused by incorrect answers and also introduces more randomness during train- ing to improve the model generalization ability. Furthermore, the ability to sample questions, com- pared to greedy selection, also improves the diver- sity of the questions asked by our agent, which is crucial for user experience.</p><p>Our contributions can be summarized as fol- lows: (1) We propose a novel RL framework to learn the optimal policy of question selection in the Q20 game without any dependencies on the existing KBs of target objects. Our trained agent is robust to noisy answers and has a good diver- sity in its selected questions. (2) To make the re- ward more meaningful, we also propose a novel neural network on reward function approximation to deliver the appropriate immediate rewards at each time-step. (3) Extensive experiments show that our RL method clearly outperforms a highly engineered baseline in the real-world Q20 games where noisy answers are common. Besides, our RL method is also competitive to that baseline on a noise-free simulation environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we first describe our RL framework for playing the Q20 game, which is shown in the <ref type="figure">Fig. 1</ref>. The user in our system is the answerer who thinks of a target object o tgt in the object set O at the beginning of the game. Our policy-based agent acts as the questioner that can ask 20 ques- tions to figure out what exactly o tgt is. Specifi- cally, an internal state vector s is maintained by our agent, which describes the confidence about o tgt . At each time-step t, the agent picks up the promising action (select a question) according to the policy π θ (a|s t ), and transits from the state s t to the next state s t+1 after receiving the answer ("Yes"/"No"/"Unknown") from the user. The his- torical trajectories s t , a t , r t+1 , s t+1 are stored in a replay memory which enables the agent to be trained on previously observed data by sampling from it. Note that only when a guess is made about o tgt at the end of game can the agent receive a re- ward signal, which makes it unable to distinguish the importance of each selected question. There- fore, we design a RewardNet to learn the more in- formative reward at each time-step and thus lead the agent to achieve the better performance.</p><p>In the rest of this section, we first describe how to formulate the Q20 game into a RL framework, and then introduce the RewardNet. Finally, we will demonstrate our training procedure in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling of the Q20 Game</head><p>In the Q20 game, the goal of our agent is to figure out the object o tgt that the user thinks of at the be- ginning of game by asking 20 questions. We for- mulate the process of question selection as a finite Markov Decision Process (MDP) which can be solved with RL. A tuple S, A, P, R, γ is defined to represent the MDP, where S is the continuous state space, A = {a 1 , a 2 , · · · , a m } is the set of all available actions, P(S t+1 = s |S t = s, A t = a) is the transition probability matrix, R(s, a) is the re- ward function and γ ∈ [0, 1] is the discount factor used to calculate the long-time return. In the RL framework, at each time-step t, the agent takes an action a t under the state s t according to the policy π θ (a|s t ). After interacting with the environment, the agent receives a reward scalar r t+1 and tran- sits to the next state s t+1 , then another time-step begins. All these trajectories s t , a t , r t+1 , s t+1 in a game constitute an episode which is an instance of the finite MDP. The long-time return G t of the time-step t is calculated as follows:</p><formula xml:id="formula_0">G t = T k=0 γ k r t+k+1</formula><p>(1)</p><p>In the following parts, we describe each compo- nent of RL corresponding to the Q20 game.</p><p>Environment. The major component of our en- vironment is the user in the Q20 game who de- cides the target object o tgt and answers questions from the agent. Besides, the environment also needs to deliver the reward based on the outcome of the game and store historical data into the replay memory (see <ref type="figure">Fig. 1</ref>).</p><p>Action. Since the agent interacts with the user by asking questions, the action a t ∈ A taken by our agent refers to selecting the question q at at time- step t, and A is the set of the indices to all available questions in the Q20 game.</p><p>State. In our method, we use the state s t to keep track of the current confidence of target object o tgt . Specifically s t ∈ R |O| and n i=1 s t,i = 1, where O = {o 1 , o 2 , · · · , o n } represents the set of all the objects that can be chosen by the user. Therefore, the state s t is a probability distribution over all the objects and s t,i is the confidence that the object o i is the target object o tgt at time-step t.</p><p>The initial state s 0 can either be a uniform dis- tribution or initialized by the prior knowledge. We observe that users typically prefer to choose popu- lar objects which are more concerned by the pub- lic. For example, the founder of Tesla Inc. and the designer of SpaceX, "Elon Musk", is more likely to be chosen compared to a CEO of a new startup. Motivated by this, we could use the yearly retrieval frequency C(o i ) of object o i on a com- mercial search engine to calculate the initial state s 0 , where s 0,i = C(o i ) / n j=1 C(o j ). Transition Dynamics. In our method, the transi- tion dynamics is deterministic. Given the object set O and the question set A, we collect the nor- malized probabilities of the answer over "Yes", "No" and "Unknown" for each object-question pair. And the rule of state transition is define as:</p><formula xml:id="formula_1">s t+1 = s t α (2)</formula><p>where α depends on the answer x t to the question q at which is selected by the agent at the step t:</p><formula xml:id="formula_2">α =    [R(1, a t ), . . . , R(|O|, a t )], x t = Y es [W (1, a t ), . . . , W (|O|, a t )], x t = N o [U (1, a t ), . . . , U (|O|, a t )], x t = U nk (3)</formula><p>where O is the object set and for each object- question pair (o i , q j ), R(i, j) and W (i, j) are cal- culated as follows:</p><formula xml:id="formula_3">R(i, j) = C yes (i, j) + δ C yes (i, j) + C no (i, j) + C unk (i, j) + λ W (i, j) = C no (i, j) + δ C yes (i, j) + C no (i, j) + C unk (i, j) + λ (4)</formula><p>R(i, j) and W (i, j) are probabilities of answering "Yes" and "No" to question q j with respect to the object o i respectively. C yes (i, j), C no (i, j) and C unk (i, j) are frequencies of answering "Yes", "No" and "Unknown" to question q j with respect to the object o i . δ and λ are smoothing parameters. Then the probability of answering "Unknown" to question q j with respect to the object o i is:</p><formula xml:id="formula_4">U (i, j) = 1 − R(i, j) − W (i, j)<label>(5)</label></formula><p>In this way, the confidence s t,i that the object o i is the target object o tgt is updated following the user's answer x t to the selected question q at at the time-step t.</p><p>Policy Network. We directly parameterize the policy π θ (a|s t ) with a neural network which maps the state s t to a probability distribution over all available actions: π θ (a|s t ) = P[a|s t ; θ]. The pa- rameters θ are updated to maximize the expected return which is received from the environment. In- stead of learning a greedy policy in value-based methods like DQN, the policy network is able to learn a stochastic policy which can increase the di- versity of questions asked by our agent and poten- tially make the agent more robust to noisy answers in the real-world Q20 game. The policy π θ (a|s) is modeled by a Multi-Layer Perceptron (MLP) and the output layer is normalized by using a masked softmax function to avoid selecting the question that has been asked before. Because asking the same question twice does not provide extra infor- mation about o tgt in a game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem of Direct Reward</head><p>For most reinforcement learning applications, it is always a critical part to design reward functions, especially when the agent needs to precisely take actions in a complex task. A good reward function can improve the learning efficiency and help the agent achieve better performances.</p><p>In the Q20 game, however, the immediate re- ward r t of selecting question q at is unknown at the time-step t (t &lt; T ) because each selected ques- tion is just answered with a simple "Yes", "No" or "Unknown" and there is no extra information pro- vided by user. Only when the game ends (t = T ) can the agent receive a reward signal of win or loss. So we intuitively consider the direct reward: r T = 30 and −30 for the win and loss respec- tively while r t = 0 for all t &lt; T . Unfortu- nately, the direct reward is not discriminative be- cause the agent receives the same immediate re- ward r t = 0 (t &lt; T ) for selecting both good and bad questions. For example, if the o tgt is "Donald Trump", then selecting question (a) "Is your role the American president?" should receive more im- mediate reward r t than selecting question (b) "Has your role been married?". The reason is that as for the o tgt , question (a) is more relevant and can nar- row down the searching space to a greater extent.</p><p>Therefore, it is necessary to design a better re- ward function to estimate a non-zero immediate reward r t , and make the long-time return G t = T k=0 γ k r t+k+1 more informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reward Function Approximation by Neural Network</head><p>To solve the problem of the direct reward, we pro- pose a reward function which employs a neural network to estimate a non-zero immediate reward r t at each time-step. So that G t can be more infor- mative, which thus leads to a better trained ques- tioner agent. The reward function takes the state-action pair (s t , a t ) as input and outputs the corresponding im- mediate reward r t+1 . In our method, we use a MLP with sigmoid output to learn the appropri- ate immediate reward during training, and this net- work is referred as RewardNet. In each episode, the long-term return G t is used as a surrogate in- dicator of r t+1 to train our RewardNet with the following loss function:</p><formula xml:id="formula_5">L 1 (σ) = (R(s t , a t ; σ) − sigmoid(G t )) 2 (6)</formula><p>where σ is the network parameters. Here we ap- ply the sigmoid function on G t so as to prevent G t from growing too large. Besides, we also use the replay memory to store both old and recent ex- periences, and then train the network by sampling mini-batches from it. The training process based on the experience replay technique can decorrelate the sample data and thus make the training of the RewardNet more efficient.</p><p>Furthermore, since the target object o tgt can be obtained at the end of each episode, we can use the extra information provided by o tgt to es- timate a better immediate reward r t . To capture the relevance between the selected questions and o tgt in an episode, we further propose a object- aware RewardNet which takes the s t , a t , o tgt tu- ple as input and produces corresponding r t+1 as output. The detailed training algorithm is shown in Algo. 1. </p><formula xml:id="formula_6">G t ← T k=0 γ k r t+k+1 13 r t+1 ← sigmoid(G t ) 14 Store (s t , a t , o i , r t+1 ) in D 1 15 if len(D 1 ) &gt; K 1 then 16</formula><p>Sample mini-batch from D 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Update σ with loss L 1 (σ) in Eq. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training the Policy-Based Agent</head><p>We train the policy network using REIN- FORCE (Williams, 1992) algorithm and the cor- responding loss function is defined as follows:</p><formula xml:id="formula_7">L 2 (θ) = −E π θ [log π θ (a t |s t )(G t − b t )]<label>(7)</label></formula><p>where the baseline b t is a estimated value of the ex- pected future reward at the state s t , which is pro- duced by a value network V η (s t ). Similarly, the value network V η (s t ) is modeled as a MLP which takes the state s t as input and outputs a real value as the expected return. By introducing the base- line b t for the policy gradient, we can reduce the variance of gradients and thus make the training process of policy network more stable. The net- work parameters η are updated by minimizing the loss function below:</p><formula xml:id="formula_8">L 3 (η) = (V η (s t ) − G t ) 2<label>(8)</label></formula><p>Note that, in our method, both the RewardNet and the value network V η (s t ) approximate the re- ward during training. But the difference lies in that the RewardNet is designed to estimate a ap- propriate non-zero reward r t and further derive the more informative return G t while V η (s t</p><note type="other">) aims to learn a baseline b t to reduce the variance of policy gradients. We combine both of two networks to improve the gradients for our policy network and thus lead to a better agent. The training procedure is described in Algo. 2. Algorithm 2: Training the Agent 1 Initialize replay memory D 2 to capacity N 2 2 Initialize policy net π with random weights θ 3 Initialize value net V with random weights η 4 Initialize RewardNet with random weights σ 5 for episode i ← 1 to Z do 6</note><p>Rollout, collect rewards, and save the history in S 2 (4-10 in Algo. 1)</p><formula xml:id="formula_9">7 for (s t , a t , r t+1 ) in S 2 do 8 G t ← T k=0 γ k r t+k+1 9</formula><p>Update RewardNet (13-17 in Algo. 1)</p><formula xml:id="formula_10">10 Store (s t , a t , G t ) in D 2 11 if len(D 2 ) &gt; K 2 then 12</formula><p>Sample mini-batch from D 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>Update η with loss L 3 in Eq. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head><p>Update θ with loss L 2 in Eq. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We use a user simulator to train our questioner agent and test the agent with the simulated an- swerer and real users. Specifically, our experi- ments answer three questions: (1) Is our method more robust in real-world Q20 games, compared to the methods based on relevance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">User Simulator</head><p>Training the RL agent is challenging because the agent needs to continuously interact with the envi- ronment. To speed up the training process of the proposed RL model, we construct a user simulator which has enough prior knowledge to choose ob- jects and answer questions selected by the agent. We collect 1,000 famous people and 500 ques- tions for them. Besides, for every person-question pair in our dataset, a prior frequency distribution over "Yes", "No" and "Unknown" is also collected from thousands of real users. For example, as for "Donald Trump", question (a) "Is your role the American president?" is answered with "Yes" for 9,500 times, "No" for 50 times and "Unknown" for 450 times. We use Eq.4 and 5 to construct three matrices R, W, U ∈ R |O| * |A| (|O| = 1000, |A| = 500) which are used for state transition in the Sec- tion. 2.1. Then given the object o i and question q j , the user simulator answers "Yes", "No" and "Un- known" when R(i, j), W (i, j), and U (i, j) has the max value among them respectively.</p><p>Constructed by the prior knowledge, the sim- ulator can give noise-free answer in most cases. Because the prior frequency distribution for each person-question pair is collected from thousands of users with the assumption that most of them do not lie when answering questions in the Q20 game.</p><p>In an episode, the simulator randomly samples a person following the object distribution s 0 , which is generated from the object popularity (see the state part of Section. 2.1), as the target object. Then the agent gives a guess when the number of selected questions reaches 20. After that, the simulator check the agent's answer and return a reward signal of win or loss. There is only one chance for the agent to guess in an episode. The win and loss reward are 30 and -30 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>While the architectures of the policy network, Re- wardNet and value network can vary in different scenarios, in this paper, we simply use the MLP with one hidden layer of size 1,000 for all of them, but with different parameters. These networks take in the state vector directly, which is a prob- ability distribution over all objects. The Reward- Net further takes in the one-hot vector of action a t . Based on the input of RewardNet, the object- aware RewardNet takes one more target object o tgt as the feature which is also a one-hot vector.</p><p>We use the ADAM optimizer ( <ref type="bibr" target="#b5">Kingma and Ba, 2014</ref>) with the learning rate 1e-3 for policy net- work and 1e-2 for both RewardNet and value net- work. The discounted factor γ for calculating the long-term return is 0.99. The model was trained up to 2,000,000 steps (2,00,000 games) and the pol- icy network was evaluated every 5,000 steps. Each evaluation records the agent's performance with a greedy policy for 2,000 independent episodes. The 2,000 target objects for these 2,000 episodes are randomly selected following the distribution s 0 , which is generated from the object popularity and kept the same for all the training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Competitor</head><p>We compare our RL method with the entropy- based model proposed by <ref type="bibr" target="#b11">Wu et al. (2018)</ref>, which utilizes the real-world answers to each object- question pair to calculate an object-question rele- vance matrix with the entropy-based method. The relevance matrix is then used for question ranking and object ranking via carefully designed formu- las and engineering tricks. Since this method is shown to be effective in their production environ- ment, we consider it to be a strong baseline to our proposed RL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulated Evaluation</head><p>We first evaluate our agent and the entropy-based baseline (referred to as EntropyModel, see Section. 3.3) by using the simulated user (Sec- tion. 3.1). To investigate which initialization strat- egy of the state s 0 is better (see the state part of Section. 2.1), we further evaluate two variants of our model: the agent with uniform distribution s 0 (RL uniform) and the agent with the distribution s 0 initialized by the prior knowledge on the object popularity (RL popularity). <ref type="figure" target="#fig_1">Fig. 2</ref> shows the curves on the win rate of these methods evaluated on 2,000 independent episodes with respect to the number of training steps. Note that, the EntropyModel only needs to update its statistics during training and has already accu- mulated a significant number of data since it has been run for over a year in their production envi- ronment. Therefore, only a small fraction of its statistics can be changed, which leads to a small rise at the beginning of training, and its win rate remains at around 95% afterwards.</p><p>On the other hand, both our RL models con- tinuously improve the win rate with the growing number of interactions with the user simulator, and they achieve 50% win rate after around 20,000 steps. As we can see, although the s 0 initial- ized with the prior knowledge of object popular- ity keeps consistent with the object selection strat- egy of the simulator, the agent with uniform dis- tribution s 0 (RL uniform) still performs clearly better than the agent with s 0 based on the prior knowledge (RL popularity). The reason is that the former can explore the Q20 game environ- ment more fully. The prior knowledge based s 0 helps the agent narrow down the candidate space more quickly when the target object is a popu- lar object. However, it also becomes misleading when the target object is not popular and makes the agent even harder to correct the confidence of the target object. On the contrary, the uniform dis- tribution s 0 makes the agent keep track of the tar- get object only based on the user's answers. And the superior performance of the RL uniform in- dicates that our question selection policy is highly effective, which means it is not necessary to use the RL popularity to increase the win rate of hot objects in the game.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, RL uniform achieves win rate 94% which is very close to EntropyModel. Compared to our RL method, EntropyModel needs more user data to calculate their entropy- based relevance matrix and involves many engi- neering tricks. The fact that RL uniform is com- petitive to EntropyModel in the noise-free sim- ulation environment indicates that our RL method is very cost-effective: it makes use of user data more efficiently and is easier to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Evaluation</head><p>To further investigate the performance of our RL method in the real-world Q20 game where noisy answers are common, we also conduct an human evaluation experiment. Specifically, we let real</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Win Rate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EntropyModel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>71.3% RL uniform</head><p>75.9% users to play the game with EntropyModel and RL uniform for 1,000 times respectively. In the real-world Q20 game, users sometimes make mis- takes when they answer the questions during the game. For example, as for the target object "Don- ald Trump", question (a) "Is your role the Ameri- can president?" is sometimes answered with "No" or "Unknown" by real users. On the contrary, the simulator hardly makes such mistakes since we have provided it with enough prior knowl- edge. As shown in <ref type="table">Table.</ref> 1, RL uniform out- performs EntropyModel by about 4.5% on win rate in the real-world Q20 games. It shows that our RL method is more robust to noisy answers than EntropyModel. Specifically, the robust- ness of our RL method to the noise is shown in the following two aspects. First, compared to the rule- based statistics update in EntropyModel, our RL model can be trained by modern neural net- work optimizers in a principled way, which results in the better generalization ability of our model. Secondly, different from the EntropyModel se- lecting the top-ranked question at each time-step, RL uniform samples a question following its question probability distribution π θ (a|s), which enables our agent to jump out of the local optimum caused by incorrect answers from users. And since more randomness is introduced by sampling from the question probability distribution during train- ing, it also improves the tolerance of our model towards the unexpected question sequences.</p><p>Besides, we also find some interesting cases during human evaluation. Sometimes, the RL agent selects a few strange questions which seems to be not that much relevant to the chosen object, but it can still find the correct answer at the end of game. This situation is caused by the fact that our method samples questions based on the output of policy net, rather than greedy selection during training. We find that this phenomenon increases the user experience since it makes the agent more unpredictable to the users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Effectiveness of RewardNet</head><p>To investigate the effectiveness of our RewardNet (Section. 2.3), we further evaluate three variants of our model in the simulation environment: the model trained with with direct reward, Reward- Net, and object-aware RewardNet, which are re- ferred to as DirectReward, RewardNet, and ObjectRewardNet respectively. They are all trained with the uniform distribution s 0 .</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, DirectReward con- verges in the early steps and has a relatively poor performance with the win rate 89%. Both RewardNet and ObjectRewardNet achieve the better performance with a win rate of 94% af- ter convergence. This clear improvement shows that the more informative long-term return, calcu- lated with the immediate reward delivered by our RewardNet method, significantly helps the train- ing of the agent.</p><p>Furthermore, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we can also see that ObjectRewardNet learns faster than RewardNet in the early steps. This indicates that ObjectRewardNet can estimate the immediate reward more quickly with the extra information provided by the target object, which leads to the faster convergence of the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Win Rate Regarding Question Numbers</head><p>In this section, we investigate how the win rate grows with the number of asked questions and whether a early-stop strategy can be adopted in the game. We use the user simulator to play the game with the RL uniform agent and two settings are taken into account: the simulator samples the tar- get object following the uniform object distribu- tion (UnifSimulator), and samples following the prior object distribution based on the object popularity (PopSimulator). We perform 1,000 simulations for each number of questions, and the win rate curve is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>As we can see that UnifSimulator achieves the win rate of 80% with only 14 questions in both settings. And the flat curves in the region after 18 questions indicate that the game can be early stopped with the almost same win rate at step 18. Since a lower win rate is acceptable sometimes, other early-stop strategies can also be derived for the better user experience with the trade-off be- tween the win rate and game steps.</p><p>Besides, the fact that RL uniform performs similarly under both settings actually shows that our RL method is robust to different objects. It also performs well on infrequent objects where we may have the limited user data for constructing a well-tuned state transition dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>When our agent is playing the game with real users, we select two cases from records. In the first case, the person that the user chooses is Cris- tiano Ronaldo, the famous football player. As we can see in Tab. 2, our agent can still figure out the target person while No.17 and No.19 questions are answered wrong by the user, which indicates our agent is robust to noisy answers. In the second case, the chosen person is Napoleon Bonaparte who was the French Emperor. Although there are some other candidates satisfied the constraints, the target person can be figured out because of the people popularity, which is shown in Tab. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Q20. The Q20 game is popularized as an elec- tronic game by the program of Robin Burgener in 1988 <ref type="bibr" target="#b1">(Burgener, 2006</ref>), which uses a object- question relevance table to rank questions and tar- get objects. <ref type="bibr" target="#b11">Wu et al. (Wu et al., 2018)</ref> improves the relevance table with entropy-based metrics, and uses complicated engineering tricks to make it perform quite well in their production environ- ment. These table-based methods use rules to update parameters, which makes them easily af- fected by noisy answers. Besides, Zhao and Max- ine (2016) also explores Q20 in their dialogue state tracking research. However, they only use a small toy Q20 setting where the designed questions are about 6 person attributes in the Knowledge Base (KB). Since their method relies on the KB for nar- rowing down the scope of target object, it is not applicable to real-world Q20 games where a well- defined object KB is often unavailable. Compared to previous approaches, our RL method is robust to the answer noise and does not rely on the KB.</p><p>Deep Reinforcement Learning. DRL has wit- nessed great success in playing complex games like Atari games ( <ref type="bibr" target="#b6">Mnih et al., 2015)</ref> , <ref type="bibr">Go (Silver et al., 2016)</ref>, and etc. In the natural language pro- cessing (NLP), DRL is also used to play text-based games ( <ref type="bibr" target="#b7">Narasimhan et al., 2015)</ref>, and used to han- dle fundamental NLP tasks like machine transla- tion ( <ref type="bibr" target="#b3">He et al., 2016</ref>) and machine comprehen- sion ( <ref type="bibr" target="#b4">Hu et al., 2017)</ref> as well. Our Q20 game lies in the intersection of the field of game and NLP. In this work, we propose a policy-based RL model that acts as the questioner in the Q20 game, and it exhibits the superior performance in our human evaluation.</p><p>Natural Language Games. In the literature, there are some works focusing on solving and generat- ing English riddles <ref type="bibr" target="#b2">(De Palma and Weiner, 1992;</ref><ref type="bibr" target="#b0">Binsted, 1996)</ref> and Chinese character riddles ( <ref type="bibr" target="#b9">Tan et al., 2016</ref>). Compared to riddles, the Q20 game is a sequential decision process which requires careful modeling of this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a policy-based RL method to solve the question selection problem in the Q20 Game. Instead of using the direct reward, we further propose an object-aware RewardNet to estimate the appropriate non-zero reward and thus make the long-time return more informative. Compared to previous approaches, our RL method is more robust to the answer noise which is com- mon in the real-world Q20 game. Besides, our RL agent can also ask various questions and does not require the existing KB and complicated engineer- ing tricks. The experiments on a noisy-free sim- ulation environment show that our RL method is competitive to an entropy-based engineering sys- tem, and clearly outperforms it on the human eval- uation where noisy answers are common.</p><p>As for the future work, we plan to explore meth- ods to use machine reading to automatically con- struct the state transition dynamics from corpora like Wikipedia. In this way, we can further build an end-to-end framework for the large-scale Q20 games in the real world.  Is the person engaged in many charity activities? Unknown Has the person once been the emperor? Yes <ref type="table">Table 3</ref>: In this case, the person that the user chooses is Napoleon Bonaparte, the French Emperor. Although there are some other candidates satisfied the constraints, our agent can figure out the target person because of the people popularity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>×Figure 2 :</head><label>2</label><figDesc>Figure 2: Win Rate Curves in Simulation Environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>×Figure 3 :</head><label>3</label><figDesc>Figure 3: Effectiveness of RewardNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Win Rate Regarding Numbers of Questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>No</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table ? (</head><label>?</label><figDesc></figDesc><table>Sec-
tion. 4.2) And how does it perform in the simu-
lation environment? (Section. 4.1) (2) Does our 
RewardNet help in the training process? (Sec-
tion. 4.3) (3) How the winning rate grows with the 
number of questions, and whether it is possible to 
stop earlier? (Section. 4.4) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Win Rate on Human Evaluation.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The person that the user chooses is Cristiano Ronaldo, the famous football player. As we can see in table, 
our agent can still figure out the target person while No.17 and No.19 are answered wrong by the user, which 
indicates our agent is robust to noisy answers. 

No. Question 
User's Answer 

1 
Is the person female? 
No 
2 
Is the person still alive? 
No 
3 
Does the person have children? 
Yes 
4 
Does the person have brothers or sisters in the family? 
Yes 
4 
Is the person very smart? 
Yes 
5 
Was the person born in America? 
No 
6 
Is the person the white man? 
Yes 
7 
Is the person's family very rich? 
No 
8 
Is the person a controversial figure in history? 
Yes 
9 
Is the person related to politics? 
Yes 
10 
Does the person have good looks? 
Unknown 
11 
Does the person have short hair? 
Yes 
12 
Is the person very famous? 
Yes 
13 
Has the person once been very powerful? 
Yes 
14 
Is the character of the person very aggressive? 
No 
15 
Has the person been the president of a country? 
Yes 
16 
Is the person a military? 
Yes 
17 
Has the person once killed men? 
No 
18 
Was the person born in Britain? 
No 
19 
Was the person one of famous leaders in the World War II? No 
20 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We gratefully thank the anonymous reviewers for their insightful comments and suggestions on the earlier version of this paper. The first author also thanks the Microsoft for providing resources for the research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Machine humour: An implemented model of puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Artificial neural network guessing method and game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Burgener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Riddles: accessibility and knowledge representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul De</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E Judith</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1121" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno>abs/1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Humanlevel control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language understanding for textbased games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Solving and generating chinese character riddles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="846" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Q20: Rinna riddles your mind by asking 20 questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Momo</forename><surname>Klyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyohei</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Chen</surname></persName>
		</author>
		<editor>Japan NLP</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
