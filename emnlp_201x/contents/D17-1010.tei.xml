<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mimicking Word Embeddings using Subword RNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Guthrie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mimicking Word Embeddings using Subword RNNs</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="102" to="112"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using dis-tributional information obtained from un-labeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIM-ICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require retraining on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntac-tic attributes. It is competitive with (and complementary to) a supervised character-based model in low-resource settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the key advantages of word embeddings for natural language processing is that they en- able generalization to words that are unseen in labeled training data, by embedding lexical fea- tures from large unlabeled datasets into a rela- tively low-dimensional Euclidean space. These low-dimensional embeddings are typically trained to capture distributional similarity, so that infor- mation can be shared among words that tend to appear in similar contexts. However, it is not possible to enumerate the en- tire vocabulary of any language, and even large un- labeled datasets will miss terms that appear in later applications. The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods. These challenges are particularly acute when working with low- resource languages, where even unlabeled data may be difficult to obtain at scale. A typical so- lution is to abandon hope, by assigning a single OOV embedding to all terms that do not appear in the unlabeled data.</p><p>We approach this challenge from a quasi- generative perspective. Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the lat- ter. We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task. We call this model the MIMICK-RNN, for its ability to read a word's spelling and mimick its distributional embedding.</p><p>Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features. As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors. Quantitative evalua- tion on the Stanford RareWord dataset ( <ref type="bibr" target="#b15">Luong et al., 2013</ref>) provides more evidence that these character-based embeddings capture word similar- ity for rare and unseen words.</p><p>As an extrinsic evaluation, we conduct ex- periments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Univer- sal Dependencies dataset <ref type="bibr" target="#b5">(De Marneffe et al., 2014</ref>). Our model shows significant improvement across the board against a single UNK-embedding backoff method, and obtains competitive results against a supervised character-embedding model, which is trained end-to-end on the target task. In low-resource settings, our approach is par- ticularly effective, and is complementary to su- pervised character embeddings trained from la- beled data. The MIMICK-RNN therefore pro- vides a useful new tool for tagging tasks in set- tings where there is limited labeled data. Models and code are available at www.github.com/ yuvalpinter/mimick .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Compositional models for embedding rare and unseen words. Several studies make use of morphological or orthographic information when training word embeddings, enabling the predic- tion of embeddings for unseen words based on their internal structure. <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref> compute word embeddings by summing over em- beddings of the morphemes; <ref type="bibr" target="#b15">Luong et al. (2013)</ref> construct a recursive neural network over each word's morphological parse; <ref type="bibr" target="#b1">Bhatia et al. (2016)</ref> use morpheme embeddings as a prior distribu- tion over probabilistic word embeddings. While morphology-based approaches make use of mean- ingful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes. Character-based approaches avoid these problems: for example, <ref type="bibr" target="#b12">Kim et al. (2016)</ref> train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; <ref type="bibr" target="#b24">Wieting et al. (2016)</ref> learn embeddings of character n- grams, and then sum them into word embeddings. In all of these cases, the model for composing em- beddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus. In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained. This is similar to the "retrofitting" ap- proach of <ref type="bibr" target="#b7">Faruqui et al. (2015)</ref>, but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.</p><p>Supervised subword models. Another class of methods learn task-specific character-based word embeddings within end-to-end supervised sys- tems. For example, <ref type="bibr" target="#b21">Santos and Zadrozny (2014)</ref> build word embeddings by convolution over char- acters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging ob- jective drives the entire learning process. <ref type="bibr" target="#b14">Ling et al. (2015)</ref> propose a multi-level long short- term memory (LSTM; Hochreiter and Schmidhu- ber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over words. <ref type="bibr" target="#b20">Plank et al. (2016)</ref> show that concatenat- ing a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging. Because these methods learn from la- beled data, they can cover only as much of the lex- icon as appears in their labeled training sets. As we show, they struggle in several settings: low- resource languages, where labeled training data is scarce; morphologically rich languages, where the number of morphemes is large, or where the mapping from form to meaning is complex; and in Chinese, where the number of characters is or- ders of magnitude larger than in non-logographic scripts. Furthermore, supervised subword models can be combined with MIMICK, offering additive improvements.</p><p>Morphosyntactic attribute tagging. We evalu- ate our method on the task of tagging word to- kens for their morphosyntactic attributes, such as gender, number, case, and tense. The task of morpho-syntactic tagging dates back at least to the mid 1990s <ref type="bibr" target="#b18">(Oflazer and Kuruöz, 1994;</ref><ref type="bibr" target="#b10">Hajič and Hladká, 1998)</ref>, and interest has been rejuvenated by the availability of large-scale multilingual mor- phosyntactic annotations through the Universal Dependencies (UD) corpus <ref type="bibr" target="#b5">(De Marneffe et al., 2014</ref>). For example, <ref type="bibr" target="#b8">Faruqui et al. (2016)</ref> propose a graph-based technique for propagating type- level morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger. In contrast, we apply a neural sequence labeling approach, in- spired by the POS tagger of <ref type="bibr" target="#b20">Plank et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MIMICK Word Embeddings</head><p>We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: re- gardless of how the original embeddings were cre- ated, we assume there is a generative wordform- based protocol for creating these embeddings. By training a model over the existing vocabulary, we can later use that model for predicting the embed- ding of an unseen word.</p><p>Formally: given a language L, a vocabulary V ⊆ L of size V , and a pre-trained embeddings table W ∈ R V ×d where each word {w k } V k=1 is assigned a vector e k of dimension d, our model is trained to find the function f : L → R d such that the projected function f | V approximates the assignments f (w k ) ≈ e k . Given such a model, a new word w k * ∈ L \ V can now be assigned an embedding e k * = f (w k * ).</p><p>Our predictive function of choice is a Word Type Character Bi-LSTM. Given a word with character sequence w = {c i } n 1 , a forward-LSTM and a backward-LSTM are run over the corre- sponding character embeddings sequence {e (c) i } n 1 . Let h n f represent the final hidden vector for the forward-LSTM, and let h 0 b represent the final hid- den vector for the backward-LSTM. The word em- bedding is computed by a multilayer perceptron:</p><formula xml:id="formula_0">(1) f (w) = O T · g(T h · [h n f ; h 0 b ] + b h ) + b T ,</formula><p>where T h , b h and O T , b T are parameters of affine transformations, and g is a nonlinear elementwise function. The model is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. The training objective is similar to that of <ref type="bibr" target="#b25">Yin and Schütze (2016)</ref>. We match the predicted em- beddings f (w k ) to the pre-trained word embed- dings e w k , by minimizing the squared Euclidean distance,</p><formula xml:id="formula_1">(2) L = f (w k ) − e w k 2 2 .</formula><p>By backpropagating from this loss, it is possible to obtain local gradients with respect to the pa- rameters of the LSTMs, the character embeddings, and the output model. The ultimate output of the training phase is the character embeddings ma- trix C and the parameters of the neural network:</p><formula xml:id="formula_2">M = {C, F, B, T h , b h , O T , b T },</formula><p>where F, B are the forward and backward LSTM component pa- rameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MIMICK Polyglot Embeddings</head><p>The pretrained embeddings we use in our ex- periments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding ef- fort. Available for dozens of languages, each dataset contains 64-dimension embeddings for the 100,000 most frequent words in a language's train- ing corpus (of variable size), as well as an UNK embedding to be used for OOV words. Even with this vocabulary size, querying words from respec- tive UD corpora (train + dev + test) yields high OOV rates: in at least half of the 23 languages in our experiments (see Section 5), 29.1% or more of the word types do not appear in the Polyglot vo- cabulary. The token-level median rate is 9.2%. 1 Applying our MIMICK algorithm to Polyglot embeddings, we obtain a prediction model for each of the 23 languages. Based on preliminary testing on randomly selected held-out develop- ment sets of 1% from each Polyglot vocabulary (with error calculated as in Equation 2), we set the following hyper-parameters for the remainder of the experiments: character embedding dimen- sion = 20; one LSTM layer with 50 hidden units; 60 training epochs with no dropout; nonlinearity function g = tanh. <ref type="bibr">2</ref> We initialize character em- beddings randomly, and use DyNet to implement the model ( <ref type="bibr" target="#b17">Neubig et al., 2017</ref>).</p><p>Nearest-neighbor examination. As a prelimi- nary sanity check for the validity of our pro- tocol, we examined nearest-neighbor samples in languages for which speakers were available: English, Hebrew, Tamil, and Spanish. <ref type="table">Ta</ref> ; (b) the model shows robustness to typos (e.g., developiong, corssing); (c) part-of-speech is learned across multiple suffixes (pesky -euphoric, ghastly); (d) word compounding is detected (e.g., lawnmower -bookmaker, postman); (e) semantics are not learned well (as is to be expected from the lack of context in training), but there are surprises (e.g., flatfish -slimy, watery). <ref type="table" target="#tab_1">Table 2</ref> presents examples from Hebrew that show learned proper- ties can be extended to nominal morphosyntactic attributes (gender, number -first two examples) and even relational syntactic subword forms such as genetive markers (third example). Names are learned (fourth example) despite the lack of cas- ing in the script. Spanish examples exhibit word- shape and part-of-speech learning patterns with some loose semantics: for example, the plural ad- jective form prenatales is similar to other family- related plural adjectives such as patrimoniales and generacionales. Tamil displays some semantic similarities as well: e.g. enjineer ('engineer') pre- dicts similarity to other professional terms such as kalviyiyal ('education'), thozhilnutpa ('techni- cal'), and iraanuva ('military').</p><p>Stanford RareWords. The Stanford RareWord evaluation corpus ( <ref type="bibr" target="#b15">Luong et al., 2013</ref>) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes. As these words are unlikely to be above the cutoff threshold for standard word embedding models, they em- phasize the performance on OOV words. For evaluation of our MIMICK model on the RareWord corpus, we trained the Varia- tional Embeddings algorithm (VarEmbed; Bha- tia et al., 2016) on a 20-million-token, 100,000- type Wikipedia corpus, obtaining 128-dimension word embeddings for all words in the test cor- pus. VarEmbed estimates a prior distribution over word embeddings, conditional on the morpholog- ical composition. For in-vocabulary words, a pos- terior is estimated from unlabeled data; for out- of-vocabulary words, the expected embedding can be obtained from the prior alone. In addition, we compare to <ref type="bibr">FastText (Bojanowski et al., 2016</ref>), a high-vocabulary, high-dimensionality embedding benchmark.</p><p>The results, shown in <ref type="table">Table 3</ref>, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the "All pairs" condition. MIMICK also outper- forms VarEmbed. FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes</head><p>The Universal Dependencies (UD) scheme <ref type="bibr" target="#b5">(De Marneffe et al., 2014</ref>) features a minimal set of 17 POS tags ( <ref type="bibr" target="#b19">Petrov et al., 2012)</ref> and supports tagging further language-specific features using attribute-specific inventories. For example, a verb in Turkish could be assigned a value for the evidentiality attribute, one which is absent from Danish. These additional morphosyn- tactic attributes are marked in the UD dataset as optional per-token attribute-value pairs.</p><p>Our approach for tagging morphosyntactic at- tributes is similar to the part-of-speech tagging model of <ref type="bibr" target="#b14">Ling et al. (2015)</ref>, who attach a projec- tion layer to the output of a sentence-level bidi- rectional LSTM. We extend this approach to mor- phosyntactic tagging by duplicating this projection layer for each attribute type. The input to our mul- tilayer perceptron (MLP) projection network is the hidden state produced for each token in the sen- tence by an underlying LSTM, and the output is Nearest neighbors TTGFM '(s/y) will come true', TPTVR '(s/y) will solve', TBTL '(s/y) will cancel', TSIR '(s/y) will remove' GIAVMTRIIM 'geometric(m-pl)'2 ANTVMIIM 'anatomic(m-pl)', GAVMTRIIM 'geometric(m-pl)'1 BQFTNV 'our request' IVFBIHM 'their(m) residents', XTAIHM 'their(m) sins', IRVFTV 'his inheritance' RIC'RDSVN 'Richardson' AVISTRK 'Eustrach', QMINQA 'Kaminka', GVLDNBRG 'Goldenberg'  <ref type="table">Table 3</ref>: Similarity results on the RareWord set, measured as Spearman's ρ × 100. VarEmbed was trained on a 20-million token dataset, Polyglot on a 1.7B-token dataset.</p><p>attribute-specific probability distributions over the possible values for each attribute on each token in the sequence. Formally, for a given attribute a with possible values v ∈ V a , the tagging prob- ability for the i'th word in a sentence is given by:</p><formula xml:id="formula_3">Pr(a w i = v) = (Softmax(φ(h i ))) v ,<label>(3)</label></formula><formula xml:id="formula_4">with (4) φ(h i ) = O a W · tanh(W a h · h i + b a h ) + b a W ,</formula><p>where h i is the i'th hidden state in the underlying LSTM, and φ(h i ) is a two-layer feedforward neu- ral network, with weights W a h and O a W . We apply a softmax transformation to the output; the value at position v is then equal to the probability of at- tribute v applying to token w i . The input to the underlying LSTM is a sequence of word embed- dings, which are initialized to the Polyglot vectors when possible, and to MIMICK vectors when nec- essary. Alternative initializations are considered in the evaluation, as described in Section 5.2.</p><p>Each tagged attribute sequence (including POS tags) produces a loss equal to the sum of nega- tive log probabilities of the true tags. One way to combine these losses is to simply compute the sum loss. However, many languages have large differences in sparsity across morpho-syntactic at- tributes, as apparent from <ref type="table">Table 4</ref> (rightmost col- umn). We therefore also compute a weighted sum loss, in which each attribute is weighted by the proportion of training corpus tokens on which it is assigned a non-NONE value. Preliminary experi- ments on development set data were inconclusive across languages and training set sizes, and so we kept the simpler sum loss objective for the remain- der of our study. In all cases, part-of-speech tag- ging was less accurate when learned jointly with morphosyntactic attributes. This may be because the attribute loss acts as POS-unrelated "noise" af- fecting the common LSTM layer and the word em- beddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Settings</head><p>The morphological complexity and composition- ality of words varies greatly across languages. While a morphologically-rich agglutinative lan- guage such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytic language such as Viet- namese may have not a single polymorphemic or inflected word in it. To see whether this property is influential on our MIMICK model and its perfor- mance in the downstream tagging task, we select languages that comprise a sample of multiple mor- phological patterns. Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well. We also considered language selection recommendations from de <ref type="bibr" target="#b13">Lhoneux and Nivre (2016)</ref> and <ref type="bibr" target="#b22">Schluter and Agi´cAgi´c (2017)</ref>.</p><p>As stated above, our approach is built on the Polyglot word embeddings. The intersection of the Polyglot embeddings and the UD dataset (ver- sion 1.4) yields 44 languages. Of these, many are under-annotated for morphosyntactic attributes; we select twenty-three sufficiently-tagged lan- guages, with the exception of Indonesian. <ref type="bibr">3</ref>  <ref type="table">Table 4</ref> presents the selected languages and their typolog- ical properties. As an additional proxy for mor-  <ref type="table">Table 4</ref>: Languages used in tagging evaluation. Languages on the right are Indo-European. *In Viet- namese script, whitespace separates syllables rather than words. phological expressiveness, the rightmost column shows the proportion of UD tokens which are an- notated with any morphosyntactic attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metrics</head><p>As noted above, we use the UD datasets for testing our MIMICK algorithm on 23 languages 4 with the supplied train/dev/test division. We measure part- of-speech tagging by overall token-level accuracy.</p><p>For morphosyntactic attributes, there does not seem to be an agreed-upon metric for reporting performance. <ref type="bibr" target="#b6">Dzeroski et al. (2000)</ref> report per- tag accuracies on a morphosyntactically tagged corpus of Slovene. <ref type="bibr" target="#b8">Faruqui et al. (2016)</ref> report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-of- speech, case, gender, tense); recall and precision were calculated for the full set of each attribute's values, pooled together. <ref type="bibr">5 Agi´cAgi´c et al. (2013)</ref> report separately on parts-of-speech and morphosyntac- tic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. <ref type="bibr" target="#b9">Georgiev et al. (2012)</ref> report token-level ac- curacy for exact all-attribute tags (e.g. 'Ncmsh' for "Noun short masculine singular definite") in Bulgarian, reaching a tagset of size 680. precision are calculated over the entire set, with F1 defined as their harmonic mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models</head><p>We implement and test the following models:</p><p>No-Char. Word embeddings are initialized from Polyglot models, with unseen words assigned the Polyglot-supplied UNK vector. Following tuning experiments on all languages with cased script, we found it beneficial to first back off to the lower- cased form for an OOV word if its embedding ex- ists, and only otherwise assign UNK.</p><p>MIMICK. Word embeddings are initialized from Polyglot, with OOV embeddings inferred from a MIMICK model (Section 3) trained on the Poly- glot embeddings. Unlike the No-Char case, back- ing off to lowercased embeddings before using the MIMICK output did not yield conclusive benefits and thus we report results for the more straightfor- ward no-backoff implementation.</p><p>CHAR→TAG. Word embeddings are initialized from Polyglot as in the No-Char model (with low- ercase backoff), and appended with the output of a character-level LSTM updated during training <ref type="bibr" target="#b20">(Plank et al., 2016</ref>). This additional module causes a threefold increase in training time.</p><p>Both. Word embeddings are initialized as in MIMICK, and appended with the CHAR→TAG LSTM.</p><p>Other models. Several non-Polyglot embed- ding models were examined, all performed sub- stantially worse than Polyglot. Two of these are notable: a random-initialization baseline, and a model initialized from FastText em- beddings (tested on English). FastText sup- plies 300-dimension embeddings for 2.51 million lowercase-only forms, and no UNK vector. <ref type="bibr">6</ref> Both of these embedding models were attempted with and without CHAR→TAG concatenation. Another model, initialized from only MIMICK output em- beddings, performed well only on the language with smallest Polyglot training corpus (Latvian). A Polyglot model where OOVs were initialized using an averaged embedding of all Polyglot vec- tors, rather than the supplied UNK vector, per- formed worse than our No-Char baseline on a great majority of the languages.</p><p>Last, we do not employ type-based tagset re- strictions. All tag inventories are computed from the training sets and each tag selection is per- formed over the full set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameters</head><p>Based on development set experiments, we set the following hyperparameters for all models on all languages: two LSTM layers of hidden size 128, MLP hidden layers of size equal to the num- ber of each attribute's possible values; momen- tum stochastic gradient descent with 0.01 learning rate; 40 training epochs (80 for 5K settings) with a dropout rate of 0.5. The CHAR→TAG models use 20-dimension character embeddings and a single hidden layer of size 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We report performance in both low-resource and full-resource settings. Low-resource training sets were obtained by randomly sampling training sen- tences, without replacement, until a predefined to- ken limit was reached. We report the results on the full sets and on N = 5000 tokens in <ref type="table" target="#tab_4">Table 5</ref> (part- of-speech tagging accuracy) and <ref type="table" target="#tab_5">Table 6</ref>  MIMICK as OOV initialization. In nearly all experimental settings on both tasks, across lan- guages and training corpus sizes, the MIMICK embeddings significantly improve over the Poly- glot UNK embedding for OOV tokens on both POS and morphosyntactic tagging. For POS, the largest margins are in the Slavic languages (Rus- sian, Czech, Bulgarian), where word order is rel- atively free and thus rich word representations are imperative. Chinese also exhibits impressive im- provement across all settings, perhaps due to the large character inventory (&gt; 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word-and character- level sparsity in the UD corpus. <ref type="bibr">7</ref> In morphosyn- tactic tagging, gains are apparent for Slavic lan- guages and Chinese, but also for agglutinative lan- guages -especially Tamil and Turkish -where the stable morpheme representation makes it easy for subword modeling to provide a type-level sig- nal. <ref type="bibr">8</ref> To examine the effects on Slavic and agglu- tinative languages in a more fine-grained view, we present results of multiple training-set size exper- iments for each model, averaged over five repeti- tions (with different corpus samples), in <ref type="figure">Figure 2</ref>.</p><p>MIMICK vs.</p><p>CHAR→TAG. In several lan- guages, the MIMICK algorithm fares better than the CHAR→TAG model on part-of-speech tagging in low-resource settings. <ref type="table">Table 7</ref> presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data. We obtain statistically signifi- cant improvements in most languages, even when CHAR→TAG is included. These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns. While test set OOVs are a strength of the CHAR→TAG model ( <ref type="bibr" target="#b20">Plank et al., 2016)</ref>, in many languages there are still considerable im- provements to be obtained from the application of MIMICK initialization. This suggests that with limited training data, the end-to-end CHAR→TAG model is unable to learn a sufficiently accurate rep- resentational mapping from orthography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a straightforward algorithm to infer OOV word embedding vectors from pre-trained,      <ref type="table">Table 7</ref>: Absolute gain in POS tagging accuracy from using MIMICK for 10,000-token datasets (all tokens for Tamil and Kazakh). Bold denotes sta- tistical significance (McNemar's test,p &lt; 0.01).</p><p>limited-vocabulary models, without need to ac- cess the originating corpus. This method is par- ticularly useful for low-resource languages and tasks with little labeled data available, and in fact is task-agnostic. Our method improves per- formance over word-based models on annotated sequence-tagging tasks for a large variety of lan- guages across dimensions of family, orthography, and morphology. In addition, we present a Bi- LSTM approach for tagging morphosyntactic at- tributes at the token level. In this paper, the MIM- ICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic char- acters ( <ref type="bibr" target="#b4">Costa-jussà et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>We thank Umashanthi Pavalanathan, Sandeep Soni, Roi Reichart, and our anonymous reviewers for their valuable input. We thank Manaal Faruqui and Ryan McDonald for their help in understand- ing the metrics for morphosyntactic tagging. The project was supported by project HDTRA1-15-1- 0019 from the Defense Threat Reduction Agency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MIMICK model architecture.</figDesc><graphic url="image-1.png" coords="3,322.92,66.19,184.26,188.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Müller et al. (2013) do the same for six other languages. We report micro F1: each token's value for each attribute is compared separately with the gold la- beling, where a correct prediction is a matching non-NONE attribute/value assignment. Recall and 4 When several datasets are available for a language, we use the unmarked corpus. 5 Details were clarified in personal communication with the authors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(mor- phosyntactic attribute tagging micro-F1). Results for additional training set sizes are shown in Fig- ure 2; space constraints prevent us from showing figures for all languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Ntrain = 5000</head><label>5000</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Full</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Nearest-neighbor examples for Hebrew (Transcriptions per</head><label>2</label><figDesc></figDesc><table>Sima'an et al. (2001)). 's/y' stands 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>POS tagging accuracy (UD 1.4 Test). Bold (Italic) indicates significant improvement (degrada-
tion) by McNemar's test, p &lt; .01, comparing MIMICK to "No-Char", and "Both" to CHAR→TAG. 
* For reference, we copy the reported results of Plank et al. (2016)'s analog to CHAR→TAG. Note that 
these were obtained on UD 1.2, and without jointly tagging morphosyntactic attributes. 

Ntrain = 5000 
Full data 

No-Char MIMICK CHAR 
Both 
No-Char MIMICK CHAR 
Both 
→TAG 
→TAG 

kk -
-
-
-
21.48 
20.07 
28.47 
20.98 
ta 
80.68 
81.96 
84.26 
85.63 79.90 
81.93 
84.55 
85.01 
lv 
56.98 
59.86 
64.81 
65.82 66.16 
66.61 
76.11 
75.44 
hu 73.13 
76.30 
73.62 
76.85 80.04 
80.64 
86.43 
84.12 
tr 
69.58 
75.21 
75.81 
78.93 78.31 
83.32 
91.51 
90.86 
el 
86.87 
86.07 
86.40 
87.50 94.64 
94.96 
96.55 
96.76 
bg 78.26 
81.77 
82.74 
84.93 91.98 
93.48 
96.12 
95.96 
sv 82.09 
84.12 
85.26 
88.16 92.45 
94.20 
96.37 
96.57 
eu 65.29 
66.00 
70.67 
70.27 82.75 
84.74 
90.58 
91.39 
ru 77.31 
81.84 
79.83 
83.53 88.80 
91.24 
93.54 
93.56 
da 80.26 
82.74 
83.59 
82.65 92.06 
94.14 
96.05 
95.96 
zh 63.29 
71.44 
63.50 
74.66 84.95 
85.70 
84.86 
85.87 
fa 
84.73 
86.07 
85.94 
81.75 95.30 
95.55 
96.90 
96.80 
he 75.35 
68.57 
81.06 
75.24 90.25 
90.99 
93.35 
93.63 
ro 84.20 
85.64 
85.61 
87.31 94.97 
96.10 
97.18 
97.14 
en 86.71 
87.99 
88.50 
89.61 95.30 
95.59 
96.40 
96.30 
ar 
84.14 
84.17 
81.41 
81.11 94.43 
94.85 
95.50 
95.37 
hi 
83.45 
86.89 
85.64 
85.27 96.15 
96.21 
96.59 
96.67 
it 
89.96 
92.07 
91.27 
92.62 97.32 
97.80 
98.18 
98.31 
es 88.11 
89.81 
88.58 
89.63 94.84 
95.44 
96.21 
96.84 
cs 68.66 
72.65 
71.02 
73.61 91.75 
93.71 
95.29 
95.31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Micro-F1 for morphosyntactic attributes (UD 1.4 Test). Bold (Italic) type indicates significant 
improvement (degradation) by a bootstrapped Z-test, p &lt; .01, comparing models as in Table 5. Note 
that the Kazakh (kk) test set has only 78 morphologically tagged tokens. </table></figure>

			<note place="foot" n="1"> Some OOV counts, and resulting model performance, may be adversely affected by tokenization differences between Polyglot and UD. Notably, some languages such as Spanish, Hebrew and Italian exhibit relational synthesis wherein words of separate grammatical phrases are joined into one form (e.g. Spanish del = de + el, &apos;from the-masc.sg.&apos;). For these languages, the UD annotations adhere to the sub-token level, while Polyglot does not perform subtokenization. As this is a real-world difficulty facing users of out-of-the-box embeddings, we do not patch it over in our implementations or evaluation. 2 Other settings, described below, were tuned on the supervised downstream tasks.</note>

			<note place="foot" n="3"> Vietnamese has no attributes by design; it is a pure analytic language.</note>

			<note place="foot" n="6"> Vocabulary type-level coverage for the English UD corpus: 55.6% case-sensitive, 87.9% case-insensitive.</note>

			<note place="foot" n="7"> Character coverage in Chinese Polyglot is surprisingly good: only eight characters from the UD dataset are unseen in Polyglot, across more than 10,000 unseen word types. 8 Persian is officially classified as agglutinative but it is mostly so with respect to derivations. Its word-level inflections are rare and usually fusional.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Natural Language Learning (CoNLL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Morphological priors for probabilistic neural word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chinese-spanish neural machine translation enhanced with character and word bitmap fonts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marta R Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aldón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the Language Resources and Evaluation Conference (LREC)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4585" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Morphosyntactic tagging of slovene: Evaluating taggers and tagsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saso</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaz</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Zavrel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the Language Resources and Evaluation Conference (LREC)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morpho-syntactic lexicon generation using graph-based semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging for morphologically complex languages: Application to Bulgarian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Zhikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="492" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbora</forename><surname>Hladká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="483" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ud treebank sampling for comparative parser evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sixth Swedish Language Technology Conference (SLTC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tagging and morphological disambiguation of turkish text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuruöz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth conference on Applied natural language processing</title>
		<meeting>the fourth conference on Applied natural language processing</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="144" to="149" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the Language Resources and Evaluation Conference (LREC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirically sampling universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agi´cagi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NoDaLiDa Workshop on Universal Dependencies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a tree-bank of modern hebrew text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Itai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoad</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Traitement Automatique des Langues</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="380" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Alon Altman, and Noa Nativ</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02789</idno>
		<title level="m">Charagram: Embedding words and sentences via character n-grams</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word meta-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
