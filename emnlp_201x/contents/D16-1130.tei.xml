<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1224" to="1233"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse relations (e.g., contrast and causality) support a set of sentences to form a coherent text. Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., "because", "but") explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connec- tives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations <ref type="bibr" target="#b20">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b1">Baldridge and Lascarides, 2005;</ref><ref type="bibr" target="#b21">Subba and Di Eugenio, 2009;</ref><ref type="bibr" target="#b5">Hernault et al., 2010;</ref><ref type="bibr" target="#b16">Pitler et al., 2009;</ref><ref type="bibr" target="#b8">Joty et al., 2012</ref>). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific learning aims, and has the potential to improve readers' reading fluency and comprehension of the text (National Institute of Child <ref type="bibr">Health and Human Development, 2000;</ref><ref type="bibr">LaBerge and Samuels, 1974)</ref>. Therefore, for the task of discourse parsing, repeated reading is necessary, as it is difficult to generalize which words are really useful on the first try and efficient features should be dynamically exploited through several passes of reading . Now, let us check one real example to elaborate the necessity of using repeated reading in discourse parsing. Arg-1 : the use of 900 toll numbers has been expanding rapidly in recent years</p><p>Arg-2 : for a while, high-cost pornography lines and services that tempt children to dial (and redial) movie or music information earned the service a somewhat sleazy image (Comparison -wsj 2100)</p><p>To identify the "Comparison" relation between the two arguments Arg-1 and Arg-2, the most crucial clues mainly lie in some content, like "expanding rapidly" in Arg-1 and "earned the service a somewhat sleazy image" in Arg-2, since there exists a contrast between the semantic meanings of these two text spans. However, it is difficult to obtain sufficient information for pinpointing these words through scanning the argument pair left to right in one pass. In such case, we follow the repeated reading strategy, where we obtain the general meaning through reading the arguments for the first time, re-read them later and gradually pay close attention to the key content. Recently, some approaches simulating repeated reading have witnessed their success in different tasks. These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine trans- lation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) and the external memory which can be read and written when processing the objects ( <ref type="bibr" target="#b22">Sukhbaatar et al., 2015)</ref>. For example, <ref type="bibr" target="#b10">Kumar et al. (2015)</ref> drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer. In computation vision, <ref type="bibr" target="#b27">Yang et al. (2015)</ref> pointed out that repeatedly giving attention to different regions of an image could gradually lead to more precise image representations.</p><p>Inspired by these recent work, for discourse parsing, we propose a model that aims to repeatedly read an argument pair and gradually focus on more fine-grained parts after grasping the global information. Specifically, we design the Neural Networks with Multi-Level Attention (NNMA) consisting of one general level and several attention levels.</p><p>In the general level, we capture the general representations of each argument based on two bidirectional long short-term memory (LSTM) models. For each attention level, NNMA generates a weight vector over the argument pair to locate the important parts related to the discourse relation. And an external short-term memory is designed to store the information exploited in previous levels and help update the argument representations. We stack this structure in a recurrent manner, mimicking the process of reading the arguments multiple times. Finally, we use the representation output from the highest attention level to identify the discourse relation. Experiments on the PDTB dataset show that our proposed model achieves the state-of-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Repeated Reading Neural Network with Multi-Level Attention</head><p>In this section, we describe how we use the neural networks with multi-level attention to repeatedly read the argument pairs and recognize implicit discourse relations. First, we get the general understanding of the arguments through skimming them. To implement this, we adopt the bidirectional Long-Short Term Memory Neural Network (bi-LSTM) to model each argument, as bi-LSTM is good at modeling over a sequence of words and can represent each word with consideration of more contextual information. Then, several attention levels are designed to simulate the subsequent multiple passes of reading. On each attention level, an external short-term memory is used to store what has been learned from previous passes and guide which words should be focused on. To pinpoint the useful parts of the arguments, the attention mechanism is used to predict a probability distribution over each word, indicating to what degree each word should be concerned. The overall architecture of our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For clarity, we only illustrate two attention levels in the figure. It is noted that we can easily extend our model to more attention levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Representing Arguments with LSTM</head><p>The Long-Short Term Memory (LSTM) Neural Network is a variant of the Recurrent Neural Network which is usually used for modeling a sequence. In our model, we adopt two LSTM neural networks to respectively model the two arguments: the left argument Arg-1 and the right argument Arg- 2.</p><p>First of all, we associate each word w in our vocabulary with a vector representation x w ∈ R De . Here we adopt the pre-trained vectors provided by <ref type="bibr">GloVe (Pennington et al., 2014)</ref>. Since an argument can be viewed as a sequence of word vectors, let x 1 i (x 2 i ) be the i-th word vector in argument Arg-1 (Arg-  2) and the two arguments can be represented as,</p><formula xml:id="formula_0">R 1 0 R 1 1 L h  1 1 h  1 i h  1 0 h  1 1 h  1 i h  1 1 L h  2 0 h  2 i h  2 1 h  2 2 L h  2 2 L h  2 i h  2 1 h  2 0 h  Discourse Relation</formula><formula xml:id="formula_1">Arg-1 : [x 1 1 , x 1 2 , · · · , x 1 L 1 ] Arg-2 : [x 2 1 , x 2 2 , · · · , x 2 L 2 ]</formula><p>where Arg-1 has L 1 words and Arg-2 has L 2 words. To model the two arguments, we briefly introduce the working process how the LSTM neural networks model a sequence of words. For the i-th time step, the model reads the i-th word x i as the input and updates the output vector h i as follows <ref type="bibr" target="#b28">(Zaremba and Sutskever, 2014</ref>).</p><formula xml:id="formula_2">i i = sigmoid(W i [x i , h i−1 ] + b i )<label>(1)</label></formula><formula xml:id="formula_3">f i = sigmoid(W f [x i , h i−1 ] + b f ) (2) o i = sigmoid(W o [x i , h i−1 ] + b o )<label>(3)</label></formula><formula xml:id="formula_4">˜ c i = tanh(W c [x i , h i−1 ] + b c ) (4) c i = i i * ˜ c i + f i * c i−1 (5) h i = o i * tanh(c i )<label>(6)</label></formula><p>where [ ] means the concatenation operation of several vectors. i, f , o and c denote the input gate, forget gate, output gate and memory cell respectively in the LSTM architecture. The input gate i determines how much the input x i updates the memory cell. The output gate o controls how much the memory cell influences the output. The forget gate f controls how the past memory c i−1 affects the current state.</p><formula xml:id="formula_5">W i , W f , W o , W c , b i , b f , b o , b c are the network parameters.</formula><p>Referring to the work of <ref type="bibr" target="#b24">Wang and Nyberg (2015)</ref>, we implement the bidirectional version of LSTM neural network to model the argument sequence. Besides processing the sequence in the forward direction, the bidirectional LSTM (bi- LSTM) neural network also processes it in the reverse direction. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, using two bi-LSTM neural networks, we can obtain</p><formula xml:id="formula_6">h 1 i = [ h 1 i , h 1 i ]</formula><p>for the i-th word in Arg-1 and</p><formula xml:id="formula_7">h 2 i = [ h 2 i , h 2 i ] for the i-th word in Arg-2, where h 1 i , h 2 i ∈ R d and h 1 i , h 2 i ∈ R d are the output vectors from two directions.</formula><p>Next, to get the general-level representations of the arguments, we apply a mean pooling operation over the bi-LSTM outputs, and obtain two vectors R 1 0 and R 2 0 , which can reflect the global information of the argument pair.</p><formula xml:id="formula_8">R 1 0 = 1 L 1 L 1 i=0 h 1 i (7) R 2 0 = 1 L 2 L 2 i=0 h 2 i (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tuning Attention via Repeated Reading</head><p>After obtaining the general-level representations by treating each word equally, we simulate the repeated reading and design multiple attention levels to gradually pinpoint those words particularly useful for discourse relation recognition. In each attention level, we adopt the attention mechanism to determine which words should be focused on.</p><p>An external short-term memory is designed to remember what has seen in the prior levels and guide the attention tuning process in current level. Specifically, in the first attention level, we concatenate R 1 0 , R 2 0 and R 1 0 −R 2 0 and apply a non-linear transformation over the concatenation to catch the general understanding of the argument pair. The use of R 1 0 −R 2 0 takes a cue from the difference between two vector representations which has been found explainable and meaningful in many applications ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>). Then, we get the memory vector M 1 ∈ R dm of the first attention level as</p><formula xml:id="formula_9">M 1 = tanh(W m,1 [R 1 0 , R 2 0 , R 1 0 −R 2 0 ])<label>(9)</label></formula><p>where W m,1 ∈ R dm×6d is the weight matrix. With M 1 recording the general meaning of the argument pair, our model re-calculates the importance of each word. We assign each word a weight measuring to what degree our model should pay attention to it. The weights are so-called "attention" in our paper. This process is designed to simulate the process that we re-read the arguments and pay more attention to some specific words with an overall understanding derived from the first-pass reading. Formally, for Arg-1, we use the memory vector M 1 to update the representation of each word with a non-linear transformation. According to the updated word representations o 1 1 , we get the attention vector a 1</p><p>1 .</p><formula xml:id="formula_10">h 1 = [h 1 0 , h 1 1 , · · · , h 1 L 1 ]<label>(10)</label></formula><formula xml:id="formula_11">o 1 1 = tanh(W 1 a,1 h 1 + W 1 b,1 (M 1 ⊗ e))<label>(11)</label></formula><formula xml:id="formula_12">a 1 1 = sof tmax(W 1 s,1 o 1 1 )<label>(12)</label></formula><p>where h 1 ∈ R 2d×L 1 is the concatenation of all LSTM output vectors of Arg-1. e ∈ R L 1 is a vector of 1s and the M 1 ⊗ e operation denotes that we repeat the vector M 1 L 1 times and generate a</p><formula xml:id="formula_13">d m × L 1 matrix. The attention vector a 1 1 ∈ R L 1 is obtained through applying a sof tmax operation over o 1 1 . W a,1 1 ∈ R 2d×2d , W b,1 1 ∈ R 2d×dm and W s,1</formula><p>1 ∈ R 1×2d are the transformation weights. It is noted that the subscripts denote the current attention level and the superscripts denote the corresponding argument. In the same way, we can get the attention vector a 2 1 for Arg-2. Then, according to a 1 1 and a 2 1 , our model re-reads the arguments and get the new representations R 1 1 and R 2 1 for the first attention level.</p><formula xml:id="formula_14">R 1 1 = h 1 (a 1 1 ) T<label>(13)</label></formula><formula xml:id="formula_15">R 2 1 = h 2 (a 2 1 ) T<label>(14)</label></formula><p>Next, we iterate the "memory-attention- representation" process and design more attention levels, giving NNMA the ability to gradually infer more precise attention vectors. The processing of the second or above attention levels is slightly different from that of the first level, as we update the memory vector in a recurrent way. To formalize, for the k-th attention level (k ≥ 2), we use the following formulae for Arg-1.</p><formula xml:id="formula_16">M k = tanh(W m,k [R 1 k−1 , R 2 k−1 , R 1 k−1 −R 2 k−1 , M k−1 ])<label>(15)</label></formula><formula xml:id="formula_17">o 1 k = tanh(W 1 a,k h 1 + W 1 b,k (M k ⊗ e))<label>(16)</label></formula><formula xml:id="formula_18">a 1 k = sof tmax(W 1 s,k o 1 k )<label>(17)</label></formula><formula xml:id="formula_19">R 1 k = h 1 (a 1 k ) T<label>(18)</label></formula><p>In the same way, we can computer o 2 k , a 2 k and R 2 k for Arg-2. Finally, we use the newest representation derived from the top attention level to recognize the discourse relations. Suppose there are totally K attention levels and n relation types, the predicted discourse relation distribution P ∈ R n is calculated as</p><formula xml:id="formula_20">P = sof tmax(W p [R 1 K , R 2 K , R 1 K −R 2 K ] + b p )<label>(19)</label></formula><p>where W p ∈ R n×6d and b p ∈ R n are the transformation weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training</head><p>To train our model, the training objective is defined as the cross-entropy loss between the outputs of the softmax layer and the ground-truth class labels. We use stochastic gradient descent (SGD) with momentum to train the neural networks.</p><p>To avoid over-fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λ e to train the neural network parameters Θ and the word embeddings Θ e referring to ( <ref type="bibr" target="#b6">Ji and Eisenstein, 2015)</ref>. λ e is set to a small value for preventing over- fitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preparation</head><p>We evaluate our model on the Penn Discourse Treebank (PDTB) ( <ref type="bibr" target="#b17">Prasad et al., 2008)</ref>. In our work, we experiment on the four top-level classes in this corpus as in previous work <ref type="bibr" target="#b19">(Rutherford and Xue, 2015)</ref>. We extract all the implicit relations of PDTB, and follow the setup of <ref type="bibr" target="#b19">(Rutherford and Xue, 2015)</ref>. We split the data into a training set ( <ref type="bibr">Sections 220)</ref>, development set (Sections 0-1), and test set (Section <ref type="bibr">[21]</ref><ref type="bibr">[22]</ref>. <ref type="table">Table 1</ref> summarizes the statistics of the four PDTB discourse relations, i.e., Comparison, Contingency, Expansion and Temporal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Train We first convert the tokens in PDTB to lowercase. The word embeddings used for initializing the word representations are provided by <ref type="bibr">GloVe (Pennington et al., 2014)</ref>, and the dimension of the embeddings D e is 50. The hyper-parameters, including the momentum δ, the two learning rates λ and λ e , the dropout rate q, the dimension of LSTM output vector d, the dimension of memory vector d m are all set according to the performance on the development set Due to space limitation, we do not present the details of tuning the hyper-parameters and only give their final settings as shown in <ref type="table" target="#tab_2">Table 2</ref>.  To evaluate our model, we adopt two kinds of experiment settings. The first one is the four- way classification task, and the second one is the binary classification task, where we build a one- vs-other classifier for each class. For the second setting, to solve the problem of unbalanced classes in the training data, we follow the reweighting method of <ref type="bibr" target="#b19">(Rutherford and Xue, 2015)</ref> to reweigh the training instances according to the size of each relation class. We also use visualization methods to analyze how multi-level attention helps our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>First, we design experiments to evaluate the effec- tiveness of attention levels and how many attention levels are appropriate. To this end, we implement a baseline model (LSTM with no attention) which directly applies the mean pooling operation over LSTM output vectors of two arguments without any attention mechanism.</p><p>Then we consider different attention levels including one-level, two- level and three-level. The detailed results are shown in <ref type="table" target="#tab_4">Table 3</ref>. For four-way classification, macro- averaged F 1 and Accuracy are used as evaluation metrics. For binary classification, F 1 is adopted to evaluate the performance on each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Four-  From <ref type="table" target="#tab_4">Table 3</ref>, we can see that the basic LSTM model performs the worst. With attention levels added, our NNMA model performs much better. This confirms the observation above that one-pass reading is not enough for identifying the discourse relations. With respect to the four-way F 1 measure, using NNMA with one-level attention produces a 4% improvement over the baseline system with no attention. Adding the second attention level gives another 2.8% improvement. We perform significance test for these two improvements, and they are both significant under one-tailed t-test (p &lt; 0.05). However, when adding the third attention level, the performance does not promote much and almost reaches its plateau. We can see that three- level NNMA experiences a decease in F 1 and a slight increase in Accuracy compared to two-level NNMA. The results imply that with more attention levels considered, our model may perform slightly better, but it may incur the over-fitting problem due to adding more parameters. With respect to the binary classification F <ref type="bibr">1</ref>   <ref type="table">Table 4</ref>: Comparison with the State-of-the-art Approaches.</p><p>that the "Comparison" relation needs more passes of reading compared to the other three relations. The reason may be that the identification of the "Comparison" depends more on some deep analysis such as semantic parsing, according to ( <ref type="bibr" target="#b30">Zhou et al., 2010)</ref>. Next, we compare our models with six state-of- the-art baseline approaches, as shown in <ref type="table">Table 4</ref>. The six baselines are introduced as follows.</p><p>• P&amp;C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set.</p><p>• J&amp;E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans.</p><p>• Zhang2015: <ref type="bibr" target="#b29">Zhang et al. (2015)</ref> proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB.</p><p>• R&amp;X2014, R&amp;X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. <ref type="bibr" target="#b19">Rutherford and Xue (2015)</ref> further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier.</p><p>• B&amp;D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy classifier.</p><p>• Liu2016: Liu et al. <ref type="formula" target="#formula_2">(2016)</ref> proposed to better classify the discourse relations by learning from other discourse-related tasks with a multi- task neural network.</p><p>• Ji2016: Ji et al. <ref type="formula" target="#formula_2">(2016)</ref> proposed a neural language model over sequences of words and used the discourse relations as latent variables to connect the adjacent sequences.</p><p>It is noted that P&amp;C2012 and J&amp;E2015 merged the "EntRel" relation into the "Expansion" rela- tion <ref type="bibr">1</ref> . For a comprehensive comparison, we also experiment our model by adding a Expa.+EntRel vs Other classification. Our NNMA model with two attention levels exhibits obvious advantages over the six baseline methods on the whole. It is worth noting that NNMA is even better than the R&amp;X2015 approach which employs extra data.</p><p>As for the performance on each discourse relation, with respect to the F 1 measure, we can see that our NNMA model can achieve the best results on the "Expansion", "Expansion+EntRel" and "Temporal" relations and competitive results on the "Contingency" relation . The performance of recognizing the "Comparison" relation is only worse than R&amp;X2014 and R&amp;X2015. As <ref type="bibr" target="#b18">(Rutherford and Xue, 2014</ref>) stated, the "Comparison" relation is closely related to the constituent parse feature of the text, like production rules. How to represent and exploit these information in our model will be our next research focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of Attention Levels</head><p>The multiple attention levels in our model greatly boost the performance of classifying implicit dis- course relations. In this subsection, we perform both qualitative and quantitative analysis on the attention levels.</p><p>First, we take a three-level NNMA model for example and analyze its attention distributions on different attention levels by calculating the mean Kullback-Leibler (KL) Divergence between any two levels on the training set. In <ref type="figure" target="#fig_2">Figure 3</ref>, we use kl ij to denote the KL Divergence between the i th and the j th attention level and use kl ui to denote the KL Divergence between the uniform distribution and the i th attention level. We can see that each attention level forms different attention distributions and the difference increases in the higher levels. It can be inferred that the 2 nd and 3 rd levels in NNMA gradually neglect some words and pay more attention to some other words in the arguments. One point worth mentioning is that Arg-2 tends to have more non-uniform attention weights, since kl u2 and kl u3 of Arg-2 are much larger than those of Arg- 1. And also, the changes between attention levels are more obvious for Arg-2 through observing the values of kl 12 , kl 13 and kl 23 . The reason may be that Arg-2 contains more information related with discourse relation and some words in it tend to require focused attention, as Arg-2 is syntactically bound to the implicit connective.</p><p>At the same time, we visualize the attention levels of some example argument pairs which are analyzed by the three-level NNMA. To illustrate the k th attention level, we get its attention weights a 1 k and a 2 k which reflect the contribution of each word and then depict them by a row of color-shaded grids in <ref type="figure">Figure 2</ref>.</p><p>We can see that the NNMA model focuses on different words on different attention levels. Interestingly, from <ref type="figure">Figure 2</ref>, we find that the 1 st and 3 rd attention levels focus on some similar words, while the 2 nd level is relatively different from them. It seems that NNMA tries to find some clues (e.g. "moscow could be suspended" in Arg-2a; "won the business" in Arg-1b; "with great aplomb he considers not only" in Arg-2c) for recognizing the discourse relation on the 1 st level, looking closely at other words (e.g. "misuse of psychiatry against dissenters" in Arg-2a; "a third party that" in Arg-1b; "and support of hitler" in Arg-2c) on the 2 nd level, and then reconsider the arguments, focus on some specific words (e.g. "moscow could be suspended" in Arg-2a; "has not only hurt" in Arg-2b) and make the final decision on the last level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implicit Discourse Relation Classification</head><p>The Penn Discourse Treebank (PDTB) <ref type="bibr" target="#b17">(Prasad et al., 2008)</ref>, known as the largest discourse corpus, is composed of 2159 Wall Street Journal articles. Each document is annotated with the predicate-argument structure, where the predicate is the discourse connective (e.g. while) and the arguments are two text spans around the connective. The discourse connective can be either explicit or implicit. In PDTB, a hierarchy of relation tags is provided for annotation. In our study, we use the four top-level tags, including Temporal, Contingency, Comparison and Expansion. These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names ( <ref type="bibr" target="#b25">Wang et al., 2012)</ref>.</p><p>Implicit discourse relation recognition is often treated as a classification problem. The first work to tackle this task on PDTB is <ref type="bibr" target="#b16">(Pitler et al., 2009</ref>). They selected several surface features to train four binary classifiers, each for one of the top-level PDTB relation classes. Extending from this work, <ref type="bibr" target="#b11">Lin et al. (2009)</ref> further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. <ref type="bibr" target="#b18">Rutherford and Xue (2014)</ref> used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Networks and Attention Mechanism</head><p>Recently, neural network-based methods have gained prominence in the field of natural language processing <ref type="bibr" target="#b9">(Kim, 2014)</ref>. Such methods are primar- ily based on learning a distributed representation for each word, which is also called a word embedding <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>). Attention mechanism was first introduced into neural models to solve the alignment problem between different modalities. Graves (2013) designed a neural network to generate handwriting based on a text. It assigned a window on the input text at each step and generate characters based on the content within the window. <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> introduced this idea into machine translation, where their model computed a probabilistic distribution over the input sequence when generating each target word. <ref type="bibr" target="#b23">Tan et al. (2015)</ref> proposed an attention- based neural network to model both questions and sentences for selecting the appropriate non-factoid answers.</p><p>In parallel, the idea of equipping the neural model with an external memory has gained increasing attention recently. A memory can remember what the model has learned and guide its subsequent actions.  presented a neural network to read and update the external memory in a recurrent manner with the guidance of a question embedding. <ref type="bibr" target="#b10">Kumar et al. (2015)</ref> proposed a similar model where a memory was designed to change the gate of the gated recurrent unit for each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>As a complex text processing task, implicit dis- course relation recognition needs a deep analysis of the arguments. To this end, we for the first time propose to imitate the repeated reading strategy and dynamically exploit efficient features through several passes of reading. Following this idea, we design neural networks with multiple levels of attention (NNMA), where the general level and the attention levels represent the first and subsequent passes of reading. With the help of external short-term memories, NNMA can gradually update the arguments representations on each attention level and fix attention on some specific words which provide effective clues to discourse relation recognition. We conducted experiments on PDTB and the evaluation results show that our model can achieve the state-of-the-art performance on recognizing the implicit discourse relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neural Network with Multi-Level Attention. (Two attention levels are given here.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: KL-divergences between attention levels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameters for Neural Network with Multi-Level Attention.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performances of NNMA with Different 
Attention Levels. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>measures, we can see</head><label></label><figDesc></figDesc><table>System 

Four-way 
Binary 
F 1 
Acc. Comp. Cont. Expa. Expa.+EntRel Temp. 
P&amp;C2012 
-
-
31.32 49.82 
-
79.22 
26.57 
J&amp;E2015 
-
-
35.93 52.78 
-
80.02 
27.63 
Zhang2015 
38.80 55.39 32.03 47.08 68.96 
80.22 
20.29 
R&amp;X2014 
38.40 55.50 39.70 54.40 70.20 
80.44 
28.70 
R&amp;X2015 
40.50 57.10 41.00 53.80 69.40 
-
33.30 
B&amp;D2015 
-
-
36.36 55.76 61.76 
-
27.30 
Liu2016 
44.98 57.27 37.91 55.88 69.97 
-
37.17 
Ji2016 
42.30 59.50 
-
-
-
-
-
NNMA(two-level) 46.29 57.17 36.70 54.48 70.43 
80.73 
38.84 
NNMA(three-level) 44.95 57.57 39.86 53.69 69.71 
80.86 
37.61 

</table></figure>

			<note place="foot" n="1"> EntRel is the entity-based coherence relation which is independent of implicit and explicit relations in PDTB. However some research merges it into the implicit Expansion relation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their insightful comments on this paper. This work was partially supported by National Key Basic Research Program of China (2014CB340504), and National Natural Science Foundation of China (61273278 and 61572049). The correspondence author of this paper is Sujian Li.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arg-1a th e w o rl d p sy ch</head><p>ia tr ic as so ci at io n vo te d at an at h e n s p ar le y to co n d it io n al ly re ad m it th e so vi e t u n io n 1 2 3</p><p>Attention <ref type="table">Level   Arg-2a   m o sc o w  co  u ld b e su  sp  en  d ed if   th e m is u se o f p sy ch  ia tr y ag  ai n st  d is se n te rs is  d is</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>u t ib m w o u ld h a v e w o n th e b u si n e ss a n y w a y a s a sa le to a th ir d p a rt y th a t w o u ld h a v e th e n le a se d th e e q u ip m e n t to th e cu st o m e r</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic head-driven parsing for discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing word representations for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>David A Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One vector is not enough: Entity-augmented distributed semantics for discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A latent variable recurrent neural network for discourse relation language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01913</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel discriminative framework for sentencelevel discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit discourse relation classification via multi-task neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving implicit discourse relation recognition through feature set optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SigDial</title>
		<meeting>SigDial</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Penn Discourse TreeBank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering implicit discourse relations through brown cluster pair representation and coreference patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the inference of implicit discourse relations via classifying explicit discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Attapol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lstmbased deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A long shortterm memory model for answer sentence selection in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit Discourse Relation Recognition by Selecting Typical Training Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">Memory networks. Proceedings of ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<title level="m">Stacked attention networks for image question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shallow convolutional neural network for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting discourse connectives for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCL</title>
		<meeting>the ICCL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
