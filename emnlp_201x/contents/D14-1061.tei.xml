<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Parallel Data: Joint Word Alignment and Decipherment Improves Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Parallel Data: Joint Word Alignment and Decipherment Improves Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="557" to="565"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Inspired by previous work, where decipher-ment is used to improve machine translation, we propose a new idea to combine word alignment and decipherment into a single learning process. We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus, but also the monolingual corpus. We apply our approach to improve Malagasy-English machine translation, where only a small amount of parallel data is available. In our experiments, we observe gains of 0.9 to 2.1 Bleu over a strong baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State-of-the-art machine translation (MT) systems ap- ply statistical techniques to learn translation rules au- tomatically from parallel data. However, this reliance on parallel data seriously limits the scope of MT ap- plication in the real world, as for many languages and domains, there is not enough parallel data to train a de- cent quality MT system.</p><p>However, compared with parallel data, there are much larger amounts of non parallel data. The abil- ity to learn a translation lexicon or even build a ma- chine translation system using monolingual data helps address the problems of insufficient parallel data. <ref type="bibr" target="#b30">Ravi and Knight (2011)</ref> are among the first to learn a full MT system using only non parallel data through deci- pherment. However, the performance of such systems is much lower compared with those trained with par- allel data. In another work, <ref type="bibr" target="#b17">Klementiev et al. (2012)</ref> show that, given a phrase table, it is possible to esti- mate parameters for a phrase-based MT system from non parallel data.</p><p>Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel data by using additional non parallel data. <ref type="bibr" target="#b29">Rapp (1995)</ref> shows that with a seed lexicon, it is possi- ble to induce new word level translations from non par- allel data. Motivated by the idea that a translation lexi- con induced from non parallel data can be used to trans- late out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data <ref type="bibr" target="#b6">(Fung and Yee, 1998;</ref><ref type="bibr" target="#b20">Koehn and Knight, 2002;</ref><ref type="bibr" target="#b11">Haghighi et al., 2008</ref>; Garera  <ref type="bibr" target="#b2">and Jagarlamudi, 2011;</ref><ref type="bibr" target="#b13">Irvine and Callison-Burch, 2013b;</ref><ref type="bibr" target="#b12">Irvine and Callison-Burch, 2013a;</ref><ref type="bibr" target="#b15">Irvine et al., 2013)</ref>.</p><p>Lately, there has been increasing interest in learn- ing translation lexicons from non parallel data with de- cipherment techniques ( <ref type="bibr" target="#b30">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b4">Dou and Knight, 2012;</ref><ref type="bibr" target="#b26">Nuhn et al., 2012;</ref><ref type="bibr" target="#b5">Dou and Knight, 2013)</ref>. Decipherment views one language as a cipher for another and learns a translation lexicon that pro- duces fluent text in the target (plaintext) language. Pre- vious work has shown that decipherment not only helps find translations for OOVs ( <ref type="bibr" target="#b4">Dou and Knight, 2012</ref>), but also improves translations of observed words <ref type="bibr" target="#b5">(Dou and Knight, 2013)</ref>.</p><p>We find that previous work using monolingual or comparable data to improve quality of machine transla- tion separates two learning tasks: first, translation rules are learned from parallel data, and then the information learned from parallel data is used to bootstrap learning with non parallel data. Inspired by approaches where joint inference reduces the problems of error propaga- tion and improves system performance, we combine the two separate learning processes into a single one, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The contributions of this work are:</p><p>• We propose a new objective function for word alignment that combines the process of word alignment and decipherment into a single learning task.</p><p>• In experiments, we find that the joint process out- performs the previous pipeline approach, and ob- serve Bleu gains of 0.9 and 2.1 on two different test sets.</p><p>• We release 15.3 million tokens of monolingual Malagasy data from the web, as well as a small Malagasy dependency tree bank containing 20k tokens.</p><p>2 Joint Word Alignment and Decipherment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A New Objective Function</head><p>In previous work that uses monolingual data to im- prove machine translation, a seed translation lexicon learned from parallel data is used to find new transla- tions through either word vector based approaches or decipherment. In return, selection of a seed lexicon needs to be careful as using a poor quality seed lexi- con could hurt the downstream process. Evidence from a number of previous work shows that a joint inference process leads to better performance in both tasks ( <ref type="bibr" target="#b16">Jiang et al., 2008;</ref><ref type="bibr" target="#b35">Zhang and Clark, 2008</ref>).</p><p>In the presence of parallel and monolingual data, we would like the alignment and decipherment models to benefit from each other. Since the decipherment and word alignment models contain word-to-word transla- tion probabilities t( f | e), having them share these pa- rameters during learning will allow us to pool infor- mation from both data types. This leads us to de- velop a new objective function that takes both learn- ing processes into account. Given our parallel data,</p><formula xml:id="formula_0">(E 1 , F 1 ), . . . , (E m , F m ), . . . , (E M , F M )</formula><p>, and monolingual data F 1 mono , . . . , F n mono , . . . , F N mono , we seek to maximize the likelihood of both. Our new objective function is defined as:</p><formula xml:id="formula_1">F joint = M m=1 log P(F m | E m ) + α N n=1 log P(F n mono ) (1)</formula><p>The goal of training is to learn the parameters that maximize this objective, that is</p><formula xml:id="formula_2">θ * = arg max θ F joint<label>(2)</label></formula><p>In the next two sections, we describe the word align- ment and decipherment models, and present how they are combined to perform joint optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Alignment</head><p>Given a source sentence F = f 1 , . . . , f j , . . . , f J and a target sentence E = e 1 , . . . , e i , . . . , e I , word alignment models describe the generative process employed to produce the French sentence from the English sentence through alignments a = a 1 , . . . , a j , . . . , a J .</p><p>The IBM models 1-2 ( <ref type="bibr" target="#b1">Brown et al., 1993</ref>) and the HMM word alignment model ( <ref type="bibr" target="#b33">Vogel et al., 1996</ref>) use two sets of parameters, distortion probabilities and translation probabilities, to define the joint probabil- ity of a target sentence and alignment given a source sentence.</p><formula xml:id="formula_3">P(F, a | E) = J j=1 d(a j | a j−1 , j)t( f j | e a j ).<label>(3)</label></formula><p>These alignment models share the same translation probabilities t( f j | e a j ), but differ in their treatment of the distortion probabilities d(a j | a j−1 , j). <ref type="bibr" target="#b1">Brown et al. (1993)</ref> introduce more advanced models for word alignment, such as Model 3 and Model 4, which use more parameters to describe the generative process. We do not go into details of those models here and the reader is referred to the paper describing them.</p><p>Under the Model 1-2 and HMM alignment models, the probability of target sentence given source sentence is:</p><formula xml:id="formula_4">P(F | E) = a J j=1 d(a j | a j−1 , j)t( f j | e a j ).</formula><p>Let θ denote all the parameters of the word align- ment model. Given a corpus of sentence pairs (E 1 , F 1 ), . . . , (E m , F m ), . . . , (E M , F M ), the standard ap- proach for training is to learn the maximum likelihood estimate of the parameters, that is,</p><formula xml:id="formula_5">θ * = arg max θ M m=1 log P(F m | E m ) = arg max θ log        a P(F m , a | E m )        .</formula><p>We typically use the EM algorithm <ref type="bibr" target="#b3">(Dempster et al., 1977)</ref>, to carry out this optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decipherment</head><p>Given a corpus of N foreign text sequences (cipher- text), F 1 mono , . . . , F n mono , . . . , F N mono , decipherment finds word-to-word translations that best describe the cipher- text. <ref type="bibr" target="#b18">Knight et al. (2006)</ref> are the first to study several natu- ral language decipherment problems with unsupervised learning. Since then, there has been increasing interest in improving decipherment techniques and its applica- tion to machine translation ( <ref type="bibr" target="#b30">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b4">Dou and Knight, 2012;</ref><ref type="bibr" target="#b26">Nuhn et al., 2012;</ref><ref type="bibr" target="#b5">Dou and Knight, 2013;</ref><ref type="bibr" target="#b27">Nuhn et al., 2013)</ref>.</p><p>In order to speed up decipherment, <ref type="bibr" target="#b4">Dou and Knight (2012)</ref> suggest that a frequency list of bigrams might contain enough information for decipherment. Accord- ing to them, a monolingual ciphertext bigram F mono is generated through the following generative story:</p><p>• Generate a sequence of two plaintext tokens e 1 e 2 with probability P(e 1 e 2 ) given by a language model built from large numbers of plaintext bi- grams.</p><p>• Substitute e 1 with f 1 and e 2 with f 2 with probabil-</p><formula xml:id="formula_6">ity t( f 1 |e 1 ) · t( f 2 |e 2 ).</formula><p>The probability of any cipher bigram F is:</p><formula xml:id="formula_7">P(F mono ) = e 1 e 2 P(e 1 e 2 ) · t( f 1 |e 1 ) · t( f 2 |e 2 ) (4)</formula><p>And the probability of the corpus is:</p><formula xml:id="formula_8">P(corpus) = N n=1 P(F n mono ) (5)</formula><p>Given a plaintext bigram language model, the goal is to manipulate t( f |e) to maximize P(corpus). Theoret- ically, one can directly apply EM to solve the problem ( <ref type="bibr" target="#b18">Knight et al., 2006</ref>). However, EM has time complex- ity O(N·V 2 e ) and space complexity O(V f ·V e ), where V f , V e are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams.</p><p>There have been previous attempts to make decipher- ment faster. <ref type="bibr" target="#b30">Ravi and Knight (2011)</ref> apply Bayesian learning to reduce the space complexity. However, Bayesian decipherment is still very slow with Gibbs sampling <ref type="bibr" target="#b10">(Geman and Geman, 1987)</ref>. Dou and Knight (2012) make sampling faster by introducing slice sam- pling <ref type="bibr" target="#b25">(Neal, 2000</ref>) to Bayesian decipherment. Besides Bayesian decipherment, <ref type="bibr" target="#b27">Nuhn et al. (2013)</ref> show that beam search can be used to solve a very large 1:1 word substitution cipher. In subsection 2.4.1, we describe our approach that uses slice sampling to compute ex- pected counts for decipherment in the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Joint Optimization</head><p>We now describe our EM approach to learn the param- eters that maximize F joint (equation 2), where the dis- tortion probabilities, d(a j | a j−1 , j) in the word align- ment model are only learned from parallel data, and the translation probabilities, t( f | e) are learned using both parallel and non parallel data. The E step and M step are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Our algorithm starts with EM learning only on par- allel data for a few iterations. When the joint inference starts, we first compute expected counts from parallel data and non parallel data using parameter values from the last M step separately. Then, we add the expected counts from both parallel data and non parallel data to- gether with different weights for the two. Finally we  <ref type="table">table and distortion table to</ref> update parameters in the new M step.</p><p>The E step for parallel part can be computed effi- ciently using the forward-backward algorithm ( <ref type="bibr" target="#b33">Vogel et al., 1996)</ref>. However, as we pointed out in Section 2.3, the E step for the non parallel part has a time com- plexity of O(V 2 ) with the forward-backward algorithm, where V is the size of English vocabulary, and is usu- ally very large. Previous work has tried to make de- cipherment scalable ( <ref type="bibr" target="#b30">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b4">Dou and Knight, 2012;</ref><ref type="bibr" target="#b27">Nuhn et al., 2013;</ref><ref type="bibr" target="#b31">Ravi, 2013)</ref>. How- ever, all of them are designed for decipherment with ei- ther Bayesian inference or beam search. In contrast, we need an algorithm to make EM decipherment scalable. To overcome this problem, we modify the slice sam- pling <ref type="bibr" target="#b25">(Neal, 2000</ref>) approach used by <ref type="bibr" target="#b4">Dou and Knight (2012)</ref> to compute expected counts from non parallel data needed for the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Draw Samples with Slice Sampling</head><p>To start the sampling process, we initialize the first sample by performing approximate Viterbi decoding using results from the last EM iteration. For each for- eign dependency bigram f 1 , f 2 , we find the top 50 can- didates for f 1 and f 2 ranked by t(e| f ), and find the En- glish sequence e 1 , e 2 that maximizes t(</p><formula xml:id="formula_9">e 1 | f 1 ) · t(e 2 | f 2 ) · P(e 1 , e 2 ).</formula><p>Suppose the derivation probability for current sam- ple e current is P(e current), we use slice sampling to draw a new sample in two steps:</p><p>• Select a threshold T uniformly between 0 and P(e current).</p><p>• Draw a new sample e new uniformly from a pool of candidates: {e new|P(e new) &gt; T }.</p><p>The first step is straightforward to implement. How- ever, it is not trivial to implement the second step. We adapt the idea from Dou and Knight (2012) for EM learning. Suppose our current sample e current contains En- glish tokens e i−1 , e i , and e i+1 at position i − 1, i, and i + 1 respectively, and f i be the foreign token at position i. Using point-wise sampling, we draw a new sample by changing token e i to a new token e . Since the rest of the sample remains the same, only the probability of the trigram P(e i−1 e e i+1 ) (The probability is given by a bigram language model.), and the channel model prob- ability t( f i |e ) change. Therefore, the probability of a sample is simplified as shown Equation 6.</p><formula xml:id="formula_10">P(e i−1 e e i+1 ) · t( f i |e ) (6)</formula><p>Remember that in slice sampling, a new sample is drawn in two steps. For the first step, we choose a threshold T uniformly between 0 and P(e i−1 e i e i+1 ) · t( f i |e i ). We divide the second step into two cases based on the observation that two types of samples are more likely to have a probability higher than T (Dou and Knight, 2012): (1) those whose trigram probability is high, and (2) those whose channel model probability is high. To find candidates that have high trigram proba- bility, <ref type="bibr" target="#b4">Dou and Knight (2012)</ref> build a top k sorted lists ranked by P(e i−1 e e i+1 ), which can be pre-computed off-line. Then, they test if the last item e k in the list satisfies the following inequality:</p><formula xml:id="formula_11">P(e i−1 e k e i+1 ) · c &lt; T<label>(7)</label></formula><p>where c is a small constant and is set to prior in their work. In contrast, we choose c empirically as we do not have a prior in our model. When the inequality in Equation 7 is satisfied, a sample is drawn in the fol- lowing way: Let set A = {e |e i−1 e e i+1 · c &gt; T } and set B = {e |t( f i |e ) &gt; c}. Then we only need to sample e uniformly from A ∪ B until P(e i−1 e e i+1 ) · t( f i |e ) is greater than T . It is easy to prove that all other candi- dates that are not in the sorted list and with t( f i |e ) ≤ c have a upper bound probability: P(e i−1 e k e i+1 )·c. There- fore, they do not need to be considered. Second, when the last item e k in the list does not meet the condition in Equation 7, we keep drawing samples e randomly until its probability is greater than the threshold T .</p><p>As we mentioned before, the choice of the small con- stant c is empirical. A large c reduces the number of items in set B, but makes the condition P(e i−1 e k e i+1 ) · c &lt; T less likely to satisfy, which slows down the sam- pling. On the contrary, a small c increases the number of items in set B significantly as EM does not encour- age a sparse distribution, which also slows down the sampling. In our experiments, we set c to 0.001 based on the speed of decipherment. Furthermore, to reduce the size of set B, we rank all the candidate translations Spanish English Parallel 10.3k 9.9k Non Parallel 80 million 400 million <ref type="table">Table 1</ref>: Size of parallel and non parallel data for word alignment experiments (Measured in number of tokens) of f i by t(e | f i ), then we add maximum the first 1000 candidates whose t( f i |e ) &gt;= c into set B. For the rest of the candidates, we set t( f i |e ) to a value smaller than c (0.00001 in experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Compute Expected Counts from Samples</head><p>With the ability to draw samples efficiently for deci- pherment using EM, we now describe how to compute expected counts from those samples. Let f 1 , f 2 be a specific ciphertext bigram, N be the number of sam- ples we want to use to compute expected counts, and e 1 , e 2 be one of the N samples. The expected counts for pairs ( f 1 , e 1 ) and ( f 2 , e 2 ) are computed as:</p><formula xml:id="formula_12">α · count( f 1 , f 2 ) N</formula><p>where count( f 1 , f 2 ) is count of the bigram, and α is the weight for non parallel data as shown in Equation 1. Expected counts collected for f 1 , f 2 are accumulated from each of its N samples. Finally, we collect ex- pected counts using the same approach from each for- eign bigram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Alignment Experiments</head><p>In this section, we show that joint word alignment and decipherment improves the quality of word alignment. We choose to evaluate word alignment performance for Spanish and English as manual gold alignments are available. In experiments, our approach improves alignment F score by as much as 8 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup</head><p>As shown in <ref type="table">Table 1</ref>, we work with a small amount of parallel, manually aligned Spanish-English data <ref type="bibr" target="#b22">(Lambert et al., 2005</ref>), and a much larger amount of mono- lingual data. The parallel data is extracted from Europarl, which consists of articles from European parliament plenary sessions. The monolingual data comes from English and Spanish versions of Gigaword corpra containing news articles from different news agencies.</p><p>We view Spanish as a cipher of English, and follow the approach proposed by <ref type="bibr" target="#b5">Dou and Knight (2013)</ref> to extract dependency bigrams from parsed Spanish and English monolingual data for decipherment. We only keep bigrams where both tokens appear in the paral- lel data. Then, we perform Spanish to English (En- glish generating Spanish) word alignment and Span- ish to English decipherment simultaneously with the method discussed in section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Results</head><p>We align all 500 sentences in the parallel corpus, and tune the decipherment weight (α) for Model 1 and HMM using the last 100 sentences. The best weights are 0.1 for Model 1, and 0.005 for HMM. We start with Model 1 with only parallel data for 5 iterations, and switch to the joint process for another 5 iterations with Model 1 and 5 more iterations of HMM. In the end, we use the first 100 sentence pairs of the corpus for evalu- ation. <ref type="figure" target="#fig_3">Figure 3</ref> compares the learning curve of alignment F-score between EM without decipherment (baseline) and our joint word alignment and decipherment. From the learning curve, we find that at the 6th iteration, 2 iterations after we start the joint process, alignment F- score is improved from 34 to 43, and this improvement is held through the rest of the Model 1 iterations. The alignment model switches to HMM from the 11th iter- ation, and at the 12th iteration, we see a sudden jump in F-score for both the baseline and the joint approach. We see consistent improvement of F-score till the end of HMM iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improving Low Density Languages Machine Translation with Joint Word Alignment and Decipherment</head><p>In the previous section, we show that the joint word alignment and decipherment process improves quality of word alignment significantly for Spanish and En- glish. In this section, we test our approach in a more challenging setting: improving the quality of machine translation in a real low density language setting. In this task, our goal is to build a system to trans- late Malagasy news into English. We have a small amount of parallel data, and larger amounts of mono- lingual data collected from online websites. We build a dependency parser for Malagasy to parse the monolin- gual data to perform dependency based decipherment ( <ref type="bibr" target="#b5">Dou and Knight, 2013)</ref>. In the end, we perform joint word alignment and decipherment, and show that the joint learning process improves Bleu scores by up to 2.1 points over a phrase-based MT baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Malagasy Language</head><p>Malagasy is the official language of Madagascar. It has around 18 million native speakers. Although Mada- gascar is an African country, Malagasy belongs to the Malayo-Polynesian branch of the Austronesian lan- guage family. Malagasy and English have very dif- ferent word orders. First of all, in contrast to En- glish, which has a subject-verb-object (SVO) word or- der, Malagasy has a verb-object-subject (VOS) word order. Besides that, Malagasy is a typical head ini- tial language: Determiners precede nouns, while other modifiers and relative clauses follow nouns (e.g. ny "the" boky "book" mena "red"). The significant dif- ferences in word order pose great challenges for both  machine translation and decipherment. <ref type="table" target="#tab_1">Table 2</ref> shows the data available to us in our experi- ments. The majority of parallel text comes from Global Voices 1 (GV). The website contains international news translated into different foreign languages. Besides that, we also have a very small amount of parallel text containing local web news, with English translations provided by native speakers at the University of Texas, Austin. The Malagasy side of this small parallel corpus also has syntactical annotation, which is used to train a very basic Malagasy part of speech tagger and depen- dency parser. We also have much larger amounts of non paral- lel data for both languages. For Malagasy, we spent two months manually collecting 15.3 million tokens of news text from local news websites in Madagascar. <ref type="bibr">2</ref> We have released this data for future research use. For English, we have 2.4 billion tokens from the Gigaword corpus. Since the Malagasy monolingual data is col- lected from local websites, it is reasonable to argue that those data contain significant amount of information re- lated to Africa. Therefore, we also collect 396 million tokens of African news in English from allAfrica.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Building A Dependency Parser for Malagasy</head><p>Since Malagasy and English have very different word orders, we decide to apply dependency based decipher- ment for the two languages as suggested by <ref type="bibr" target="#b5">Dou and Knight (2013)</ref>. To extract dependency relations, we need to parse monolingual data in Malagasy and En- glish. For English, there are already many good parsers available. In our experiments, we use Turbo parser <ref type="bibr" target="#b24">(Martins et al., 2013</ref>) trained on the English Penn Tree- bank ( <ref type="bibr" target="#b23">Marcus et al., 1993)</ref> to parse all our English monolingual data. However, there is no existing good parser for Malagasy.</p><p>The quality of a dependency parser depends on the amount of training data available. State-of-the-art En- glish parsers are built from Penn Treebank, which con- tains over 1 million tokens of annotated syntactical  trees. In contrast, the available data for training a Mala- gasy parser is rather limited, with only 168 sentences, and 2.8k tokens, as shown in <ref type="table" target="#tab_1">Table 2</ref>. At the very be- ginning, we use the last 120 sentences as training data to train a part of speech (POS) tagger using a toolkit provided by <ref type="bibr" target="#b9">Garrette et al. (2013)</ref> and a dependency parser with the Turbo parser. We test the performance of the parser on the first 48 sentences and obtain 72.4% accuracy.</p><p>One obvious way to improve tagging and parsing ac- curacy is to get more annotated data. We find more data with only part of speech tags containing 465 sentences and 10k tokens released by ( <ref type="bibr" target="#b9">Garrette et al., 2013)</ref>, and add this data as extra training data for POS tagger. Also, we download an online dictionary that contains POS tags for over 60k Malagasy word types from mala- gasyword.org. The dictionary is very helpful for tag- ging words never seen in the training data.</p><p>It is natural to think that creation of annotated data for training a POS tagger and a parser requires large amounts of efforts from annotators who understand the language well. However, we find that through the help of parallel data and dictionaries, we are able to create more annotated data by ourselves to improve tagging and parsing accuracy. This idea is inspired by previ- ous work that tries to learn a semi-supervised parser by projecting dependency relations from one language (with good dependency parsers) to another <ref type="bibr" target="#b34">(Yarowsky and Ngai, 2001;</ref><ref type="bibr" target="#b7">Ganchev et al., 2009</ref>). However, we find those automatic approaches do not work well for Malagasy.</p><p>To further expand our Malagasy training data, we first use a POS tagger and parser with poor perfor- mance to parse 788 sentences (20k tokens) on the Malagasy side of the parallel corpus from Global Voices. Then, we correct both the dependency links and POS tags based on information from dictionaries 3 and the English translation of the parsed sentence. We spent 3 months to manually project English dependen- cies to Malagasy and eventually improve test set pars- ing accuracy from 72.4% to 80.0%. We also make this data available for future research use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Machine Translation Experiments</head><p>In this section, we present the data used for our MT experiments, and compare three different systems to justify our joint word alignment and decipherment ap- proach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Baseline Machine Translation System</head><p>We build a state-of-the-art phrase-based MT system, PBMT, using Moses ( <ref type="bibr" target="#b21">Koehn et al., 2007)</ref>. PBMT has 3 models: a translation model, a distortion model, and a language model. We train the other models using half of the Global Voices parallel data (the rest is re- served for development and testing), and build a 5- gram language model using 834 million tokens from AFP section of English Gigaword, 396 million tokens from allAfrica, and the English part of the parallel cor- pus for training. For alignment, we run 10 iterations of Model 1, and 5 iterations of HMM. We did not run Model 3 and Model 4 as we see no improvements in Bleu scores from running those models. We do word alignment in two directions and use grow-diag-final- and heuristic to obtain final alignment. During decod- ing, we use 8 standard features in Moses to score a can- didate translation: direct and inverse translation proba- bilities, direct and inverse lexical weighting, a language model score, a distortion score, phrase penalty, and word penalty. The weights for the features are learned on the tuning data using minimum error rate training (MERT) <ref type="bibr" target="#b28">(Och, 2003)</ref>.</p><p>To compare with previous decipherment approach to improve machine translation, we build a second base- line system. We follow the work by <ref type="bibr" target="#b5">Dou and Knight (2013)</ref> to decipher Malagasy into English, and build a translation lexicon T decipher from decipherment. To im- prove machine translation, we simply use T decipher as an additional parallel corpus. First, we filter T decipher by keeping only translation pairs ( f, e), where f is ob- served in the Spanish part and e is observed in the En- glish part of the parallel corpus. Then we append all the Spanish and English words in the filtered T decipher to the end of Spanish part and English part of the paral- lel corpus respectively. The training and tuning process is the same as the baseline machine translation system PBMT. We call this system Decipher-Pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Joint Word Alignment and Decipherment for Machine Translation</head><p>When deciphering Malagasy to English, we extract Malagasy dependency bigrams using all available Malagasy monolingual data plus the Malagasy part of the Global Voices parallel data, and extract English dependency bigrams using 834 million tokens from English Gigaword, and 396 million tokens from al- lAfrica news to build an English dependency language model. In the other direction, we extract English de- pendency bigrams from English part of the entire paral- lel corpus plus 9.7 million tokens from allAfrica news 4 , and use 17.3 million tokens Malagasy monolingual data (15.3 million from the web and 2.0 million from Global Voices) to build a Malagasy dependency lan- guage model. We require that all dependency bigrams only contain words observed in the parallel data used to train the baseline MT system. During learning, we run Model 1 without decipher- ment for 5 iterations. Then we perform joint word alignment and decipherment for another 5 iterations with Model 1 and 5 iterations with HMM. We tune decipherment weights (α) for Model 1 and HMM us- ing grid search against Bleu score on a development set. In the end, we only extract rules from one di- rection P(English|Malagasy), where the decipherment weights for Model 1 and HMM are 0.5 and 0.005 re- spectively. We chose this because we did not find any benefits to tune the weights on each direction, and then use grow-diag-final-end heuristic to form final align- ments. We call this system Decipher-Joint.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>We tune each system three times with MERT and choose the best weights based on Bleu scores on tuning set. <ref type="table">Table 4</ref> shows that while using a translation lexicon learnt from decipherment does not improve the quality of machine translation significantly, the joint approach improves Bleu score by 0.9 and 2.1 on Global Voices test set and web news test set respectively. The results show that the parsing quality correlates with gains in Bleu scores. Scores in the brackets in the last row of the table are achieved using a dependency parser with 72.4% attachment accuracy, while scores outside the brackets are obtained using a dependency parser with 80.0% attachment accuracy.</p><p>We analyze the results and find the gain mainly comes from two parts. First, adding expected counts from non parallel data makes the distribution of trans- lation probabilities sparser in word alignment models. The probabilities of translation pairs favored by both parallel data and decipherment becomes higher. This gain is consistent with previous observation where a sparse prior is applied to EM to help improve word alignment and machine translation ( <ref type="bibr" target="#b32">Vaswani et al., 2012)</ref>. Second, expected counts from decipherment also help discover new translation pairs in the paral- lel data for low frequency words, where those words are either aligned to NULL or wrong translations in the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We propose a new objective function for word align- ment to combine the process of word alignment and decipherment into a single task. In, experiments, we find that the joint process performs better than previous pipeline approach, and observe Bleu gains of 0.9 and 2.1 point on Global Voices and local web news test sets, respectively. Finally, our research leads to the release of 15.3 million tokens of monolingual Malagasy data from the web as well as a small Malagasy dependency tree bank containing 20k tokens.</p><p>Given the positive results we obtain by using the joint approach to improve word alignment, we are in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decipherment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Tune (GV) Test (GV) Test (Web) None PBMT (Baseline) 18.5 17.1 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.7 Separate</head><p>Decipher-Pipeline 18.5 17.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Joint</head><p>Decipher-Joint 18.9 (18.7) 18.0 (17.7) 9.8 (8.5) <ref type="table">Table 4</ref>: Decipher-Pipeline does not show significant improvement over the baseline system. In contrast, Decipher- Joint using joint word alignment and decipherment approach achieves a Bleu gain of 0.9 and 2.1 on the Global Voices test set and the web news test set, respectively. The results in brackets are obtained using a parser trained with only 120 sentences. (GV: Global Voices) spired to apply this approach to help find translations for out of vocabulary words, and to explore other pos- sible ways to improve machine translation with deci- pherment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Combine word alignment and decipherment into a single learning process.</figDesc><graphic url="image-1.png" coords="1,351.61,204.54,129.60,187.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Joint Word Alignment and Decipherment with EM</figDesc><graphic url="image-2.png" coords="3,333.61,62.81,165.61,244.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 globalvoicesonline.org 2 aoraha.com, gazetiko.com, inovaovao.com, expressmada.com, lakroa.com</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curve showing our joint word alignment and decipherment approach improves word alignment quality over the traditional EM without decipherment (Model 1: Iteration 1 to 10, HMM: Iteration 11 to 15)</figDesc><graphic url="image-3.png" coords="6,118.77,62.80,360.01,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Size of Malagasy and English data used in our 
experiments (Measured in number of tokens) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Size of training, tuning, and testing data in 
number of tokens (GV: Global Voices) 

</table></figure>

			<note place="foot" n="3"> an online dictionary from malagasyword.org, as well as a lexicon learned from the parallel data</note>

			<note place="foot" n="4"> We do not find further Bleu gains by using more English monolingual data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by NSF Grant 0904684 and ARO grant W911NF-10-1-0533. The authors would like to thank David Chiang, Malte Nuhn, Victoria Fos-sum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and Hui Zhang (in no particular order) for their comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons using the visual similarity of labeled web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Three</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Three</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation for machine translation by mining unseen words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale decipherment for out-of-domain machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependencybased decipherment for resource-limited machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An IR approach for translating new words from nonparallel, comparable texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Lo Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dependency grammar induction via bitext projection constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Garera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-world semi-supervised learning of postaggers for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. In Readings in computer vision: issues, problems, principles, and paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL08: HLT. Association for Computational Linguistics</title>
		<meeting>ACL08: HLT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining bilingual and comparable corpora for low resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>August</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised bilingual lexicon induction with multiple monolingual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013</title>
		<meeting>the 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monolingual marginal matching for translation model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT. Association for Computational Linguistics</title>
		<meeting>ACL-08: HLT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised analysis for decipherment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishit</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL</title>
		<meeting>the COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Main Conference Poster Sessions</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition. Association for Computational Linguistics</title>
		<meeting>the ACL-02 Workshop on Unsupervised Lexical Acquisition. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Guidelines for word alignment evaluation and manual alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriá</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">B</forename><surname>Mariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="285" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Turning on the Turbo: Fast third-order nonprojective Turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><surname>Neal</surname></persName>
		</author>
		<title level="m">Slice sampling. Annals of Statistics</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deciphering foreign language by combining language models and context vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beam search for solving substitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying word translations in non-parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting of Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting of Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable decipherment for machine translation via hash sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Smaller alignment models for better translations: Unsupervised word alignment with the l0norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HMM-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Computational Linguistics</title>
		<meeting>the 16th Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies. Association for Computational Linguistics</title>
		<meeting>the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint word segmentation and POS tagging using a single perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
