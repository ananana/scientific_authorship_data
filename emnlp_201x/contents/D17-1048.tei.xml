<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Cognition Based Attention Model for Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xiang</surname></persName>
							<email>Rong.Xiang@amd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
							<email>churen.huang@polyu.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Chinese and Bilingual Studies</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Cognition Based Attention Model for Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="462" to="471"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Attention models are proposed in sentiment analysis because some words are more important than others. However, most existing methods either use local context based text information or user preference information. In this work, we propose a novel attention model trained by cognition grounded eye-tracking data. A reading prediction model is first built using eye-tracking data as dependent data and other features in the context as independent data. The predicted reading time is then used to build a cognition based attention (CBA) layer for neural sentiment analysis. As a comprehensive model, We can capture attentions of words in sentences as well as sentences in documents. Different attention mechanisms can also be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis is critical for many applica- tions such as sentimental product recommenda- tion ( <ref type="bibr" target="#b7">Dong et al., 2013)</ref>, public opinion detec- tion ( <ref type="bibr" target="#b26">Pang et al., 2008)</ref>, and human-machine inter- action ( <ref type="bibr" target="#b4">Clavel and Callejas, 2016)</ref>, etc.Sentiment analysis has been well-explored ( <ref type="bibr" target="#b25">Pang et al., 2002;</ref><ref type="bibr" target="#b38">Vanzo et al., 2014;</ref><ref type="bibr" target="#b33">Tang et al., 2015a;</ref><ref type="bibr" target="#b18">Maas et al., 2011</ref>).Recently, deep learning based methods have further elevated the perfor- mance of sentiment analysis without the need for labor intensive feature engineering.</p><p>Attention models are incorporated into senti- ment analysis because not all words are created equal. Some words are more important than oth- ers in conveying the message in a sentence. Simi- larly, some sentences are more important than oth- ers in a document. Although the overall reading time as a cognitive process may reflect the syn- tax and discourse complexity, reading time of in- dividual words is also an indicator of their seman- tic importance in text <ref type="bibr" target="#b29">(Roseman, 2001;</ref><ref type="bibr" target="#b6">Demberg and Keller, 2008)</ref>. Previous attention models are built using information embedded in text including users, products and text in local context for senti- ment classification ( <ref type="bibr" target="#b34">Tang et al., 2015b;</ref><ref type="bibr" target="#b40">Yang et al., 2016;</ref><ref type="bibr" target="#b9">Gui et al., 2016)</ref>. How- ever, attention models using local context based text through distributional similarity lack theoret- ical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model.</p><p>In this paper, we propose a novel cognition based attention(CBA) model for sentiment analy- sis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring ei- ther the point of gaze or the motion of an eye rel- ative to the head <ref type="bibr">1</ref> . In psycho-linguistics experi- ments, <ref type="bibr" target="#b1">Barrett(2016)</ref> shows that readers are less likely to fixate on close-class words that are pre- dictable from context. Readers also fixate longer on words which play significant semantic roles <ref type="bibr" target="#b6">(Demberg and Keller, 2008</ref>) in addition to infre- quent words, ambiguous words, and morphologi- cal complex words <ref type="bibr" target="#b28">(Rayner, 1998)</ref>. Since reading time can be learned from an eye-tracking dataset, predicted reading time of words in its context can be used as indicators of attention weights.</p><p>We first build a regression model to map syn- tax, and context features of a word to its reading time based on eye-tracking data. We then apply the model to sentiment analysis text to obtain the estimated reading time of words at the sentence level. The estimated reading time can then be used as the attention weights in its context to build the attention layer in a neural network based senti- ment analysis model. Evaluation on the four sen- timent analysis benchmark datasets (IMDB, <ref type="bibr">Yelp 13, Yelp 14 and IMDB2)</ref> show that our proposed model can significantly improve the performance compared to the state-of-the-art attention methods.</p><p>To sum up, we have two major contributions: (1) We propose a novel cognition grounded at- tention model to improve the state-of-the-art neu- ral network based sentiment analysis models by learning attention information from eye-tracking data. This is one of the first attempts to use cog- nition grounded data in sentiment analysis. The CBA model not only can capture attention of words at the sentence level, it can also be aggre- gated to work at the document level. (2) Evalu- ation on several real-world datasets in sentiment analysis shows that our method outperforms other state-of-the-art methods significantly. This work validates the effectiveness of cognition grounded data in building attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>The basic task in sentiment analysis can be formu- lated as a classification problem. Class labels can either be binary (positive/negative) or polarity ei- ther as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc..</p><p>In recent years, deep learning based methods have greatly improved the performance of senti- ment analysis. Commonly used models include Convolutional Neural Networks <ref type="bibr" target="#b31">(Socher et al., 2011</ref>), Recursive Neural Network ( <ref type="bibr" target="#b32">Socher et al., 2013)</ref>, and Recurrent Neural Networks ( <ref type="bibr" target="#b11">Irsoy and Cardie, 2014)</ref>. RNN naturally benefits sentiment classification because of its ability to capture se- quential information in text. However, standard RNN suffers from the gradient vanishing problem ( <ref type="bibr" target="#b2">Bengio et al., 1994</ref>) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool- ing and then feed into the next layer. Experiments in datasets which contain long documents and sen- tences demonstrate that the LSTM model outper- forms the traditional RNN ( <ref type="bibr">Tang et al., 2015a,c)</ref>.</p><p>Not all words contribute equally to the seman- tics of a sentence . Atten- tion based neural networks are proposed to high- light their difference in contribution ( <ref type="bibr" target="#b40">Yang et al., 2016)</ref>. In document level sentiment classifica- tion, both sentence level attention and document level attention are proposed. In the sentence level attention layer, an attention mechanism identi- fies words that are important. Those informative words are aggregated as attention weights to form sentence embedding representation. This method is generally called local context attention method. Similarly, some sentences can also be highlighted to indicate their importance in a document.</p><p>Apart from local context attention, user/product attentions are also included in deep learning based methods either in a separate network <ref type="bibr" target="#b9">(Gui et al., 2016</ref>) or a unified network ( <ref type="bibr" target="#b35">Tang et al., 2015c;</ref><ref type="bibr" target="#b9">Gui et al., 2016)</ref>. Some feature engineering method to some specific datasets can also achieve very good result( <ref type="bibr" target="#b30">Sadeghian and Sharafat, 2015)</ref>. However, they are not suited for other genre of text as user- product information are not generally available.</p><p>Attention models can be built not only from lo- cal text or user/product information but also from cognitive grounded data, especially eye-tracking data <ref type="bibr" target="#b28">(Rayner, 1998;</ref><ref type="bibr" target="#b0">Allopenna et al., 1998</ref>). Joshi (2014) proposes a novel metric called Sentiment Annotation Complexity for measuring sentiment annotation complexity based on eye-tracking data.  presents a cognitive study of senti- ment detection from the perspective of AI where readers are tested as sentiment readers. Mishra ( <ref type="bibr" target="#b24">Mishra et al., 2016b</ref>) recently proposes a model in sentiment analysis and sarcasm detection by using eye-tracking data as a feature in addition to text features using Naive-Bayes and SVM classifiers.</p><p>In other NLP tasks, <ref type="bibr" target="#b13">Joshi (2013)</ref> shows that Word-Sense-Disambiguation can make use of si- multaneous eye-tracking. Eye-tracking data are also used to measure the difficulty in translation annotation ( <ref type="bibr" target="#b21">Mishra et al., 2013)</ref>. <ref type="bibr" target="#b1">Barrett (2016)</ref> finds that gaze patterns during reading are strongly influenced by the role a word plays in terms of syn- tax, semantic, and discourse.</p><p>Among different available eye-tracking datasets, the Dundee corpus, GECO (the Ghent Eye-Tracking Corpus), and <ref type="bibr" target="#b24">Mishra et al. (Mishra et al., 2016b</ref>) are considered high-quality re- sources <ref type="bibr" target="#b15">(Kennedy, 2003;</ref><ref type="bibr" target="#b5">Cop et al., 2016;</ref><ref type="bibr" target="#b24">Mishra et al., 2016b</ref>). The Dundee corpus contains eye movement data from English and French news- papers <ref type="bibr" target="#b15">(Kennedy, 2003)</ref>. Measurements were taken while 10 participants read 20 newspaper articles. GECO is an English-Dutch bilingual corpus with eye-tracking data from 17 participants collected from reading the complete novel The Mysterious Affair at Styles. The corpus has 4,934 sentences, 774,015 tokens, and 9,876 words. The Mishra( <ref type="bibr" target="#b23">Mishra et al., 2016a</ref>) dataset contains 994 text snippets with 383 positive and 611 negative examples from newspaper clippings, sampled from seven native speakers.</p><p>To predict reading time using eye-tracking data, <ref type="bibr" target="#b37">Tomanek et al. (2010)</ref> proposes a regression model using linguistic features related to syntax and semantics for calibration. <ref type="bibr" target="#b14">Hahn (2016)</ref> pro- poses a novel approach to model both skipping and reading using unsupervised method which com- bines neural attention with auto-encoding trained on raw text using reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our proposed CBA model</head><p>The basic idea of our method is to add a CBA model into a neural-network based LSTM senti- ment classifier. Let D be a collection of docu-</p><formula xml:id="formula_0">ments. A document d k , d k ∈ D, has m number of sentences S 1 , S 2 , ...S j , ..., S m . A sentence S j is formed by a sequence of words S j = w j 1 w j 2 ...w j l j , where l j is the length of S j . The features of a word w i ∈ D form a feature vector v w i = [F 1 w i , F 2 w i ....F n w i ]</formula><p>where n is the feature space size. The purpose of document level sentiment classification is to project a document d k into the target space of L class labels. Similarly, at the sen- tence level, the purpose is to project a sentence S j into the target class space.</p><p>To build the CBA model, we need to first build a reading time prediction model for words within each sentence. Reading time is predicted based on word features and text features calibrated by eye- tracking data. Note that reading time from an eye- tracking dataset cannot be used directly because the text of any eye-tracking dataset is too small for sufficient coverage. Consequently, our method has four tasks: (1) to predict the reading time of words using eye-tracking data and v w i as features; <ref type="formula" target="#formula_3">(2)</ref> to build attention models based on predicted reading time at sentence level and document level; (3) to integrate attentions from other attention models; and <ref type="formula" target="#formula_5">(4)</ref> to add the attention model into the LSTM based sentiment classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling of reading time</head><p>To learn the reading time of words in a sentence, our method is based on regression analysis us- ing eye-tracking data as dependent variables and context information in v w∈S j as independent vari- ables. In the eye-tracking process, a number of different time measures such as first fixation dura- tion, gaze duration, and total reading time. In this work, we only use the total reading time.</p><p>Since a document set is always available for sentiment analysis, we use features extracted from these documents to train the regression model. We select features based on the works from Dem- berg <ref type="bibr">(2008)</ref> and <ref type="bibr" target="#b37">Tomanek (2010)</ref> to include word features such as word length and POS tags as well as context level syntax and semantic features such as the total number of dominated nodes in a depen- dency parsing three, the maximum dependency distance, semantic category etc..</p><p>Given a word w in a sentence S j , w ∈ S j , and its feature vector</p><formula xml:id="formula_1">v w∈S j = [F w 1 , F w 2 , ..., F w n ]</formula><p>where n is the dimension size in feature space, the regression model on eye-tracking data is a map- ping function g between reading time t w∈S j and v w∈S j as defined below:</p><formula xml:id="formula_2">t w∈S j = g(α 1 F w 1 + α 2 F w 2 + ... + α n F w n + b),<label>(1)</label></formula><p>where t w∈S j is the predicted reading time for w, α i is the weight of feature F w i , and b is a con- stant. Note that the set of α i (i = 1...n) forms the weight vector α w for t w∈S j . When v w∈S j takes scalar values, g can be an identity function and thus this model becomes a typical linear regres- sion model. When t w∈S j takes discrete values, g can be a logistic function and this model becomes a typical logistic regression model. we set g to be the identity function. The objec- tive function then becomes:</p><formula xml:id="formula_3">min α n a i ∈ α ||t w∈S j − y w∈S j || 2 2 + λR( α),<label>(2)</label></formula><p>where y w∈S j is the true eye-tracking values of reading time, R( α) is the regularization of α, and λ is the regularization weight. When λ = 0, the model degrades to a linear regression function. In this work, we evaluate the use of both the linear regression model and the Ridge regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Building the attention based model</head><p>Once we have predicted reading time for words used in sentences, the attention model can be built with two components. The first component works at the sentence level to give different words differ- ent emphasis in a sentence. The second compo- nent works at the document level to give different sentences different emphasis in a document.</p><p>For a sentence S j = w 1 w 2 ...w i ...w l j with length l j , each word w i in S j has a corresponding reading time t w i . Let t S j denote the total reading time of S j . Then,</p><formula xml:id="formula_4">t S j = l j i=1,w i ∈S j t w i .<label>(3)</label></formula><p>For sentence level attention, the CBA weight for w i in S j , denoted as A S j :w i , can be defined as:</p><formula xml:id="formula_5">A S j :w i = t w i t S j .<label>(4)</label></formula><p>This sentence level attention model defined above gives more weights to words that have longer reading time relative to the total reading time of the sentence.</p><p>Let a document d k , d k ∈ D, be formed by a set of sentences S j = w 1 w 2 ...w i ...w l j . Now the CBA weight for a sentence S j in d k is defined as:</p><formula xml:id="formula_6">A d k :S j = t s j m i=1 t S i .<label>(5)</label></formula><p>This aggregated document level attention model gives more weights to the sentences that have longer reading time relative to the total reading time of the document. Let A d k denote the doc- ument level attention weight vector. The size of A d k should be m, the number of sentences in d k .</p><p>Let S j denote the embedding of S j in N dimen- sional space, where S j ∈ d k . Then, the set of sen- tence representations for d k should be a matrix of size m × N , denoted byˆSbyˆ byˆS d k . After the inclusion of the attention model, ˆ S d k should be:</p><formula xml:id="formula_7">ˆ S d k = A d k S T j .<label>(6)</label></formula><p>Let</p><formula xml:id="formula_8">d k denote the document embedding of d k . Since d k is an N dimensional vector, d k can now</formula><p>be defined by the adjusted attention model as</p><formula xml:id="formula_9">( d k ) i = m j=1 ( ˆ S d k ) i,j .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Incorporation of other attention models</head><p>Since document embedding representation allows the combined use of multiple attention mecha- nisms, it is to our advantage to incorporate dif- ferent attention mechanisms which may help to capture different aspects of attentions. Generally speaking, different attention mechanisms can be incorporated either serially or in parallel.</p><p>In principle, any number of attention models can be included. As an an example to illustrate the capability of our proposed method, we choose one state-of-the-art local attention model(shorthanded as LA). The model is a semantic-based local at- tention model proposed by <ref type="bibr" target="#b40">Yang (2016)</ref> and also used by . For inclusion serially, the attention weight is formulated as follows:</p><formula xml:id="formula_10">A s S j : w i = LA S j :w i * A S j :w i ,<label>(8)</label></formula><p>where LA s j :w i the sentence level attention model by the local attention model. To incorporate LA in parallel mode, the attention weight can be formu- lated by:</p><formula xml:id="formula_11">A p S j : w i = LA S j : w i + A S j : w i .<label>(9)</label></formula><p>Similar methods can be used at document level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">General sentiment analysis model</head><p>We take the neural network based LSTM senti- ment classifier <ref type="bibr" target="#b8">(Gers, 2001</ref>) to be applied in both the sentence level and the document level because of its excellent performance on long sentences ( <ref type="bibr" target="#b33">Tang et al., 2015a</ref> At the sentence level, each word w i in a sen- tence S j is represented by its word embedding w i in the N dimensional space. The LSTM cell state c i and the hidden state h S j :w i can be updated in two steps. In the first step, the previous hidden state h S j :w i−1 uses a hyperbolic function to form c i as defined below.</p><formula xml:id="formula_12">c i = tanh( ˆ W c * [ h S j :w i−1 * w i ] + ˆ b),<label>(10)</label></formula><p>wherê W c is a parameter matrix, h S j :w i−1 is the previous hidden state and w i is the word vector. ˆ b is the regularization parameter matrix. In the sec- ond step, c i is updated by c i and its previous state c i−1 to form c i according to the below formula:</p><formula xml:id="formula_13">c i = f i c i−1 + i i c i .<label>(11)</label></formula><p>The hidden state of w i can be obtained by</p><formula xml:id="formula_14">h S j :w i = o i tanh( f i c i ).<label>(12)</label></formula><p>The forget gate f i is designed to keep the long term memory. A series of hidden states h 1 h 2 ... h i can serve as input to the attention layer to obtain sentence representation S j . In the document level, similar method is used to get the sentence matrixˆS matrixˆ matrixˆS in the document level LSTM layer to obtain the final document representation d k . In our work, the final document representation d k encodes both the sentence level information and the document level information. In the LSTM model, we use a hidden layer to project the final document vector</p><formula xml:id="formula_15">d f k through a hyperbolic function. d f k = tanh( ˆ W h d k + ˆ b h ),<label>(13)</label></formula><p>wherê W h is the hidden layer weight matrix andˆb andˆandˆb h is the regularization matrix.</p><p>Finally, sentiment prediction for any label lL obtained by the softmax function defined below:</p><formula xml:id="formula_16">P (y = l| d f k ) = e d f T k W l L l=1 e d f T k W l<label>(14)</label></formula><p>where W l is the softmax weight for each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance evaluation</head><p>Our proposed CBA for sentiment classification is evaluated on four document sets: The first three datasets IMDB, Yelp 13, and Yelp14 which are review texts including user/product informa- tion developed by Tang (2015a). The last dataset IMDB2 is a plain text by Maas (2011). All four datasets are tokenized through the Stanford NLP tool ( ). <ref type="table">Table 1</ref> list the statistics of the datasets in- cluding number of classes, number of docu- ments, and average length of sentence. We split train/development/test set in the rate of 8:1:1. The best configuration of the development dataset is used in the test set to obtain the final result. Two commonly used performance evaluation metrics are used. The first one is accuracy and the second one is rooted mean square error (RMSE) <ref type="bibr">3</ref> . Let GR i be the golden sentiment ratings, P R i be the predicted sentiment rating, and T be the num- ber of documents where GR i = P R i . Accuracy is then defined by</p><formula xml:id="formula_17">Accuracy = T N ,<label>(15)</label></formula><p>and RMSE is defined by</p><formula xml:id="formula_18">RM SE = N i=1 (GR i − P R i ) 2 * 1 N .<label>(16)</label></formula><p>We train the skip-gram word embedding ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) on each dataset separately to initialize the word vectors. All embedding sizes on the model are set to 200, a commonly used size.</p><p>Three sets of experiments are conducted. The first is on the selection of the regression model for reading time prediction. The second set of experi- ments compares our proposed CBA with another sentiment analysis method which use text only. The third set of experiments evaluates the effec- tiveness of combining different attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reading time prediction</head><p>The training for the regression model for reading time prediction using eye-tracking data requires the learning from text and context features as dis- cussed in Section 3.1. We compare our regression model with more complex deep learning based re- gression models in each of the three eye-tracking datasets. <ref type="bibr">4</ref> We take the first 90% of sentences as training data and the rest 10% as test data. The configu- ration that performs the best is selected and pred- icated on the document sentiment analysis dataset to obtain estimated reading time. Ideally, an eye- tracking corpus built from on-line reviews is more suitable for our experiments. But, we can only work with what is available.</p><p>In addition to the linear regression model(LL) and the Ridge regression model(RR), we also choose the Recurrent Neural Network (RNN) model and the Long Short Time Memory (LSTM) model for regression learning. For both models, there are two versions. The basic version inputs the extracted feature sets as word representation, labeled as RNN-1 and LSTM-1, respective. The second version takes word embedding <ref type="bibr" target="#b27">(Pennington et al., 2014</ref>) as the initial word representa- tion input, labeled as RNN-2 and LSTM-2, respec- tively. The RMSE results are listed in  <ref type="table" target="#tab_2">Table 2</ref>: RMSE for reading time predic- tion(Unit:Milliseconds)</p><p>Note that Ridge Regression(RR) has the best performance on all the three datasets because reg- ularization in RR reduces over-fitting problem.In three eye tracking datasets, the RR can achieve co- efficient of determination 5 of 0.32, 0.30 and 0.27 in three eye tracking datasets. The features, their types and the corresponding coefficients in RR are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The more complicated deep learning models suffer from serious over-fitting problem. And the result of Deep learning model with word embed- ding initialization partly supports the fact that the reading time are more depend on the micro level syntax and semantic feature for the word, such as number of letters in word and complexity score of the word instead of the deep level context features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of different sentiment classification methods</head><p>Because the features used in our model are all text based, we compare CBA with two groups   <ref type="table" target="#tab_2">Type Cofficient  Number of letters  Num  22.441  Start with capital letter  Bool  1.910  Capital letters only  Bool  161.580  Have alphanumeric letters  Bool  6.020  Is punctuation  Bool  -8.930  Is abbreviation  Bool  10.551  Is entity-critical word  Bool  7.612  Number of dominated</ref>  of baseline methods which also only use review text for learning. Group 1 methods include com- monly known linguistic and context features for SVM classifiers. Group 2 includes recent senti- ment classification algorithms which are top per- formers using review text for training including one method that uses local attention model. Be- low is the list of Group1 methods.</p><note type="other">nodes Num 0.980 Max dependency distance Num 1.982 Inverse document frequency Num -9.291 Number of senses in wordnet Num 7.494 Complexity score Num 57.240 Constant Num 239.910</note><p>• Majority -A simple majority based classi- fier based on sentence labels.</p><p>• Trigram -A SVM classifier using uni- grams/bigrams/trigram as features.</p><p>• Text feature -A SVM classifier using word level and context level features, such as n- gram and sentiment lexicons.</p><p>• AvgWordvec -A SVM classifier that takes the average of word embeddings in Word2Vec as document embedding.</p><p>Here is a list of Group 2 methods:</p><p>• SSWE ( <ref type="bibr" target="#b36">Tang et al., 2014</ref>) -A SVM clas- sifier using sentiment specific word embed- ding.</p><p>• RNTN+RNN (Socher et al., 2013) -A Re- cursive Neural Tensor Network(RNTN) to represent sentences and trained using RNN.</p><p>• Paragraph vector ( <ref type="bibr" target="#b17">Le and Mikolov, 2014</ref>) -A SVM classifier using document embed- ding as features.</p><p>• LSTM+LA ( ) -State-of- the-art LSTM using local context as attention mechanism in both sentence level and docu- ment level.  • LSTM+UPA (Chen et al., 2016) -State- of-the-art LSTM including LA as well as user/product as attention mechanism at both sentence level and document level.</p><p>Our proposed CBA model has several variations as explained below.</p><p>• LSTM+CBA -The LSTM classifier us- ing only CBA model at sentence level and document level. Based on the three eye-tracking datasets(GECO, DUNDEE and Mishra's) for reading time prediction, we label the same model by different training data as LSTM+CBA G ,LSTM+CBA D and LSTM+CBA M .</p><p>• LSTM+CBA+LA G -The LSTM based classifier using both the CBA model and the local text context based attention model(LA) . Since combining method can either be serial or in parallel, there are actually two corresponding variations: LSTM+CBA+LA G s and LSTM+CBA+LA G p .</p><p>• LSTM+CBA+UPA G -The same frame- work to LSTM+CBA+LA G with additional user/product attention. The two correspond- ing variations are LSTM+CBA+UPA G s and LSTM+CBA+UPA G p . <ref type="table" target="#tab_5">Table 4</ref> shows the performance of the three groups using review text without user/product in- formation on only the first three datasets meth- ods in Group 1 and Group 2 do not have evalua- tions on IMDB2. Among all the reference meth- ods that do not use any attention mechanism in- cluding all methods in Group 1 and Group 2(ex- cept LSTM+LA), LSTM is the best performer. LSTM+LA (2016), which is the state-of-the-art method, uses local attention mechanism to im- prove performance significantly. Among our CBA based variations, using the GECO dataset gives the best result outperforming LSTM+LA in all three datasets. LSTM+CBA G has significant improve- ment over LSTM+LA with p values of p &lt; 0.016 on IMDB, p &lt; 0.0019 on Yelp 13, and p &lt; 0.00023 on Yelp 14. LSTM+CBA G has the best result compared to the other two variations be- cause GECO has larger participant size. Its text genre is also closer to the review datasets for sen- timent analysis.</p><p>In the third set of experiment, we compare our LSTM+CBA model with the combination of other attention models including the LA model and the UPA model as shown in <ref type="table">Table 5</ref>. In the second set of experiment, since the GECO dataset gives the best performance, <ref type="table">Table 5</ref>   <ref type="table">Table 5</ref>: Evaluation on sentiment classification on using dual attention only if user/product information is available. Such data is provided in the first three sets of data. <ref type="table">Table 5</ref> shows that among all three single atten- tion models, UPA outperforms both LA and CBA in the first three datasets. This is easier to under- stand as UPA already included LA and it has more explicit information from users and products for its attention model compared to CBA which needs to learn hidden attention information. The com- bined method of CBA with UPA can still further improve performance. When CBA+UPA are com- bined in parallel, it has the best performance for both Yelp13 and Yelp14 (with p value of 0.027 and 0.032 respectively compare to LSTM+UPA). In the IMDB dataset, however, UPA has the best per- formance. This may be because user/product in- formation is more effective in movie review IMDB dataset which is more subjective.</p><p>However, the UPA model works only if user and product information is available. Thus for IMDB2 where user/product information is not available, only CBA and LA models work and the combined use of CBA+LA gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case study</head><p>A random sentence sample 'The Shelton hotel is lucky to receive 2stars from me considering ...' is taken from the Yelp13 dataset to demonstrate the difference in the two attention mechanisms, i.e. lo- cal text(LA), and cognition-based(CBA). <ref type="figure" target="#fig_2">Figure 1</ref> shows visually the difference in attention weights of the two models.</p><p>The attention weights of words in the LA model does not change much. CBA, on the other hand, gives higher weights to the sentiment linked word 2stars and the verb receive. This two words do play significant roles as an indirect object and a main verb, respectively. This case shows that CBA does a better job in capturing micro level informa- tion in the sentence level. This support the experi- mental results in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table">Table 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future works</head><p>In this paper, we propose a novel cognition based attention model to improve the state-of-the-art neural sentiment analysis model through cognition grounded eye-tracking data. A simple and effec- tive regression model is used to predict reading time using both eye-tracking data and local text features. The predicted reading time is then used to build an attention layer in neural sentiment anal- ysis models. The attention model considers both reading time and other syntactic and context fea- tures. It works in both the sentence level and the document level sentiment analysis.</p><p>Evaluation on benchmarking datasets validates the effectiveness of our method in sentiment anal- ysis as our method clearly outperforms other state- of-the-art methods that use local context informa- tion to build their attention models. Our CBA mechanism can also be combined with other at- tention mechanisms to provide room for further improvement. Future work includes using other eye-tracking information such as saccade and fix- ation. The incorporation of other information such as user-product information can also be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5</head><label></label><figDesc>https : //en.wikipedia.org/wiki/Coef f icientof d etermination</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature Name</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Case Study on attention weights</figDesc><graphic url="image-1.png" coords="8,307.28,264.55,226.77,175.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>GECO DUNDEE Mishra 
LR 
72.47 
73.52 
87.25 
RR 
69.47 
70.52 
84.22 
RNN-1 
75.47 
83.52 
96.23 
LSTM-1 79.47 
84.52 
114.25 
RNN-2 
79.57 
86.47 
101.25 
LSTM-2 83.88 
95.88 
122.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Major features used by the Ridge Regres-
sion Model 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>IMDB Yelp13 Yelp14 ACC RMSE ACC RMSE ACC RMSE</head><label></label><figDesc></figDesc><table>General baseline 
(Group 1) 

Majority 
0.196 
2.495 0.411 
1.060 0.392 1.097 
Trigram 
0.399 
1.783 0.569 
0.814 0.577 0.804 
TextFeature 
0.402 
1.793 0.556 
0.845 0.572 0.801 
AveWord2vec 
0.304 
1.985 0.526 
0.898 0.531 0.893 

Recently developed 
methods 
(Group 2) 

SSWE+SVM 
0.312 
1.973 0.549 
0.849 0.557 0.851 
Paragraph Vector 0.314 
1.814 0.554 
0.832 0.564 0.802 
RNTN+RNN 
0.401 
1.764 0.574 
0.804 0.582 0.821 
CLSTM 
0.421 
1.549 0.592 
0.769 0.594 0.766 
B-CLSTM 
0.462 
1.453 0.619 
0.705 0.592 0.741 
LSTM 
0.443 
1.465 0.627 
0.701 0.637 0.686 
LSTM+LA 
0.487 
1.381 0.631 
0.706 0.631 0.715 

CBA based models 
LSTM+CBA M 
0.447 
1.495 0.610 
0.746 0.613 0.768 
LSTM+CBA D 
0.468 
1.419 0.623 
0.706 0.628 0.702 
LSTM+CBA G 
0.489 
1.365 0.638 
0.697 0.641 0.678 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Evaluation on sentiment classification using review text for training 

• CLSTM (Xu et al., 2016) -Cached LSTM 
to capture the overall semantic information in 
long text. The two variations include regular 
CLSTM and bi-directional B-CLSTM. 

</table></figure>

			<note place="foot" n="1"> https://en.wikipedia.org/wiki/Eye-tracking</note>

			<note place="foot" n="3"> Normally accuracy is a problematic measure in highly unbalanced data sets. But in In IMDB, the largest class only takes less than 20% of all instances out of classes. The most imbalanced data are Yelp 13 whose largest class is 41% among 5 classes and second largest is about 30%. IMDB has a 50/50 split for 2-classes. 4 Mishra et.al (Mishra et al., 2016a) only provides fixation time. So, fixation time is used when training by this set of eye-tracking data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is partially supported by the research grants from Hong Kong Polytechnic University (PolyU RTVU) and GRF grant( CERG PolyU 15211/14E).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Paul D Allopenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of memory and language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="439" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised part-ofspeech tagging using eye-tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">584</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiment analysis: from opinion mining to human-agent interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoraida</forename><surname>Callejas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Presenting geco: An eyetracking corpus of monolingual and bilingual sentence reading. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uschi</forename><surname>Cop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Dirix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Drieghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Duyck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data from eyetracking corpora as evidence for theories of syntactic processing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentimental product recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>O&amp;apos;mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="411" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long short-term memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Universität Hannover</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intersubjectivity and sentiment: from language to knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2789" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05604</idno>
		<title level="m">Modeling human reading with neural attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="page" from="720" to="728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nivvedan Senthamilselvan, and Pushpak Bhattacharyya</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
	<note>Measuring sentiment annotation complexity of text</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">More than meets the eye: Study of human cognition in sense annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diptesh</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="733" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling human reading with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael Hahn Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The dundee corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>cd-rom</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Psychology</forename><surname>Department</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Dundee</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically predicting sentence translation difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibc</forename><surname>Critt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="346" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A cognitive study of subjectivity extraction in sentiment annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting readers&apos; sarcasm understandability by modeling gaze behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diptesh</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3747" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Leveraging cognitive features for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diptesh</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seema</forename><surname>Nagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuntal</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">156</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">372</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A model of appraisal in the emotion system: Integrating theory, research, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ira J Roseman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bag of words meets bags of popcorn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Reza</forename><surname>Sharafat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A cognitive cost model of annotations based on eye-tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Lohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1158" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A context-based model for sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vanzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2345" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cached long short-term memory neural networks for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuangjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04989</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
