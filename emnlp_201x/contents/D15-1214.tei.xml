<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity in Spectral Learning for Natural Language Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9LE</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9LE</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity in Spectral Learning for Natural Language Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F 1 score of 90.18, and for German we achieve the F 1 score of 83.38.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or re- combined in order to improve the accuracy in var- ious problems <ref type="bibr" target="#b13">(Henderson and Brill, 1999</ref>). Such problems include machine translation <ref type="bibr" target="#b20">(Macherey and Och, 2007)</ref>, syntactic parsing <ref type="bibr" target="#b3">(Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b30">Sagae and Lavie, 2006;</ref><ref type="bibr" target="#b11">Fossum and Knight, 2009;</ref><ref type="bibr" target="#b28">Petrov, 2010;</ref><ref type="bibr" target="#b4">Choe et al., 2015</ref>) and others <ref type="bibr" target="#b35">(Van Halteren et al., 2001</ref>).</p><p>The main argument behind the use of such a di- verse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. There- fore, recombination or reranking of solutions in that set will further optimize the choice of a solu- tion, combining together the information from all solutions.</p><p>In this paper, we explore another angle for the use of a set of parse tree predictions, where all pre- dictions are made for the same sentence. More specifically, we describe techniques to exploit di- versity with spectral learning algorithms for natu- ral language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the deriva- tion of word embeddings ( <ref type="bibr" target="#b19">Luque et al., 2012;</ref><ref type="bibr" target="#b7">Cohen et al., 2013;</ref><ref type="bibr" target="#b34">Stratos et al., 2014;</ref><ref type="bibr" target="#b9">Dhillon et al., 2015;</ref><ref type="bibr" target="#b29">Rastogi et al., 2015;</ref><ref type="bibr" target="#b25">Nguyen et al., 2015;</ref><ref type="bibr" target="#b18">Lu et al., 2015)</ref>. <ref type="bibr" target="#b7">Cohen et al. (2013)</ref> showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectation- maximization algorithm ( <ref type="bibr" target="#b23">Matsuzaki et al., 2005</ref>). Their result still lags behind state of the art in natu- ral language parsing, with methods such as coarse- to-fine ( <ref type="bibr" target="#b27">Petrov et al., 2006</ref>).</p><p>We further advance the accuracy of natural lan- guage parsing with spectral techniques and L- PCFGs, yielding a result that outperforms the orig- inal Berkeley parser from <ref type="bibr" target="#b26">Petrov and Klein (2007)</ref>. Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse.</p><p>The main contributions of this paper are two- fold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of <ref type="bibr" target="#b6">Cohen et al. (2012)</ref>, but simpler to understand and imple- ment. This algorithm has value for readers who are interested in learning more about spectral al- gorithms -it demonstrates some of the core ideas in spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models.</p><p>Second, we describe how a diverse set of predic- tors can be used with spectral learning techniques.</p><p>Our approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states. Our noise schemes are similar to those described by <ref type="bibr" target="#b37">Wang et al. (2013)</ref>. We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; <ref type="bibr" target="#b7">Cohen et al., 2013)</ref>, and repeat this process mul- tiple times. We then use the set of parses we get from all models in a recombination step.</p><p>The rest of the paper is organized as follows. In §2 we describe notation and background about L-PCFG parsing. In §3 we describe our new spec- tral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In §4 we describe the various noise schemes we use with our spectral algorithm and the spectral algorithm of <ref type="bibr" target="#b7">Cohen et al. (2013)</ref>. In §5 we describe how to decode with multiple mod- els, each arising from a different noise setting. In §6 we describe our experiments with natural lan- guage parsing for English and German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Notation</head><p>We denote by [n] the set of integers {1, . . . , n}.</p><p>For a statement Γ, we denote by <ref type="bibr">[[Γ]</ref>] its indicator function, with values 0 when the assertion is false and 1 when it is true. An L-PCFG is a 5-tuple (N , I, P, m, n) where:</p><p>• N is the set of nonterminal symbols in the grammar. I ⊂ N is a finite set of intermi- nals. P ⊂ N is a finite set of preterminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of nonter- minals into two subsets.</p><p>• [m] is the set of possible hidden states.</p><p>• [n] is the set of possible words.</p><p>• For all a ∈ I,</p><formula xml:id="formula_0">b ∈ N , c ∈ N , h 1 , h 2 , h 3 ∈ [m], we have a binary context-free rule a(h 1 ) → b(h 2 ) c(h 3 ).</formula><p>• For all a ∈ P, h ∈ [m], x ∈ [n], we have a lexical context-free rule a(h) → x.</p><p>Latent-variable PCFGs are essentially equiv- alent to probabilistic regular tree grammars (PRTGs; <ref type="bibr" target="#b16">Knight and Graehl, 2005</ref>) where the righthand side trees are of depth 1. With gen- eral PRTGs, the righthand side can be of arbitrary depth, where the leaf nodes of these trees corre- spond to latent states in the L-PCFG formulation above and the internal nodes of these trees corre- spond to interminal symbols in the L-PCFG for- mulation.</p><p>Two important concepts that will be used throughout of the paper are that of an "inside tree" and an "outside tree." Given a tree, the inside tree for a node contains the entire subtree below that node; the outside tree contains everything in the tree excluding the inside tree. See <ref type="figure" target="#fig_0">Figure 1</ref> for an example. Given a grammar, we denote the space of inside trees by T and the space of outside trees by O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clustering Algorithm for Estimating L-PCFGs</head><p>We assume two feature functions, φ : T → R d and ψ : O → R d , mapping inside and outside trees, respectively, to a real vector. Our training data consist of examples (a (i) , t (i) , o (i) , b (i) ) for i ∈ {1 . . . M }, where a (i) ∈ N ; t (i) is an inside tree; o (i) is an outside tree; and b (i) = 1 if a (i) is the root of tree, 0 otherwise. These are obtained by splitting all trees in the training set into inside and outside trees at each node in each tree. We then define Ω a ∈ R d×d :</p><formula xml:id="formula_1">Ω a = M i=1 [[a (i) = a]]φ(t (i) )(ψ(o (i) )) M i=1 [[a (i) = a]]<label>(1)</label></formula><p>This matrix is an empirical estimate for the cross-covariance matrix between the inside trees and the outside trees of a given nonterminal a. An inside tree and an outside tree are conditionally in- dependent according to the L-PCFG model, when the latent state at their connecting point is known. This means that the latent state can be identified by finding patterns that co-occur together in in- side and outside trees -it is the only random vari- able that can explain such correlations. As such, Inputs: An input treebank with the following additional in- formation: training examples (a (i) , t (i) , o (i) , b (i) ) for i ∈ {1 . . . M }, where a (i) ∈ N ; t (i) is an inside tree; o (i) is an outside tree; and b (i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature- vectors φ(t) ∈ R d . A function ψ that maps outside trees o to feature-vectors ψ(o) ∈ R d . An integer k denoting the thin-SVD rank. An integer m denoting the number of latent states.</p><p>Algorithm: (Step 1: Singular Value Decompositions)</p><p>• Calculate SVD on Ω a to getˆUgetˆ</p><formula xml:id="formula_2">getˆU a ∈ R (d×k) andˆVandˆ andˆV a ∈ R (d ×k) for each a ∈ N . (Step 1: Projection) • For all i ∈ [M ], compute y (i) = ( ˆ U a i ) φ(t (i) ) and z (i) = ( ˆ V a i ) ψ(o (i) ).</formula><p>• For all i ∈ <ref type="bibr">[M ]</ref>, set x (i) to be the concatenation of y (i) and z (i) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Step 2: Cluster Projections)</head><p>• For all a ∈ N , cluster the set</p><formula xml:id="formula_3">{x (i) | a (i) = a} to get a clustering function γ : R 2k → [m] that maps a projected vector x (i) to a cluster in [m].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Step 3: Compute Final Parameters)</head><p>• Annotate each node in the treebank with γ(x (i) ).</p><p>• Compute the probability of a rule</p><formula xml:id="formula_4">p(a[h1] → b[h2] c[h3] | a[h1])</formula><p>as the relative frequency of its ap- pearance in the cluster-annotated treebank.</p><p>•  if we reduce the dimensions of Ω a using singu- lar value decomposition (SVD), we essentially get representations for the inside trees and the outside trees that correspond to the latent states. This intuition leads to the algorithm that appears in <ref type="figure" target="#fig_2">Figure 2</ref>. The algorithm we describe takes as in- put training data, in the form of a treebank, decom- posed into inside and outside trees at each node in each tree in the training set.</p><p>The algorithm first performs SVD for each of the set of inside and outside trees for all nontermi- nals. <ref type="bibr">1</ref> This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words ( <ref type="bibr" target="#b9">Dhillon et al., 2015</ref>; <ref type="bibr">1</ref> We normalize features by their variance. <ref type="bibr" target="#b29">Rastogi et al., 2015)</ref>. The algorithm then takes the representations induced by the SVD step, and clusters them -we use k-means to do the clus- tering. Finally, it maps each SVD representation to a cluster, and as a result, gets a cluster identi- fier for each node in each tree in the training data. These clusters are now treated as latent states that are "observed." We subsequently follow up with frequency count maximum likelihood estimate to estimate the probabilities of each parameter in the L-PCFG.</p><p>Consider for example the estimation of rules of the form a → x. Following the clustering step we obtain for each nonterminal a and latent state h a set of rules of the form a[h] → x. Each such in- stance comes from a single training example of a lexical rule. Next, we compute the probability of the rule a[h] → x by counting how many times that rule appears in the training instances, and nor- malize by the total count of a[h] in the training instances. Similarly, we compute probabilities for binary rules of the form a → b c.</p><p>The features that we use for φ and ψ are sim- ilar to those used in <ref type="bibr" target="#b7">Cohen et al. (2013)</ref>. These features look at the local neighborhood surround- ing a given node. More specifically, we indicate the following information with the inside features (throughout these definitions assume that a → b c is at the root of the inside tree t):</p><p>• The pair of nonterminals (a, b). E.g., for the inside tree in <ref type="figure" target="#fig_0">Figure 1</ref> this would be the pair (VP, V).</p><p>• The pair (a, c). E.g., (VP, NP).</p><p>• The rule a → b c. E.g., VP → V NP.</p><p>• The rule a → b c paired with the rule at the node b. E.g., for the inside tree in <ref type="figure" target="#fig_0">Figure 1</ref> this would correspond to the tree fragment (VP (V saw) NP).</p><p>• The rule a → b c paired with the rule at the node c. E.g., the tree fragment (VP V (NP D N)).</p><p>• The head part-of-speech of t paired with a.</p><p>E.g., the pair (VP, V).</p><p>• The number of words dominated by t paired with a. E.g., the pair (VP, 3).</p><p>In the case of an inside tree consisting of a sin- gle rule a → x the feature vector simply indicates the identity of that rule.</p><p>For the outside features, we use:</p><p>• The rule above the foot node. E.g., for the outside tree in <ref type="figure" target="#fig_0">Figure 1</ref> this would be the rule S → NP VP * (the foot nonterminal is marked with * ).</p><p>• The two-level and three-level rule fragments above the foot node. These features are ab- sent in the outside tree in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>• The label of the foot node, together with the label of its parent. E.g., the pair (VP, S).</p><p>• The label of the foot node, together with the label of its parent and grandparent.</p><p>• The part-of-speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of the foot node.</p><p>• The width of the spans to the left and to the right of the foot node, paired with the label of the foot node.</p><p>Other Spectral Algorithms The SVD step on the Ω a matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG esti- mation algorithms. <ref type="bibr" target="#b6">Cohen et al. (2012)</ref> used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transfor- mation. Their algorithm generalizes the work of <ref type="bibr" target="#b14">Hsu et al. (2009)</ref> and <ref type="bibr" target="#b0">Bailly et al. (2010)</ref>. <ref type="bibr" target="#b5">Cohen and Collins (2014)</ref> also developed an al- gorithm that makes use of an SVD step on the inside-outside. It relies on the idea of "pivot features" -features that uniquely identify latent states.</p><p>Louis and Cohen (2015) used a clustering al- gorithm that resembles ours but does not sepa- rate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and lin- ear context-free rewriting systems. Their applica- tion was the analysis of hierarchical structure of conversations in online forums.</p><p>In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of <ref type="bibr" target="#b7">Cohen et al. (2013)</ref>. We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do hard clustering instead of soft cluster- ing. However, we detected that the clustering algo- rithm gives a more diverse set of solutions, when the features are perturbed. As such, in the next sections, we explain how to perturb the models we get from the clustering algorithm (and the spectral algorithm) in order to improve the accuracy of the clustering and spectral algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Spectral Estimation with Noise</head><p>It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP <ref type="bibr" target="#b13">(Henderson and Brill, 1999</ref>). Usually a k-best list from a single model is used to exploit model diversity. Instead, we es- timate multiple models, where the underlying fea- tures are filtered with various noising schemes.</p><p>We try three different types of noise schemes for the algorithm in Gaussian (additive): Let σ &gt; 0. For each x (i) , we draw a vector ε ∈ R 2k of Gaussians with mean 0 and variance σ 2 , and then set</p><formula xml:id="formula_5">x (i) ← x (i) + ε.</formula><p>Gaussian (multiplicative): Let σ &gt; 0. For each x (i) , we draw a vector ε ∈ R 2k of Gaussians with mean 0 and variance σ 2 , and then set</p><formula xml:id="formula_6">x (i) ← x (i) ⊗ (1 + ε),</formula><p>where ⊗ is coordinate-wise mul- tiplication.</p><p>Note the distinction between the dropout noise and the Gaussian noise schemes: the first is per- formed on the feature vectors before the SVD step, and the second is performed after the SVD step. It is not feasible to add Gaussian noise prior to the SVD step, since the matrix Ω a will no longer be sparse, and its SVD computation will be computa- tionally demanding.</p><p>Our use of dropout noise here is inspired by "dropout" as is used in neural network training, where various connections between units in the neural network are dropped during training in or- der to avoid overfitting of these units to the data ( <ref type="bibr" target="#b33">Srivastava et al., 2014</ref>).</p><p>The three schemes we described were also used by <ref type="bibr" target="#b37">Wang et al. (2013)</ref> to train log-linear models. <ref type="bibr">Wang et al.'</ref>s goal was to prevent overfitting by introducing this noise schemes as additional reg- ularizer terms, but without explicitly changing the training data. We do filter the data through these noise schemes, and show in §6 that all of these noise schemes do not improve the performance of our estimation on their own. However, when mul- tiple models are created with these noise schemes, and then combined together, we get an improved performance. As such, our approach is related to the one of <ref type="bibr" target="#b28">Petrov (2010)</ref>, who builds a commit- tee of latent-variable PCFGs in order to improve a natural language parser.</p><p>We also use these perturbation schemes to cre- ate multiple models for the algorithm of <ref type="bibr" target="#b6">Cohen et al. (2012)</ref></p><note type="other">. The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure. After noising the pro- jections of the inside and outside feature functions we get from the SVD step, we use these projected noised features as a new set of inside and outside feature functions, and re-run the spectral algorithm of Cohen et al. (2012) on them.</note><p>We are required to add this extra SVD step be- cause the spectral algorithm of Cohen et al. as- sumes the existence of linearly transformed pa- rameter estimates, where the parameters of each nonterminal a is linearly transformed by unknown invertible matrices. These matrices cancel out when the inside-outside algorithm is run with the spectral estimate output. In order to ensure that these matrices still exactly cancel out, we have to follow with another SVD step as described above. The latter SVD step is performed on a dense Ω a ∈ R m×m but this is not an issue considering m (the number of latent states) is much smaller than d or d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Decoding with Multiple Models</head><p>Let G 1 , . . . , G p be a set of L-PCFG grammars. In §6, we create such models using the noising tech- niques described above. The question that remains is how to combine these models together to get a single best output parse tree given an input sen- tence.</p><p>With L-PCFGs, decoding a single sentence re- quires marginalizing out the latent states to find the best skeletal tree 2 for a given string. Let s be a sentence. We define t(G i , s) to be the output tree according to minimum Bayes risk decoding. This means we follow Goodman (1996), who uses dy- namic programming to compute the tree that maxi- mizes the sum of all marginals of all nonterminals in the output tree. Each marginal, for each span a, i, j (where a is a nonterminal and i and j are endpoints in the sentence), is computed by using the inside-outside algorithm.</p><p>In addition, let µ(a, i, j|G k , s) be the marginal, as computed by the inside-outside algorithm, for the span a, i, j with grammar G k for string s. We use the notation a, i, j ∈ t to denote that a span a, i, j is in a tree t.</p><p>We suggest the following three ways for decod- ing with multiple models G 1 , . . . , G p :</p><p>Maximal tree coverage: Using dynamic pro- gramming, we return the tree that is the solution to:</p><formula xml:id="formula_7">t * = arg max t a,i,j∈t p k=1 [[a, i, j ∈ t(G k , s)]].</formula><p>This implies that we find the tree that maximizes its coverage with respect to all other trees that are decoded using G 1 , . . . , G p .</p><p>Maximal marginal coverage: Using dynamic programming, we return the tree that is the solution to:</p><formula xml:id="formula_8">t * = arg max t a,i,j∈t p k=1 µ(a, i, j|G k , s).</formula><p>This is similar to maximal tree coverage, only in- stead of considering just the single decoded tree for each model among G 1 , . . . , G p , we make our decoding "softer," and rely on the marginals that each model gives.</p><p>MaxEnt reranking: We train a MaxEnt reranker on a training set that includes outputs from mul- tiple models, and then, during testing time, de- code with each of the models, and use the trained reranker to select one of the parses. We use the reranker of Charniak and Johnson (2005). 3</p><p>As we see later in §6, it is sometimes possible to extract more information from the training data by using a network, or a hierarchy of the above tree combination methods. For example, we get our best result for parsing by first using MaxEnt with several subsets of the models, and then combining the output of these MaxEnt models using maximal tree coverage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we describe parsing experiments with two languages: English and German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results for English</head><p>For our English parsing experiments, we use a standard setup. More specifically, we use the Penn WSJ treebank <ref type="bibr" target="#b21">(Marcus et al., 1993</ref>) for our experi- ments, with sections 2-21 as the training data, and section 22 used as the development data. Section 23 is used as the final test set. We binarize the trees in training data, but transform them back be- fore evaluating them.</p><p>For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic pro- gramming chart. The parser takes part-of-speech tagged sentences as input. We tag all datasets us- ing Turbo Tagger ( <ref type="bibr" target="#b22">Martins et al., 2010)</ref>, trained on sections 2-21. We use the F 1 measure according to the PARSEVAL metric ( <ref type="bibr">Black et al., 1991)</ref> for the evaluation.</p><p>Preliminary experiments We first experiment with the number of latent states for the clustering algorithm without perturbations. We use k = 100 for the SVD step. Whenever we need to cluster a set of points, we run the k-means algorithm 10 times with random restarts and choose the clus- tering result with the lowest objective value. On section 22, the clustering algorithm achieves the following results (F 1 measure): m = 8: 84.30%, m = 16: 85.98%, m = 24: 86.48%, m = 32: 85.84%, m = 36: 86.05%, m = 40: 85.43%. As we increase the number of states, performance improves, but plateaus at m = 24. For the rest of our experiments, both with the spectral algorithm of <ref type="bibr" target="#b6">Cohen et al. (2012)</ref> and the clustering algorithm presented in this paper, we use m = 24.</p><p>Compact models One of the advantage of the clustering algorithm is that it leads to much more compact models. The number of nonzero param- eters with m = 24 for the clustering algorithm is approximately 97K, while the spectral algorithms lead to a significantly larger number of nonzero parameters with the same number of latent states: approximately 54 million.</p><p>Oracle experiments To what extent do we get a diverse set of solutions from the different mod- els we estimate? This question can be answered by testing the oracle accuracy in the different settings. For each type of noising scheme, we generated 80 models, 20 for each σ ∈ {0.05, 0.1, 0.15, 0.2}. Each noisy model by itself lags behind the best model (see <ref type="figure" target="#fig_4">Figure 3)</ref>. However, when choosing the best tree among these models, the additively- noised models get an oracle accuracy of 95.91% on section 22; the multiplicatively-noised models get an oracle accuracy of 95.81%; and the dropout- noised models get an oracle accuracy of 96.03%. Finally all models combined get an oracle accu- racy of 96.67%. We found out that these oracle scores are comparable to the one Charniak and Johnson (2005) report.</p><p>We also tested our oracle results, comparing the spectral algorithm of <ref type="bibr" target="#b7">Cohen et al. (2013)</ref> to the clustering algorithm. We generated 20 mod- els for each type of noising scheme, 5 for each σ ∈ {0.05, 0.1, 0.15, 0.2}) for the spectral al- gorithm. <ref type="bibr">4</ref> Surprisingly, even though the spectral models were smoothed, their oracle accuracy was lower than the accuracy of the clustering algo-  <ref type="table">Table 1</ref>: Results on section 22 (WSJ). MaxTre denotes decoding using maximal tree coverage, MaxMrg denotes decoding using maximal marginal coverage, and MaxEnt denotes the use of a discriminative reranker. Add, Mul and Dropout denote the use of additive Gaussian noise, multiplicative Gaussian noise and dropout noise, respectively. The number of models used in the first three rows for the clustering algorithm is 80: 20 for each σ ∈ {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each σ (see footnotes). The number of latent states is m = 24. For All, we use all models combined from the first three rows. The "No noise" baseline for the spectral algorithm is taken from <ref type="bibr" target="#b7">Cohen et al. (2013)</ref>. The best figure in each algorithm block is in boldface.</p><note type="other">Clustering Spectral (smoothing) Spectral (no smoothing) MaxTre MaxMrg</note><p>rithm: 92.81% vs. 95.73%. <ref type="bibr">5</ref> This reinforces two ideas: (i) that noising acts as a regularizer, and has a similar role to backoff smoothing, as we see be- low; and (ii) the noisy estimation for the clustering algorithm produces a more diverse set of parses than that produced with the spectral algorithm.  <ref type="table">Table 2</ref>: Results on section 23 (English). The first three results (Best) are taken with the best model in each corresponding block in <ref type="table">Table 1</ref>. The last three results (Hier) use a hierarchy of the above tree combination methods in each block. It com- bines all MaxEnt results using the maximal tree coverage (see text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>It is also important to note that the high ora- cle accuracy is not just the result of k-means not finding the global maximum for the clustering ob- jective. If we just run the clustering algorithms with 80 models as before, without perturbing the features, the oracle accuracy is 95.82%, which is lower than the oracle accuracy with the additive and dropout perturbed models. To add to this, we see below that perturbing the training set with the spectral algorithm of Cohen et al. improves the ac-curacy of the spectral algorithm. Since the spectral algorithm of Cohen et al. does not maximize any objective locally, it shows that the role of the per- turbations we use is important.</p><p>Results Results on the development set are given in <ref type="table">Table 1</ref> with our three decoding methods. We present the results from three algorithms: the clustering algorithm and the spectral algorithms (smoothed and unsmoothed). <ref type="bibr">6</ref> It seems that dropout noise for the spectral algo- rithm acts as a regularizer, similarly to the back- off smoothing techniques that are used in <ref type="bibr" target="#b7">Cohen et al. (2013)</ref>. This is evident from the two spectral algorithm blocks in <ref type="table">Table 1</ref>, where dropout noise does not substantially improve the smoothed spec- tral model (Cohen et al. report accuracy of 88.53% with smoothed spectral model for m = 24 without noise) -the accuracy is 88.64%-88.71%-89.47%, but the accuracy substantially improves for the un- smoothed spectral model, where dropout brings an accuracy of 86.47% up to 89.52%.</p><p>All three blocks in <ref type="table">Table 1</ref> demonstrate that decoding with the MaxEnt reranker performs the best. Also it is interesting to note that our results continue to improve when combining the output of previous combination steps further. The best re- sult on section 22 is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in <ref type="table">Ta</ref>  <ref type="table">Table 3</ref>: Results on the development set for German. See <ref type="table">Table 1</ref> for interpretation of MaxTre, MaxMrg, MaxEnt and Add, Mul and Dropout. The number of models used in the first three rows for the clustering algorithm is 80: 20 for each σ ∈ {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each σ.</p><p>The number of latent states is m = 8. For All, we use all models combined from the first three rows. The best figure in each algorithm block is in boldface.</p><p>ble 1). This yields a 90.68% F 1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See <ref type="table">Table 2</ref> for results on section 23.</p><p>Our results are comparable to state-of-the-art results for parsing. For example, <ref type="bibr" target="#b30">Sagae and Lavie (2006)</ref>, <ref type="bibr" target="#b11">Fossum and Knight (2009)</ref> and  report an accuracy of 93.2%-93.3% us- ing parsing recombination; <ref type="bibr" target="#b31">Shindo et al. (2012)</ref> report an accuracy of 92.4 F 1 using a Bayesian tree substitution grammar; <ref type="bibr" target="#b28">Petrov (2010)</ref> reports an accuracy of 92.0% using product of L-PCFGs; <ref type="bibr" target="#b3">Charniak and Johnson (2005)</ref> report accuracy of 91.4 using a discriminative reranking model; <ref type="bibr" target="#b2">Carreras et al. (2008)</ref> report 91.1 F 1 accuracy for a discriminative, perceptron-trained model; <ref type="bibr" target="#b26">Petrov and Klein (2007)</ref> report an accuracy of 90.1 F 1 . <ref type="bibr" target="#b8">Collins (2003)</ref> reports an accuracy of 88.2 F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results for German</head><p>For the German experiments, we used the NEGRA corpus ( <ref type="bibr" target="#b32">Skut et al., 1997</ref>). We use the same setup as in <ref type="bibr" target="#b28">Petrov (2010)</ref>, and use the first 18,602 sen- tences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank.</p><p>Our German experiments follow the same set- ting as in our English experiments. For the clus- tering algorithm we generated 80 models, 20 for each σ ∈ {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, we generate 20 models, 5 for each σ.</p><p>For the reranking experiment, we had to modify the BLLIP parser <ref type="bibr" target="#b3">(Charniak and Johnson, 2005)</ref> to use the head features from the German tree- bank. We based our modifications on the docu- mentation for the NEGRA corpus (our modifica- tions are based mostly on mapping of nontermi- nals to coarse syntactic categories).</p><p>Preliminary experiments For German, we also experiment with the number of latent states. On the development set, we observe that the F 1 mea- sure is: 75.04% for m = 8, 73.44% for m = 16 and 70.84% for m = 24. For the rest of our experi- ments, we fix the number of latent states at m = 8.  <ref type="table">Table 4</ref>: Results on the test set for the German data. The first three results (Best) are taken with the best model in each corresponding block in Ta- ble 3. The last three results (Hier) use a hierarchy of the above tree combination methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Oracle experiments The additively-noised models get an oracle accuracy of 90.58% on the development set; the multiplicatively-noised models get an oracle accuracy of 90.47%; and the dropout-noised models get an oracle accuracy of 90.69%. Finally all models combined get an oracle accuracy of 92.38%. We compared our oracle results to those given by the spectral algorithm of <ref type="bibr" target="#b7">Cohen et al. (2013)</ref>. With 20 models for each type of noising scheme, all spectral models combined achieve an oracle ac- curacy of 83.45%. The clustering algorithm gets the oracle score of 90.12% when using the same number of models.</p><p>Results Results on the development set and on the test set are given in <ref type="table">Table 3 and Table 4</ref> re- spectively.</p><p>Like English, in all three blocks in <ref type="table">Table 3</ref>, de- coding with the MaxEnt reranking performs the best. Our results continue to improve when fur- ther combining the output of previous combina- tion steps. The best result of 82.04% on the devel- opment set is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in <ref type="table">Table 3</ref>). This also leads to the best result of 83.38% on the test set. See <ref type="table">Table 4</ref> for results on the test set.</p><p>Our results are comparable to state-of-the-art results for German parsing. For example, <ref type="bibr" target="#b28">Petrov (2010)</ref> reports an accuracy of 84.5% using prod- uct of L-PCFGs; <ref type="bibr" target="#b26">Petrov and Klein (2007)</ref> report an accuracy of 80. <ref type="bibr">1 F 1 ;</ref><ref type="bibr" target="#b10">and Dubey (2005)</ref> reports an accuracy of 76.3 F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>From a theoretical point of view, one of the great advantages of spectral learning techniques for latent-variable models is that they yield consis- tent parameter estimates. Our clustering algorithm for L-PCFG estimation breaks this, but there is a work-around to obtain an algorithm which would be statistically consistent.</p><p>The main reason that our algorithm is not a con- sistent estimator is that it relies on k-means clus- tering, which maximizes a non-convex objective using hard clustering steps. The k-means algo- rithm can be viewed as "hard EM" for a Gaussian mixture model (GMM), where each latent state is associated with one of the mixture components in the GMM. This means that instead of following up with k-means, we could have identified the param- eters and the posteriors for a GMM, where the ob- servations correspond to the vectors that we clus- ter. There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees ( <ref type="bibr" target="#b36">Vempala and Wang, 2004;</ref><ref type="bibr" target="#b15">Kannan et al., 2005;</ref><ref type="bibr" target="#b24">Moitra and Valiant, 2010</ref>).</p><p>With theoretical guarantees on the correctness of the posteriors from this step, the subsequent use of maximum likelihood estimation step could yield consistent parameter estimates. The con- sistency guarantees will largely depend on the amount of information that exists in the base fea- ture functions about the latent states according to the L-PCFG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a novel estimation algorithm for latent-variable PCFGs. This algorithm is based on clustering of continuous tree representations, and it also leads to sparse grammar estimates and compact models. We also showed how to get a diverse set of parse tree predictions with this algo- rithm and also older spectral algorithms. Each pre- diction in the set is made by training an L-PCFG model after perturbing the underlying features that estimation algorithm uses from the training data. We showed that such a diverse set of predictions can be used to improve the parsing accuracy of En- glish and German.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The inside tree (left) and outside tree (right) for the nonterminal VP in the parse tree (S (NP (D the) (N dog)) (VP (V saw) (NP (D the) (N woman)))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Similarly, compute the root probabilities π(a[h]) and preterminal rules p(a[h] → x | a[h]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The clustering estimation algorithm for L-PCFGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Dropout noise: Let σ ∈ [0, 1]. We set each el- ement in the feature vectors φ(t) and ψ(o) to 0 with probability σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: F 1 scores of noisy models. Each data point gives the F 1 accuracy of a single model on the development set, based on the legend. The xaxis enumerates the models (80 in total for each noise scheme).</figDesc><graphic url="image-1.png" coords="6,72.00,62.81,240.38,242.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>-</head><label></label><figDesc></figDesc><table>Clustering 

Spectral (smoothing) 
Spectral (no smoothing) 
MaxTre MaxMrg MaxEnt 
MaxTre MaxMrg MaxEnt 
MaxTre MaxMrg MaxEnt 
Add 
77.34 
76.87 
80.01 
77.76 
77.85 
78.09 
77.44 
77.56 
77.91 
Mul 
77.80 
77.80 
80.34 
77.80 
77.76 
78.89 
77.62 
77.85 
78.94 
Dropout 
77.37 
77.17 
80.94 
77.94 
78.06 
79.02 
77.97 
78.17 
79.18 
All 
77.71 
77.51 
80.86 
78.04 
77.89 
79.46 
77.73 
77.91 
79.66 
No noise 
75.04 
77.71 
77.07 

</table></figure>

			<note place="foot" n="2"> A skeletal tree is a derivation tree without latent states decorating the nonterminals.</note>

			<note place="foot" n="3"> Implementation: https://github.com/BLLIP/ bllip-parser. More specifically, we used the programs extract-spfeatures, cvlm-lbfgs and best-indices. cvlm-lbfgs was used with the default hyperparameters from the Makefile.</note>

			<note place="foot" n="4"> There are two reasons we use a smaller number of models with the spectral algorithm: (a) models are not compact (see text) and (b) as such, parsing takes comparatively longer. However, in the above comparison, we use 20 models for the clustering algorithm as well.</note>

			<note place="foot" n="5"> Oracle scores for the clustering algorithm: 95.73% (20 models for each noising scheme) and 96.67% (80 models for each noising scheme).</note>

			<note place="foot" n="6"> Cohen et al. (2013) propose two variants of spectral estimation for L-PCFGs: smoothed and unsmoothed. The smoothed model uses a simple backedoff smoothing method which leads to significant improvements over the unsmoothed one. Here we compare our clustering algorithm against both of these models. However unless specified otherwise, the spectral algorithm of Cohen et al. (2013) refers to their best model, i.e. the smoothed model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank David McClosky for his help with running the BLLIP parser and the three anonymous reviewers for their helpful com-ments. This research was supported by an EPSRC grant (EP/L02411X/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A spectral approach for probabilistic grammatical inference on trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ALT</title>
		<meeting>ALT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezra</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Gdaniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Ingria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">L</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Y</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DARPA Workshop on Speech and Natural Language</title>
		<meeting>DARPA Workshop on Speech and Natural Language</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Syntactic parse fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Do Kook Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A provably correct learning algorithm for latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eigenwords: Spectral word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramveer</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What to do when lexicalization fails: Parsing German with suffix analysis and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining constituent parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting diversity in natural language processing: Combining parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The spectral method for general mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Salmasian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3559</biblScope>
			<biblScope unit="page" from="444" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An overview of probabilistic tree transducers for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics and intelligent text processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3406</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conversation trees: A grammar model for topic structure in forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep multilingual correlation for improved word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral learning for nondeterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An empirical study on computing consensus translations from multiple machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TurboParsers: Dependency parsing by approximate variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Settling the polynomial learnability of mixtures of gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>IEEE Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Is your anchor going up or down? Fast and accurate supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiview LSA: Representation learning via generalized CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parser combination by reparsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An annotation scheme for free word order languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigitte</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ANLP</title>
		<meeting>ANLP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Kyum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving accuracy in word class tagging through the combination of machine learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Zavrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="229" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="841" to="860" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature noising for log-linear structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">K-best combination of syntactic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew</forename><surname>Lim Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
