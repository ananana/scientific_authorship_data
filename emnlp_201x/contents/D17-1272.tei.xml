<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counterfactual Learning from Bandit Feedback under Deterministic Logging: A Case Study in Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolin</forename><surname>Lawrence</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Development Center &amp;</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Counterfactual Learning from Bandit Feedback under Deterministic Logging: A Case Study in Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2566" to="2576"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deter-ministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from determinis-tic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Commercial SMT systems allow to record large amounts of interaction log data at no cost. Such logs typically contain a record of the source, the translation predicted by the system, and the user feedback. The latter can be gathered di- rectly if explicit user quality ratings of transla- tions are supported, or inferred indirectly from the interaction of the user with the translated content. Indirect feedback in form user clicks on displayed ads has been shown to be a valu- able feedback signal in response prediction for display advertising ( <ref type="bibr">Bottou et al., 2013</ref>). Similar to the computational advertising scenario, one could imagine a scenario where SMT systems are optimized from partial information in form of user feedback to predicted translations, instead of from manually created reference translations. This learning scenario has been investigated in the areas of bandit learning <ref type="bibr">(Bubeck and CesaBianchi, 2012</ref>) or reinforcement learning (RL) <ref type="bibr" target="#b24">(Sutton and Barto, 1998)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the learning protocol using the terminology of ban- dit structured prediction ( <ref type="bibr" target="#b22">Sokolov et al., 2016;</ref><ref type="bibr" target="#b11">Kreutzer et al., 2017</ref>), where at each round, a system (corresponding to a policy in RL terms) makes a prediction (also called action in RL, or pulling an arm of a bandit), and receives a re- ward, which is used to update the system. Counterfactual learning attempts to reuse ex- isting interaction data where the predictions have been made by a historic system different from the target system. This enables offline or batch learning from logged data, and is important if online experiments that deploy the target system are risky and/or expensive. Counterfactual learn- ing tasks include policy evaluation, i.e. estimat- ing how a target policy would have performed if it had been in control of choosing the predictions for which the rewards were logged, and policy optimization (also called policy learning), i.e. optimizing parameters of a target policy given the logged data from the historic system. Both tasks are called counterfactual, or off-policy in RL terms, since the target policy was actually not in control during logging. <ref type="figure" target="#fig_1">Figure 2</ref> shows the learning protocol for off-policy learning from partial feedback. The crucial trick to obtain unbiased estimators to evaluate and to optimize the off-policy sys- tem is to correct the sampling bias of the log- ging policy. This can be done by importance sampling where the estimate is corrected by the inverse propensity score <ref type="bibr" target="#b20">(Rosenbaum and Rubin, 1983</ref>) of the historical algorithm, mitigating the problem that predictions there were favored by the historical system are over-represented in the logs. As shown by <ref type="bibr" target="#b12">Langford et al. (2008)</ref> or <ref type="bibr" target="#b23">Strehl et al. (2010)</ref>, a sufficient exploration of the output space by the logging system is a prerequisite for counterfactual learning. If the logging policy acts stochastically in predicting outputs, this condition is satisfied, and inverse propensity scoring can be applied to correct the sampling bias. However, commercial SMT sys- tems usually try to avoid any risk and only log the most probable translation. This effectively results in deterministic logging policies, making theory and practice of off-policy methods inap- plicable to counterfactual learning in SMT.</p><p>This paper presents a case study in counter- factual learning for SMT that shows that pol- icy optimization from deterministic bandit logs is possible despite these seemingly contradic- tory theoretical requirements. We formalize our learning problem as an empirical risk minimiza- tion over logged data. While a simple empiri- cal risk minimizer can show degenerate behav- ior where the objective is minimized by avoiding or over-representing training samples, thus suf- fering from decreased generalization ability, we show that the use of control variates can remedy this problem. Techniques such as doubly-robust policy evaluation and learning <ref type="bibr" target="#b3">(Dudik et al., 2011</ref>) or weighted importance sampling <ref type="bibr" target="#b7">(Jiang and Li, 2016;</ref><ref type="bibr" target="#b27">Thomas and Brunskill, 2016)</ref> can be interpreted as additive <ref type="bibr" target="#b21">(Ross, 2013)</ref> or multi- plicative control variates <ref type="bibr" target="#b10">(Kong, 1992</ref>) that serve for variance reduction in estimation. We observe that a further effect of these techniques is that of smoothing out deterministic components by taking the whole output space into account. Fur- thermore, we conjecture that while outputs are logged deterministically, the stochastic selection of inputs serves as sufficient exploration in pa- rameter optimization over a joint feature repre- sentation over inputs and outputs. We present experiments using simulated bandit feedback for two different SMT tasks, showing improvements of up to 2 BLEU in SMT domain adaptation from deterministically logged bandit feedback. This result, together with a comparison to the standard case of policy learning from stochas- tically logged simulated bandit feedback, con- firms the effectiveness our proposed techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Counterfactual learning has been known under the name of off-policy learning in various fields that deal with partial feedback, namely contex- tual bandits ( <ref type="bibr" target="#b12">Langford et al. (2008)</ref>; <ref type="bibr" target="#b23">Strehl et al. (2010)</ref>; <ref type="bibr" target="#b3">Dudik et al. (2011)</ref>; <ref type="bibr" target="#b13">Li et al. (2015)</ref>, inter alia), reinforcement learning ( <ref type="bibr" target="#b24">Sutton and Barto (1998)</ref>; <ref type="bibr" target="#b19">Precup et al. (2000)</ref>; <ref type="bibr" target="#b7">Jiang and Li (2016)</ref>; <ref type="bibr" target="#b27">Thomas and Brunskill (2016)</ref>, inter alia), and structured prediction <ref type="bibr">(Swaminathan and Joachims (2015a,b)</ref>, inter alia). The idea behind these approaches is to first perform pol- icy evaluation and then policy optimization, un- der the assumption that better evaluation leads to better optimization. Our work puts a focus on policy optimization in an empirical risk mini- mization framework for deterministically logged data. Since our experiment is a simulation study, we can compare the deterministic case to the standard scenario of policy optimization and evaluation under stochastic logging.</p><p>Variance reduction by additive control variates has implicitly been used in doubly robust tech- niques <ref type="bibr" target="#b3">(Dudik et al., 2011;</ref><ref type="bibr" target="#b7">Jiang and Li, 2016)</ref>. However, the connection to Monte Carlo tech- niques has not been made explicit until <ref type="bibr" target="#b27">Thomas and Brunskill (2016)</ref>, nor has the control vari- ate technique of optimizing the variance reduc- tion by adjusting a linear interpolation scalar <ref type="bibr" target="#b21">(Ross, 2013)</ref> been applied in off-policy learn- ing. Similarly, the technique of weighted im- portance sampling has been used as variance re- duction technique in off-policy learning ( <ref type="bibr" target="#b19">Precup et al., 2000;</ref><ref type="bibr" target="#b7">Jiang and Li, 2016;</ref><ref type="bibr" target="#b27">Thomas and Brunskill, 2016)</ref>. The connection to multiplica- tive control variates <ref type="bibr" target="#b10">(Kong, 1992)</ref> has been made explicit in <ref type="bibr" target="#b26">Swaminathan and Joachims (2015b)</ref>. To our knowledge, our analysis of both control variate techniques from the perspective of avoid- ing degenerate behavior in learning from deter- ministically logged data is novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Counterfactual Learning from</head><p>Deterministic Bandit Logs Problem Definition. The problem of counter- factual learning (in the following used in the sense of counterfactual optimization) for ban- dit structured prediction can be described as fol- lows: Let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆ : Y → [0, 1] be a reward func- tion (and δ = −∆ be the corresponding task loss function) 1 quantifying the quality of struc- tured outputs. We are given a data log of triples D = {(x t , y t , δ t )} n t=1 where outputs y t for in- puts x t were generated by a logging system, and loss values δ t were observed only at the gener- ated data points. In case of stochastic logging with probability π 0 , the inverse propensity scor- ing approach <ref type="bibr" target="#b20">(Rosenbaum and Rubin, 1983</ref>) uses importance sampling to achieve an unbiased es- timate of the expected loss under the parametric target policy π w :</p><formula xml:id="formula_0">ˆ R IPS (π w ) = 1 n n t=1 δ t π w (y t |x t ) π 0 (y t |x t ) (1) ≈ E p(x) E π 0 (y|x) [δ(y) π w (y|x) π 0 (y|x) ] = E p(x) E πw(y|x) [δ(y)].</formula><p>In case of deterministic logging, we are con- fined to empirical risk minimization:</p><formula xml:id="formula_1">ˆ R DPM (π w ) = 1 n n t=1 δ t π w (y t |x t ).<label>(2)</label></formula><p>Equation <ref type="formula" target="#formula_1">(2)</ref> assumes deterministically logged outputs with propensity π 0 = 1, t = 1, . . . , n of the historical system. We call this objective the deterministic propensity matching (DPM) objec- tive since it matches deterministic outputs of the logging system to outputs in the n-best list of the target system. For optimization under deter- ministic logging, a sampling bias is unavoidable since objective (2) does not correct it by impor- tance sampling. Furthermore, the DPM estima- tor may show a degenerate behavior in learning. This problem can be remedied by the use of con- trol variates, as we will discuss in Section 5.</p><p>Learning Principle: Doubly Controlled Em- pirical Risk Minimization. Our first modifi- cation of Equation <ref type="formula" target="#formula_1">(2)</ref> has been originally moti- vated by the use of weighted importance sam- pling in inverse propensity scoring because of <ref type="bibr">1</ref> We will use both terms, reward and loss, in order to be consistent with the respective literature. its observed stability and variance reduction ef- fects ( <ref type="bibr" target="#b19">Precup et al., 2000;</ref><ref type="bibr" target="#b7">Jiang and Li, 2016;</ref><ref type="bibr" target="#b27">Thomas and Brunskill, 2016)</ref>. We call this ob- jective the reweighted deterministic propensity matching (DPM+R) objective:</p><formula xml:id="formula_2">2568ˆR 2568ˆ 2568ˆR DPM = 1 n n t=1 δ t π w (y t |x t ) log π w (y t |x t ). ˆ R DPM+R = 1 n n t=1 [δ t ¯ π w (y t |x t )( log π w (y t |x t ) − n u=1 ¯ π w (y u |x u ) log π w (y u |x u ))]. ˆ R ˆ c DC = 1 n n t=1 [(δ t − ˆ c ˆ δ t )¯ π w (y t |x t )( log π w (y t |x t ) − n u=1 ¯ π w (y u |x u ) log π w (y u |x u )) +ˆc+ˆc y∈Y(xt) ˆ δ(x t , y)π w (y|x t ) log π w (y|x t )].</formula><formula xml:id="formula_3">ˆ R DPM+R (π w ) = 1 n n t=1 δ t ¯ π w (y t |x t ) (3) = 1 n n t=1 δ t π w (y t |x t ) n t=1 π w (y t |x t ) .</formula><p>From the perspective of Monte Carlo simu- lation, the advantage of this modification can be explained by viewing reweighting as a mul- tiplicative control variate <ref type="bibr" target="#b26">(Swaminathan and Joachims, 2015b</ref>). Let Z = δ t π w (y t |x t ) and W = π w (y t |x t ) be two random vari- ables, then the variance of r = 1 n n t=1 Z 1 n n t=1 W can be approximately written as follows <ref type="bibr" target="#b10">(Kong, 1992)</ref>: Var(r) ≈ 1 n (r 2 Var(W ) + Var(Z) − 2r Cov(W, Z)). This shows that a positive cor- relation between the variable W , representing the target model probability, and the variable Z, representing the target model scaled by the task loss function, will reduce the variance of the es- timator. Since there are exponentially many out- puts to choose from for each input during log- ging, variance reduction is useful in counterfac- tual learning even in the deterministic case. Un- der a stochastic logging policy, a similar modifi- cation can be done to objective (1) by reweight- ing the ratio ρ t = πw(yt|xt) π 0 (yt|xt) as ¯ ρ t = ρt t ρt . We will use this reweighted IPS objective, called IPS+R, in our comparison experiments that use stochastically logged data.</p><p>A further modification of Equation <ref type="formula">(3)</ref> is motivated by the incorporation of a direct re- ward estimation method in the inverse propen- sity scorer as proposed in the doubly-robust es- timator <ref type="bibr" target="#b3">(Dudik et al., 2011;</ref><ref type="bibr" target="#b7">Jiang and Li, 2016;</ref><ref type="bibr" target="#b27">Thomas and Brunskill, 2016)</ref>. LetˆδLetˆ Letˆδ(x t , y t ) be a regression-based reward model trained on the logged data, and letˆcletˆ letˆc be a scalar that allows to optimize the estimator for minimal variance <ref type="bibr" target="#b21">(Ross, 2013)</ref>. We define a doubly controlled em- pirical risk minimization objectivê R ˆ c DC as fol- lows (forˆcforˆ forˆc = 1 we arrive at a similar objective calledˆRcalledˆ calledˆR DC ):</p><formula xml:id="formula_4">ˆ R ˆ c DC (π w ) = 1 n n t=1 (δ t − ˆ c ˆ δ t ) ¯ π w (y t |x t ) (4) + ˆ c y∈Y(xt) ˆ δ(x t , y) π w (y|x t ) .</formula><p>From the perspective of Monte Carlo simu- lation, the doubly robust estimator can be seen as variance reduction via additive control vari- ates <ref type="bibr" target="#b21">(Ross, 2013)</ref>. Let X = δ t and Y = ˆ δ t be two random variables. Then</p><formula xml:id="formula_5">¯ Y = y∈Y(xt) ˆ δ(x t , y) π w (y|x t )</formula><p>is the expectation 2 of Y , and Equation (4) can be rewritten as <ref type="bibr" target="#b21">Ross (2013)</ref>, Chap. 9.2). Again this shows that variance of the estimator can be reduced if the variable X, representing the re- ward function, and the variable Y , represent- ing the regression-based reward model, are posi- tively correlated. The optimal scalar parameterˆcparameterˆ parameterˆc can be derived easily by taking the derivative of variance term, leading tô</p><formula xml:id="formula_6">E ¯ πw(x) (X − ˆ c Y )+ ˆ c ¯ Y . The variance of the term X −ˆ c Y is Var(X −ˆ c Y ) = Var(X)+ˆc+ˆc 2 Var(Y )− 2ˆc2ˆc Cov(X, Y ). (</formula><formula xml:id="formula_7">tô c = Cov(X, Y ) Var(Y ) .<label>(5)</label></formula><p>In case of stochastic logging the reweighted target probability ¯ π w (y t |x t ) is replaced by a reweighted ratio ¯ ρ t . We will use such reweighted models of the original doubly robust model, with and without optimaî c, called DR andˆcandˆ andˆc DR, in our experiments that use stochastic logging.</p><p>Learning Algorithms. Applying a stochastic gradient descent update rule w t+1 = w t − η ˆ R(π w ) t to the objective functions defined above leads to a variety of algorithms. The gradi- ents of the objectives can be derived by using the score function gradient estimator ( <ref type="bibr" target="#b5">Fu, 2006</ref>) and are shown in <ref type="table" target="#tab_0">Table 1</ref>. Stochastic gradient descent algorithms apply to any differentiable policy π w , thus our methods can be applied to a variety of systems, including linear and non-linear mod- els. Since previous work on off-policy methods in RL and contextual bandits has been done in the area of linear classification, we start with an adaptation of off-policy methods to linear SMT models in our work. We assume a Gibbs model</p><formula xml:id="formula_8">π w (y t |x t ) = e α(w φ(xt,yt)) y∈Y(xt) e α(w φ(xt,y)) ,<label>(6)</label></formula><p>based on a feature representation φ : X × Y → R d , a weight vector w ∈ R d , and a smoothing parameter α ∈ R + , yielding the following sim- ple derivative log π w (y t |x t ) = α φ(x t , y t ) − y∈Y(xt) φ(x t , y)π w (y t |x t ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Setup. In our experiments, we aim to simu- late the following scenario: We assume that it is possible to divert a small fraction of the user interaction traffic for the purpose of policy eval- uation and to perform stochastic logging on this small data set. The main traffic is assumed to be logged deterministically, following a conserva- tive regime where one-best translations are used  for an SMT system that does not change fre- quently over time. Since our experiments are simulation studies, we will additionally perform stochastic logging, and compare policy learn- ing for the (realistic) case of deterministic log- ging with the (theoretically motivated) case of stochastic logging.</p><p>In our deterministic-based policy learning ex- periments, we evaluate the empirical risk mini- mization algorithms derived from objectives (3) (DPM+R) and (4). For the doubly controlled ob- jective we employ two variants: First, ˆ c is set to 1 as in (Dudik et al., 2011) (DC). Second, we calculatê c as described in Equation (5) (ˆ c DC). The algorithms used in policy evaluation and for stochastic-based policy learning are variants of these objectives that replace ¯ π by ¯ ρ to yield es- timators IPS+R, DR, andˆcandˆ andˆc DR of the expected loss.</p><p>All objectives will be employed in a domain adaptation scenario for machine translation. A system trained on out-of-domain data will be used to collect feedback on in-domain data. This data will serve as the logged data D in the learning experiments. We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a con- catenation of the Europarl corpus ( <ref type="bibr" target="#b8">Koehn, 2005)</ref>, the Common Crawl corpus <ref type="bibr">3</ref> and the News Com- mentary corpus <ref type="bibr" target="#b9">(Koehn and Schroeder, 2007)</ref>. The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus <ref type="bibr" target="#b28">(Tiedemann, 2012)</ref>. A second task uses the French-to-English Europarl data with the goal of domain adaptation to news arti- cles with the News Commentary corpus ( <ref type="bibr" target="#b9">Koehn and Schroeder, 2007)</ref>. We split off two parts from the TED corpus to be used as validation and test data for the learning experiments. As valida- tion data for the News Commentary corpus we use the splits provided at the WMT shared task, namely nc-devtest2007 as validation data and nc-test2007 as test data. An overview of the data statistics can be seen in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>As baseline, an out-of-domain system is built using the SCFG framework CDEC ( <ref type="bibr" target="#b4">Dyer et al., 2010</ref>) with dense features (10 standard features and 2 for the language model). After tokeniz- ing and lowercasing the training data, the data were word aligned using CDEC's fast align. A 4-gram language model is build on the tar- get languages for the out-of-domain data using KENLM ( <ref type="bibr" target="#b6">Heafield et al., 2013</ref>). For News, we additionally assume access to in-domain target language text and train another in-domain lan- guage model on that data, increasing the number of features to 14 for News.</p><p>The framework uses a standard linear Gibbs model whose distribution can be peaked using a parameter α (see Equation (6)): Higher value of α will shift the probability of the one-best trans- lation closer to 1 and all others closer to 0. Using α &gt; 1 during training will promote to learn mod- els that are optimal when outputting the one-best translation. In our experiments, we found α = 5 to work well on validation data.</p><p>Additionally, we tune a system using CDEC's MERT implementation <ref type="bibr" target="#b16">(Och, 2003)</ref> on the in- domain data with their references. This full- information in-domain system conveys the best possible improvement using the given training data. It can thus be seen as the oracle system for the systems which are learnt using the same input-side training data, but have only bandit feedback available to them as a learning signal. All systems are evaluated using the corpus-level BLEU metric ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>   the original out-of-domain systems, and logging the one-best translation. For the stochastic ex- periments, the translations are sampled from the model distribution. The feedback to the logged translation is simulated using the reference and sentence-level BLEU ( <ref type="bibr" target="#b14">Nakov et al., 2012</ref>).</p><p>Direct Reward Estimation. When creating the logged data D, we also record the feature vectors of the translations to train the direct re- ward estimate that is needed for (ˆ c)DC. Using the feature vector as input and the per-sentence BLEU as the output value, we train a regression- based random forest with 10 trees using scikit- learn <ref type="bibr" target="#b18">(Pedregosa et al., 2011</ref>). To measure per- formance, we perform 5-fold cross-validation and measure the macro average between esti- mated rewards and the true rewards from the log:</p><formula xml:id="formula_9">| 1 n δ(x t , y t ) − 1 n ˆ δ(x t , y t )|.</formula><p>We also report the micro average which quantifies how far off one can expect the model to be for a random sample: 1 n |δ(x t , y t ) − ˆ δ(x t , y t )|. The final model used in the experiments is trained on the full training data. Cross-validation results for the regression-based direct reward model can be found in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Policy Evaluation. Policy evaluation aims to use the logged data D to estimate the perfor- mance of the target system π w . The small logged data D eval that is diverted for policy evalua- tion is created by translating only 10k sentences of the in-domain training data with the out-of- domain system and sample translations accord- ing to the model probability. Again we record the sentence-level BLEU as the feedback. The reference translations that also exist for those 10k sentences are used to measure the ground truth BLEU value for translations using the full- information in-domain system. The goal of eval- uation is to achieve a value of IPS+R, DR, andˆc andˆ andˆc DR on D eval that are as close as possible to the ground truth BLEU value.</p><p>To be able to measure variance, we create five folds of D eval , differing in random seeds. We re- port the average difference between the ground truth BLEU score and the value of the log-based policy evaluation, as well as the standard devi- ation in <ref type="table" target="#tab_5">Table 4</ref>. We see that IPS+R underesti- mates the BLEU value by 7.78 on News. DR overestimates instead. ˆ c DR achieves the closest estimate, overestimating the true value by less than 1 BLEU. On TED, all policy evaluation results are overestimates. For the DR variants the overestimation result can be explained by the random forests' tendency to overestimate. Opti- maî c DR can correct for this, but not always in a sufficient way.</p><p>Policy Learning. In our learning experiments, learning starts with the weights w 0 from the out- of-domain model. As this was the system that produced the logged data D, the first iteration will have the same translations in the one-best position. After some iterations, however, the translation that was logged may not be in the first position any more. In this case, the n-best list is searched for the correct translation. Due to speed reasons, the scores of the translation system are normalized to probabilities using the first 1,000 unique entries in the n-best list, rather than using the full hypergraph. Our experiments showed that this did not impact the quality of learning.</p><p>In order for the multiplicative control variate to be effective, the learning procedure has to uti- lize mini-batches. If the mini-batch size is cho- sen too small, the estimates of the control vari- ates may not be reliable. We test mini-batch sizes of 30k and 10k examples, whereas 30k on News means that we perform batch training since the mini-batch spans the entire training set. Mini- batch size β and early stopping point where se- lected by choosing the setup and iteration that achieved the highest BLEU score on the one-best translations for the validation data. The learning rate η was selected in the same way, whereas the possible values were 1e−4, 1e−5, 1e−6 or, al- ternatively, Adadelta <ref type="bibr" target="#b29">(Zeiler, 2012)</ref>, which sets the learning rate on a per-feature basis. The re- sults on both validation and test set are reported in <ref type="table">Table 5</ref>. Statistical significance of the out- of-domain system compared to all other systems is measured using Approximate Randomization testing <ref type="bibr" target="#b15">(Noreen, 1989)</ref>.</p><p>For the deterministic case, we see that in general DPM+R shows the lowest increase but can still significantly outperform the baseline. An explanation of why DPM+R cannot improve any further, will be addressed separately below. DC yields improvements of up to 1.5 BLEU points, whilê c DC obtains improvements of up to 2 BLEU points over the out-of-domain base- line. In more detail on the TED data, DC can close the gap of nearly 3 BLEU by half between the out-of-domain and the full-information in- domain system. ˆ c DC can improve by further 0.6 BLEU which is a significant improvement at p = 0.0017. Also note that, whilê c DC takes more iterations to reach its best result on the val- idation data, ˆ c DC already outperforms DC at the stopping iteration of DC. At this pointˆcpointˆ pointˆc DC is better by 0.18 BLEU on the validation set and continues to increase until its own stopping it- eration. The final results ofˆcofˆ ofˆc DC falls only 0.8 BLEU behind the oracle system that had refer- ences available during its learning process. Con- sidering the substantial difference in information that both systems had available, this is remark-  <ref type="table">Table 5</ref>: BLEU increases for learning, over the out-of-domain baseline on validation and test set. Out- of-domain is the baseline and starting system and in-domain is the oracle system tuned on in-domain data with references. For the deterministic case, all results are statistically significant at p ≤ 0.001 with regards to the baseline. For the stochastic case, all results are statistically significant at p ≤ 0.002 with regards to the baseline, except for IPS+R on the News corpus.</p><p>able. The improvements on the News corpus show similar tendencies. Again there is a gap of nearly 3 BLEU to close and with an improve- ment of 1.05 BLEU points, DC can achieve a no- table result. ˆ c DC was able to further improve on this but not as successfully as was the case for the TED corpus. Analyzing the actuaî c values that were calculated in both experiments allows us to gain an insight as to why this was the case: For TED, ˆ c is on average 1.35. In the case of News, however, ˆ c has a maximum value of 1.14 and thus stays quite close to 1, which would equate to using DC. It is thus not surprising that there is no significant difference between DC andˆcandˆ andˆc DC.</p><p>Comparison to the Stochastic Case. Even if not realistic for commercial applications of SMT, our simulation study allows us to stochas- tically log large amounts of data in order to com- pare learning from deterministic logs to the stan- dard case. As shown in <ref type="table">Table 5</ref>, the relations be- tween algorithms and even the absolute improve- ments are similar for stochastic and determin- istic logging. Significance tests between each deterministic/stochastic experiment pair show a significant difference only in case of DC/DR on TED data. However, the DR result still does not significantly outperform the best determinis- tic objective on TED (ˆ c DC). The p values for all other experiment pairs lie above 0.1. From this we can conclude that it is indeed an acceptable practice to log deterministically. <ref type="bibr" target="#b12">Langford et al. (2008)</ref> show that counterfactual learning is impossible unless the logging sys- tem sufficiently explores the output space. This condition is seemingly not satisfied if the log- ging systems acts according to a deterministic policy. Furthermore, since techniques such as "exploration over time" <ref type="bibr" target="#b23">(Strehl et al., 2010)</ref> are not applicable to commercial SMT systems that are not frequently changed over time, the case of counterfactual learning for SMT seems hope- less. However, our experiments present evidence to the contrary. In the following, we present an analysis that aims to explain this apparent con- tradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Implicit Exploration. In an experimental comparison between stochastic and determinis- tic logging for bandit learning in computational advertising, <ref type="bibr" target="#b2">Chapelle and Li (2011)</ref> observed that varying contexts (representing user and page visited) induces enough exploration into ad se- lection such that learning becomes possible. A similar implicit exploration can also be attributed to the case of SMT: An identical input word or phrase can lead, depending on the other words and phrases in the input sentence, to different output words and phrases. Moreover, an identi- cal output word or phrase can appear in different output sentences. Across the entire log, this im- plicitly performs the exploration on phrase trans- lations that seems to be missing at first glance.</p><p>Smoothing by Multiplicative Control Vari- ates. The DPM estimator can show a degen- erate behavior in that the objective can be mini- mized simply by setting the probability of every logged data point to 1.0. This over-represents logged data that received low rewards, which is undesired. Furthermore, systems optimized with this objective cannot properly discriminate be- tween the translations in the output space. This can be seen as a case of translation invariance of the objective, as has been previously noted by <ref type="bibr" target="#b26">Swaminathan and Joachims (2015b)</ref>: Adding a small constant c to the probability of every data point in the log increases the overall value of the objective without improving the discrimina- tive power between high-reward and low-reward translations.</p><p>DPM+R solves the degeneracy of DPM by defining a probability distribution over the logged data by reweighting via the multiplica- tive control variate. After reweighting, the ob- jective value will decrease if the probability of a low-reward translation increased, as it takes away probability mass from other, higher reward samples. Because of this trade-off, balancing the probabilities over low-reward and high-reward samples becomes important, as desired.</p><p>Smoothing by Additive Control Variates. Despite reweighting, DPM+R can still show a degenerate behavior by setting the probabili- ties of only the highest-reward samples to 1.0, while avoiding all other logged data points. This clearly hampers the generalization ability of the model since inputs that have been avoided in training will not receive a proper ranking of their translations.</p><p>The use of an additive control variate can solve this problem by using a reward estimate that takes the full output space into account. The objective will now be increased if the probabil- ity of translations with high estimated reward is increased, even if they were not seen in train- ing. This will shift probability mass to unseen data with high estimated-reward, and thus im- prove the generalization ability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we showed that off-policy learn- ing from deterministic bandit logs for SMT is possible if smoothing techniques based on con- trol variates are used. These techniques will avoid degenerate behavior in learning and im- prove generalization of empirical risk minimiza- tion over logged data. Furthermore, we showed that standard off-policy evaluation is applicable to SMT under stochastic logging policies.</p><p>To our knowledge, this is the first application of counterfactual learning to a complex struc- tured prediction problem like SMT. Since our objectives are agnostic of the choice of the un- derlying model π w , it is also possible to transfer our techniques to non-linear models such as neu- ral machine translation. This will be a desidera- tum for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Online learning from partial feedback.</figDesc><graphic url="image-1.png" coords="1,355.43,568.28,109.80,58.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Offline learning from partial feedback.</figDesc><graphic url="image-2.png" coords="2,127.32,313.41,109.80,137.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Gradients of counterfactual objectives. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TED DE -EN News FR-EN</head><label>DE</label><figDesc></figDesc><table>train 
122k 
30k 
validation 
3k 
1k 
test 
3k 
2k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number of sentences for in-domain data 
splits of SMT train, validation, and test data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>). The logged data D is created by translating the in-domain training data of the corpora using</figDesc><table>TED News 

macro avg. 0.67 
0.23 
micro avg. 15.03 10.87 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation of regression-based reward 
estimation by average BLEU differences be-
tween estimated and true rewards. 

IPS+R 
DRˆc DRˆ DRˆc DR 

TED 
avg. estimate +4.00 +7.98 +6.07 
std. dev. 
0.64 
3.83 
2.06 

News 
avg. estimate -7.78 +6.63 +0.95 
std. dev. 
0.97 
4.13 
2.33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Policy evaluation by macro averaged 
difference between estimated and ground truth 
BLEU on 10k stochastically logged data, aver-
aged over 5 runs. 

</table></figure>

			<note place="foot" n="2"> Note that we introduce a slight bias by using πw versus ¯ πw in sampling probability and control variate.</note>

			<note place="foot" n="3"> http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research reported in this paper was sup-ported in part by the German research founda-tion (DFG), and in part by a research coopera-tion grant with the Amazon Development Center Germany.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quiñonerocandela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><forename type="middle">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanakar</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<editor>Patrice Simard, and Ed Snelson. 2013</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastian</forename><surname>Bubeck Andnicoì O Cesa-Bianchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical evaluation of Thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Doubly robust policy evaluation and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)<address><addrLine>Bellevue, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gradient estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook in Operations Research and Management Science</title>
		<editor>S.G. Henderson and B.L. Nelson</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="575" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Doubly robust offpolicy value evaluation for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Translation Summit</title>
		<meeting>the Machine Translation Summit<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experiments in domain adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Translation (WMT)</title>
		<meeting>the Workshop on Machine Translation (WMT)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A note on importance sampling using standardized weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustine</forename><surname>Kong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>Illinois</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Chicago</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report 348</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bandit structured prediction for neural sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploration scavenging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML)<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Counterfactual estimation and optimization of click metrics in search engines: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunbao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Kleban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference</title>
		<meeting>the International World Wide Web Conference<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing for sentence-level bleu+1 yields short translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 24th International Conference on Computational Linguistics (COLING)<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computer Intensive Methods for Testing Hypotheses: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and the 3rd Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<meeting>the Human Language Technology Conference and the 3rd Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)<address><addrLine>Edmonton, Cananda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics (ACL)<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning (ICML)</title>
		<meeting>the Seventeenth International Conference on Machine Learning (ICML)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simulation, fifth edition. Elsevier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic structured prediction under bandit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from logged implicit exploration data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Sytems (NIPS)</title>
		<imprint>
			<publisher>Canada</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Vancouver</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reinforcement Learning. An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch learning from logged bandit feedback through counterfactual risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1731" to="1755" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The self-normalized estimator for counterfactual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dataefficient off-policy policy evaluation for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>ArXiv:1212.5701 [cs.LG</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
