<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
							<email>tianze@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Oregon State University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<email>liang.huang.sh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Oregon State University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
							<email>llee@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Oregon State University</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="12" to="23"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiper-wasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case Opn 3 q exact decoders for arc-hybrid and arc-eager transition systems. With our minimal features, we also present Opn 3 q global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the &quot;second-best-in-class&quot; result on the English Penn Treebank.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It used to be the case that the most accurate de- pendency parsers made global decisions and em- ployed exact decoding. But transition-based de- pendency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly ( <ref type="bibr" target="#b23">Weiss et al., 2015;</ref><ref type="bibr">Dyer et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016</ref>). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to ac- cess rich information about particular positions in the stack and buffer of the current parser configu- ration. But consulting many positions means that although polynomial-time exact-decoding algo- rithms do exist, having been introduced by <ref type="bibr">Huang and Sagae (2010)</ref> and <ref type="bibr">Kuhlmann et al. (2011)</ref>, un- fortunately, they are prohibitively costly in prac- tice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly re- duced set of nine positions, but the worst-case run- ning time for the exact-decoding version of their algorithm is Opn 6 q (originally reported as Opn 7 q) for a length-n sentence. As an extreme case, <ref type="bibr">Dyer et al. (2015)</ref> use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming.</p><p>Recently, Kiperwasser and Goldberg (2016a) and <ref type="bibr">Cross and Huang (2016a)</ref> applied bi- directional long short-term memory networks ( <ref type="bibr">Graves and Schmidhuber, 2005</ref>, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid pars- ing system (K&amp;G), and three suffice for arc- standard (C&amp;H). <ref type="bibr">1</ref> Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it con- tains only two positional bi-LSTM vectors, suf- fers almost no loss in performance in comparison to larger sets, and out-performs a single position. (Details regarding the situation with arc-standard can be found in §2.)</p><p>Our minimal feature set plugs into Huang and Sagae's and <ref type="bibr">Kuhlmann et al.'</ref>s dynamic program-ming framework to produce the first implementa- tion of Opn 3 q exact decoders for arc-hybrid and arc-eager parsers. We also enable and implement Opn 3 q global training methods. Empirically, en- sembles containing our minimal-feature, globally- trained and exactly-decoded models produce the best unlabeled attachment score (UAS) reported (to our knowledge) on the Chinese Treebank and the "second-best-in-class" result on the English Penn Treebank. <ref type="bibr">2</ref> Additionally, we provide a slight update to the theoretical connections previously drawn by <ref type="bibr">Gómez-Rodríguez, Carroll, and</ref><ref type="bibr">Weir (2008, 2011)</ref> between TBDPs and the graph-based dependency parsing algorithms of <ref type="bibr">Eisner (1996)</ref> and <ref type="bibr">Eisner and Satta (1999)</ref>, including results regarding the arc-eager parsing system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Minimal Feature Set</head><p>TBDPs incrementally process a sentence by mak- ing transitions through search states representing parser configurations. Three of the main transition systems in use today (formal introduction in §3.1) all maintain the following two data structures in their configurations: (1) a stack of partially parsed subtrees and (2) a buffer (mostly) of unprocessed sentence tokens.</p><p>To featurize configurations for use in a scoring function, it is common to have features that extract information about the first several elements on the stack and the buffer, such as their word forms and part-of-speech (POS) tags. We refer to these as po- sitional features, as each feature relates to a partic- ular position in the stack or buffer. Typically, mil- lions of sparse indicator features (often developed via manual engineering) are used.</p><p>In contrast, <ref type="bibr">Chen and Manning (2014)</ref> intro- duce a feature set consisting of dense word-, POS-, and dependency-label embeddings. While dense, these features are for the same 18 positions that have been typically used in prior work. Re- cently, <ref type="bibr">Kiperwasser and Goldberg (2016a)</ref> and <ref type="bibr">Cross and Huang (2016a)</ref> adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the num- ber of positions considered down to four and three, <ref type="bibr">2</ref> Our ideas were subsequently adapted to the labeled set- ting by Shi, <ref type="bibr" target="#b13">Wu, Chen, and Cheng (2017)</ref> in their submis- sion to the CoNLL 2017 shared task on Universal Dependen- cies parsing. Their team achieved the second-highest labeled attachment score in general and had the top average perfor- mance on the surprise languages.   <ref type="table">Table 1</ref>: Top: English PTB dev-set UAS% for progressively smaller sets of positional features, for greedy parsers with different transition sys- tems. The "double-arrow" notation indicates vec- tors produced by a bi-directional LSTM. Internal lines highlight large performance drop-offs when a feature is deleted. Bottom: sizes of the minimal feature sets in <ref type="bibr">Kiperwasser and Goldberg (2016a)</ref>, <ref type="bibr">Cross and Huang (2016a)</ref>, and our work.</p><p>for different transition systems, respectively. This naturally begs the question, what is the lower limit on the number of positional features necessary for a parser to perform well? <ref type="bibr">Kiperwasser and Goldberg (2016a)</ref> reason that for the arc-hybrid system, the first and second items on the stack and the first buffer item -denoted by s 0 , s 1 , and b 0 , respectively -are required; they addi- tionally include the third stack item, s 2 , because it may not be adjacent to the others in the origi- nal sentence. For arc-standard, <ref type="bibr">Cross and Huang (2016a)</ref> argue for the necessity of s 0 , s 1 , and b 0 .</p><p>We address the lower-limit question empiri- cally, and find that, surprisingly, two positions suffice for the greedy arc-eager and arc-hybrid parsers. We also provide empirical support for Cross and Huang's argument for the necessity of three features for arc-standard. In the rest of this section, we explain our experiments, run only on an English development set, that support this con- clusion; the results are depicted in <ref type="table">Table 1</ref>. We later explore the implementation implications in §3-4 and then test-set parsing-accuracy in §6.</p><p>We employ the same model architecture as <ref type="bibr">Kiperwasser and Goldberg (2016a)</ref>. Specifically, we first use a bi-LSTM to encode an n-token sen- tence, treated as a sequence of per-token concate- nations of word-and POS-tag embeddings, into a sequence of vectors r ÑÐ w 1 , . . . ,  is the output of the bi-LSTM at time step i. (The double-arrow notation for these vectors empha- sizes the bi-directionality of their origin). Then, for a given parser configuration, stack positions are represented by ÑÐ s j , defined as ÑÐ w ips j q where ips j q gives the position in the sentence of the to- ken that is the head of the tree in s j . Similarly, buffer positions are represented by ÑÐ b j , defined as ÑÐ w ipb j q for the token at buffer position j. Finally, as in <ref type="bibr">Chen and Manning (2014)</ref>, we use a multi- layer perceptron to score possible transitions from the given configuration, where the input is the con- catenation of some selection of the ÑÐ s j s and ÑÐ b k s. We use greedy decoders, and train the models with dynamic oracles <ref type="bibr">(Goldberg and Nivre, 2013)</ref>. <ref type="table">Table 1</ref> reports the parsing accuracy that re- sults for feature sets of size four, three, two, and one for three commonly-used transition systems. The data is the development section of the English Penn Treebank (PTB), and experimental settings are as described in our other experimental section, §6. We see that we can go down to three or, in the arc-hybrid and arc-eager transition systems, even two positions with very little loss in performance, but not further. We therefore call t  <ref type="table">Table 1</ref> for a summary. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic Programming for TBDPs</head><p>As stated in the introduction, our minimal fea- ture set from §2 plugs into Huang and Sagae and <ref type="bibr">Kuhlmann et al.'</ref>s dynamic programming (DP) framework. To help explain the connection, this section provides an overview of the DP frame- work. We draw heavily from the presentation of <ref type="bibr">Kuhlmann et al. (2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Three Transition Systems</head><p>Transition-based parsing <ref type="bibr" target="#b8">(Nivre, 2008;</ref><ref type="bibr">Kübler et al., 2009</ref>) is an incremental parsing framework based on transitions between parser configura- <ref type="bibr">3</ref> We tentatively conjecture that the following might ex- plain the observed phenomena, but stress that we don't cur- rently see a concrete way to test the following hypothesis. With t ÑÐ s 0, ÑÐ b 0u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing in- formation. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, reñ, introduced in §3.1); perhaps ÑÐ s 1 is therefore not so cru- cial for arc-hybrid in practice?</p><p>tions. For a sentence to be parsed, the system starts from a corresponding initial configuration, and attempts to sequentially apply transitions un- til a configuration corresponding to a full parse is produced. Formally, a transition system is defined as S " pC, T, c s , C τ q, where C is a nonempty set of configurations, each t P T : C á C is a transi- tion function between configurations, c s is an ini- tialization function that maps an input sentence to an initial configuration, and C τ Ď C is a set of terminal configurations.</p><p>All systems we consider share a common tri- partite representation for configurations: when we write c " pσ, β, Aq for some c P C, we are re- ferring to a stack σ of partially parsed subtrees; a buffer β of unprocessed tokens and, optionally, at its beginning, a subtree with only left descendants; and a set A of elements ph, mq, each of which is an attachment (dependency arc) with head h and modifier m. <ref type="bibr">4</ref> We write m ð h to indicate that m left-modifies h, and h ñ m to indicate that m right- modifies h. For a sentence w " w 1 , ..., w n , the initial configuration is pσ 0 , β 0 , A 0 q, where σ 0 and A 0 are empty and β 0 " rROOT|w 1 , ..., w n s; ROOT is a special node denoting the root of the parse tree 5 (vertical bars are a notational convenience for indicating different parts of the buffer or stack; our convention is to depict the buffer first element leftmost, and to depict the stack first element right- most). All terminal configurations have an empty buffer and a stack containing only ROOT.</p><p>Arc-Standard The arc-standard system <ref type="bibr" target="#b7">(Nivre, 2004</ref>) is motivated by bottom-up parsing: each de- pendent has to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (re ñ , reduce and attach a right modifier), and left-reduce (re ð , reduce and attach a left modifier), are defined as:</p><p>shrpσ, b 0 |β, Aqs " pσ|b 0 , β, Aq re ñ rpσ|s 1 |s 0 , β, Aqs " pσ|s 1 , β, A Y tps 1 , s 0 quq re ð rpσ|s 1 |s 0 , β, Aqs " pσ|s 0 , β, A Y tps 0 , s 1 quq Arc-Hybrid The arc-hybrid system <ref type="bibr" target="#b25">(Yamada and Matsumoto, 2003;</ref><ref type="bibr">Gómez-Rodríguez et al., 2008;</ref><ref type="bibr">Kuhlmann et al., 2011</ref>) has the same defi- nitions of sh and re ñ as arc-standard, but forces the collection of left modifiers before right modi- fiers via its b 0 -modifier re ð transition. This con- trasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack.</p><p>shrpσ, b 0 |β, Aqs " pσ|b 0 , β, Aq re ñ rpσ|s 1 |s 0 , β, Aqs " pσ|s 1 , β, A Y tps 1 , s 0 quq</p><formula xml:id="formula_0">re ð rpσ|s 0 , b 0 |β, Aqs " pσ, b 0 |β, A Y tpb 0 , s 0 quq</formula><p>Arc-Eager In contrast to the former two sys- tems, the arc-eager system <ref type="bibr" target="#b6">(Nivre, 2003</ref>) makes attachments as early as possible -even if a modi- fier has not yet received all of its own modifiers. This behavior is accomplished by decomposing the right-reduce transition into two independent transitions, one making the attachment (ra) and one reducing the right-attached child (re).</p><p>shrpσ, b 0 |β, Aqs " pσ|b 0 , β, Aq  <ref type="bibr" target="#b10">Pereira and Warren, 1983;</ref><ref type="bibr" target="#b14">Shieber et al., 1995)</ref>, wherein transitions serve as inference rules; these are given as the lefthand sides of the first three sub- figures in <ref type="figure" target="#fig_7">Figure 1</ref>. For a given w " w 1 , ..., w n , assertions take the form ri, j, ks (or, when applica- ble, a two-index shorthand to be discussed soon), meaning that there exists a sequence of transi- tions that, starting from a configuration wherein head ps 0 q " w i , results in an ending configura- tion wherein head ps 0 q " w j and head pb 0 q " w k . If we define w 0 as ROOT and w n`1 as an end- of-sentence marker, then the goal theorem can be stated as r0, 0, n ` 1s.</p><formula xml:id="formula_1">re ð rpσ|s 0 , b 0 |β, Aqs " pσ, b 0 |β, A Y tpb 0 , s 0 quq</formula><p>For arc-standard, we depict an assertion ri, h, ks as a subtree whose root (head) is the token at h. Assertions of the form ri, i, ks play an important role for arc-hybrid and arc-eager, and we employ the special shorthand ri, ks for them in <ref type="figure" target="#fig_7">Figure 1</ref>. In that figure, we also graphically depict such sit- uations as two consecutive half-trees with roots w i and w k , where all tokens between i and k are al- ready attached. The superscript b in an arc-eager assertion ri b , js is an indicator variable for whether w i has been attached to its head (b " 1) or not (b " 0) after the transition sequence is applied. <ref type="bibr">Kuhlmann et al. (2011)</ref> show that all three de- duction systems can be directly "tabularized" and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incor- porating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, as the two-index shorthand ri, js sug- gests, arc-eager and arc-hybrid systems can be im- plemented to take Opn 2 q space and Opn 3 q time; the arc-standard system requires Opn 3 q space and Opn 4 q time (if one applies the so-called hook trick <ref type="bibr">(Eisner and Satta, 1999)</ref>).</p><p>Since an Opn 4 q running time is not sufficiently practical even in the simple-feature case, in the re- mainder of this paper we consider only the arc- hybrid and arc-eager systems, not arc-standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Practical Optimal Algorithms Enabled By Our Minimal Feature Set</head><p>Until now, no one had suggested a set of positional features that was both information-rich enough for accurate parsing and small enough to obtain the Opn 3 q running-time promised above. Fortunately, our bi-LSTM-based t ÑÐ s 0 , ÑÐ b 0 u feature set qualifies, and enables the fast optimal procedures described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exact Decoding</head><p>Given an input sentence, a TBDP must choose among a potentially exponential number of cor- responding transition sequences. We assume ac- cess to functions f t that score individual configu- rations, where these functions are indexed by the transition functions t P T . For a fixed transition sequence t " t 1 , t 2 , . . ., we use c i to denote the configuration that results after applying t i .</p><p>Typically, for efficiency reasons, greedy left-to- right decoding is employed: the next transition t ˚ i out of c i´1 is arg max t f t pc i´1 q, so that past and future decisions are not taken into account. The score F ptq for the transition sequence is induced by summing the relevant f t i pc i´1 q values.</p><p>However, our use of minimal feature sets en- ables direct computation of an argmax over the en- tire space of transition sequences, arg max t F ptq, via dynamic programming, because our positions don't rely on any information "outside" the deduc- tion rule indices, thus eliminating the need for ad-   ditional state-keeping. We show how to integrate the scoring functions for the arc-eager system; the arc-hybrid system is handled similarly. The score-annotated rules are as follows:</p><note type="other">reñ ri, h1, ks rk, h2, js ri, h1, js i h1 k h2 j i h1 j h ñ 1 h2 reð ri, h1, ks rk, h2, js ri, h2, js i h1 k h2 j i h2 j h ð 1 h2 Goal r0, 0, n ` 1s 0 0 n ` 1 (a) Arc-standard Axiom r0, 1s 0 1 Inference Rules sh ri, js rj, j ` 1s i j j j `1 j ď n reñ rk, is ri, js rk, js k i i j k j k ñ i reð rk, is ri, js rk, js k i i j k j i ð j Goal r0, n ` 1s</note><formula xml:id="formula_2">0 n ` 1 (b) Arc-hybrid Axiom r0 0 , 1s 0 0 1 Inference Rules sh ri b , js rj 0 , j ` 1s i b j j 0 j `1 j ď n ra ri b , js rj 1 , j ` 1s i b j j 1 j `1 i ñ j j ď n reð rk b , is ri 0 , js rk b , js k b i i 0 j k b j i ð j re rk b , is ri 1 , js rk b , js k b i i 1 j k b j Goal r0 0 , n ` 1s 0 0 n ` 1 (c) Arc-eager Axioms i i ` 1 j j 0 ď i, j ď n</formula><formula xml:id="formula_3">ri b , js : v rj 0 , j ` 1s : 0 pshq rk b , is : v 1 ri 0 , js : v 2 rk b , js : v 1 ` v 2 ` ∆ pre ð q where ∆ " f sh p ÑÐ w k , ÑÐ w i q ` f reð p ÑÐ w i , ÑÐ</formula><p>w j q -abus- ing notation by referring to configurations by their features. The left-reduce rule says that we can first take the sequence of transitions asserted by rk b , is, which has a score of v 1 , and then a shift transition moving w i from b 0 to s 0 . This means that the ini- tial condition for ri 0 , js is met, so we can take the sequence of transitions asserted by ri 0 , js -say it has score v 2 -and finally a left-reduce transition to finish composing the larger transition sequence. Notice that the scores for sh and ra are 0, as the scoring of these transitions is accounted for by re- duce rules elsewhere in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Global Training</head><p>We employ large-margin training that considers each transition sequence globally. Formally, for a training sentence w " w 1 , . . . , w n with gold tran- sition sequence t gold , our loss function is max t ´ F ptq`costptptq`ptq`costpt gold , tq´Ftq´tq´F pt gold q ¯ where costpt gold , tq is a custom margin for tak- ing t instead of t gold -specifically, the number of mis-attached nodes. Computing this max can again be done efficiently with a slight modifica- tion to the scoring of reduce transitions:</p><formula xml:id="formula_4">rk b , is : v 1 ri 0 , js : v 2 rk b , js : v 1 ` v 2 ` ∆ 1 pre ð q</formula><p>where ∆ 1 " ∆ ` 1 phead pw i q ‰ w j q. This loss- augmented inference or cost-augmented decoding ( <ref type="bibr" target="#b18">Taskar et al., 2005</ref>; Smith, 2011) technique has previously been applied to graph-based parsing by <ref type="bibr">Kiperwasser and Goldberg (2016a)</ref>.</p><p>Efficiency Note The computation decomposes into two parts: scoring all feature combinations, and using DP to find a proof for the goal theorem in the deduction system. Time-complexity analy- sis is usually given in terms of the latter, but the former might have a large constant factor, such as 10 4 or worse for neural-network-based scoring functions. As a result, in practice, with a small n, scoring with the feature set t ÑÐ s 0 , ÑÐ b 0 u (Opn 2 q) can be as time-consuming as the decoding steps (Opn 3 q) for the arc-hybrid and arc-eager systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical Connections</head><p>Our minimal feature set brings implementation of practical optimal algorithms to TBDPs, whereas previously only graph-based dependency parsers (GBDPs) -a radically different, non-incremental paradigm -enjoyed the ability to deploy them. Interestingly, for both the transition-and graph- based paradigms, the optimal algorithms build de- pendency trees bottom-up from local structures. It is thus natural to wonder if there are deeper, more formal connections between the two.</p><p>In previous work, <ref type="bibr">Kuhlmann et al. (2011)</ref> re- lated the arc-standard system to the classic CKY algorithm <ref type="bibr">(Cocke, 1969;</ref><ref type="bibr">Kasami, 1965;</ref><ref type="bibr" target="#b26">Younger, 1967</ref>) in a manner clearly suggested by <ref type="figure" target="#fig_7">Figure 1a</ref>; CKY can be viewed as a very simple graph-based approach. <ref type="bibr">Gómez-Rodríguez et al. (2008</ref><ref type="bibr">Gómez-Rodríguez et al. ( , 2011</ref> formally prove that sequences of steps in the edge- factored GBDP <ref type="bibr">(Eisner, 1996)</ref> can be used to em- ulate any individual step in the arc-hybrid system <ref type="bibr" target="#b25">(Yamada and Matsumoto, 2003)</ref> and the Eisner and Satta (1999, <ref type="figure" target="#fig_7">Figure 1d</ref>) version. However, they did not draw an explicitly direct connection between Eisner and Satta (1999) and TBDPs.</p><p>Here, we provide an update to these previous findings, stated in terms of the expressiveness of scoring functions, considered as parameterization.</p><p>For the edge-factored GBDP, we write the score for an edge as f G p ÑÐ h, ÑÐ mq, where h is the head and m the modifier. A tree's score is the sum of its edge scores. We say that a parameterized depen- dency parsing model A contains model B if for ev- ery instance of parameterization in model B, there exists an instance of model A such that the two models assign the same score to every parse tree. We claim: Lemma 1. The arc-eager model presented in §4.1 contains the edge-factored model. Proof Sketch. Consider a given edge-factored GBDP parameterized by f G . For any parse tree, every edge i ð j involves two deduction rules, and their contribution to the score of the final proof is  </p><formula xml:id="formula_5">f sh ( ÑÐ w k , ÑÐ w i ) ` f reð p ÑÐ w i , ÑÐ w j q. We set f sh ( ÑÐ w k , ÑÐ w i ) " 0 and f reð p ÑÐ w i , ÑÐ w j q " f G p ÑÐ w j , ÑÐ w i q.</formula><formula xml:id="formula_6">f ra ( ÑÐ w k , ÑÐ w i ) " f G p ÑÐ w k , ÑÐ w i q and f re p ÑÐ w i , ÑÐ w j q " 0.</formula><p>The parameterization we arrive at emulates ex- actly the scoring model of f G .</p><p>We further claim that the arc-eager model is more expressive than not only the edge-factored GBDP, but also the arc-hybrid model in our paper.</p><p>Lemma 2. The arc-eager model contains the arc- hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Data and Evaluation We experimented with English and Chinese. For English, we used the Stanford Dependencies (de Marneffe and Man- ning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank ( <ref type="bibr">Marcus et al., 1993, PTB)</ref>. As is standard, we used §2-21 of the Wall Street Journal for training, §22 for development, and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger ( <ref type="bibr" target="#b20">Toutanova et al., 2003</ref>). For Chinese, we used the Penn Chinese Treebank 5.1 ( <ref type="bibr" target="#b24">Xue et al., 2002</ref>, CTB), with the same splits and head-finding rules for conversion to dependencies as <ref type="bibr" target="#b27">Zhang and Clark (2008)</ref>. We adopted the CTB's gold- standard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentence- level unlabeled exact match (UEM). Following prior work, all punctuation is excluded from eval- uation. For each model, we initialized the network parameters with 5 different random seeds and re- port performance average and standard deviation. Implementation Details Our model structures reproduce those of <ref type="bibr">Kiperwasser and Goldberg (2016a)</ref>. We use 2-layer bi-directional LSTMs with 256 hidden cell units. Inputs are concatena- tions of 28-dimensional randomly-initialized part- of-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors <ref type="bibr" target="#b9">(Pennington et al., 2014</ref>) (English) and pre-trained skip- gram-model vectors ( <ref type="bibr" target="#b4">Mikolov et al., 2013</ref>) (Chi- nese). The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hid- den units and activation function tanh. We set the dropout rate for the bi-LSTM ( <ref type="bibr">Gal and Ghahramani, 2016</ref>) and MLP ( <ref type="bibr" target="#b16">Srivastava et al., 2014</ref>) for each model according to development-set perfor- mance. <ref type="bibr">6</ref> All parameters except the word embed- í ½í³ Our best local í ¼í¼Our arc-eager DP í ¼í¼Our arc-hybrid DP í ½í²¼15 Our all global í ½í²¼20 KBKDS16 í ½í²¼5 Our arc-eager DP í ½í²¼5 dings are initialized uniformly <ref type="bibr">(Glorot and Bengio, 2010)</ref>. Approximately 1,000 tokens form a mini-batch for sub-gradient computation. We train each model for 20 epochs and perform model se- lection based on development UAS. The proposed structured loss function is optimized via Adam ( <ref type="bibr">Kingma and Ba, 2015)</ref>. The neural network com- putation is based on the python interface to DyNet <ref type="bibr">(Neubig et al., 2017)</ref>, and the exact decoding al- gorithms are implemented in Cython. 7</p><p>Main Results We implement exact decoders for the arc-hybrid and arc-eager systems, and present the test performance of different model configu- rations in <ref type="table" target="#tab_2">Table 2</ref>, comparing global models with local models. All models use the same decoder for testing as during the training process. Though no global decoder for the arc-standard system has been explored in this paper, its local models are listed for comparison. We also include an edge- factored graph-based model, which is convention- ally trained globally. The edge-factored model scores bi-LSTM features for each head-modifier pair; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores. For this model, we use Dozat and Man- <ref type="bibr">7</ref> See https://github.com/tzshi/dp-parser-emnlp17 .</p><p>ning's (2017) biaffine scoring model, although in our case the model size is smaller. 8 Analogously to the dev-set results given in §2, on the test data, the minimal feature sets perform as well as larger ones in locally-trained models. And there exists a clear trend of global models out- performing local models for the two different tran- sition systems on both datasets. This illustrates the effectiveness of exact decoding and global train- ing. Of the three types of global models, the arc- eager arguably has the edge, an empirical finding resonating with our theoretical comparison of their model expressiveness.</p><p>Comparison with State-of-the-Art Models <ref type="figure" target="#fig_9">Figure 2</ref> compares our algorithms' results with those of the state-of-the-art. <ref type="bibr">9</ref> Our models are competitive and an ensemble of 15 globally- trained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reach-ing the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work Not Yet Mentioned</head><p>Approximate Optimal Decoding/Training Be- sides dynamic programming <ref type="bibr">(Huang and Sagae, 2010;</ref><ref type="bibr">Kuhlmann et al., 2011</ref>), various other ap- proaches have been proposed for approaching global training and exact decoding. Best-first and A* search <ref type="bibr" target="#b20">(Klein and Manning, 2003;</ref><ref type="bibr" target="#b11">Sagae and Lavie, 2006;</ref><ref type="bibr" target="#b12">Sagae and Tsujii, 2007;</ref><ref type="bibr" target="#b29">Zhao et al., 2013;</ref><ref type="bibr" target="#b19">Thang et al., 2015;</ref><ref type="bibr">Lee et al., 2016)</ref> give optimality certificates when solutions are found, but have the same worst-case time com- plexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search ( <ref type="bibr" target="#b28">Zhang and Clark, 2011)</ref>, dynamic oracles ( <ref type="bibr">Nivre, 2012, 2013;</ref><ref type="bibr">Cross and Huang, 2016b</ref>) and error states <ref type="bibr" target="#b21">(Vaswani and Sagae, 2016)</ref>. Beam search records the k best-scoring transition pre- fixes to delay local hard decisions, while the lat- ter two leverage configurations deviating from the gold transition path during training to better simu- late the test-time environment.</p><p>Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers <ref type="bibr">(Henderson, 2003</ref><ref type="bibr">(Henderson, , 2004</ref><ref type="bibr">Chen and Manning, 2014;</ref><ref type="bibr" target="#b23">Weiss et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr">Dozat and Manning, 2017</ref>) because of their ex- pressive representation power. Recently, <ref type="bibr" target="#b17">Stern et al. (2017)</ref> have proposed minimal span-based features for constituency parsing.</p><p>Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree ( <ref type="bibr">Le and Zuidema, 2014;</ref><ref type="bibr">Dyer et al., 2015;</ref><ref type="bibr">Kiperwasser and Goldberg, 2016b)</ref>, but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Concluding Remarks</head><p>In this paper, we have shown the following.</p><p>• The bi-LSTM-powered feature set t ÑÐ s 0 , ÑÐ b 0 u is minimal yet highly effective for arc-hybrid and arc-eager transition-based parsing.</p><p>• Since DP algorithms for exact decoding <ref type="bibr">(Huang and Sagae, 2010;</ref><ref type="bibr">Kuhlmann et al., 2011</ref>) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn 3 q, feasible for practice.</p><p>• Combining exact decoding with global train- ing -which is also enabled by our minimal feature set -with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and second- best results to date on these data sets among "purely" dependency-based approaches.</p><p>There are many directions for further explo- ration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition sys- tems. It would also be interesting to further in- vestigate relationships between graph-based and dependency-based parsing. In §5 we have men- tioned important earlier work in this regard, and provided an update to those formal findings.</p><p>In our work, we have brought exact decoding, which was formerly the province solely of graph- based parsing, to the transition-based paradigm. We hope that the future will bring more inspira- tion from an integration of the two perspectives. <ref type="bibr">Liang Huang and Kenji Sagae. 2010</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Features</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ÑÐ w n s, where each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ÑÐ s 0 , ÑÐ b 0 u our minimal feature set with respect to arc-hybrid and arc-eager, and empirically confirm that Cross and Huang's t ÑÐ s 0 , ÑÐ s 1 , ÑÐ b 0 u is minimal for arc-standard; see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>precondition: s 0 not attached to any word) rarpσ|s 0 , b 0 |β, Aqs " pσ|s 0 |b 0 , β, A Y tps 0 , b 0 quq rerpσ|s 0 , β, Aqs " pσ, β, Aq (precondition: s 0 has been attached to its head) 3.2 Deduction and Dynamic Programming Kuhlmann et al. (2011) reformulate the three tran- sition systems just discussed as deduction systems (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>d) Edge-factored graph-based parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 1a-1c: Kuhlmann et al.'s inference rules for three transition systems, together with CKY-style visualizations of the local structures involved and, to their right, conditions for the rule to apply. 1d: the edge-factored graph-based parsing algorithm (Eisner and Satta, 1999) discussed in §5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing our UAS results with results from the literature. x-axis: PTB; y-axis: CTB. Most datapoint labels give author initials and publication year; citations are in the bibliography. Ensemble datapoints are annotated with ensemble size. Weiss et al. (2015) and Andor et al. (2016) achieve UAS of 94.26 and 94.61 on PTB with beam search, but did not report CTB results, and are therefore omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Arc-standard Arc-hybrid Arc-eager</head><label></label><figDesc></figDesc><table>t 

ÑÐ 

s 2, 

ÑÐ 

s 1, 

ÑÐ 

s 0, 

ÑÐ 

b 0u 93.95˘0.12 94.08˘0.13 93.92˘0.04 

t 

ÑÐ 

s 1, 

ÑÐ 

s 0, 

ÑÐ 

b 0u 94.13˘0.06 94.08˘0.05 93.91˘0.07 

t 

ÑÐ 

s 0, 

ÑÐ 

b 0u 54.47˘0.36 94.03˘0.12 93.92˘0.07 

t 

ÑÐ 

b 0u 47.11˘0.44 52.39˘0.23 79.15˘0.06 

Min positions Arc-standard Arc-hybrid Arc-eager 

K&amp;G 2016a 
-
4 
-
C&amp;H 2016a 
3 
-
-
our work 
3 
2 
2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Similarly, for edges k ñ i in the other direction, we set</figDesc><table>Model 

Training 
Features 
PTB 
CTB 
UAS (%) UEM (%) 
UAS (%) UEM (%) 

Arc-standard 
Local 

t 

ÑÐ 

s 2, 

ÑÐ 

s 1, 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

93.95 ˘0.12 52.29 ˘0.66 
88.01 ˘0.26 36.87 ˘0.53 

Arc-hybrid 
Local 

t 

ÑÐ 

s 2, 

ÑÐ 

s 1, 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

93.89 ˘0.10 50.82 ˘0.75 
87.87 ˘0.17 35.47 ˘0.48 
Local 

t 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

93.80 ˘0.12 49.66 ˘0.43 
87.78 ˘0.09 35.09 ˘0.40 
Global 

t 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

94.43 ˘0.08 53.03 ˘0.71 
88.38 ˘0.11 36.59 ˘0.27 

Arc-eager 
Local 

t 

ÑÐ 

s 2, 

ÑÐ 

s 1, 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

93.80 ˘0.12 49.66 ˘0.43 
87.49 ˘0.20 33.15 ˘0.72 
Local 

t 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

93.77 ˘0.08 49.71 ˘0.24 
87.33 ˘0.11 34.17 ˘0.41 
Global 

t 

ÑÐ 

s 0, 

ÑÐ 

b 0u 

94.53 ˘0.05 53.77 ˘0.46 
88.62 ˘0.09 37.75 ˘0.87 

Edge-factored 
Global 

t 

ÑÐ 

h , 

ÑÐ 

mu 

94.50 ˘0.13 53.86 ˘0.78 
88.25 ˘0.12 36.42 ˘0.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Test set performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure.</figDesc><table></table></figure>

			<note place="foot" n="1"> We note that K&amp;G were not focused on minimizing positions, although they explicitly noted the implications of doing so: &quot;While not explored in this work, [fewer positions] results in very compact state signatures, [which is] very appealing for use in transition-based parsers that employ dynamicprogramming search&quot; (pg. 319). C&amp;H also noted in their follow-up (Cross and Huang, 2016b) the possibility of future work using dynamic programming thanks to simple features.</note>

			<note place="foot" n="4"> For simplicity, we only present unlabeled parsing here. See Shi et al. (2017) for labeled-parsing results. 5 Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013).</note>

			<note place="foot">Proof Sketch. We leverage the fact that the arceager model divides the sh transition in the archybrid model into two separate transitions, sh and ra. When we constrain the parameters f sh &quot; f ra in the arc-eager model, the model hypothesis space becomes exactly the same as arc-hybrid&apos;s. The extra expressiveness of the arc-eager model comes from the scoring functions f sh and f re that capture structural contexts other than headmodifier relations. Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions.</note>

			<note place="foot" n="6"> For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u.</note>

			<note place="foot" n="8"> The same architecture and model size as other transitionbased global models is used for fair comparison. 9 We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses. They produce higher PTB UAS, but access more training information and do not directly apply to datasets without constituency annotation.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training with exploration improves a greedy stack LSTM parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2005" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going to the roots of dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="13" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. DyNet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies</title>
		<meeting>the 8th International Workshop on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing as deduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 21st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A best-first probabilistic shift-reduce parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dependency parsing and domain adaptation with LR models and parser ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1044" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining global models for parsing Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principles and implementation of deductive parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Schabes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic Structure Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituent parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal shift-reduce constituent parsing with structured perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Quang Le Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1534" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient structured inference for transition-based parsing with neural networks and error states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a large-scale annotated Chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies</title>
		<meeting>the 8th International Workshop on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recognition and parsing of context-free languages in time n 3. Information and Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Younger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="189" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal incremental parsing via best-first dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="768" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
