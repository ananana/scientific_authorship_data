<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social Media Text Classification under Negative Covariate Shift</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geli</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Chicago Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@cs.uic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Chicago Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Social Media Text Classification under Negative Covariate Shift</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In a typical social media content analysis task, the user is interested in analyzing posts of a particular topic. Identifying such posts is often formulated as a classification problem. However, this problem is challenging. One key issue is covariate shift. That is, the training data is not fully representative of the test data. We observed that the covariate shift mainly occurs in the negative data because topics discussed in social media are highly diverse and numerous, but the user-labeled negative training data may cover only a small number of topics. This paper proposes a novel technique to solve the problem. The key novelty of the technique is the transformation of document representation from the traditional n-gram feature space to a center-based similarity (CBS) space. In the CBS space, the covariate shift problem is significantly mitigated, which enables us to build much better classifiers. Experiment results show that the proposed approach markedly improves classification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Applications using social media data, such as reviews, discussion posts, and (micro) blogs are becoming increasingly popular. We observed from our collaborations with social science and health science researchers that in a typical appli- cation, the researcher first need to obtain a set of posts of a particular topic that he/she wants to study, e.g., a political issue. Keyword search is often used as the first step. However, that is not sufficient due to low precision and low recall. A post containing the keyword "politics" may not be a political post while a post that does not con- tain the keyword may be a political post. Thus, text classification is needed to make more so- phisticated decisions to improve accuracy.</p><p>For classification, the user first manually la- bels a set of relevant posts (positive data) about the political issue and irrelevant posts (negative data) not about the political issue and then builds a classifier by running a learning algorithm, e.g. SVM or naïve Bayes. However, the resulting classifier may not be satisfactory. There may be many reasons. One key reason we observed is that the labeled negative training data is not fully representative of the negative test data.</p><p>Let the user-interested topic be P (positive), and the set of all other irrelevant topics discussed in a social media source be T = {T 1 , T 2 , …, T n }, which forms the negative data. n is usually large. However, due to the labor-intensive effort of manual labeling, the user can label only a certain number of training posts. Then the labeled nega- tive training posts may cover only a small num- ber of irrelevant topics S of T (S ⊆ T) as nega- tive. Further, due to the highly dynamic nature of social media, it is probably impossible to label all possible negative topics. In testing, when posts of other negative topics in T−S show up, their classification can be unpredictable. For ex- ample, in an application, the training data has no negative examples about sports. However, in testing, some sports posts show up. These unex- pected sports posts may be classified arbitrarily, which results in low classification accuracy. In this paper, we aim to solve this problem.</p><p>In machine learning, this problem is called covariate shift, a type of sample selection bias. In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution. However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different <ref type="bibr" target="#b13">(Heckman 1979;</ref><ref type="bibr" target="#b34">Shimodaira 2000;</ref><ref type="bibr" target="#b46">Zadrozny 2004;</ref><ref type="bibr" target="#b14">Huang et al. 2007;</ref><ref type="bibr" target="#b36">Sugiyama et al. 2008;</ref><ref type="bibr" target="#b3">Bickel et al. 2009</ref>). In general, the sample selec- tion bias problem is not solvable because the two distributions can be arbitrarily far apart from each other. Various assumptions were made to solve special cases of the problem. One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test data sets <ref type="bibr" target="#b34">(Shimodaira 2000;</ref><ref type="bibr" target="#b14">Huang et al. 2007;</ref><ref type="bibr" target="#b3">Bickel et al. 2009</ref>). This gives the covariate shift problem.</p><p>In this paper, we focus on a special case of the covariate shift problem. We assume that the co- variate shift problem occurs mainly in the nega- tive training and test data, and no or minimum covariate shift exists in the positive training and test data. This assumption is reasonable because the user knows the type of posts/documents that s/he is looking for and can label many of them.</p><p>Following the notations in ( <ref type="bibr" target="#b3">Bickel et al. 2009</ref>), our special case of the covariate shift problem can be stated formally as follows: let the set of training examples be {(x 1 , y 1 ), (x 2 , y 2 ), …, (x k , y k )}, where x i is the data/feature vector and y i is the class label of x i . Let the set of test cases be {x k+1 , x k+2 , …, x n }, which have no class labels. Since we are interested in binary classification, y i is either 1 (positive class) or -1 (negative class). The labeled training data and the unseen test data have the same target conditional distribution p(y|x) and the marginal distributions of the posi- tive data in both the training and testing are also the same. But the marginal distributions of the negative data in the training and testing are dif- ferent, i.e., í µí± ! (í µí°± ! ) ≠ í µí± ! (í µí°± ! ), where L, T, and - represent the labeled training data, test data, and the negative class respectively.</p><p>Existing methods for addressing the covariate shift problem basically work as follows (see the Related Work section). First, they estimate the bias of the training data based on the given test data using some statistical techniques. Then, a classifier is trained on a weighted version of the original training set based on the estimated bias. Requiring the test data to be available in training is, however, a major weakness. In the social me- dia post classification setting, the system needs to constantly classify the incoming data. It is in- feasible to perform training constantly.</p><p>In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific na- ture of our problem, i.e., the positive training data does not have the covariate shift issue.</p><p>One obvious solution to this problem is one- class classification ( <ref type="bibr" target="#b33">Schölkopf et al. 1999;</ref><ref type="bibr" target="#b37">Tax and Duin, 1999a)</ref>, i.e., one-class SVM. We simp- ly discard the negative training posts/documents completely because they have the covariate shift problem. Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly. Although it is conceivable to use an unsuper- vised method such clustering, SVD ( <ref type="bibr" target="#b0">Alter et al., 2000</ref>) or LDA ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>, supervised learning usually give much higher accuracy.</p><p>In our proposed method, instead of perform- ing supervised learning in the original document space based on n-grams, we perform learning in a similarity space. Thus, the key novelty of the method is the transformation from the original document space (DS) to a center-based similarity space (CBS). In the new space, the covariate shift problem is significantly mitigated, which enables us to build more accurate classifiers. The reason for this is that in CBS based learning the vectors in the similarity space enable SVM (which is the learning algorithm that we use) to find a good boundary of the positive class data based on similarity and to separate it from all possible negative class data, including those neg- ative data that is not represented in training. We will explain this in greater detail in Section 3.5 after we present the proposed algorithm, which we call CBS-L (for CBS Learning).</p><p>This paper makes three contributions: First, it formulates a special case of the covariate shift problem. This case occurs frequently in social media data classification as we discussed above. Second, it proposes a novel CBS space based learning method, CBS-L, which avoids the co- variate shift problem to a large extent because it is able to find a good similarity boundary of the positive data. Third, it experimentally demon- strates the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional supervised learning assumes that the training and test examples are drawn from the same distribution. However, this assumption can be violated in many applications. This is espe- cially the case for social media data because of the high topic diversity and constant changes of topics. This problem is known as covariate shift, which is a form of sample selection bias.</p><p>Sample selection bias was first introduced in econometrics by <ref type="bibr" target="#b13">Heckman (1979)</ref>. It came into the field of machine learning through the work of <ref type="bibr" target="#b46">Zadrozny (2004)</ref>. The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to compensate for the bias ( <ref type="bibr" target="#b3">Bickel et al. 2009</ref>).</p><p>For example, <ref type="bibr" target="#b34">Shimodaira (2000)</ref> and Sugiya- ma and <ref type="bibr" target="#b35">Muller (2005)</ref> proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training exam- ples. <ref type="bibr" target="#b10">Dudik et al. (2005)</ref> and <ref type="bibr" target="#b4">Bickel and Scheffer (2007)</ref> used maximum entropy density estima- tion, while <ref type="bibr" target="#b14">Huang et al. (2007)</ref> proposed kernel mean matching. <ref type="bibr" target="#b36">Sugiyama et al. (2008)</ref> and <ref type="bibr" target="#b41">Tsuboi et al. (2008)</ref> estimated the weights for the training instances by minimizing the Kullback- Leibler divergence between the test and the weighted training distributions. <ref type="bibr" target="#b3">Bickel et al. (2009)</ref> proposed an integrated model. As we dis- cussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn't have this restriction.</p><p>As mentioned in the introduction, one-class classification is a suitable approach to solve the problem. <ref type="bibr" target="#b37">Duin (1999a and</ref><ref type="bibr" target="#b38">1999b)</ref> pro- posed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the mini- mum radius. In order to balance between model over-fitting and under-fitting, <ref type="bibr" target="#b39">Tax and Duin (2001)</ref> proposed a method that tries to use artifi- cially generated outliers to optimize the model parameters. However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 di- mensions. Also, as pointed out by <ref type="bibr" target="#b17">(Khan and Madden, 2010;</ref><ref type="bibr" target="#b18">2014)</ref>, one drawback of their methods is that they often require a large dataset and the methods become very inefficient in high dimensional feature spaces. Since text documents are usually represented in a much higher dimen- sional space, these methods are less suitable for text applications. <ref type="bibr" target="#b27">Manevitz and Yousef (2001)</ref> performed one-class text classification using one-class SVM as proposed by <ref type="bibr" target="#b33">Schölkopf et al. (1999)</ref>. The method is based on identifying outli- er data that are representative of the second class. Instead of assuming the origin is the only mem- ber of the outlier class, it assumes those data points with few non-zero entries are also outliers. However, as reported in the paper, their methods produce quite weak results ( <ref type="bibr" target="#b33">Schölkopf et al., 1999;</ref><ref type="bibr" target="#b34">2000)</ref>.  presented an im- proved version of one-class SVM for detecting anomalies. Their idea is to consider all data points that are close to the origin as outliers. <ref type="bibr">Both (Yang and Madden, 2007)</ref> and <ref type="bibr" target="#b40">(Tian and Gu, 2010)</ref> tried to refine Schölkopf's models by searching optimal parameters. <ref type="bibr" target="#b26">Luo et al., (2007)</ref> proposed a cost-sensitive one-class SVM algo- rithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method.</p><p>In this work, we propose to represent docu- ments in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications ( <ref type="bibr" target="#b9">Radev et al., 2000;</ref><ref type="bibr" target="#b12">He et al., 2004;</ref><ref type="bibr" target="#b19">Lebanon 2006;</ref><ref type="bibr" target="#b30">Ranzato and</ref><ref type="bibr">Szummer, 2008, Wang and</ref><ref type="bibr" target="#b42">Domeniconi, 2008</ref>). In ( <ref type="bibr" target="#b9">Radev et al., 2000</ref>), although the centroid sen- tence/document vector was computed, it was not transformed to a similarity space vector represen- tation. <ref type="bibr" target="#b42">Wang and Domeniconi (2008)</ref> proposed to use external knowledge to build semantic ker- nels for documents in order to improve text clas- sification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the en- riched document representations could help solve our problem.</p><p>Our work is also related to learning from posi- tive and unlabeled examples, also known as PU learning <ref type="bibr" target="#b8">(Denis, 1998;</ref><ref type="bibr" target="#b45">Yu et al. 2002;</ref><ref type="bibr" target="#b24">Liu et al. 2003;</ref><ref type="bibr" target="#b20">Lee and Liu, 2003;</ref><ref type="bibr" target="#b11">Elkan and Noto, 2008;</ref><ref type="bibr" target="#b23">Li et al. 2010)</ref>. In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data.</p><p>Our problem is also very different from do- main adaption as we work in the same domain. Due to the use of document similarity, our meth- od has some resemblance to learning to rank <ref type="bibr" target="#b21">(Li, 2011;</ref><ref type="bibr" target="#b25">Liu, 2011</ref>). However, CBS-L is very dif- ferent because we perform supervised classifica- tion. Our similarity is also center-based rather than pair-wise document similarity, which is also used in (Qian and Liu 2013) for spam detection. and a simple example. The detailed algorithm follows. In Section 3.5, we explain why CBS-L is better than DS-based learning when unex- pected negative data appear in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Idea</head><p>In the proposed CBS-L formulation, each docu- ment d is still represented as a feature vector, but the vector no longer represents the document d itself based on n-grams. Instead, it represents a set of similarity values between document d and the center of the positive documents. Specifically, the learning consists of the following steps: where y:z represents a ds-feature y (e.g., a word) and its feature value (e.g., term fre- quency, tf). We want to transform the follow- ing positive document d 1 and negative docu- ment d 2 (ds-vectors) to their cbs-vectors (the first number is the class): We now have a binary classification problem in the CBS space. This step simply runs a classification algorithm, e.g., SVM, to build a classifier. We use SVM in our work.</p><formula xml:id="formula_0">d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CBS Based Learning</head><p>We are given a binary text classification problem. Let D = {(d 1 , y 1 ), (d 2 , y 2 ), …, (d n , y n )} be the set of training examples, where d i is a document and y i ∈ {1, -1} is its class label. Traditional classi- fication directly uses D to build a binary classifi- er. However, in the CBS space, we learn a classi- fier that returns 1 for documents that are "close enough" to the center of the training positive documents and -1 for documents elsewhere. We now detail the proposed technique. As we mentioned above, instead of using one single ds- vector to represent a document d i ∈D, we use a set</p><formula xml:id="formula_1">R d of p ds-vectors R d = {í µí°± ! ! , í µí°± ! ! , …, í µí°± ! ! }.</formula><p>Each vector í µí°± ! ! denotes one document space rep- resentation of the document, e.g., unigram repre- sentation. We then compute the center of positive training documents, which is represented as a set of í µí± centroids C = {c 1 , c 2 , …, c p }, each of which corresponds to one document space representa- tion in R d . The way to compute each center c i is similar to that in the Rocchio relevance feedback method in information retrieval <ref type="bibr" target="#b31">(Rocchio, 1971;</ref><ref type="bibr" target="#b28">Manning et al. 2008)</ref>, which uses the correspond- ing ds-vectors of all training positive and nega- tive documents. The detail will be given below. Based on R d for document d and the center C, we can transform a document d from its document space representations R d to one center-based sim- ilarity vector cbs-v by applying a similarity func- tion í µí±í µí±í µí± on each element í µí°± ! ! of R d and its corre- sponding center c i . We now detail document transformation.</p><p>Training document transformation: The train-ing data transformation from ds-vectors to cbs- vectors performs the following two steps:</p><p>Step 1: Compute the set C of centroids for the positive class. Each centroid vector c i ∈C is for one document representation í µí°± ! ! . And it is computed by applying the Rocchio method to the corresponding ds-vectors of all documents in both positive and negative training data.</p><formula xml:id="formula_2">í µí° ! = í µí»¼ í µí°· ! í µí°± ! ! í µí°± ! ! í µí°í µí°¬ ! ! ∈! ! − í µí»½ |í µí°· − í µí°· ! | í µí°± ! ! í µí°± ! ! í µí°± ! ! ∈!!! !</formula><p>where í µí°· ! is the set of documents in the posi- tive class and |.| is the size function. í µí»¼ and í µí»½ are parameters, which are usually set empiri- cally. It is reported that using tf-idf representa- tion, í µí»¼ = 16 and í µí»½ = 4 usually work quite well ( <ref type="bibr" target="#b2">Buckley et al. 1994</ref>). The subtraction is used to reduce the influence of those terms that are not discriminative (i.e., terms appear- ing in both positive and negative documents).</p><p>Step 2: Compute the similarity vector cbs-v d (center-based similarity space vector) for each document d ∈D based on its set of document space vectors R d and the corresponding cen- troids C of the positive documents.</p><formula xml:id="formula_3">cbs-v d = Sim(R d , C)</formula><p>Sim has a set of similarity measures, and each measure m j is applied to p document represen- tations í µí°± ! ! in R d and their corresponding cen- ters í µí° ! in C to generate p similarity features (cbs-features) in cbs-v d . We discuss the ds- features and similarity measures for compu- ting cbs-features in the next two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity:</head><p>The data transformation step is clearly linear in the number of examples, i.e., n.</p><p>Test document transformation: For each test document d, we can use step 2 above to produce a cbs-vector for d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DS-Features</head><p>In order to compute cbs-features (center-based similarity space features) for each document, we need to have the ds-features of a document and the center of the positive class. We discuss ds- features first, which are extracted from each doc- ument itself.</p><p>Since our task is document classification, we use the popular unigram, bigram and trigram with tf-idf weighting as the ds-features for a doc- ument. These three types of ds-features also give us three different document representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CBS-Features</head><p>Ds-vectors are transformed into cbs-vectors by applying a set of similarity measures on each document space vector and the corresponding center vector. In this work, we employed five similarity measures from <ref type="bibr" target="#b5">(Cha, 2007)</ref> to gauge the similarity of two vectors. Based on these measures, we produce 15 CBS features using the unigram, bigram, and trigrams representations of each document. The similarity measures we used are listed in <ref type="table" target="#tab_0">Table 1</ref>, where P and Q are two vec- tors and d represents the dimension of P and Q. <ref type="table" target="#tab_0">Table 1</ref>: similarity measures for CBS-Features</p><formula xml:id="formula_4">í µí± !"# = í µí± ! í µí± ! ! !!! í µí± ! ! ! !!! í µí± ! ! ! !!! í µí± !"# = 1 − 1 í µí± í µí± ! í µí± ! ! ! !!! − í µí± ! í µí± ! ! ! !!! ! !!! í µí± !"# = 1 − í µí±í µí± 1 + í µí± ! − í µí± ! ! !!! í µí± !"#$ = 2 í µí± ! í µí± ! ! !!! í µí± ! ! ! !!! + í µí± ! ! ! !!! í µí± !"# = í µí± ! í µí± ! ! !!! í µí± ! ! ! !!! + í µí± ! ! ! !!! − í µí± ! í µí± ! ! !!!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Why Does CBS Space Learning Work?</head><p>We now try to explain why CBS learning (CBS- L) can deal with the covariate shift problem, and thus can perform better than document space learning. The reason is that due to the use of sim- ilarity features, CBS-L is essentially trying to generate a boundary for the positive training data because similarity is not directional and thus co- vers all directions in a spherical shape in the space. In classification, the negative data from anywhere or direction outside the spherical shape can be detected. The covariate shift problem will not affect the classification much. Many types of documents that are not represented in the nega- tive training data will still be detected due to their low similarity. For example, in <ref type="figure" target="#fig_1">Figure 1</ref>, we want to build a SVM classifier to separate posi- tive data represented as black squares and nega- tive data represented as empty circles. The con- structed CBS-L classifier would look like a circle (in dashed line) in the original document space covering the positive data. The size of this (boundary) circle depends on the separation mar- gin between the two classes. Although data points represented by empty triangles are not represented in the negative training data (which has only empty circles) in building the classifier, our classifier is able to identify them as not posi- tive at the test time because they are outside the boundary circle. If we had used the document space (DS) features to build a SVM classifier, the classifier would be a line (see <ref type="figure" target="#fig_1">Figure 1</ref>) between the positive data (black squares) and the negative data (empty cir- cles). This line unfortunately will not be able to identify data points represented as empty trian- gles as not positive because the triangles actually lie on the positive side and would be classified as positive, which is clearly wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed learning in the center-based similarity space (CBS-L) and compare it with baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Dataset</head><p>As stated at the beginning of the paper, this work was motivated by the real-life problem of identi- fying the right social media posts or documents for specific applications. For an effective evalua- tion, we need a large number of classes in the data to reflect the topic richness and diversity of the social media. The whole data also has to be labeled for evaluation. Using online reviews of a large number of products is a natural choice be- cause there are many types of products and ser- vices and there is no need to do manual labeling, which is very labor intensive, time consuming, and error prone. We obtained the Amazon review database from the authors of (Jindal and Liu 2008), and constructed a dataset with reviews of 50 types of products, which we also call 50 top- ics. Each topic (a type of products) have 1000 reviews. For each topic, we randomly sampled 700 reviews/documents for training and the re- maining 300 reviews for testing. Note that alt- hough we use this product review collection, we do not perform sentiment classification. Instead, we still perform the traditional topic based classi- fication. That is, given a review, the system de- cides what type of product the review is about. In our experiments, we use every topic as the posi- tive class. This gives us 50 classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We use three baselines in our evaluation.</p><p>Document space one-class SVM (ds-osvm): As we discussed earlier, due to the covariate shift problem in the negative training data, one solu- tion is to drop the negative training data com- pletely to build a one-class classifier. One-class SVM is the state-of-the-art one-class classifica- tion algorithm. We apply one-class SVM to the documents in the document space as one of the baselines. One-class SVM was first introduced by <ref type="bibr" target="#b33">Schölkopf et al. (1999;</ref><ref type="bibr" target="#b34">2000)</ref>, which is based on the assumption that the origin is the only member of the second class. The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is em- ployed to construct a hyper-plane that separates the data and the original with maximum margin. As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by <ref type="bibr" target="#b37">Tax and Duin (1999a;</ref><ref type="bibr" target="#b38">1999b)</ref>. SVDD seeks to distinguish the positive class from all other possible data in space. It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius. It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in ( <ref type="bibr" target="#b18">Khan and Madden, 2014)</ref> demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied. Thus in this paper, we just use one- class SVM, which is one of the SVM-based clas- sification tools in the LIBSVM 1 library (version 3.20) (Chang and Lin, 2011).</p><p>Center-based similarity space one-class SVM (cbs-osvm): Instead of applying one-class SVM to documents in the original document space, this baseline applies it to the CBS space after the documents are transformed to CBS vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM:</head><p>This baseline is the SVM applied in the original document space. Although in this case, there is covariate shift problem, we want to see how serious the problem might be, and how the proposed CBS-L technique can deal with the problem. We use the SVM tool in LIBSVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Kernels and Parameters</head><p>As <ref type="bibr" target="#b18">Khan and Madden (2014)</ref> pointed out that one-class SVM performs the best when Gaussian kernel is used, we use Gaussian kernel as well. <ref type="bibr" target="#b27">Manevitz and Yousef (2001)</ref> applied one-class SVM to text classification, and the authors re- ported that one-class SVM works the best with binary feature weighting scheme compared to tf or tf-idf weighting schemes. Also, they reported that a small number of features (10) with highest document frequency performed the best with Gaussian kernel. We also use binary representa- tion, but found that 10 features are already too many in our case. In fact, 5 features give the best results. Using a small number of features is intui- tive because to find the boundary of a very high dimensional space is very difficult. We also tried more features but they were poorer. For SVM classification in the document space, we use the linear kernel as it has been shown by many researchers that the linear kernel performs the best (e.g., <ref type="bibr" target="#b15">Joachims, 1998;</ref><ref type="bibr" target="#b7">Colas and Brazdil, 2006</ref>). We experimented with RBF kernels ex- tensively, but they did not perform well with the traditional document representation. The term weighting scheme is tf-idf ( <ref type="bibr" target="#b7">Colas and Brazdil, 2006</ref>) with no feature selection.</p><p>For our proposed method CBS-L, we use tf-idf values of unigram, bigram and trigram to repre- sent a document in three ways in the document space. As mentioned earlier, five document simi- larity functions are used to transform document space vectors to CBS space vectors. And in order to filter out less useful features for the center vector of the positive class, we performed feature selection in the document space using the classic information gain method <ref type="bibr" target="#b43">(Yang and Pedersen, 1997</ref>) to empirically choose the most effective 100 features for the positive class. For all the kernels, we use the default parame- ter settings in the LIBSVM systems. We tried to tune the parameters, but did not get better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We now present the experiment results. As men- tioned above, we treat each topic as the positive class. This gives 50 tests. To test the effect of covariate shift, we also vary the number of topics in the negative class. We used 10, 20, 30, and 40 topics in the training negative class. The test set always has 49 topics of negative data.</p><p>For each setting, we give three sets of results for the positive class, which is the target topic data that we are interested in obtaining through classification. Each set of results includes the standard measures of precision, recall, and F1- score for the positive class. The three sets are: 1. In-training: In this case, the test negative data <ref type="table" target="#tab_0">In-training  Out-of-training  Combined  precision recall  F1-score precision recall  F1-score precision recall F1-</ref>  <ref type="table">Table 2</ref>: Summary results of the 50 topics contains only data from those topics used in training. This is the classical supervised learn- ing setting where the training and test data are randomly drawn from the same distribution. 2. Not-in-training: In this case, the test negative set contains only data from the other topics not used in training. The classical setting of supervised learning does not deal with this problem. This represents covariate shift. 3. Combined: In this case, the test data contains both in-training and not-in-training negative topics. Due to the use of not-in-training test data, this is also not the classical setting. Due to a large number of experiment results, we cannot report all the details. <ref type="table">Table 2</ref> summarizes the results. Notice that for ds-osvm, it does not make sense to have in-training and not-in- training results because it does not use any train- ing negative data. Thus, there is only one set of results for "Combined," which is duplicated in the table for easy comparison. However, note that cbs-osvm uses negative data for training in order to compute the center for the positive class.</p><p>From the table, we can make the following observations (since there are many numbers, we only focus on F1-scores). 1. The proposed CBS-L method performs mark- edly better than all baselines. For the results of in-training, not-in-training, and combined, CBS-L is consistently better in all cases than all baselines. Even for in-training, CBS-L per- form better than SVM. This clearly shows the superiority of the proposed CBS-L method. 2. ds-osvm performs poorly. cbs-osvm is much better because it uses the negative data in fea- ture selection and center computation. 3. SVM in the document space performed poorly (Combined) when only a small number of negative topics are used in training. It gets better than both one-class SVM baselines when more negative topics are used in train- ing (see the reason in the next point). 4. Finally, we can also see that with the number of training negative topics increases, the re- sults of the combined case of both SVM and CBS-L improve. This is expected because with the increased number of negative topics for training, the number of not-in-training negative topics for testing decreases and the covariate shift problem gets smaller. We can also see that cbs-osvm, SVM and CBS-L's F1-scores for not-in-training improve with the increased training negative topics due to the same reason. However, their F1-scores drop for in-training because with more negative   <ref type="table">Table 3</ref>: F1-score for each positive topic or class in the combined case topics, the data becomes more skewed, which hurts in-training classification.</p><p>To give a flavor of the detailed results for each topic (product), we give the full results for one setting with 30 randomly selected topics as the training negative data <ref type="table">(Table 3)</ref>. The results in the table are F1-scores of the combined case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The ability to get relevant posts accurately about a topic from social media is a challenging prob- lem. This paper attempted to solve this problem by identifying and dealing with the technical is- sue of covariate shift. The key idea of our tech- nique is to transform document representation from the traditional n-gram feature space to a similarity based space. Our experimental results show that the proposed method CBS-L outper- formed strong baselines by large margins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>If we use cosine as the first similarity measure in Sim, we can generate a cbs-feature 1:0.50 for d 1 (as cosine(c, d 1 ) = 0.50) and a cbs- feature 1:0.27 for d 2 (as cosine(c, d 2 ) = 0.27). If we have more similarity measures, more cbs-features will be produced. The resulting cbs-vectors for d 1 and d 2 with their class la- bels, 1 and -1, are: d 1 : 1 1:0.50 … d 2 : -1 1:0.27 … 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CBS learning vs. DS learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 . Each document d (in the positives d (called a cbs-vector). s d consists of a set of similarity values beis called an cbs-feature. s d still has the same original class label as d. Let us</head><label>1</label><figDesc></figDesc><table>or negative 
class) is first represented with a set of docu-
ment representations, i.e., document space 
vectors (ds-vectors) based on the document it-
self as in traditional text classification. Each 
vector denotes one representation of the doc-
ument. For example, one representation may 
be based on only unigrams, and another rep-
resentation may be based on only bigrams. 
For simplicity, we use only one representa-
tion/vector x (e.g., unigrams) here to represent 
d. Note that we use bold lower case letters to 
represent vectors. Each feature in a ds-vector 
is called a ds-feature. 
2. A center vector c is then computed for each 
document representation for the positive class 
documents using the ds-vectors of all positive 
and negative documents of that representation. 
c is thus also a ds-vector. 
3. Each document d in the positive and negative 
class is then transformed to a center-based 
similarity space vector -
tween document d's set of ds-vectors, i.e., {x} 
in our case here (since we use only one repre-
sentation), and the set of corresponding posi-
tive class center vectors, i.e., {c} in our case: 

s d =Sim({x}, {c}), 

where Sim is a similarity function consisting 
of a set of similarity measures. Each feature in 
s d see an actual 
example. We assume that our single center 
vector for the positive class has been comput-
ed (see Section 3.2) based on the unigram rep-
resentation of documents: 

c: 1:1 2:1 6:2 

</table></figure>

			<note place="foot" n="3"> The Proposed CBS Learning We now formulate the proposed supervised learning in the CBS space, called CSB-L. The key difference between CBS learning and the classic document space (DS) learning is in the document representation, which applies to both training and testing documents or posts. In the next subsection, we first give the intuitive idea</note>

			<note place="foot" n="1"> http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by the NSF grants IIS-1111092 and IIS-1407927, and a Google faculty award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Singular Value Decomposition for GenomeWide Expression Data Processing and Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nat&apos;,l Academy of Science</title>
		<meeting>Nat&apos;,l Academy of Science</meeting>
		<imprint>
			<date type="published" when="2000-08" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="10101" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Effect of Adding Relevance Information in a Relevance Feedback Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR Conference</title>
		<meeting>SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative learning under covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scheffer</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dirichletenhanced spam filtering based on biased samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Models and Methods in Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="300" to="307" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Comparison of SVM and some older classification algorithms in text classification tasks. Artificial Intelligence in Theory and Practice. IFIP International Federation for Information Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Brazdil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">PAC learning from positive statistical queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>ALT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budzikowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLP/NAACL Workshop on Summarization</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias in maximum entropy density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="213" to="220" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Locality Preserving Indexing for Document Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Of SIGIR</title>
		<meeting>Of SIGIR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Opinion Spam and Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Web Search and Data Mining</title>
		<meeting>the ACM Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of recent trends in one class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Cognitive Science</title>
		<imprint>
			<biblScope unit="page" from="188" to="197" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">One-Class Classification: Taxonomy of Study and Review of Techniques. The Knowledge Engineering Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sequential document representations and simplicial curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>UAI</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with Positive and Unlabeled Examples Using Weighted Logistic Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to Rank for Information Retrieval and Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Morgan &amp; Claypool publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving One-class SVM for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the Second International conference on Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3077" to="3081" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Negative Training Data can be Harmful to Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-K</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">P</forename></persName>
		</author>
		<title level="m">Building text classifiers using positive and unlabeled examples. ICDM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to Rank for Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Research on cost-sensitive learning in one-class anomaly detection algorithms. Autonomic and Trusted Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4610</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">One-class SVMs for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manevitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">M</forename><surname>Yousef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning research</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying Multiple Userids of the Same Author</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semisupervised learning of compact document representations with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The smart retrieval system: experiments in automatic document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
		<editor>G. Salton</editor>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
	<note>Relevant feedback in information retrieval</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<idno>MSR- TR-99-87</idno>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inputdependent estimation of generalization error under covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Decision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="279" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Von Bunau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawanabe</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data domain description using support vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ESAN99, Brussels</title>
		<meeting>ESAN99, Brussels</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Support vector domain description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1191" to="1199" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Uniform object generation for optimizing one-class classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="155" to="173" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Anomaly detection combining one-class SVMs and particle swarm optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Dynamics</title>
		<imprint>
			<biblScope unit="page" from="303" to="310" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Direct density ratio estimation for large-scale covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Building semantic kernels for text classification using Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">One-class support vector machine calibration using particle swarm optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>AICS</publisher>
			<pubPlace>Dublin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PEBL: Positive example based learning for Web page classification using SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning and evaluating classifiers under s ample selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
