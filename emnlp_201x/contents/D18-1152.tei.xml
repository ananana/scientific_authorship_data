<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rational Recurrences</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>}</roleName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
							<email>sthomson@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">}</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">|</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rational Recurrences</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1203" to="1214"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Despite the tremendous empirical success of neural models in natural language processing , many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent base-lines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural models, and in particular gated variants of recurrent neural networks (RNNs, e.g., Hochre- iter and <ref type="bibr" target="#b23">Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Cho et al., 2014</ref>), have become a core building block for state- of-the-art approaches in NLP <ref type="bibr" target="#b22">(Goldberg, 2016)</ref>. While these models empirically outperform clas- sical NLP methods on many tasks ( <ref type="bibr" target="#b64">Zaremba et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b13">Dyer et al., 2016;</ref><ref type="bibr" target="#b50">Peng et al., 2017</ref>, inter alia), they typically lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neu- ral models are more interpretable than previously</p><formula xml:id="formula_0">q 1 / ¯ 1 q 0 8↵/ ¯ 1 8↵/(↵)</formula><p>8↵/µ(↵) <ref type="figure">Figure 1</ref>: A two-state WFSA B described in §2. It is closely related to several models studied in this paper ( §4.1). Bold circles indicate initial states, and double circles final states, which are associ- ated with final weights. Arrows represent transi- tions, labeled by the symbols ↵ they consume, and the weights as a function of ↵. Arcs not drawn are assumed to have weight ¯ 0. For brevity, 8↵ means 8↵ 2 ⌃, with ⌃ being the alphabet.</p><p>thought, by drawing connections to weighted fi- nite state automata (WFSAs). We study several re- cently proposed RNN architectures and show that one can use WFSAs to characterize their recur- rent updates. We call such models rational re- currences ( §3). <ref type="bibr">1</ref> Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates the development of new ones. In recent work, <ref type="bibr" target="#b57">Schwartz et al. (2018)</ref> intro- duced SoPa, an RNN constructed from WFSAs, and thus rational by our definition. They also showed that a single-layer max-pooled CNN <ref type="bibr" target="#b30">(LeCun, 1998)</ref> can be simulated by a set of simple WFSAs (one per output dimension), and accord- ingly are also rational. In this paper we broaden such efforts, and show that rational recurrences are in frequent use ( <ref type="bibr" target="#b44">Mikolov et al., 2014;</ref><ref type="bibr" target="#b1">Balduzzi and Ghifary, 2016;</ref><ref type="bibr">Lei et al., 2016</ref><ref type="bibr" target="#b33">Lei et al., , 2017a</ref><ref type="bibr" target="#b4">Bradbury et al., 2017;</ref><ref type="bibr" target="#b16">Foerster et al., 2017</ref>). For instance, we will show in §4 that the WFSA diagrammed in <ref type="figure">Figure 1</ref> has strong connections to several of the models mentioned above.</p><p>Based on these observations, we then discuss potential approaches to deriving novel neural ar- chitectures from WFSAs ( §5). As a case study, we present a new model motivated by the in- terpolation of a two-state WFSA and a three- state one, capturing (soft) unigram and bigram features, respectively. Our experiments show that in two tasks-language modeling and text classification-the proposed model outperforms recently proposed rational models ( §6). Fur- ther extensions might lead to larger gains, and the rational recurrence view could facilitate eas- ier exploration of such extensions. To pro- mote such exploration, we publicly release our implementation at https://github.com/ Noahs-ARK/rational-recurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Weighted Finite State Automata (WFSAs)</head><p>This section reviews weighted finite-state au- tomata and semirings, which underly our anal- yses in §3. WFSAs extend nondeterministic unweighted finite-state automata by assigning weights to transitions, start states, and final states. Instead of simply accepting or rejecting a string, a WFSA returns a score for the string, and this score summarizes the weights along all paths through the WFSA that consume the string. In order for this summary score to be efficiently computable, weights are taken from a semiring.</p><p>Definition 1 ( <ref type="bibr" target="#b28">Kuich and Salomaa, 1986)</ref>. A semiring is a set K along with two associative bi- nary operations on K, (addition) and ⌦ (multi- plication), and two identity elements: ¯ 0 for addi- tion, and ¯ 1 for multiplication. Semirings also re- quire that addition is commutative, multiplication distributes over addition, and that multiplication by ¯ 0 annihilates (i.e., ¯ 0 ⌦ a = a ⌦ ¯ 0 = ¯ 0).</p><p>One common semiring is the real (or plus- times) semiring: hR, +, ·, 0, 1i. The other one used in this work is the max-plus semir- ing hR [ {1}, max, +, 1, 0i. We refer the reader to <ref type="bibr" target="#b28">Kuich and Salomaa (1986)</ref> for others. Definition 2. A weighted finite-state automaton (WFSA) over a semiring K is a 5-tuple, A = h⌃, Q, ⌧, , ⇢i, 2 with:</p><p>• a finite input alphabet ⌃;</p><p>• a finite state set Q;</p><p>• transition weights ⌧ :</p><formula xml:id="formula_1">Q ⇥ Q ⇥ (⌃ [ {"}) ! K;</formula><p>• initial weights : Q ! K;</p><p>• and final weights ⇢ : Q ! K.</p><p>" / 2 ⌃ marks special "-transitions that may be taken without consuming any input. A assigns a score AJxK to a string x = x 1 . . . x n 2 ⌃ ⇤ by summing over the scores of all possible paths de- riving x. The score of each individual path is the product of the weights of the transitions it consists of. Formally:</p><p>Definition 3 (path score). Let ⇡ = ⇡ 1 . . . ⇡ n be a sequence of adjacent transitions in A, with each transition</p><formula xml:id="formula_2">⇡ i = (q i , q i+1 , z i ) 2 Q⇥Q⇥(⌃ [ {"}).</formula><p>The path ⇡ derives string x 2 ⌃ ⇤ , which is the substring of z = z 1 z 2 . . . z n that excludes " symbols (for example, if z = a"bc"""d, then x = abcd). ⇡'s score in A is given by</p><formula xml:id="formula_3">A[⇡] = (q 1 ) ⌦ n O i=1 ⌧ (⇡ i ) ! ⌦ ⇢(q n+1 ). (1)</formula><p>Definition 4 (string score). Let ⇧(x) denote the set of all paths in A that derive x. Then the score assigned by A to x is defined to be</p><formula xml:id="formula_4">AJxK = M ⇡2⇧(x) A[⇡].<label>(2)</label></formula><p>Because K is a semiring, AJxK can be com- puted in time linear in |x| by the Forward algo- rithm ( <ref type="bibr" target="#b2">Baum and Petrie, 1966)</ref>. Here, for simplic- ity, we describe the Forward algorithm without "- transitions. <ref type="bibr">3</ref> Its dynamic program is given by:</p><formula xml:id="formula_5">⌦ 0 (q) = (q),<label>(3a)</label></formula><formula xml:id="formula_6">⌦ i+1 (q) = M q 0 2Q ⌦ i (q 0 ) ⌦ ⌧ (q 0 , q, x i ),<label>(3b)</label></formula><formula xml:id="formula_7">AJxK = M q2Q ⌦ n (q) ⌦ ⇢(q).<label>(3c)</label></formula><p>⌦ i (q) gives the total score of all paths that derive x 1 . . . x i and end in state q. Example 5. <ref type="figure">Figure 1</ref> diagrams a WFSA B, con- sisting of two states. A path starts from the initial state q 0 (with (q 0 ) = ¯ 1); it then takes any num- states respectively. Our definition is equivalent, giving the weight functions value ¯ 0 wherever they were undefined. <ref type="bibr">3</ref> "-transitions can be handled with a slight modifica- tion ( <ref type="bibr" target="#b57">Schwartz et al., 2018)</ref>. Note though that if A contains a cycle of "-transitions, then either K must follow the star semiring laws ( <ref type="bibr" target="#b28">Kuich and Salomaa, 1986)</ref>, or the number of consecutive "-transitions allowed must be capped. ber of "self-loop" transitions, each consuming an input without changing the path score (since it's weighted by ¯ 1); it then consumes an input sym- bol ↵ and takes a transition weighted by µ(↵), and reaches the final state q 1 (with ⇢(q 1 ) = ¯ 1); it may further consume more input by taking self- loops at q 1 , updating the path score by multiply- ing it by (↵) for each symbol ↵. Then from Definition 4, we can calculate that B gives the empty string score ¯ 0, and gives any nonempty string x = x 1 . . .</p><formula xml:id="formula_8">x n 2 ⌃ + score BJxK = n1 M i=1 0 @ µ(x i ) ⌦ n O j=i+1 (x j ) 1 A µ(x n ).<label>(4)</label></formula><p>B can be seen as capturing soft unigram pat- terns ( <ref type="bibr" target="#b11">Davidov et al., 2010)</ref>, in the sense that it consumes one input symbol to reach the final state from the initial state. It is straightforward to de- sign WFSAs capturing longer patterns by includ- ing more states ( <ref type="bibr" target="#b57">Schwartz et al., 2018)</ref>, as we will discuss later in §4 and §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rational Recurrences</head><p>Before formally defining rational recurrences in §3.2, we highlight the connection between WFSAs and RNNs using a motivating example ( §3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Motivating Example</head><p>We describe a simplified RNN which strips away details of some recent RNNs, in order to highlight the behaviors of the forget gate and the input.</p><p>Example 6. For an input sequence x = x 1 . . . x n , let the word embedding vector for x t be v t . As in many gated RNN variants <ref type="bibr" target="#b23">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Cho et al., 2014</ref>), we use a forget gate f t , which is computed with an affine transformation followed by an elementwise sig- moid function . The current input representation u t is similarly computed, but with an optional non- linearity (e.g., tanh) g. The hidden state c t can be seen as a weighted sum of the previous state and the new input, controlled by the forget gate.</p><formula xml:id="formula_9">f t = W f v t + b f ,<label>(5a)</label></formula><formula xml:id="formula_10">u t = (1 f t ) g W u v t + b u ,<label>(5b)</label></formula><formula xml:id="formula_11">c t = f t c t1 + u t .<label>(5c)</label></formula><p>The hidden state c t can then be used in down- stream computation, e.g., to calculate output state h t = tanh(c t ), which is then fed to an MLP clas- sifier. We focus only on the recurrent computation.</p><p>In Example 6, both f t and u t depend only on the current input token x t (through v t ), and not the previous state. Importantly, the interaction with the previous state c t1 is not via affine trans- formations followed by nonlinearities, as in, e.g., an Elman network <ref type="bibr" target="#b15">(Elman, 1990)</ref>, where c t = tanh(W c c t1 + W v v t + b c ). As we will discuss later, this is important in relating this recurrent up- date function to WFSAs.</p><p>Since the recurrent update in Equation 5c is el- ementwise, for simplicity we focus on just the ith dimension. Unrolling it in time steps, we get</p><formula xml:id="formula_12">[c t ] i = [f t ] i [c t1 ] i + [u t ] i = t1 X j=1 0 @ [u j ] i t Y k=j+1 [f k ] i 1 A + [u t ] i ,<label>(6)</label></formula><p>where <ref type="bibr">[·]</ref> i denotes the ith dimension of a vector. As noted by <ref type="bibr" target="#b31">Lee et al. (2017)</ref>, the hidden state at time step t can be seen as a sum of previous in- put representations, weighted by the forget gate; longer histories typically get a smaller weight, since the forget gate values are between 0 and 1 due to the sigmoid function. Let's recall the WFSA B ( <ref type="figure">Figure 1</ref> and Exam- ple 5) using the real semiring hR, +, ·, 0, 1i. Equa- tion 6 is recovered by parameterizing B's weight functions µ and with</p><formula xml:id="formula_13">µ(x t ) = [u t ] i , (x t ) = [f t ] i .<label>(7)</label></formula><p>Denote the resulting WFSA by B i , and we have:</p><p>Proposition 7. Running a single layer RNN in Ex- ample 6 over any nonempty input string x 2 ⌃ + , the ith dimension of its hidden state at time step t equals the score assigned by B i to x :t :</p><formula xml:id="formula_14">[c t ] i = B i Jx :t K.<label>(8)</label></formula><p>In other words, the ith dimension of the RNN in Example 6 can be seen as a WFSA structurally equivalent to B. Its weight functions are imple- mented as the ith dimension of Equations 5, and the learned parameters are the ith row of W and b. Then it is straightforward to recover the full d- dimensional RNN, by collecting d such WFSAs, each of which is parametrized by a row in the Ws and bs. Based on this observation, we are now ready to formally define rational recurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrences and Rationality</head><p>For a function c: ⌃ ⇤ ! K d , its recurrence is said to be the dependence of c(x :t ) on c(x :t1 ), for input sequence 8x 2 ⌃ + . We discuss a class of recur- rences that can be characterized by WFSAs. The mathematical counterpart of WFSAs are rational power series <ref type="bibr" target="#b3">(Berstel and Reutenauer, 1988)</ref>, jus- tifying naming such recurrences rational:</p><p>Definition 8 (rational recurrence). The recurrence of c : ⌃ ⇤ ! K d is said to be rational, if there ex- ists a set of weighted finite state automata</p><formula xml:id="formula_15">{A i } d i=1</formula><p>over alphabet ⌃ and semiring hK, , ⌦, ¯ 0, ¯ 1i with both and ⌦ taking constant time and space, such that</p><formula xml:id="formula_16">8x 2 ⌃ ⇤ , ⇥ c(x) ⇤ i = A i JxK, 8i 2 {1, 2, . . . , d}. 4 (9)</formula><p>It directly follows from Proposition 7 that Corollary 9. The recurrence in Example 6 is ra- tional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relationship to Existing Neural Models</head><p>This section studies several recently proposed neu- ral architectures, and relates them to rational re- currences. §4.1 begins by relating some of them to the RNN defined in Example 6, and then to the WFSA B (Example 5). We then describe a WFSA similar to B, but with one additional state, and dis- cuss how it provides a new view of RNN mod- els motivated by n-gram features ( §4.2). In §4.3 we study rational recurrences that are not elemen- twise, using an existing model. In the following discussion, we shall assume the real seimiring, unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural Architectures Related to B</head><p>Despite its simplicity, Example 6 corresponds to several existing neural architectures. For instance, quasi-RNN (QRNN; <ref type="bibr" target="#b4">Bradbury et al., 2017)</ref> and simple recurrent unit (SRU; <ref type="bibr" target="#b35">Lei et al., 2017b</ref>) aim to speed up the recurrent computation. To do so, they drop the matrix multiplication depen- dence on the previous hidden state, resulting in similar recurrences to that in Example 6. <ref type="bibr">5</ref> Other works start from different motivations, but land on similar recurrences, e.g., strongly-typed RNNs (T- RNN; <ref type="bibr" target="#b1">Balduzzi and Ghifary, 2016)</ref> and its gated variants (T-LSTM and T-GRU), and structurally constrained RNNs (SCRN; <ref type="bibr" target="#b44">Mikolov et al., 2014</ref>).</p><p>The analysis in §3.1 directly applies to SRU, T-RNN, and SCRN. In fact, Example 6 presents a slightly more complicated version of them. In these models, input representations are computed without the bias term or any nonlinearity: u t = W u v t . By Proposition 7 and Corollary 9: Corollary 10. The recurrences of single-layer SRU, T-RNN, and SCRN architectures are ratio- nal.</p><p>It is slightly more complicated to analyze the recurrences of the QRNN, T-LSTM, and T-GRU. Although their hidden states c t are updated in the same way as Equation 5c, the input representa- tions and gates may depend on previous inputs. For example, in T-LSTM and T-GRU, the forget gate is a function of two consecutive inputs:</p><formula xml:id="formula_17">f t = (V f v t1 + W f v t + b f ) .<label>(10)</label></formula><p>QRNNs are similar, but may depend on up to K tokens, due to the K-window convolutions. Eis- ner (2002) discuss finite state machines for sec- ond (or higher) order probabilistic sequence mod- els. Following the same intuition, we sketch the construction of WFSAs corresponding to QRNNs with 2-window convolutions in Appendix A, and summarize the key results here: Proposition 11. The recurrences of single-layer T-GRU, T-LSTM, and QRNN are rational. In par- ticular, a single-layer d-dimensional QRNN using K-window convolutions can be recovered by a set of d WFSAs, each with O(2 |⌃| K1 ) states.</p><p>The size of WFSAs needed to recover QRNN grows exponentially in the window size. There- fore, at least for QRNNs, Proposition 11 has more conceptual value than practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">More than Two States</head><p>So far our discussion has centered on B, a two- state WFSA capturing unigram patterns (Exam- ple 5). In the same spirit as going from unigram to n-gram features, one can use WFSAs with more states to capture longer patterns ( <ref type="bibr" target="#b57">Schwartz et al., 2018)</ref>. In this section we augment B by intro- ducing more states, and explore its relationship to some neural architectures motivated by n-gram features. We start with a three-state WFSA as an example, and then discuss more general cases. <ref type="figure">Figure 2</ref> diagrams a WFSA C, augmenting B with another state. To reach the final state q 2 , at</p><formula xml:id="formula_18">q 0 8↵/1 q 1 8↵/ 1 (↵) q 2 /1 8↵/ 2 (↵) 8↵/µ 2 (↵) 8↵/µ 1 (↵)</formula><p>Figure 2: A three-state WFSA C discussed in §4.2. least two transitions must be taken, in contrast to one in B. History information is decayed by the self-loop at the final state q 2 , assuming 2 is be- tween 0 and 1. C has another self-loop over q 1 , weighted by 1 2 (0, 1). The motivation is to allow (but down-weight) nonconsecutive bigrams, as we will soon show.</p><p>The scores assigned by C can be inductively computed by applying the Forward algorithm ( §2). Given input sequence x longer than one, let</p><formula xml:id="formula_19">CJx :0 K = 0, then CJx :t+1 K = CJx :t K 2 (x t+1 ) + t µ 2 (x t+1 ),<label>(11)</label></formula><p>where</p><formula xml:id="formula_20">t = t1 1 (x t ) + µ 1 (x t ),<label>(12)</label></formula><formula xml:id="formula_21">and 0 = 0. Unrolling t in time, we get t = t1 X j=1 0 @ µ 1 (x j ) t Y k=j+1 1 (x k ) 1 A + µ 1 (x t ).<label>(13)</label></formula><p>Due to the self-loop over state q 1 , t can be seen as a weighted sum of the µ 1 terms up to x t (Equal- tion 13). The second product term in Equation 11 then provides multiplicative interactions between µ 2 , and the weighted sum of µ 1 s. In this sense, it captures nonconsecutive bigram features.</p><p>At a first glance, Equations 11 and 12 resemble recurrent convolutional neural net- works (RCNN; <ref type="bibr">Lei et al., 2016)</ref>. RCNN is in- spired by nonconsecutive n-gram features and low rank tensor factorization. It is later studied from a string kernel perspective ( <ref type="bibr" target="#b33">Lei et al., 2017a</ref>). Here we review its nonlinear bigram version:</p><formula xml:id="formula_22">c (1) t = c (1) t1 t + u (1) t ,<label>(14a)</label></formula><formula xml:id="formula_23">c (2) t = c (2) t1 t + c (1) t1 u (2) t ,<label>(14b)</label></formula><p>where the u (j) t s are computed similarly to Equa- tion 5b, and c (2) t is used as output for onward computation. Different strategies to computing t were explored ( <ref type="bibr" target="#b32">Lei et al., 2015</ref><ref type="bibr">Lei et al., , 2016</ref>. When t is a constant, or depends only on x t , e.g., t = (W v t +b ), the ith dimension of Equations 14</p><formula xml:id="formula_24">8↵/µ 1,2 (↵) 8↵/µ 2,1 (↵) q 1 8↵/µ 1,1 (↵) 8↵/1 8↵/⌘ 1 (↵) 8↵/µ 2,2 (↵) 8↵/1 8↵/⌘ 2 (↵) q 2 /1 q 3 q 4</formula><p>Figure 3: WFSA D 1 discussed in §4.3. Two initial states q 1 and q 4 are used here.</p><p>can be recovered from Equation 11, by letting</p><formula xml:id="formula_25">µ j (x t ) = [u (j) t ] i , j (x t ) = [ t ] i , j = 1, 2. (15)</formula><p>It is straightforward to generalize the above dis- cussion to higher order cases: n-gram RCNN cor- responds to WFSAs with n + 1 states, constructed similarly to how we build C from B (Appendix B).</p><p>Proposition 12. For a single-layer RCNN with t being a constant or depending only on x t , the re- currence is rational.</p><p>As noted later in §4.3, its recurrence may not be rational when t = (W c c t1 + W v t + b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Beyond Elementwise Operations</head><p>So far we have discussed rational recurrences for models using elementwise recurrent updates (e.g., Equation 5c). This section uses an existing model as an example, to study a rational recurrence that is not elementwise. We focus on the input switched affine network (ISAN; <ref type="bibr" target="#b16">Foerster et al., 2017)</ref>. Aim- ing for efficiency and interpretability, it does not use any explicit nonlinearity; its affine transforma- tion parameters depend only on the input:</p><formula xml:id="formula_26">c t = W xt c t1 + b xt .<label>(16)</label></formula><p>Due to the matrix multiplication, the recurrence of a single-layer ISAN is not elementwise. Yet, we argue that it is rational. We will sketch the proof for a 2-dimensional case, and it is straightforward to generalize to higher dimensions (Appendix C). We define two WFSAs, each recovering one di- mension of ISAN's recurrent updates. <ref type="figure">Figure 3</ref> diagrams one of them, D 1 . The other one, D 2 , is identical (including shared weights), except us- ing q 3 instead of q 2 as the final state. For any nonempty input sequence x 2 ⌃ + , the scores assigned by D 1 and D 2 can be inductively com- puted by applying the Forward algorithm. Letting</p><formula xml:id="formula_27">D 1 Jx :0 K = D 2 Jx :0 K = 0, for t 1  D 1 Jx :t K D 2 Jx :t K = f W xt  D 1 Jx :t1 K D 2 Jx :t1 K + e b xt ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_28">f W xt =  µ 1,1 (x t ) µ 1,2 (x t ) µ 2,1 (x t ) µ 2,2 (x t ) , e b xt =  ⌘ 1 (x t ) ⌘ 2 (x t ) .<label>(18)</label></formula><p>Then Equation 16, in the case of hidden size 2, is recovered by letting W xt = f W xt and b xt = e b xt . Proposition 13. The recurrence of a single-layer ISAN is rational.</p><p>Corollary 14. For a single-layer Elman network, in the absence of any nonlinearity, the recurrence is rational.</p><p>Discussion. It is known that an Elman network can approximate any recursively computable par- tial function ( <ref type="bibr" target="#b58">Siegelmann and Sontag, 1995)</ref>. On the other hand, in their single-layer cases, WFSAs (and thus models with rational recurrences) are re- stricted to rational series <ref type="bibr" target="#b56">(Schützenberger, 1961)</ref>. Therefore, we hypothesize that models like Elman networks, LSTMs, and GRUs, where the recur- rences depend on previous states through affine transformations followed by nonlinearities, are not rational.</p><p>This work does not intend to propose rational recurrences as a concept general enough to include most existing RNNs. Rather, we wish to study a more constrained class of methods to better under- stand the connections between WFSAs and RNNs. Therefore in Definition 8, we restrict the semir- ings to be "simple," in the sense that both opera- tions take constant time and space. Such a restric- tion aims to exclude the possibility of hiding arbi- trarily complex computations inside the semiring, which might allow RNNs to satisfy the definition in a trivial and unilluminating way.</p><p>Such theoretical limitations might be less se- vere than they appear, since it is not yet entirely clear what they correspond to in practice, espe- cially when multiple vertical layers of these mod- els are used ( <ref type="bibr" target="#b37">Leshno and Schocken, 1993)</ref>. We defer to future work the further study of the con- nections between WFSAs and Elman-style RNNs.</p><p>Closing this section, <ref type="table" target="#tab_0">Table 1</ref> summarizes the discussed recurrent neural architectures and their corresponding WFSAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deriving Neural Models from WFSAs</head><p>Rational recurrences provide a new view of several recently proposed neural models. Based on such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Recurrence Function WFSA §4.1 observations, this section aims to explore potential approaches to designing neural architectures in a more interpretable and intuitive way: by deriving them from WFSAs. §5.1 studies an interpolation of unigram and bigram features by combining 2- state and 3-state WFSAs <ref type="figure">(Figures 1 and 2</ref>). We then explore alternative semirings ( §5.2), an ap- proach orthogonal to what we've discussed so far. We note that our goal is not to devise new state- of-the-art architectures. Rather, we illustrate a new design process for neural architectures that draws inspiration from WFSAs. That said, in our experiments ( §6), one of our new architectures performs as well as or better than strong baselines.</p><formula xml:id="formula_29">SRU, SCRN ct = ft ct1 + ut B T-RNN, QRNN §4.2 RCNN c (1) t = c (1) t1 t + u (1) t c (2) t = c (2) t1 t + c (1) t1 u (2) t C §4.3 ISAN ct = Wx t ct1 + bx t D1, D2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Aggregating Different Length Patterns</head><p>We start by presenting a straightforward extension to 2-state and 3-state rational models: one com- bining both. It is inspired by many classical NLP models, where unigram features and higher-order ones are interpolated. <ref type="figure">Figure 4</ref> diagrams a 4-state WFSA F. Com- pared to C <ref type="figure">(Figure 2</ref>), F uses q 1 as a second fi- nal state, aiming to capture both unigram and bi- gram patterns, since a path is allowed to stop at q 1 after consuming one input. The final states are weighted by ⇢ 1 and ⇢ 2 respectively. Another no- table modification is the additional state q 3 , which is used to create a "shortcut" to reach q 2 , together with an "-transition. Specifically, starting from q 0 , a path can now take the "-transition and reach q 3 , and then take a transition with weight µ 2 to reach q 2 . Recall from §2, that "-transitions do not con- sume any input, yet they can still be weighted by a (parameterized) function not depending on the inputs. The "-transition allows for skipping the</p><formula xml:id="formula_30">q 0 8↵/1 8↵/ 1 (↵) 8↵/ 2 (↵) 8↵/µ 2 (↵) 8↵/µ 1 (↵) q 3 "/ 8↵/µ 2 (↵) q 1 /⇢ 1 q 2 /⇢ 2</formula><p>Figure 4: A WFSA F that combines both unigram and bigram features ( §5.1). Two final states q 1 and q 2 are used, with weights ⇢ 1 and ⇢ 2 , respectively.</p><p>first word in a bigram. It can be discouraged by using 2 (0, 1), just as we do in our experiments.</p><p>Deriving the neural architecture. As in §3, we relate hidden states of an RNN to the scores as- signed by WFSAs to input strings. We then derive the neural architecture with a dynamic program.</p><p>Here we keep the discussion self-contained by ex- plicitly overviewing the procedure. It is a direct application of the Forward algorithm ( §2), though now in a form that deals with the "-transition.</p><p>Such an approach applies, of course, to more gen- eral cases, as noted by <ref type="bibr" target="#b57">Schwartz et al. (2018)</ref>. Given an input string x 2 ⌃ + , let z (j) t denote the total score of all paths landing in state q j just after consuming x t . Let z (j) 0 = 0, then for t 1,</p><formula xml:id="formula_31">z (0) t = 1 z (1) t = z (1) t1 1 (x t ) + z (0) t1 µ 1 (x t ) z (3) t = z (0) t z (2) t = z (2) t1 2 (x t ) + (z (1) t1 + z (3) t1 ) µ 2 (x t ) FJx :t K = ⇢ 1 z (1) t + ⇢ 2 z (2) t .</formula><p>We now collect d of these WFSAs to construct an RNN, and we parameterize their weight functions with the technique we've been using:</p><formula xml:id="formula_32">c (1) t = c (1) t1 f (1) t + u (1) t ,<label>(19a)</label></formula><formula xml:id="formula_33">c (2) t = c (2) t1 f (2) t + (c (1) t1 + r) u (2) t , (19b) c t = p (1) c (1) t + p (2) c (2) t ,<label>(19c)</label></formula><p>where</p><formula xml:id="formula_34">f (j) t = W (j) f v t + b (j) f ,<label>(20a)</label></formula><formula xml:id="formula_35">u (j) t = (1 f (j) t ) g W (j) u v t + b (j) u ,<label>(20b)</label></formula><formula xml:id="formula_36">p (j) = (b (j) p ), r = (b r ).<label>(20c)</label></formula><p>The p vectors correspond to the final state weights ⇢ 1 and ⇢ 2 . Despite the similarities, p are different from output gates <ref type="bibr" target="#b4">(Bradbury et al., 2017)</ref>, since the former do not depend on the input, and are param- p . The same applies to r and b r , which correspond to the weights for "-transitions .</p><formula xml:id="formula_37">Model Unigram Bigram Semiring RRNN(B) X real RRNN(B) m+ X max-plus RRNN(C) X real RRNN(F) X X real</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Alternative Semirings</head><p>Our new understanding of rational recurrences al- lows us to consider a different kind of extension: replacing the semiring. We introduce an example, which modifies Example 6 by replacing its real (plus-times) semiring with the max-plus semir- ing hR [ {1}, max, +, 1, 0i:</p><formula xml:id="formula_38">Example 15. f t = log W f v t + b f ,<label>(21a)</label></formula><formula xml:id="formula_39">u t = g W u v t + b u ,<label>(21b)</label></formula><formula xml:id="formula_40">c t = max{f t + c t1 , u t }.<label>(21c)</label></formula><p>Example 15 does not use the forget gate when computing u t (Equation 21b), which is differ- ent from its plus-times counterpart, where</p><formula xml:id="formula_41">u t = (1 f t ) g W u v t + b u .</formula><p>The reason is that, un- like the real semiring, the max-plus semiring lacks a well-defined negation. Possible alternatives in- clude taking the log of a separate input gate, or using log(1 f t ), which we leave for future work. Example 15 can be seen as replacing sum- pooling with max-pooling. Both max and sum- pooling have been used successfully in vision and NLP models. Intuitively, max-pooling "detects" the occurrence of a pattern while sum-pooling "counts" the occurrence of a pattern. One advan- tage of max operator is that the model's decisions can be back-traced and interpreted, as argued by <ref type="bibr" target="#b57">Schwartz et al. (2018)</ref>. Such a technique is appli- cable to all the models with rational recurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>This section evaluates four rational RNNs on language modeling ( §6.2) and text categoriza- tion ( §6.3). Our goal is to compare the behaviors of models derived from different WFSAs, show- ing that our understanding of WFSAs allows us to improve existing rational models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Compared Models</head><p>Our comparisons focus on the recurrences of the models, i.e., how the hidden states c t are com- puted (e.g., Equations 5c and 19c). Therefore we follow <ref type="bibr" target="#b35">Lei et al. (2017b)</ref> and use u</p><formula xml:id="formula_42">(j) t = W (j) u v (j) t</formula><p>across all compared models, listed below and as well as in <ref type="table" target="#tab_1">Table 2:</ref> • RRNN(B), with real semiring ( §4.1);</p><p>• RRNN(B) m+ , with max-plus semiring ( §5.2);</p><p>• RRNN(C), with real semiring ( §4.2);</p><p>• RRNN(F), with real semiring ( §5.1).</p><p>We also compare to an LSTM baseline. Aiming to control for comfounding factors, we do not use highway connections in any of the models. <ref type="bibr">6</ref> In the interest of space, the full architectures and hyper- parameters are detailed in Appendices D and E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Language Modeling</head><p>Dataset and implementation. We experiment with the Penn Treebank corpus (PTB; <ref type="bibr" target="#b40">Marcus et al., 1993)</ref>. We use the preprocessing and splits from <ref type="bibr" target="#b45">Mikolov et al. (2010)</ref>, resulting in a vocabu- lary size of 10K and 1M tokens.</p><p>Following standard practice, we treat the train- ing data as one long sequence, split into mini batches, and train using BPTT truncated to 35 time steps <ref type="bibr" target="#b63">(Williams and Peng, 1990</ref>). The in- put embeddings and output softmax weights are tied <ref type="bibr" target="#b53">(Press and Wolf, 2017)</ref>.</p><p>Results. Following <ref type="bibr" target="#b8">Collins et al. (2017)</ref> and <ref type="bibr" target="#b42">Melis et al. (2018)</ref>, we compare models con- trolling for parameter budget. <ref type="table" target="#tab_3">Table 3</ref> sum- marizes language modeling perplexities on PTB test set. The middle block compares all mod- els with two layers and 10M trainable parameters. <ref type="bibr">RRNN(B)</ref> and RRNN(C) achieve roughly the same performance; interpolating both unigram and bi- gram features, RRNN(F) outperforms others by more than 2.9 test perplexity. For the three-layer and 24M setting (the bottom block), we observe similar trends, except that RRNN(C) slightly under- performs RRNN(B). Here RRNN(F) outperforms others by more than 2.1 perplexity.</p><p>Using a max-plus semiring, RRNN(B) m+ un- derperforms RRNN(B) under both settings. Possi- ble reasons could be the suboptimal design choice <ref type="bibr">6</ref> Thus <ref type="bibr">RRNN(B)</ref> is essentially an SRU without highway connections. We denote it differently, to note its differences from the original implementation ( <ref type="bibr" target="#b35">Lei et al., 2017b)</ref>. Simi- larly, we do not denote RRNN(C) as RCNN ( <ref type="bibr">Lei et al., 2016</ref>   for computing input representations in the for- mer ( §5.2). Finally, most compared models out- perform the LSTM baselines, whose numbers are taken from <ref type="bibr" target="#b35">Lei et al. (2017b)</ref>. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Text Classification</head><p>Implementation. We use unidirectional 2-layer architectures for all compared models. To build the classifiers, we feed the final RNN hidden states into a 2-layer tanh-MLP. Further implementation details are described in Appendix E.</p><p>Datasets. We experiment with four binary text classification datasets, described below.</p><p>• Amazon (electronic product review corpus; <ref type="bibr" target="#b41">McAuley and Leskovec, 2013)</ref>. <ref type="bibr">8</ref> We focus on the positive and negative reviews.</p><p>• SST (Stanford sentiment treebank; <ref type="bibr" target="#b59">Socher et al., 2013)</ref>. <ref type="bibr">9</ref> We focus on the binary classification task. SST provides labels for syntactic phrases; we experiment with a more realistic setup, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Amazon SST subj CR  consider only complete sentences at either train- ing or evaluating time.</p><p>• subj (Subjectivity dataset; <ref type="bibr" target="#b48">Pang and Lee, 2004</ref>).</p><p>As subj doesn't come with official splits, we randomly split it to train (80%), development (10%), and test (10%) sets.</p><p>• CR (customer reviews dataset; <ref type="bibr" target="#b24">Hu and Liu, 2004</ref>). <ref type="bibr">10</ref> As with subj, we randomly split this dataset using the same ratio. <ref type="table" target="#tab_4">Table 4</ref> summarizes the sizes of the datasets.</p><p>Results. <ref type="table" target="#tab_6">Table 5</ref> summarizes text classification test accuracy. We report the average perfor- mance of 5 trials different only in random seeds. <ref type="bibr">RRNN(F)</ref> outperforms all other models on 3 out of the 4 datasets. For Amazon, the largest one, we do not observe significant differences between <ref type="bibr">RRNN(F)</ref> and RRNN(C), while both outperform others. This may suggest that the interpolation of unigram and bigram features by <ref type="bibr">RRNN(F)</ref> is especially useful in small data setups. As in the language modeling experiments, RRNN(B) m+ un- derperforms all other models in most cases, and in particular <ref type="bibr">RRNN(B)</ref>. These results provide ev- idence that replacing the real semiring in rational models might be challenging. We leave further ex- ploration to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Weighted finite state automata. WFSAs were once popular among many sequential tasks ( <ref type="bibr" target="#b46">Mohri et al., 2002;</ref><ref type="bibr" target="#b29">Kumar and Byrne, 2003;</ref><ref type="bibr" target="#b9">Cortes et al., 2004;</ref><ref type="bibr" target="#b49">Pardo and Birmingham, 2005;</ref><ref type="bibr" target="#b47">Moore et al., 2006</ref>, inter alia), and are still successful in mor- phology <ref type="bibr" target="#b12">(Dreyer, 2011;</ref><ref type="bibr" target="#b10">Cotterell et al., 2015;</ref><ref type="bibr">Rastogi et al., 2016, inter alia)</ref>. Compared to neural networks, WFSAs are better understood theoreti-cally and arguably more interpretable. They were recently revisited in combination with the for- mer in, e.g., text generation ( <ref type="bibr" target="#b19">Ghazvininejad et al., 2016</ref><ref type="bibr" target="#b20">Ghazvininejad et al., , 2017</ref><ref type="bibr" target="#b39">Lin et al., 2017)</ref> and automatic music accompaniment <ref type="bibr" target="#b17">(Forsyth, 2016)</ref>.</p><p>Recurrent neural networks. RNNs <ref type="bibr" target="#b15">(Elman, 1990;</ref><ref type="bibr" target="#b25">Jordan, 1989)</ref> prove to be strong mod- els for sequential data ( <ref type="bibr" target="#b58">Siegelmann and Sontag, 1995)</ref>. Besides the perhaps most notable gated variants <ref type="bibr" target="#b23">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Cho et al., 2014</ref>), extensive efforts have been devoted to developing alternatives ( <ref type="bibr" target="#b1">Balduzzi and Ghifary, 2016;</ref><ref type="bibr" target="#b43">Miao et al., 2016;</ref><ref type="bibr" target="#b65">Zoph and Le, 2017;</ref><ref type="bibr" target="#b31">Lee et al., 2017;</ref><ref type="bibr" target="#b33">Lei et al., 2017a;</ref><ref type="bibr" target="#b60">Vaswani et al., 2017;</ref><ref type="bibr">Gehring et al., 2017, inter alia)</ref>. Departing from the above approaches, this work derives RNN ar- chitectures drawing inspiration from WFSAs.</p><p>Another line of work studied the connections between WFSAs and RNNs in terms of model- ing capacity, both empirically <ref type="bibr" target="#b27">(Kolen, 1993;</ref><ref type="bibr" target="#b21">Giles et al., 1992;</ref><ref type="bibr" target="#b62">Weiss et al., 2018</ref>, inter alia) and the- oretically ( <ref type="bibr" target="#b7">Cleeremans et al., 1989;</ref><ref type="bibr" target="#b61">Visser et al., 2001;</ref><ref type="bibr" target="#b5">Chen et al., 2018</ref>, inter alia).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented rational recurrences, a new con- struction to study the recurrent updates in RNNs, drawing inspiration from WFSAs. We showed that rational recurrences are in frequent use by several recently proposed recurrent neural archi- tectures, providing new understanding of them. Based on such connections, we discussed ap- proaches to deriving novel neural architectures from WFSAs. Our empirical results demonstrate the potential of doing so. We publicly release our implementation at https://github.com/ Noahs-ARK/rational-recurrences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Recurrent neural network architectures 
discussed in  §4 and their corresponding WFSAs. 
 §4.1: SRU (Lei et al., 2017b), SCRN (Mikolov 
et al., 2014), T-RNN and its gated variants (Bal-
duzzi and Ghifary, 2016), and QRNN (Bradbury 
et al., 2017);  §4.2: RCNN (Lei et al., 2016);  §4.3: 
ISAN (Foerster et al., 2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Rational recurrent neural architectures 
compared in the experiments ( §6.1). 

eterized (through a sigmoid) by two leanred vec-
tors b 

(j) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>ModeìModeì # Params. Dev. Test 

LSTM 
2 
24M 
73.3 71.4 
LSTM 
3 
24M 
78.8 76.2 

RRNN(B) 

2 
10M 
73.1 69.2 
RRNN(B) m+ 2 
10M 
75.1 71.7 

RRNN(C) 

2 
10M 
72.5 69.5 

RRNN(F) 

2 
10M 
69.5 66.3 

RRNN(B) 

3 
24M 
68.7 65.2 
RRNN(B) m+ 3 
24M 
70.8 66.9 

RRNN(C) 

3 
24M 
70.0 67.0 

RRNN(F) 

3 
24M 
66.0 63.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Language modeling perplexity on PTB 
test set (lower is better). LSTM numbers are taken 
from Lei et al. (2017b). ` denotes the number of 
layers. Bold font indicates best performance. 

Split Amazon SST subj CR 

Train 
20K 
6.9K 8K 3.0K 
Dev. 
05K 
0.9K 1K 0.4K 
Test 
25K 
1.8K 1K 0.4K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Number of instances in the text classifica-
tion datasets ( §6.3). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Text classification test accuracy averaged 
over 5 runs. ± denotes standard deviation, and 
bold font indicates best averaged performance. 

</table></figure>

			<note place="foot" n="1"> Where the term regular is used with unweighted FSAs (e.g., regular languages, regular expressions), rational is the weighted analog (e.g., rational series, Sakarovitch, 2009; rational kernels, Cortes et al., 2004).</note>

			<note place="foot" n="2"> Some authors define ⌧ , , and ⇢ to be partial functionsapplying only a subset of transitions, initial states, and final</note>

			<note place="foot" n="4"> We restrict that both operations take constant time and space, to exclude the use of arbitrarily complex semirings ( §4.3). 5 The SRU architecture discussed through this work is based on Lei et al. (2017b). In a later updated version, Lei et al. (2018) introduce diagonal matrix multiplication interaction in the hidden state updates, inspired by (Li et al., 2018), which yields a recurrence not obviously rational.</note>

			<note place="foot" n="7"> Melis et al. (2018) point out that carefully tuning LSTMs can achieve much stronger performance, at the cost of exceptionally large amounts of computational resources for tuning. 8 http://riejohnson.com/cnn_data.html 9 nlp.stanford.edu/sentiment/index.html</note>

			<note place="foot" n="10"> http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jason Eisner, Luheng He, Tao Lei, Omer Levy, members of the ARK lab at the University of Washington, and researchers at the Allen In-stitute for Artificial Intelligence for their helpful comments on an early version of this work, and the anonymous reviewers for their valuable feedback. We also thank members of the Aristo team at the Allen Institute for Artificial Intelligence for their support with the Beaker experimentation system. This work was supported in part by NSF grant IIS-1562364 and by the NVIDIA Corporation through the donation of a Tesla GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strongly-typed recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state Markov chains. The Annals of Mathematical Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Petrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Berstel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Reutenauer</surname></persName>
		</author>
		<title level="m">Rational Series and Their Languages</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent neural networks as weighted language recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finite state automata and simple recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Cleeremans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Servan-Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="381" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capacity and trainability in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rational kernels: Theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1035" to="1062" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling word forms using latent underlying morphs and phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced sentiment learning using twitter hashtags and smileys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Non-parametric Model for the Discovery of Inflectional Paradigms from Plain Text Using Graphical Models over Strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parameter estimation for probabilistic finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intelligible language modeling with input switched affine networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic musical accompaniment using finite state machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>New York University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating topical poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hafez: an interactive poetry generation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL, System Demonstrations</title>
		<meeting>of ACL, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning and extracting finite state automata with second-order recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinghen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Zheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Chun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Serial order: A parallel, distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Connectionist Theory: Speech. Erlbaum</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fool&apos;s gold: Extracting finite state machines from recurrent network dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semirings, Automata, Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Kuich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Salomaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A weighted finite state transducer implementation of the alignment template model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient-based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07393</idno>
		<title level="m">Recurrent additive networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Molding CNNs for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Alessandro Moschitti, and Lluís M` arquez. 2016. Semi-supervised question retrieval with gated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<title level="m">Training RNNs as fast as CNNs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (IndRNN): Building A longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial ranking for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ting</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys</title>
		<meeting>of RecSys</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simplifying long short-term memory acoustic models for fast training and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7753</idno>
		<title level="m">Learning longer memory in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luks</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Juicer: A weighted finite-state transducer speech decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jithendra</forename><surname>Vepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MLMI</title>
		<meeting>of MLMI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Modeling form for on-line following of musical performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Birmingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep multitask learning for semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Backpropagating through structured argmax using a spigot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rational and recognisable power series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Sakarovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Weighted Automata</title>
		<editor>Manfred Droste, Werner Kuich, and Heiko Vogler</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="105" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the definition of a family of automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Paul</forename><surname>Schützenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="245" to="270" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SoPa: Bridging CNNs, RNNs, and weighted finite-state machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="150" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hidden markov model interpretations of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Maartje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raijmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models of Learning, Development and Evolution</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the practical computational power of finite precision RNNs for language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An efficient gradient-based algorithm for online training of recurrent network trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
