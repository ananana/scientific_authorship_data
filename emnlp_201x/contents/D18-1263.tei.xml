<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantics as a Foreign Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Paul G</orgName>
								<orgName type="institution" key="instit1">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Bar-Ilan University Computer Science Department</orgName>
								<address>
									<addrLine>Ramat Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantics as a Foreign Language</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2412" to="2421"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2412</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multilingual machine translation , where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our lin-earization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many sentence-level representations were devel- oped with the goal of capturing the sentence's proposition structure and making it accessible for downstream applications <ref type="bibr" target="#b22">(Montague, 1973;</ref><ref type="bibr" target="#b6">Carreras and M` arquez, 2005;</ref><ref type="bibr" target="#b3">Banarescu et al., 2013;</ref><ref type="bibr" target="#b0">Abend and Rappoport, 2013)</ref>. See <ref type="bibr" target="#b1">Abend and Rappoport (2017)</ref>, for a recent survey.</p><p>While syntactic grammars <ref type="bibr" target="#b20">(Marcus et al., 1993;</ref><ref type="bibr" target="#b23">Nivre, 2005</ref>) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider no- tion of propositions (e.g, nominalizations, adjec- tivals, or appositives). In particular, we will focus on the three graph-based semantic repre- sentations collected in the Broad-Coverage Se- mantic Dependency Parsing SemEval shared task (SDP) <ref type="bibr">(Oepen et al., 2015)</ref>: (1) DELPH-IN Bi- * Work performed while at Bar-Ilan University.</p><p>Lexical Dependencies (DM) <ref type="bibr" target="#b11">(Flickinger, 2000</ref>), <ref type="bibr">1</ref> (2) Enju Predicate-Argument Structures (PAS) ( ), and (3) Prague Semantic Dependencies (PSD) ( <ref type="bibr" target="#b13">Hajic et al., 2012</ref>). These annotations have garnered recent attention (e.g., <ref type="bibr" target="#b5">(Buys and Blunsom, 2017;</ref><ref type="bibr" target="#b27">Peng et al., 2017a)</ref>), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Jour- nal corpus ( <ref type="bibr" target="#b7">Charniak et al., 2000</ref>).</p><p>In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP <ref type="bibr" target="#b27">(Peng et al., 2017a</ref>) by treating the differ- ent semantic formalisms as foreign target dialects, while having English a as a common source lan- guage <ref type="bibr">(Section 3</ref>). Subsequently, we devise a neu- ral MT sequence-to-sequence framework that is suited for the task.</p><p>In order to apply sequence-to-sequence mod- els for structured prediction, a linearization func- tion is required to interpret the model's sequen- tial input and output. Initial work on structured prediction sequence-to-sequence modeling has fo- cused on tree structures ( <ref type="bibr" target="#b31">Vinyals et al., 2015;</ref><ref type="bibr" target="#b2">Aharoni and Goldberg, 2017)</ref>, as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank ( <ref type="bibr" target="#b20">Marcus et al., 1993)</ref>). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS ( <ref type="bibr" target="#b28">Peng et al., 2017b</ref>; <ref type="bibr" target="#b4">Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b17">Konstas et al., 2017;</ref><ref type="bibr" target="#b5">Buys and Blunsom, 2017)</ref>. However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization -they reduce the input graph to a tree using lossy heuristics, which are specifi-cally tailored for their target representation.</p><p>In contrast, we design a novel deterministic and lossless linearization (Section 4), which is appli- cable to any graph with ordered nodes (e.g., sen- tence word order). To that end, we devise so- lutions for the various obstacles for linearizing a graph structure, such as reentrancies (or multi- ple heads), non-connected components, and non- projective relations. This lineariztion allows us to follow the spirit of <ref type="bibr" target="#b14">Johnson et al. (2017)</ref> in train- ing all source-target combinations in a multi-task approach (Section 5). These combinations include the three traditional text to semantic parsing tasks, as well as six additional inter-representation trans- lation tasks, constituting of all binary combina- tions of the target representations (e.g., PSD to PAS, or DM to PSD).</p><p>Following, we design an encoder-decoder model which has two shared encoders, one for raw English sentences and another for linearized graphs, and a single global graph decoder. Inter- estingly, we show that training on the auxiliary inter-representation translation tasks greatly im- proves the performance on the original SDP tasks, without requiring any additional manual annota- tion effort (Section 6).</p><p>Our contributions are two-fold. First, we show that novel sequence-to-sequence models are able to effectively capture and recover general graph structures, making them a viable and easily exten- sible approach towards the SDP task. Second, be- yond SDP, as the inclusion of syntactic lineariza- tion was shown beneficial in various tasks <ref type="bibr" target="#b2">(Aharoni and Goldberg, 2017;</ref>) so does our approach prompt easy integration of graph- based representations as complementary semantic signal in various downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We begin this section by presenting the corpus we use to train and test our model (the SDP cor- pus) and the current state-of-the-art in predicting semantic dependencies. Then, we discuss pre- vious work on sequence-to-sequence models for tree prediction, which this work extends to general graph structures. Finally, we briefly describe the multilingual translation approach, which we bor- row and adapt to the semantic parsing task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Dependencies</head><p>In general, the development of most semantic for- malisms was carried out by disjoint and indepen- dent efforts. However, the 2014 and 2015 Se- mEval shared tasks ( <ref type="bibr">Oepen et al., , 2015</ref> SDP has enabled the application of machine learn- ing models for the task. <ref type="bibr" target="#b27">Peng et al. (2017a)</ref> have set the state-of-the-art results on all three tasks, using techniques inspired by graph-based depen- dency parsing models <ref type="bibr" target="#b15">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b10">Dozat and Manning, 2016;</ref><ref type="bibr" target="#b18">Kuncoro et al., 2016)</ref>. Their best results were obtained by lever- aging the fact that SDP was annotated on parallel texts. They reached 88% average labeled F1 score across the SDP representations on an in-domain test set, via joint prediction of the three representa- tions using higher-order cross-representation fea- tures. The first row in <ref type="table">Table 4</ref> summarizes their performance for the three prediction tasks.</p><p>In this work we will take a different approach to structured prediction of the SDP corpus. We will design a novel sequence-to-sequence model, not necessitating parallel annotations, which are often unavailable for multi-task learning.  Figure 1: Example of gold annotations for the three sentence-level representations in the SDP corpus (DM, PAS, and PSD) on the same sentence, which was slightly shortened for presentation. Arcs in each of the representations appear above the sentence. Our "SHIFT" edges, which appear dashed below it, were introduced in Section 4 to ensure that all nodes are reachable from the first word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structured Prediction using Sequence-to-Sequence Models</head><p>In contrast to the graph-parsing algorithms dis- cussed in Section 2.1, a recent line of work has explored the usage of more general sequence-to- sequence models to perform structured prediction, focusing specifically on predicting tree structures. These approaches devise a task-specific lineariza- tion function which converts the structured repre- sentation to a sequential string, which is then used to train the recurrent neural network. During infer- ence, the inverted linearization function is applied to the output to recover the desired structure. <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> showed that sequence-to- sequence phrase-based constituency parsing can be achieved using a tree depth-first search (DFS) traversal as a linearization function. <ref type="bibr">2</ref> Following this work, several recent efforts have employed a similar DFS approach to AMR and MRS pars- ing ( <ref type="bibr" target="#b4">Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b17">Konstas et al., 2017;</ref><ref type="bibr" target="#b28">Peng et al., 2017b;</ref><ref type="bibr" target="#b5">Buys and Blunsom, 2017)</ref>, after reducing AMR to trees by removing re-entrencies.</p><p>Several recent works have found syntactic lin- earization useful outside of neural parsers. For ex- ample, in neural machine translation, <ref type="bibr" target="#b2">Aharoni and Goldberg (2017)</ref> showed that predicting target- side linearized syntactic trees can improve the quality and grammatically of the predicted transla- tions. In Section 4, we show for the first time that the DFS approach is a viable linearization function also for semantic dependencies, by extending it to account for the challenges introduced by the richer graph structures in SDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-lingual Machine Translation</head><p>Multi-Task Learning (MTL) is a modeling ap- proach which shares and tunes the model param- eters across several tasks. In some instances of MTL, a subset of the tasks may be defined as the "main tasks", while the other tasks are treated as auxiliaries which improve performance on the main tasks by contributing to their training sig- nal. MTL had regained popularity in recent years thanks to its easy and wide-spread applicability in neural networks <ref type="bibr" target="#b8">(Collobert et al., 2011;</ref><ref type="bibr" target="#b30">Sogaard and Goldberg, 2016)</ref>.</p><p>Perhaps most relevant to this work is Google's neural machine translation system by <ref type="bibr" target="#b14">Johnson et al. (2017)</ref>, which trained a single sequence-to- sequence model to translate between multiple lan- guages. They introduced the usage of a special tag in the source sentence to specify the desired target language. For example, &lt;2es&gt; indicates that the model should translate the input sentence to Span- ish.</p><p>In Section 4, we adapt the MTL strategy to train a single model for all SDP formalisms. We use a similar "to" and "from" tags to indicate source and desired target representations, and show that introducing auxiliary inter-task translations can improve performance on the main target tasks, namely parsing semantic representations for raw input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>We define the task of semantic translation, as converting to, and between, different sentence- level semantic representations.</p><p>Formally, a sentence-level semantic representation according to formalism R is a tuple, M R = (S, G), where S = {w 1 , ..., w n } is a raw sentence, and G = (V, E | V = {v 1 , ..., v n }, E ⊆ V 2 ) is a labeled graph whose vertices have a one-to-one correspon- dence with the words in S, 3 while its edges rep- resent binary semantic relations, adhering to R's specifications.</p><p>Using these notations, our input is defined as a triplet (source, target, M source ). Preceding the input semantic representation are identifiers for source and target representation schemes (e.g., "PAS", "DM" or "PSD"). The semantic transla- tion task is then to produce M target . I.e., the sen- tence's representation under the target formalism.</p><p>This definition is broad enough to encapsulate many sentence-level representations, and in this work we will use the three SDP representations, as well as an empty "RAW" representation (where E(G) = ∅ for all sentences) to allow for transla- tions from raw input sentences. We note that fu- ture work may extend this framework with other graph-based sentence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph Linearization</head><p>As discussed in Section 2, structured prediction in a sequence-to-sequence framework requires a lin- earization function, from the desired structure to a linear sequence, and vice versa.</p><p>Oftentimes, such linearization consists of node traversal along the edges of the input graph. While previous work have had certain structural con- straints on their input (e.g., imposing tree or non- cyclic constructions), in this work, we construct a lossless function which allows us to feed the sequence-to-sequence network with a linearized general graph representation and expect a lin- earized graph in its output.</p><p>In this section, we describe our linearization traversal order, which generalizes the DFS traver- sal applied previously only for trees. We do this by converting an SDP graph such that all nodes are reachable from node v 1 . We then outline the challenging aspects of graph properties (which do not exist in trees), show that they are prevalent in the SDP corpus, and describe our proposed solu- tions. To the best of our knowledge, this is the first work which tackles the task of general graph linearization.</p><p>While our linearization can be predicted with good accuracy (as we show in following sections), there is ample room to experiment with represen- tational variations, which we start exploring in Section 6. Our conversion code is made publicly available, 4 allowing further experimentation with general graph linearization for SDP and other re- lated tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Traversing Graphs with Non-Connected Components</head><p>The DFS approach is an applicable linearization of trees since a recursive traversal, which starts at the root and explores all outgoing edges, is guaranteed to visit all of the graph's nodes. However, DFS linearization is not directly applicable to SDP, as its graphs often consist of several non-connected components.</p><p>For such graphs, there exists no starting node from which all of the nodes are reachable via DFS traversal, and certain nodes are bound to be left out of the traditional DFS encoding. For example, the words "can" and "greatest" in <ref type="figure">Figure 1a</ref> reside in different components, and therefore no single path (which traverses along the graph's edge direction) will discover both of them.</p><p>To overcome this limitation, we make sure that all nodes are reachable from node v 1 , correspond- ing to the first word in the sentence, from which we start our traversal. This is achieved by intro- ducing an artificial SHIFT edge between any two consecutive nodes v i , v i+1 for which there is no directed path already connecting them. Follow- ing, it is easy to see, by induction, that all nodes are reachable from v 1 , as for every node v i there exists a directed path (v 1 , v 2 , ..., v i−1 , v i ). For ex- ample, revisit the previously mentioned "can" and "greatest" nodes in <ref type="figure">Figure 1a</ref>, which are connected using "SHIFT" edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Linearizing a DFS Graph Traversal</head><p>Intuitively, our linearization is a pre-order DFS, generalizing <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref>'s approach to syntactic linearization. We start from v 1 and ex- plore all paths from it, in a depth-first manner. Once a path is exhausted, either by reaching a node with no outgoing edges 5 or by reaching an already visited node, we use special backtracking edges to form a path backwards "up" the graph, until we hit a node which still has unexplored outgoing edges.</p><p>Formally, our linearization of a given DFS traversal is composed of 3 types of elements (see <ref type="figure" target="#fig_5">Figure 2</ref> for example):</p><p>First, a Node reference identifies a node in the graph, which in turn corresponds to word in the SDP formalism. We identify nodes using two to- kens: (1) Their position in the sentence, relative to the previous node in the path (while the first position in the linearization is written in absolute terms, as "0"), and (2) Explicitly writing the word corresponding to the node.</p><p>For example, in <ref type="figure" target="#fig_5">Figure 2</ref>, traversing the ARG1 edge from "easy" lands at "ind/2 understand", whose outgoing ARG2 edge ar- rives at "ind/-4 success".</p><p>Second, an Edge reference, identifies an edge label. These are denoted by a single token, com- posed of 2 parts: (1) The edge's formalism (in our case, the SDP representation to which it pertains), and (2) The edge label. Traversing an edge (u, v) with label L will be encoded by placing the edge reference between the node references of u and v. For instance, in <ref type="figure" target="#fig_5">Figure 2</ref>, moving from node 0 to node 1 through the edge labeled "poss" is en- coded with the following string: "ind/0 Their PAS/poss ind/1 success".</p><p>Finally, Backtracking edges, signify a step "backward" in the traversal. These are de- noted with a single token, similarly to edge ref- erences, with the addition of a "BACK" suffix. For example, in <ref type="figure" target="#fig_5">Figure 2</ref>, we backtrack from the already visited node "understand" by writing: "PAS/ARG1/BACK". This linearization can be deterministically and efficiently inverted back to the graph structure. This is done by building the graph while read- ing the linearization, adding to it nodes and edges when they first appear, and omitting possible node recurrences in the linearization (due to cycles or backtracking edges), such as "success", which ap- pears twice in the <ref type="figure" target="#fig_5">Figure 2</ref>.</p><p>Redundancy in encoding We note that certain items in our proposed linearization are redun- dant. First, writing down the explicit word in the traversal is not necessary, as the positional in- dex is sufficient to uniquely identify a node. Sec- ond, a single backtracking tag would have been enough to identify the specific edge which is cur- rently being backtracked (e.g., BACK instead of PAS/verb ARG1/BACK). The latter is similar to the redundancy in the syntactic linearization of <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref>, who specify the type of closing bracket, e.g., NP(...)NP instead of NP(...).</p><p>In Section 6 we show empirically that our model benefits from explicitly generating these redun- dancies during decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DFS Traversal Order</head><p>A graph DFS traversal does not dictate an order in which to explore the different outgoing paths at each branching point. Consider, as a recurring example, the branching point at the word "vote" in <ref type="figure">Figure 1c</ref>, in which we need to choose an order amongst its four neighbors.</p><p>While syntactic linearization conveniently fol- lows the ordering of the words in the sentence, <ref type="bibr" target="#b17">Konstas et al. (2017)</ref> have noted that different child visiting linearization orders affect the perfor- mance of text generation from AMR. In particular, they found that following the order of annotation of a human expert worked best.</p><p>Intuitively, since different graph traversals af- fect the sequence of encoded nodes during train-  Random permutation (play, for, jocks, now) *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence order</head><p>Neighbor's index in the sentence (jocks, now, for, play)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closest words</head><p>Neighbor's absolute distance (now, for, play, jocks)</p><p>Smaller-first # nodes reachable from the neighbor (now, play, for, jocks) Under the name of each order type, we list the key by which we sort each node's neighbors. The "Ex- ample" column shows the corresponding ordering of "vote"'s neighbors in <ref type="figure">Figure 1c</ref>. * An example of one possible random permutation.</p><p>ing, the network will inevitably have to learn dif- ferent weights and attention when presented with different orderings. Therefore, some traversal or- derings may be easier to learn than others, leading to better (hopefully more semantic) abstractions.</p><p>To the best of our knowledge, the human an- notation order is not available for the SDP anno- tations, and there is no clear a priori optimal or- dering. We therefore experiment with several vis- iting orders, as described in <ref type="table" target="#tab_2">Table 2</ref>. Notably, Sentence order is equivalent to the ordering used by <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> for syntactic lineariza- tion, while Closest words orders child nodes from short to longer range-dependencies (commonly as- sociated with syntactic versus semantic relations), and Smaller first is motivated by the easy-first ap- proach ( <ref type="bibr" target="#b12">Goldberg and Elhadad, 2010)</ref>, first encod- ing paths which are shorter (and easier to memo- rize), before longer, more complicated sequences.</p><p>In Section 6 we evaluate the effect of these vari- ations on the SDP parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model</head><p>We start by describing our model architecture, in- spired by recent MT architectures, while allow- ing for different types of inputs, namely English sentences and linearized graphs. Following, we present our methods for training and testing, and specific hyper-parameter configuration and imple- mentation details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture</head><p>Our architecture, depicted in <ref type="figure" target="#fig_6">Figure 3</ref>, consists of a sequence-to-sequence model using a bi-LSTM encoder-decoder with attention on input and out- put tokens, similar to that used by <ref type="bibr" target="#b14">Johnson et al. (2017)</ref> for multi-lingual MT. As described in Sec- tion 3, it is trained on 9 translation tasks in paral- lel. We split these into two groups, consisting of 3 primary tasks and 6 auxiliary tasks, as follows:</p><formula xml:id="formula_0">PRIMARY = {(RAW, tgt) | tgt ∈ (DM, PAS, PSD)} AUXILIARY = {(src, tgt) ∈ {DM, PAS, PSD} 2 | src = tgt}</formula><p>The PRIMARY tasks deal with converting raw sentences to linearized graph structures, which we can compare to previous published baselines and are therefore our main interest. Conversely, while the AUXILIARY tasks provide additional training signal to tune our model, they are also interesting from an analytic point-of-view, which we examine in depth in Section 6.</p><p>To allow the model to differentiate between the different tasks, we prefix each input sample with two tags (see example in <ref type="figure" target="#fig_6">Figure 3)</ref>. First, similarly to <ref type="bibr" target="#b14">Johnson et al. (2017)</ref>, we add a tag indicating the desired target representation, e.g., &lt;to:DM&gt;. Second, In contrast to multi-lingual MT which omits the source language (to allow for Simplified sketch of our sequence-to-sequence architecture. The figure depicts encoding and decoding of two input training samples, one from raw text to PSD (lower left), and the other from DM to PSD (top left). The OR gate denotes choosing only one sample to encode at each training step. While the two samples use different encoders, they share a single global SDP decoder (right) which outputs a the graph structure. As denoted by dashed edges, at every decode step we can deterministically interject and override the softmax probabilities for redundant elements, based on previous predictions. For simplicity sake, a small number of units is showed for encoders and decoder and the attention and deep encoder-decoder layers are omitted.</p><p>code switching), we explicitly denote the source representation, e.g., &lt;from:PSD&gt;. This addition further strengthens the correlation between inputs from the same representation. <ref type="bibr">6</ref> Further deviating from the current practice in MT, our architecture uses two encoders and a single decoder (while common MT regards the encoder-decoder as a single unit). The first shared encoder specializes in encoding raw text for all PRIMARY tasks, while a second encodes linearized graph structures for the AUXILIARY tasks. Both encoders are linked to a single de- coder which converts their output representations to a linearized graph.</p><p>Intuitively, the two encoders correspond to the different nature of input to the PRIMARY tasks (an English sentence) versus that of the AUXILIARY tasks (a linearized graph), while a single decoder allows for a common linearized graphs output for- mat. Since the decoder is trained across all 9 tasks, both encoders are optimized to arrive at similar latent representations which are geared towards graph prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training and Inference</head><p>The overall size of multi-task training data is 320, 913 samples. This constitutes a 9-fold in-crease over a single-model for SDP (35, 657 sen- tences in the SDP corpus) and a 3-fold increase over a standard MTL approach to SDP (without the AUXILIARY tasks). During training, we pe- nalize the model on all predicted elements, in- cluding the redundant elements discussed in Sec- tion 4.2. During inference, however, these re- dundancies may cause contradictions leading to incoherent sequences. Namely, a word may not conform to the previous word index, and a back- tracking edge may point to a different relation. To overcome this we artificially increase the soft- max probabilities (dashed edges in <ref type="figure" target="#fig_6">Figure 3</ref>) so that they reflect the DFS path decoded up until that point. Specifically, we override the predicted word according to the previous index, and back- track "up" the corresponding edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>All of our hyper-parameters were tuned on a held out partition of 1000 sentences in the training set. In particular, we use 3 hidden layers for both of the encoders, and 2 hidden layers for the de- coder. English word embeddings were fixed with 300-dimensional GloVe embeddings ( <ref type="bibr" target="#b29">Pennington et al., 2014</ref>), while the graph elements, which con- sist of a lexicon of roughly 400 tokens across three representations, were randomly initialized. We trained the model until convergence, roughly 20  epochs, in about 12 hours on a GPU (NVIDIA GeForce GTX 1080 Ti), in batches of 50 sen- tences. All of these sentences belong to the same task, which is chosen at random before each batch. Finally, our models were developed using the OpenNMT-py library ( <ref type="bibr" target="#b16">Klein et al., 2017)</ref>, and are made available. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We perform several evaluations, testing the im- pact of alternative configurations, including the different DFS traversal orders and MTL versus single-task approach, as well as our model's per- formance against current state-of-the-art on each of the PRIMARY tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results</head><p>The results of our different analyses are reported in <ref type="table" target="#tab_4">Tables 3-6</ref>, as elaborated below. For all evalu- ations, we use the in-domain test partition of the SDP corpus, containing 1, 410 sentences. Follow- ing <ref type="bibr" target="#b27">Peng et al. (2017a)</ref> we report performance us- ing labeled F1 scores as well as average scores across representations. We compare the produced graphs, after applying the inverted linearization function, rather than comparing the DFS path di- rectly, as there may be several DFS graph traver- sals encoding the same relations.</p><p>DFS order matters - <ref type="table" target="#tab_4">Table 3</ref> depicts our model's performance when linearizing the graphs according to the different traversal orders dis- cussed and exemplified in <ref type="table" target="#tab_2">Table 2</ref>. Overall, we find that the "smaller-first" approach performs best across all datasets, and that imposing one of our orders is always preferable over random permu- tations. Intuitively, the "smaller-first" approach presents shorter, and likely easier, paths first, thus minimizing the amount of error-propagation for following decoding steps. Due to its better per- formance, we will report only the smaller-first's performance in all following evaluations.</p><p>From English to SDP - <ref type="table">Table 4</ref> presents the performance of our complete model ("MTL PRI- MARY+AUX") versus <ref type="bibr" target="#b27">Peng et al. (2017a)</ref>. On average, our model performs within 1% F1 point from the state-of-the art (outperforming it on the harder PSD task), despite using the more gen- eral sequence-to-sequence approach instead of a dedicated graph-parsing algorithm. In addition, an ablation study shows that multi-tasking the PRIMARY tasks is beneficial over a single task set- ting, which in turn is outperformed by the inclu- sion of the AUXILIARY tasks.</p><p>Simulating disjoint annotations -In contrast with SDP's complete overlap of annotated sen- tences, multi-task learning often deals with dis- joint training data. To simulate such scenario, we retrained the models on a randomly selected set of 33% of the train sentences for each representation (11, 886 sentences), such that the three representa- tions overlap on only 10% (3, 565 sentences). The results in <ref type="table" target="#tab_7">Table 5</ref> show that our approach is more resilient to the decrease in annotation overlap, out- performing the state-of-the-art model on the DM and PSD task, as well as on the average score. We hypothesize that this is in part thanks to our ability to use the inter-task translations, even when these exist only for part of the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Translating Between Representations</head><p>As a byproduct of training on the AUXILIARY tasks, our model can also be tested on translating between the different representations. This is done by presenting it with a linearized graph of one rep- resentation and asking it to translate it to another. To the best of our knowledge, this is the first work which tries to accomplish this.</p><p>We report the performance of all source-target combinations in <ref type="table">Table 6</ref>. These evaluations pro- vide several interesting comparisons between the representations: (1) For all representations, trans- lating from any of the other two is easier than pars- ing from raw text, (2) The PAS and DM represen- tations can be converted between them with high accuracy (95.7% and 96.1%, respectively). This can be due to their structural resemblance, noted in previous work <ref type="bibr" target="#b27">(Peng et al., 2017a;</ref><ref type="bibr">Oepen et al., 2015)</ref>, and (3) While PSD serves as a viable in- put for conversion to DM and PAS (92.1% F1 on   <ref type="table">Table 4</ref>: Evaluation of our model (labeled F1 score) versus the current state of the art. "Sin- gle" denotes training a different encoder-decoder for each task. "MTL PRIMARY" reports the performance of multi-task learning on only the PRIMARY tasks. "MTL PRIMARY+AUX" shows the performance of our full model, including MTL with the AUXILIARY tasks.</p><p>DM PAS PSD Avg.  92.6 91.9 92.1 <ref type="table">Table 6</ref>: Performance (labeled F1 score) of inter- task translations. Each column depicts the perfor- mance converting from a specific source represen- tation, while each row denotes the corresponding target representation.</p><p>average), it is relatively harder to convert either of them to PSD (88.6%). This might indicate that PSD subsumes some of the information in DM and PAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We presented a novel sequence-to-sequence ap- proach to the task of semantic dependency parsing, by casting the problem as multi-lingual machine translation. To that end, we introduced a DFS- based graph linearization function which general- izes several previous works on tree linearization. Following, we showed that our model, inspired by neural MT, benefits from the inter-task training signal, reaching performance almost on-par with current state of the art in several scenarios. Future work can employ this linearization func- tion within downstream applications, as was done with syntactic linearization, or extend this frame- work with other graph-based representations, such as universal dependencies <ref type="bibr" target="#b24">(Nivre et al., 2016</ref>) or AMR ( <ref type="bibr" target="#b3">Banarescu et al., 2013</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) have culminated in the Semantic Dependency Parsing (SDP) resource, a consistent and large corpus (roughly 39K sentences), annotated in parallel with three well-established formalisms: DELPH-IN MRS-Derived Bi-Lexical Dependen- cies (DM) (Flickinger, 2000), Enju Predicate- Argument Structures (PAS) (Miyao et al., 2014), and Prague Semantic Dependencies (PSD) (Hajic et al., 2012). While varying in their labels and annotation guidelines, all three representations in- duce a graph structure, where each node corre- sponds to a single word in the sentence. See Ta- ble 1 for more details on this corpus, and Fig- ure 1 for examples of the three SDP formalisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>vote's PSD neighbors)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Their success is easy to understand poss ARG2 ARG1 SHIFT SHIFT SHIFT SHIFT (a) Gold PAS representation from the SDP corpus. Original gold edges appear above the words, while our introduced edges appear below them. ind/0 Their PAS/poss ind/1 success SHIFT ind/1 is SHIFT ind/1 easy PAS/ARG1 ind/2 understand PAS/ARG2 ind/-4 success PAS/ARG2/BACK ind/4 understand PAS/ARG1/BACK ind/-2 easy SHIFT ind/1 to SHIFT ind/1 understand (b) Our linearization scheme for the sentence in 2a. Each node is represented by its relative index and surface form. Back- wards traversing edges (marked with BACK) appear in italics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of gold PAS representation from the development partition of the SDP corpus (top), and our corresponding linearization (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Simplified sketch of our sequence-to-sequence architecture. The figure depicts encoding and decoding of two input training samples, one from raw text to PSD (lower left), and the other from DM to PSD (top left). The OR gate denotes choosing only one sample to encode at each training step. While the two samples use different encoders, they share a single global SDP decoder (right) which outputs a the graph structure. As denoted by dashed edges, at every decode step we can deterministically interject and override the softmax probabilities for redundant elements, based on previous predictions. For simplicity sake, a small number of units is showed for encoders and decoder and the attention and deep encoder-decoder layers are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Peng</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Couch -potato jocks watching Monday Night Football can now vote for the greatest play in 20 years</head><label>Couch</label><figDesc></figDesc><table>compound 
ARG1 

ARG2 

compound compound 

ARG1 

ARG1 

ARG2 
ARG1 

BV 

ARG1 
ARG1 

ARG2 

ARG2 

SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 

(a) DELPH-IN Minimal Recursion Semantics-derived bi-lexical dependencies (DM). 

Couch-potato jocks watching Monday Night Football can now vote for the greatest play in 20 years 

noun ARG1 
verb ARG1 

verb ARG2 

noun ARG1 

noun ARG1 

aux ARG2 

aux ARG1 

adj ARG1 

verb ARG1 

prep ARG1 

prep ARG2 

det ARG1 

adj ARG1 
prep ARG1 

prep ARG2 

adj ARG1 

SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 
SHIFT 

(b) Enju Predicate-Argument Structures (PAS). 

Couch-potato jocks watching Monday Night Football can now vote for the greatest play in 20 years 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Different neighbor exploration orders. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation of different DFS orderings, in 
labeled F1 score, across the different tasks. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance (labeled F1 score) of our 
model versus the state of the art, when reducing 
the amount of overlap in the training data to 10%. 

To \From DM PAS PSD Avg. 

DM 
96.1 92.4 
94.3 
PAS 
95.7 
91.7 
93.7 
PSD 
89.5 87.6 
88.6 
Avg. 
</table></figure>

			<note place="foot" n="1"> DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999).</note>

			<note place="foot" n="2"> DFS for rooted trees is equivalent to the bracketed notation of the Penn Treebank.</note>

			<note place="foot" n="3"> The one-to-one node-to-word correspondence follows SDP&apos;s formulation, but can be relaxed to adjust for other graph structures.</note>

			<note place="foot" n="4"> https://github.com/gabrielStanovsky/ semantics-as-foreign-language</note>

			<note place="foot" n="5"> Note that after introducing the artificial &quot;SHIFT&quot; edges, only vn may have no outgoing edges.</note>

			<note place="foot" n="6"> Moreover, &quot;code-switching&quot; between semantic representations is inherently undesired.</note>

			<note place="foot" n="7"> https://github.com/gabrielStanovsky/ semantics-as-foreign-language</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by grants from the MAGNET program of the Israeli Office of the Chief Scientist (OCS); the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1) and the Is-rael Science Foundation (grant No. 1157/16).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (UCCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference of the Association for Computational Linguistics</title>
		<meeting>the 2013 conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The state of the art in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on amr parsing accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guntis</forename><surname>Barzdins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didzis</forename><surname>Gosko</surname></persName>
		</author>
		<editor>SemEval@NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust incremental neural semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CONLL</title>
		<meeting>CONLL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Blaheta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Bllip 1987-89 wsj corpus release 1. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Minimal recursion semantics: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><forename type="middle">A</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On building a more effcient grammar by exploiting types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Announcing prague czech-english dependency treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hajicová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarmila</forename><surname>Panevová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Fucíková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Mikulová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Pajas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Popelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distilling an ensemble of greedy dependency parsers into one mst parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving sequence to sequence neural machine translation by utilizing syntactic dependency information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>An Nguyen Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akifumi</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In-house: An ensemble of pre-existing offthe-shelf parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The proper treatment of quantification in ordinary english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approaches to natural language</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1973" />
			<biblScope unit="page" from="221" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dependency grammar and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<meeting><address><addrLine>Natalia Silveira, Reut Tsarfaty, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<title level="m">Hajic, and Zdenka Uresová. 2015. Semeval 2015 task 18: Broad-coverage semantic dependency parsing</title>
		<imprint>
		</imprint>
	</monogr>
	<note>SemEval@NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semeval 2014 task 8: Broad-coverage semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multitask learning for semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sogaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
