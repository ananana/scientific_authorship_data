<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Topic Model for Twitter Considering Dynamics of User Interests and Topic Trends</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Sasaki</surname></persName>
							<email>sasaki@cmplx.cse.nagoya-u.ac.jp yoshikawa, furuhashi@cse.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Yoshikawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Furuhashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Topic Model for Twitter Considering Dynamics of User Interests and Topic Trends</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1977" to="1985"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Latent Dirichlet allocation (LDA) is a topic model that has been applied to various fields, including user profiling and event summarization on Twitter. When LDA is applied to tweet collections, it generally treats all aggregated tweets of a user as a single document. Twitter-LDA, which assumes a single tweet consists of a single topic, has been proposed and has shown that it is superior in topic semantic coherence. However, Twitter-LDA is not capable of online inference. In this study, we extend Twitter-LDA in the following two ways. First, we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user. Second, we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model (TTM), which models consumer purchase behaviors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microblogs such as Twitter, have prevailed rapidly in our society recently. Twitter users post a mes- sage using 140 characters, which is called a tweet. The characters limit allows users to post tweets easily about not only personal interest or real life but also public events such as traffic accidents or earthquakes. There have been many studies on how to extract and utilize such information on tweets ( <ref type="bibr" target="#b5">Diao et al., 2012;</ref><ref type="bibr" target="#b12">Pennacchiotti and Popescu, 2011;</ref><ref type="bibr" target="#b13">Sakaki et al., 2010;</ref><ref type="bibr" target="#b17">Weng et al., 2010)</ref>.</p><p>Topic models, such as latent Dirichlet alloca- tion (LDA) ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref> are widely used to identify latent topic structure in large collections of documents. Recently, some studies have ap- plied LDA to Twitter for user classification <ref type="bibr" target="#b12">(Pennacchiotti and Popescu, 2011</ref>), detection of influ- ential users <ref type="bibr" target="#b17">(Weng et al., 2010)</ref>, and so on. LDA is a generative document model, which assumes that each document is represented as a probabil- ity distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated as a single docu- ment. This direct application does not work well because a tweet is very short compared with tradi- tional media such as newspapers. To deal with the shortness of a tweet, some studies aggregated all the tweets of a user as a single document <ref type="bibr" target="#b8">(Hong and Davison, 2010;</ref><ref type="bibr" target="#b12">Pennacchiotti and Popescu, 2011;</ref><ref type="bibr" target="#b17">Weng et al., 2010</ref>). On the other hand, <ref type="bibr" target="#b19">Zhao et al. (2011)</ref> proposed "Twitter-LDA," which is a model that considers the shortness of a tweet. Twitter-LDA assumes that a single tweet consists of a single topic, and that tweets consist of topic and background words. <ref type="bibr" target="#b19">Zhao et al. (2011)</ref> show that it works well at the point of semantic coher- ence of topics compared with LDA. However, as with the case of LDA, Twitter-LDA cannot con- sider a sequence of tweets because it assumes that samples are exchangeable. In Twitter, user inter- ests and topic trends are dynamically changing. In addition, when new data comes along, a new model must be generated again with all the data in Twitter-LDA because it does not assume online inference. Therefore, it cannot efficiently analyze the large number of tweets generated everyday. To overcome these difficulties, a model that considers the time sequence and has the capability of online inference is required.</p><p>In this study, we first propose an improved model based on Twitter-LDA, which assumes that the ratio between topic and background words dif- fers for each user. This study evaluates the pro- posed method based on perplexity and shows the efficacy of the new assumption in the improved model. Second, we propose a new topic model called "Twitter-TTM" by extending the improved model based on the topic tracking model (TTM) ( <ref type="bibr" target="#b9">Iwata et al., 2009</ref>), which models the purchase behavior of consumers and is capable of online inference. Finally, we demonstrate that Twitter- TTM can effectively capture the dynamics of user interests and topic trends in Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Improvement of Twitter-LDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improved-Model</head><p>Figure 1(a) shows the graphical representation of Twitter-LDA based on the following assumptions. There are K topics in Twitter and each topic is rep- resented by a topic word distribution. Each user has his/her topic interests ϕ u represented by a dis- tribution over K topics. Topic k is assigned to each tweet of user u depending on the topic inter- ests ϕ u . Each word in the tweet assigned by topic k is generated from a background word distribu- tion θ B or a topic word distribution θ k . Whether the word is a background word or a topic word is determined by a latent value y. When y = 0, the word is generated from the background word distribution θ B , and from the topic word distribu- tion θ k when y = 1. The latent value y is chosen according to a distribution π. In other words, the ratio between background and topic words is de- termined by π.</p><p>In Twitter-LDA, π is common for all users, meaning that the rate between background and topic words is the same for each user. However, this assumption could be incorrect, and the rate could differ for each user. Thus, we develop an improved model based on Twitter-LDA, which as- sumes that π is different for each user, as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. In the improved model, the rate between background and topic words for user u is determined by a user-specific distribution π u . The improved model is expected to infer the generative process of tweets more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiment for Improved Model</head><p>We performed an experiment to compare the pre- dictive performances of LDA, TTM, and the im- proved model shown in Section 2.1. In this ex- periment, LDA was applied as the method to ag- gregate all tweets of a user as a single document. as the same as other general tweets because they reflected the user's interests. After the above preprocessing, we obtained the final dataset with 14,139 users, 252,842 tweets, and 7,763 vocab- ularies. Each model was inferred with collapsed Gibbs sampling ( <ref type="bibr" target="#b6">Griffiths and Steyvers, 2004</ref>) and the iteration was set at 500. For a fair comparison, the hyper parameters in these models were opti- mized in each Gibbs sampling iteration by max- imizing likelihood using fixed iterations <ref type="bibr" target="#b11">(Minka, 2000)</ref>.</p><p>This study employs perplexity as the evaluation index, which is the standard metric in information retrieval literature. The perplexity of a held-out test set is defined as</p><formula xml:id="formula_0">perplexity = exp ( − 1 N ∑ u log p(w u ) )<label>(1)</label></formula><p>where w u represents words are contained in the tweets of user u and N is the number of words in the test set. A lower perplexity means higher pre- dictive performance. We set the number of topics K at 50, 100, 150, 200, and 250 and evaluated the perplexity for each model in each K via a 10-fold cross-validation.</p><p>The results are shown in <ref type="table" target="#tab_1">Table 1</ref>, which shows that the improved model performs better than the other models for any K. Therefore, the new as- sumption of the improved model, that the rate be- tween background and topic words is different for each user, could be more appropriate. LDA per- formance worsens with an increase in K because the aggregated tweets of a single user neglect the topic of each tweet. <ref type="table" target="#tab_2">Table 2</ref> shows examples of the tweets of users with high and low rates of background words. The users with a high background words rate tend to use basic words that are often used in any top- ics, such as "like," "about," and "people," and they tend to tweet about their personal lives. On the other hand, for users with a low background words rate, topical words are often used such as "Arse- nal," "Justin," and "Google". They tend to tweet about their interests, including music, sports, and movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Twitter-TTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Extension based on Topic Tracking Model</head><p>We extend the improved model shown in Section 2.1 considering the time sequence and capabil-    <ref type="bibr" target="#b15">Wang and McCallum, 2006</ref>). DTM is a model for analyzing the time evolution of top- ics in time-ordered document collections. It does not track the interests of each user as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a) because it assumes that a user (doc- ument) has only one time stamp. ToT requires all the data over time for inference, thus, it is not ap- propriate for application to continuously generated data such as Twitter. We consider a model must be capable of online inference and track the dynam- ics of user interests and topic trends for modeling tweets. Since TTM has these abilities, we adapt it to the improved model described in Section 2.</p><p>Figure 2(b) shows the graphical representation of TTM. TTM assumes that the mean of user in- terests at the current time is the same as that at the previous time, unless new data is observed. For- mally, the current interest ϕ t,u are drawn from the following Dirichlet distribution in which the mean is the previous interestˆϕinterestˆ interestˆϕ t−1,u and the precision is </p><formula xml:id="formula_1">α t,u p(ϕ t,u | ˆ ϕ t−1,u , α t,u ) ∝ ∏ k ϕ αt,u ˆ ϕ t−1,u,k −1 t,u,k<label>(2)</label></formula><p>where ϕ t,u,k represents the probability that user u is interested in topic k at time t. t is a discrete variable and can be arbitrarily set as the unit time interval, e.g., at one day or one week. The preci- sion α t,u represents the interest persistence of how consistently user u maintains his/her interests at time t compared with the previous time t − 1. α t,u is estimated for each time period and each user because interest persistence depends on both time and users. As mentioned above, the current topic trend θ t,k is drawn from the following Dirichlet distribution with the previous trendˆθtrendˆ trendˆθ t−1,k</p><formula xml:id="formula_2">p(θ t,k | ˆ θ t−1,k , β t,k ) ∝ ∏ v θ β t,k ˆ θ t−1,k,v −1 t,k,v<label>(3)</label></formula><p>where θ t,k,v represents the probability that word v is chosen in topic k at time t.</p><p>Here our proposed Twitter-TTM adapts the above TTM assumptions to the improved model. That is, we extend the improved model whereby user interest ϕ t,u and topic trend θ t,k depend on previous states. Time dependency is not consid- ered on θ B and π u because they can be regarded as being independent of time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Inference</head><p>We use a stochastic expectation-maximization al- gorithm for Twitter-TTM inference, as described in <ref type="bibr" target="#b14">Wallach (2006</ref></p><note type="other">) in which Gibbs sampling of la- tent values and maximum joint likelihood estima- tion of parameters are alternately iterated. At time t, we estimate user interests Φ</note><formula xml:id="formula_3">t = { ˆ ϕ t,u } U u=1 , topic trends Θ t = { ˆ θ t,k } K k=1</formula><p>, background word distribution θ t,B , word usage rate distribution π t,u , interest persistence parameters α t = {α t,u } U u=1 , and trend persistence parameters β t = {β t,k } K k=1 using the previous time interestsˆΦinterestsˆ interestsˆΦ t−1 and trendsˆΘ trendsˆ trendsˆΘ t−1 .</p><p>We employ collapsed Gibbs sampling to infer the latent variables. Let D t be a set of tweets and Z t , Y t be a set of latent variables z, y at time t. We can integrate the parameters in the joint distribu-tion as follows:</p><formula xml:id="formula_4">p(D t , Y t , Z t | ˆ Φ t−1 , ˆ Θ t−1 , α t , β t , λ, γ) = ( Γ(2γ) Γ(γ) 2 ) U ∏ u Γ(γ + n t,u,B )Γ(γ + n t,u,K ) Γ(2γ + n t,u ) × Γ(V λ) Γ(λ) V ∏ v Γ(n t,B,v + λ) Γ(n t,B + V λ) × ∏ k Γ(β t,k ) Γ(n t,k + β t,k ) ∏ v Γ(n t,k,v + β t,k ˆ θ t−1,k,v ) Γ(β t,k ˆ θ t−1,k,v ) × ∏ u Γ(α t,u ) Γ(c t,u + α t,u ) ∏ k Γ(c t,u,k + α t,u ˆ ϕ t−1,u,k ) Γ(α t,u ˆ ϕ t−1,u,k ) ,<label>(4)</label></formula><p>where n t,u,B and n t,u,K are the number of back- ground and topic words of user u at time t, n t,B,v is the number of times that word v is assigned as a background word at time t, n t,k,v is the num- ber of times that word v is assigned to topic k at time t, c t,u,k is the number of tweets assigned to topic k for user u at time t. In addition, n t,u = n t,u,B + n t,uK ,</p><formula xml:id="formula_5">n t,B = ∑ v n t,B,v , n t,K = ∑ k n t,k = ∑ k ∑ v n t,k,v , n t,u = ∑ k n t,u,k</formula><p>, and c t,u = ∑ k c t,u,k . Given the assignment of all other latent vari- ables, we derive the following formula calculated from eq.(4) to infer a latent topic,</p><formula xml:id="formula_6">p(z i = k|D t , Y t , Z t\i , ˆ Φ t−1 , ˆ Θ t−1 , α t , β t ) ∝ c t,u,k\i + α t,u ˆ ϕ t−1,u,k c t,u\i + α t,u Γ(n t,k\i + β t,k ) Γ(n t,k + β t,k ) × ∏ v Γ(n t,k,v + β t,k ˆ θ t−1,k,v ) Γ(n t,k,v\i + β t,k ˆ θ t−1,k,v ) ,<label>(5)</label></formula><p>where i = (t, u, s), thus z i represents a topic as- signed to the s-th tweet of user u at time t, and \i represents a count excluding the i-th tweet. Then, when z i = k is given, we derive the fol- lowing formula to infer a latent variable y j ,</p><formula xml:id="formula_7">p(y j = 0|D t , Y t\j , Z t , λ, γ) ∝ n t,B,v\j + λ n t,B\j + V λ n t,u,B\j + γ n t,u\j + 2γ ,<label>(6)</label></formula><formula xml:id="formula_8">p(y j = 1|D t , Y t\j , Z t , ˆ Θ t−1 , β t , γ) ∝ n t,k,v\j + β t,k ˆ θ t−1,k,v n t,k\j + β t,k n t,u,K\j + γ n t,u\j + 2γ ,<label>(7)</label></formula><p>where j = (t, u, s, n), thus y j represents a latent variable assigned to the n-th word in the s-th tweet of user u at time t, and \j represents a count ex- cluding the j-th word. The persistence parameters α t and β t are esti- mated by maximizing the joint likelihood eq.(4), using a fixed point iteration <ref type="bibr" target="#b11">(Minka, 2000</ref>). The update formulas are as follows:</p><formula xml:id="formula_9">α new t,u = α t,u ∑ k ˆ ϕ t−1,u,k A t,u,k Ψ(c t,u + α t,u ) − Ψ(α t,u ) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">A t,u,k = Ψ(c t,u,k + α t,u ˆ ϕ t−1,u,k ) − Ψ(α t,u ˆ ϕ t−1,u,k )</formula><p>, and</p><formula xml:id="formula_11">β new t,k = β t,k ∑ v ˆ θ t−1,k,v B t,k,v Ψ(n t,k + β t,k ) − Ψ(β t,k ) ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">B t,k,v = Ψ(n t,k,v + β t,k ˆ θ t−1,k,v ) − Ψ(β t,k ˆ θ t−1,k,v )</formula><p>. We can estimate latent variables Z t , Y t , and parameters α t and β t by iterating Gibbs sampling with eq. <ref type="formula" target="#formula_6">(5)</ref>, eq. <ref type="formula" target="#formula_7">(6)</ref>, and eq. <ref type="formula" target="#formula_8">(7)</ref> and maximum joint likelihood with eq. <ref type="formula" target="#formula_9">(8)</ref> and eq.(9). After the iterations, the means of ϕ t,u,k and θ t,k,v are obtained as follows.</p><formula xml:id="formula_13">ˆ ϕ t,u,k = c t,u,k + α t,u ˆ ϕ t−1,u,k c t,u + α t,u ,<label>(10)</label></formula><formula xml:id="formula_14">ˆ θ t,k,v = n t,k,v + β t,k ˆ θ t−1,k,v n t,k + β t,k .<label>(11)</label></formula><p>These estimates are used as the hyper parameters of the prior distributions at the next time period t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Recently, topic models for Twitter have been pro- posed. <ref type="bibr" target="#b5">Diao et al. (2012)</ref> proposed a topic model that considers both the temporal informa- tion of tweets and user's personal interests. They applied their model to find bursty topics from Twitter. <ref type="bibr" target="#b18">Yan et al. (2013)</ref> proposed a biterm topic model (BTM), which assumes that a word- pair is independently drawn from a specific topic. They demonstrated that BTM can effectively cap- ture the topics within short texts such as tweets compared with LDA. <ref type="bibr" target="#b4">Chua and Asur (2013)</ref> pro- posed two topic models considering time order and tweet intervals to extract the tweets summa- rizing a given event. The models mentioned above do not consider the dynamics of user interests, nor 1. Draw θ t,B ∼Dirichlet(λ) 2. For each topic k = 1, ..., K, (a) draw θ t,k ∼Dirichlet(β t,k ˆ θ t−1,k )</p><p>3. For each user u = 1, ..., U , (a) draw ϕ t,u ∼Dirichlet(α t,u ˆ ϕ t−1,u ) (b) draw π t,u ∼Beta(γ) (c) for each tweet s = 1, ..., N u i. draw z t,u,s ∼Multinomial(ϕ t,u ) ii. for each word n = 1, ..., N u,s A. draw y t,u,s,n ∼Bernoulli(π t,u ) B. draw w t,u,s,n ∼ Multinomial(θ t,B ) if y t,u,s,n = 0 or Multinomial(θ t,zt,u,s ) if y t,u,s,n = 1</p><p>Figure 3: Generative process of tweets in Twitter- TTM do they have the capability of online inference; thus, they cannot efficiently model the large num- ber of tweets generated everyday, whereas Twitter- TTM can capture the dynamics of user interests and topic trends and has the capability of online inference.</p><p>Some online topic models have also been pro- posed. TM-LDA was proposed by <ref type="bibr" target="#b16">Wang et al. (2012)</ref>, which can efficiently model online the top- ics and topic transitions that naturally arise in a tweet stream. Their model learns the transition parameters among topics by minimizing the pre- diction error on topic distribution in subsequent tweets. However, the TM-LDA does not con- sider dynamic word distributions. In other words, their model can not capture the dynamics of topic trends. <ref type="bibr" target="#b10">Lau et al. (2012)</ref> proposed a topic model implementing a dynamic vocabulary based on on- line LDA (OLDA) <ref type="bibr" target="#b1">(AlSumait et al., 2008</ref>) and ap- plied it to track emerging events on Twitter. An online variational Bayes algorithm for LDA is also proposed <ref type="figure" target="#fig_0">(Hoffman et al., 2010)</ref>. However, these methods are based on LDA and do not consider the shortness of a tweet. Twitter-TTM tackles the shortness of a tweet by assuming that a single tweet consists of a single topic. This assumption is based on the following observation: a tweet is much shorter than a normal document, so a single tweet rarely contains multiple topics but rather a single one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setting</head><p>We evaluated the effectiveness of the proposed Twitter-TTM using an actual Twitter data set. The original Twitter data set contains 15,962 users and 4,146,672 tweets collected from October 18 to 31, 2013. We then removed words that occurred less than 30 times and stop words. After this prepro- cessing, we obtained the final data set with 15,944 users, 3,679,481 tweets, and 30,096 vocabularies. We compared the predictive performance of Twitter-TTM with LDA, TTM, Twitter-LDA, Twitter-LDA+TTM, and the improved model based on the perplexity for the next time tweets. Twitter-LDA+TTM is a combination of Twitter- LDA and TTM. It is equivalent to Twitter-TTM, except that the rate between background and topic words is different for each user. We set the num- ber of topics K at 100, the iteration of each model at 500, and the unit time interval at one day. The hyper parameters in these models were optimized in each Gibbs sampling iteration by maximizing likelihood using fixed iterations <ref type="bibr" target="#b11">(Minka, 2000</ref>). The inferences of LDA, Twitter-LDA, and the im- proved model were made for current time tweets. <ref type="figure" target="#fig_4">Figure 5</ref> shows the perplexity of each model for each time, where t = 1 in the horizontal axis rep- resents October 18, t = 2 represents October 19, ..., and t = 13 represents October 31. The perplex- ity at time t represents the predictive performance of each model inferred by previous time tweets to the current time tweets. Note that at t = 1, the per- formance of LDA and TTM, that of Twitter-LDA and Twitter-LDA+TTM, and that of Twitter-TTM and the improved model were found to be equiva- lent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Result</head><p>As shown in <ref type="figure" target="#fig_4">Figure 5</ref>(a), the proposed Twitter- TTM shows lower perplexity compared with con- ventional models, such as LDA, Twitter-LDA, and TTM at any time, which implies that Twitter-TTM can appropriately model the dynamics of user in- terests and topic trends in Twitter. TTM could not have perplexity lower than LDA although it considers the dynamics. If LDA could not ap- propriately model the tweets, then the user inter- estsˆΦestsˆ estsˆΦ t−1 and topic trendsˆΘtrendsˆ trendsˆΘ t−1 in the previous time are not estimated well in TTM. <ref type="figure" target="#fig_4">Figure 5(b)</ref> shows the perplexities of the improved model and Twitter-TTM. From t = 2, Twitter-TTM shows lower perplexity than the improved model for each time. The reason for the high perplexity of the im- proved model is that it does not consider the dy- namics. Twitter-TTM also shows lower perplexity than Twitter-LDA+TTM for each time, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(c), because Twitter-TTM's assumption that the rate between background and topic words is different for each user is more appropriate, as demonstrated in Section 2.2. These results imply that Twitter-TTM also outperforms other conven- tional methods, such as DTM, OLDA, and TM- LDA, which do not consider the shortness of a tweet or the dynamics of user interests or topic trends . <ref type="table" target="#tab_3">Table 3</ref> shows two topic examples of the topic evolution analyzed by Twitter-TTM, and <ref type="figure" target="#fig_5">Figure 6</ref> shows the trend persistence parameters β of each topic at each time. The persistence parameters of the topic "Football" are lower than those of "Birth- day" because it is strongly affected by trends in the real world. In fact, the top words in "Football" change more dynamically than those of "Birth- day." For example, in the "Football" topic, though 'Arsenal' is usually popular, 'Madrid' becomes more popular on October 24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We first proposed an improved model based on Twitter-LDA, which estimates the rate be- tween background and topic words for each user. We demonstrated that the improved model could model tweets more efficiently than LDA and Twitter-LDA. Next we proposed a novel proba- bilistic topic model for Twitter, called Twitter- TTM, which can capture the dynamics of user in- terests and topic trends and is capable of online inference. We evaluated Twitter-TTM using an ac- tual Twitter data set and demonstrated that it could model more accurately tweets than conventional methods.</p><p>The proposed method currently needs to prede- termine the number of topics each time, and it is fixed. In future work, we plan to extend the pro- posed method to capture the birth and death of topics along the timeline with a variable number of topics, such as the model proposed by <ref type="bibr" target="#b0">Ahmed (Ahmed and Xing, 2010)</ref>. We also plan to ap- ply the proposed method to content recommenda- tions and trend analysis in Twitter to investigate this method further.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of Twitter-LDA and Improved-model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of DTM and TTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures</head><label></label><figDesc>Figures 3 and 4 show the generative process and a graphical representation of Twitter-TTM, respectively. Twitter-TTM can capture the dynamics of user interests and topic trends in Twitter considering the features of tweets online. Moreover, Twitter-TTM can be extended to capture long-term dependences, as described in Iwata et al. (2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graphical model of Twitter-TTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Perplexity for each time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Trend persistence parameters β of each topic at each time estimated by Twitter-TTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The original Twitter data set contains 14,305 users and 292,105 tweets collected on October 18, 2013. We then removed words that occurred less than 20 times and stop words. Retweets 1 were treated 1 Republishing a tweet written by another Twitter user.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Perplexity of each model in 10 runs 
Number of topic K LDA 
Twitter-LDA Improved-model 
50 
1586.7 (14.4) 2191.0 (28.4) 1555.3 (36.7) 
100 
1612.7 (11.9) 1933.9 (23.6) 1471.7 (22.3) 
150 
1635.3 (11.2) 1760.1 (15.7) 1372.3 (20.0) 
200 
1655.2 (13.0) 1635.4 (22.1) 1289.5 (13.3) 
250 
1672.7 (17.2) 1542.8 (12.5) 1231.1 (11.9) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example of tweets of users with high and low rate of background words 
High rate of background words Low rate of background words 
I hope today goes quickly 
Team Arsenal v will Ozil be 
I want to work in a cake 
Making Justin smile and laugh as he is working on music 
All need your support please 
Google nexus briefly appears in Google play store 

ity of online inference based on TTM (Iwata et 
al., 2009). TTM is a probabilistic consumer pur-
chase behavior model based on LDA for track-
ing the interests of each user and the trends in 
each topic. Other topic models considering the dy-
namics of topics include the dynamic topic model 
(DTM) (Blei and Lafferty, 2006) and topic over 
time (ToT) (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Two examples of topic evolution analyzed by Twitter-TTM 
Label 
Date Top words 
Birthday 10/18 birthday,happy,maria,hope,good,love,thanks,bday,lovely,enjoy 
10/19 happy,birthday,good,hope,thank,enjoy,love,bday,lovely,great 
10/20 birthday,happy,hope,good,love,lovely,great,enjoy,thank,beautiful 
10/21 birthday,happy,hope,good,beautiful,love,lovely,bday,great,thank 
10/22 birthday,happy,hope,good,beautiful,love,bless,thank,today,bday 
10/23 birthday,happy,thank,good,love,hope,beautiful,enjoy,channing,wish 
10/24 birthday,happy,thank,love,hope,good,beautiful,fresh,thanks,jamz 
Football 10/18 arsenal,ozil,game,team,cazorla,league,wenger,play,season,good 
10/19 goal,liverpool,gerrard,arsenal,ozil,league,newcastle,suarez,goals,team 
10/20 arsenal,ozil,goal,ramsey,norwich,goals,league,wilshere,mesut,premier 
10/21 arsenal,goal,goals,league,townsend,spurs,player,season,wenger,ozil 
10/22 arsenal,goal,wenger,ozil,league,arsene,goals,birthday,happy,team 
10/23 arsenal,dortmund,ozil,fans,wilshere,borussia,ramsey,lewandowski,giroud,league 
10/24 madrid,goals,ronaldo,cska,real,league,city,moscow,champions,yaya </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 26th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loulwah</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlotta</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine learning (ICML</title>
		<meeting>the 23rd International Conference on Machine learning (ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic summarization of events from social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Freddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitaram</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM)</title>
		<meeting>the International AAAI Conference on Weblogs and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding bursty topics from microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="536" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empirical study of topic modeling in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Social Media Analytics (SOMA)</title>
		<meeting>the First Workshop on Social Media Analytics (SOMA)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic tracking model for analyzing consumer purchase behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conferences on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1427" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On-line trend analysis with topic models:# twitter trends detection topic model online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeyhan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23th International Conference on Computational Linguistics (COLING</title>
		<meeting>the 23th International Conference on Computational Linguistics (COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1519" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Estimating a Dirichlet distribution Technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A machine learning approach to Twitter user classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM)</title>
		<meeting>the International AAAI Conference on Weblogs and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Earthquake shakes Twitter users: realtime event detection by social sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic modeling: beyond bag-of-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 23rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topics over time: a non-Markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD</title>
		<meeting>the International Conference on Knowledge Discovery and Data Mining (KDD</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TM-LDA: efficient online modeling of the latent topic transitions in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Benz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Twitterrank: finding topic-sensitive influential twitterers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee</forename><surname>Peng Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the 3rd ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A biterm topic model for short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1445" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
