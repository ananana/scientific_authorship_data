<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Sentence Kernel from Word Embeddings for Short Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Kim</surname></persName>
							<email>john.jonghoon.kim@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Sentence Kernel from Word Embeddings for Short Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a convolutional sentence kernel based on word embeddings. Our kernel overcomes the sparsity issue that arises when classifying short documents or in case of little training data. Experiments on six sentence datasets showed statistically significant higher accuracy over the standard linear kernel with n-gram features and other proposed models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the proliferation of text data available on- line, text categorization emerged as a prominent research topic. Traditionally, words (unigrams) and phrases (n-grams) have been considered as document features and subsequently fed to a clas- sifier such as an SVM <ref type="bibr">(Joachims, 1998)</ref>. In the SVM dual formulation that relies on kernels, i. e. similarity measures between documents, a linear kernel can be interpreted as the number of ex- act matching n-grams between two documents. Consequently, for short documents or when lit- tle training data is available, sparsity issues due to word synonymy arise, e. g., the sentences 'John likes hot beverages' and 'John loves warm drinks' have little overlap and therefore low linear kernel value (only 1) in the n-gram feature space, even with dependency tree representations and down- ward paths for n-grams as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We propose to relax the exact matching between words by capitalizing on distances in word embed- dings. We smooth the implicit delta word kernel, i. e. a Dirac similarity function between unigrams, behind the traditional linear document kernel to capture the similarity between words that are dif- ferent, yet semantically close. We then aggregate these word and phrase kernels into sentence and documents kernels through convolution resulting in higher kernel values between semantically re- lated sentences (e. g., close to 7 compared to 1 with bigram downward paths in <ref type="figure" target="#fig_0">Figure 1</ref>). Ex- periments on six standard datasets for sentiment analysis, subjectivity detection and topic spotting showed statistically significant higher accuracy for our proposed kernel over the bigram approaches. Our main goal is to demonstrate empirically that word distances from a given word vector space can easily be incorporated in the standard kernel be- tween documents for higher effectiveness and lit- tle additional cost in efficiency.</p><p>The rest of this paper is structured as follows. Section 2 reviews the related work. Section 3 gives the detailed formulation of our kernel. Section 4 describes the experimental settings and the results we obtained on several datasets. Finally, Section 5 concludes our paper and mentions future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Siolas and d <ref type="bibr">'Alché Buc (2000)</ref> pioneered the idea of semantic kernels for text categorization, cap- italizing on WordNet <ref type="bibr" target="#b5">(Miller, 1995)</ref> to propose continuous word kernels based on the inverse of the path lengths in the tree rather than the com- mon delta word kernel used so far, i. e. exact matching between unigrams. <ref type="bibr">Bloehdorn et al. (2006)</ref> extended it later to other tree-based simi- larity measures from WordNet while <ref type="bibr" target="#b2">Mavroeidis et al. (2005)</ref> exploited its hierarchical structure to define a Generalized Vector Space Model kernel.</p><p>In parallel, <ref type="bibr">Collins and Duffy (2001)</ref> devel- oped the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. <ref type="bibr">Culotta and Sorensen (2004)</ref> used them as Dependency Tree Kernels (DTK) to capture syntactic similar- ities while <ref type="bibr">Bloehdorn and Moschitti (2007)</ref> and <ref type="bibr">Croce et al. (2011)</ref> used them on parse trees with respectively Semantic Syntactic Tree Ker- nels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on Word- Net to capture semantic similarities but limiting to comparisons between words of the same POS tag.</p><p>Similarly, <ref type="bibr">Gärtner et al. (2003)</ref> developed graph kernels based on random walks and <ref type="bibr" target="#b11">Srivastava et al. (2013)</ref> used them on dependency trees with Vector Tree Kernels (VTK), adding node simi- larity based on word embeddings from SENNA <ref type="bibr">(Collobert et al., 2011</ref>) and reporting improve- ments over SSTK. The change from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representa- tions of words that are better suited for computing distances between words. Actually, in our exper- iments, word2vec by <ref type="bibr" target="#b3">Mikolov et al. (2013a)</ref> led to better results than with SENNA for both VTK and our kernels. Moreover, it possesses an addi- tional additive compositionality property obtained from the Skip-gram training setting ( <ref type="bibr" target="#b4">Mikolov et al., 2013b</ref>), e. g., the closest word to 'Germany' + 'capital' in the vector space is found to be 'Berlin'.</p><p>More recently, for short text similarity, <ref type="bibr" target="#b10">Song and Roth (2015)</ref> and <ref type="bibr">Kenter and de Rijke (2015)</ref> proposed additional semantic meta-features based on word embeddings to enhance classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Formulation</head><p>We denote the embedding of a word w by w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Kernel (WK)</head><p>We define a kernel between two words as a poly- nomial kernel over a cosine similarity in the word embedding space:</p><formula xml:id="formula_0">WK(w 1 , w 2 ) = 1 2 1 + w 1 , w 2 w 1 w 2 α (1)</formula><p>where α is a scaling factor. We also tried Gaus- sian, Laplacian and sigmoid kernels but they led to poorer results in our experiments. Note that a delta word kernel, i. e. the Dirac function 1 w 1 =w 2 , leads to a document kernel corresponding to the standard linear kernel over n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phrase Kernel (PhK)</head><p>Next we define a kernel between phrases consist- ing of several words. In our work, we considered two types of phrases: (1) co-occurrence phrases defined as contiguous sequences of words in the text; and (2) syntactic phrases defined as down- ward paths in the dependency tree representation, e. g., respectively 'hot beverages' and 'beverages hot' in <ref type="figure" target="#fig_0">Figure 1</ref>. With this dependency tree in- volved, we expect to have phrases that are syntac- tically more meaningful. Note that VTK consid- ers random walks in dependency trees instead of downward paths, i. e. potentially taking into ac- count same nodes multiple times for phrase length greater than two, phenomenon known as tottering.</p><p>Once we have phrases to compare, we may con- struct a kernel between them as the product of word kernels if they are of the same length l. That is, we define the Product Kernel (PK) as:</p><formula xml:id="formula_1">PK(p 1 , p 2 ) = l i=1 WK(w 1 i , w 2 i ) (2)</formula><p>where w j i is the i-th word in phrase p j of length l. Alternatively, in particular for phrases of different lengths, we may embed phrases into the embed- ding space by taking a composition operation on the constituent word embeddings. We considered two common forms of composition (Blacoe and Lapata, 2012): vector addition (+) and element- wise multiplication (). Then we define the Com- position Kernel (CK) between phrases as:</p><formula xml:id="formula_2">CK(p 1 , p 2 ) = WK(p 1 , p 2 ) (3)</formula><p>where p j , the embedding of the phrase p j , can be obtained either by addition (</p><formula xml:id="formula_3">p j = l i=1 w j i ) or by element-wise multiplication (p j = l i=1 w j i</formula><p>) of its word embeddings. For CK, we do not require the two phrases to be of the same length so the kernel has a desirable property of being able to compare 'Berlin' with 'capital of Germany' for instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Kernel (SK)</head><p>We can then formulate a sentence kernel in a sim- ilar way to <ref type="bibr" target="#b19">Zelenko et al. (2003)</ref>. It is defined through convolution as the sum of all local phrasal similarities, i. e. kernel values between phrases contained in the sentences:</p><formula xml:id="formula_4">SK(s 1 , s 2 ) = p 1 ∈φ(s 1 ), p 2 ∈φ(s 2 ) λ 1 λ 2 η PhK(p 1 , p 2 ) (4)</formula><p>where φ(s k ) is the set of either statistical or syn- tactic phrases (or set of random walks for VTK) in sentence s k , λ 1 is a decaying factor penaliz- ing longer phrases, = max{|p 1 |, |p 2 |} is the max- imum length of the two phrases, λ 2 is a distortion parameter controlling the length difference η be- tween the two phrases (η = ||p 1 | − |p 2 ||) and PhK is a phrase kernel, either PK, CK + or CK .</p><p>Since the composition methods we consider are associative, we employed a dynamic programming approach in a similar fashion to <ref type="bibr" target="#b19">Zelenko et al. (2003)</ref> to avoid duplicate computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Kernel</head><p>Finally, we sum sentence kernel values for all pairs of sentences between two documents to get the document kernel. Once we have obtained all doc- ument kernel values K ij between documents i and j, we may normalize them by K ii K jj as the length of input documents might not be uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated our kernel with co-occurrence and syntactic phrases on several standard text catego- rization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and <ref type="bibr" target="#b8">Lee, 2005</ref>) and a product review dataset (Amazon) of 2,000 multi- line documents for 4 different product groups ( <ref type="bibr">Blitzer et al., 2007</ref>) (we will report the average ef- fectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets clas- sified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) ( <ref type="bibr" target="#b7">Pang and Lee, 2004</ref>) and another of 11,640 sentences (MPQA) ( <ref type="bibr" target="#b15">Wiebe et al., 2005</ref>); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental settings</head><p>In all our experiments, we used the FANSE parser ( <ref type="bibr" target="#b12">Tratz and Hovy, 2011</ref>) to generate dependency trees and the pre-trained version of word2vec 1 , a 300 dimensional representation of 3 million En- glish words trained over a Google News dataset 1 https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5. While fine-tuning the em- beddings to a specific task or on a given dataset may improve the result for that particular task or dataset ( <ref type="bibr" target="#b1">Levy et al., 2015)</ref>, it makes the expected results less generalizable and the method harder to use as an off-the-shelf solution -re-training the neural network to obtain task-specific embeddings requires a certain amount of training data, admit- tedly unlabeled, but still not optimal under our sce- nario with short documents and little task-specific training data available. Moreover, tuning the hy- perparameters to maximize the classification accu- racy needs to be carried out on a validation set and therefore requires additional labeled data. Here, we are more interested in showing that distances in a given word vector space can enhance classi- fication in general. As for the dependency-based word embeddings proposed by <ref type="bibr" target="#b0">Levy and Goldberg (2014)</ref>, we do not think they are better suited for the problem we are tackling. As we will see in the results, we do benefit from the dependency tree structure in the phrase kernel but we still want the word kernel to be based on topical similarity rather than functional similarity.</p><p>To train and test the SVM classifier, we used the LibSVM library (Chang and Lin, 2011) and employed the one-vs-one strategy for multi-class tasks. To prevent overfitting, we tuned the pa- rameters using cross-validation on 80% of PL05 dataset (α = 5, λ 1 = 1 for PK since there is no need for distortion as the phrases are of the same length by definition, and λ 1 = λ 2 = 0.5 for CK) and used the same set of parameters on the remain- ing datasets. We performed normalization for our kernel and baselines only when it led to perfor- mance improvements on the training set (PL05, News, PL04 and MPQA).</p><p>We report accuracy on the remaining 20% for PL05, on the standard test split for Twitter (25%) and News (50%) and from 5-fold cross-validation for the other datasets (Amazon, PL04 and MPQA). We only report accuracy as the macro-average F1- scores led to similar conclusions (and except for Twitter and News, the class label distributions are balanced). Results for phrase lengths longer than two were omitted since they were marginally different at best. Statistical significance of im- provement over the bigram baseline with the same phrase definition was assessed using the micro sign test (p &lt; 0.01) (Yang and Liu, 1999). <ref type="table">Table 1</ref>: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best performance in the column. * indicates statistical significance at p &lt; 0.01 using micro sign test against the bigram baseline (delta word kernel) of the same column and with the same phrase definition.  <ref type="table">Table 1</ref> presents results from our convolutional sentence kernel and the baseline approaches. Note again that a delta word kernel leads to the typi- cal unigram and bigram baseline approaches (first three rows). The 3 rd row corresponds to DTK <ref type="bibr">(Culotta and Sorensen, 2004</ref>) and the 4 th one to VTK <ref type="bibr" target="#b11">(Srivastava et al., 2013)</ref> -the difference with our model on the 9 th row lies in the function φ(·) that enumerates all random walks in the dependency tree representation following <ref type="bibr">Gärtner et al. (2003)</ref> whereas we only consider the downward paths. Overall, we obtained better results than the n- gram baselines, DTK and VTK, especially with syntactic phrases. VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel. Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic lin- earity and ontological relation encoded in the em- bedding is not sufficient enough and treating them separately is more beneficial. However, we be- lieve CK has more room for improvement with the use of more accurate phrase embeddings such as the ones from <ref type="bibr">Le and Mikolov (2014)</ref>, <ref type="bibr" target="#b17">Yin and Schütze (2014)</ref> and <ref type="bibr" target="#b18">Yu and Dredze (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>There was little contribution to the accuracy from non-unigram features, indicating that large part of the performance improvement is credited to the word embedding resolving the sparsity issue.  This can be well observed with the following ex- periment on the number of training examples. <ref type="figure" target="#fig_2">Fig- ure 2</ref> shows the accuracy on the same test set (20% of the dataset) when the learning was done on 1% to 100% of the training set (80% of the dataset) for the bigram baseline and our bigram PK phrase kernel, both with dependency tree representation, on PL04. We see that our kernel starts to plateau earlier in the learning curve than the baseline and also reaches the maximum baseline accuracy with only about 1,500 training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational complexity</head><p>Solving the SVM in the primal for the baselines requires O(N nL) time where N is the number of training documents, n is the number of words in the document and L is the maximum phrase length considered. The computation of VTK re- duces down to power series computation of the adjacency matrix of the product graph, and since we require kernel values between all documents, it requires O(N 2 (n 2 d + n 4 L)) time where d is the dimension of the word embedding space. Our kernel is the sum of phrase kernels (PhK) starting from every pair of nodes between two sen- tences, for all phrase lengths (l) and distortions (λ 2 ) under consideration. By storing intermedi- ate values of composite vectors, a phrase kernel can be computed in O(d) time regardless of the phrase length, therefore the whole computation process has O(N 2 n 2 L 2 d) complexity. Although our kernel has the squared terms of the baseline's complexity, we are tackling the sparsity issue that arises with short text (small n) or when little train- ing data is available (small N ). Moreover, we were able to get better results with only bigrams (small L). Hence, the loss in efficiency is accept- able considering significant gains in effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel convolutional sentence kernel based on word embeddings that overcomes the sparsity issue, which arises when classifying short documents or when little training data is available. We described a general frame- work that can encompass the standard n-gram baseline approach as well as more relaxed ver- sions with smoother word and phrase kernels. It achieved significant improvements over the base- lines across all datasets when taking into account the additional information from the latent word similarity (word embeddings) and the syntactic structure (dependency tree).</p><p>Future work might involve designing new ker- nels for syntactic parse trees with appropriate sim- ilarity measures between non-terminal nodes as well as exploring recently proposed phrase em- beddings for more accurate phrase kernels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency tree representations of semantically related sentences yet with little overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Test accuracy vs. number of training examples for our kernel and the bigram baseline.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Mavroeidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases, ECML PKDD &apos;05</title>
		<meeting>the 9th European conference on Principles and Practice of Knowledge Discovery in Databases, ECML PKDD &apos;05<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at International Conference on Learning Representations, ICLR &apos;13</title>
		<meeting>Workshop at International Conference on Learning Representations, ICLR &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26, NIPS &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 2: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation</title>
		<meeting>the 7th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support Vector Machines Based on a Semantic Kernel for Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Siolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florence D&amp;apos;alché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="205" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised Sparse Vector Densification for Short Text Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-Short &apos;15</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-Short &apos;15</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1275" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Fast, Accurate, Non-projective, Semantically-enriched Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of Short Texts by Deploying Topical Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th</title>
		<meeting>the 34th</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">European Conference on Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="376" to="387" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Re-examination of Text Categorization Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;99</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;99</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Exploration of Embeddings for Generalized Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Student Research Workshop, ACLstudent &apos;14</title>
		<meeting>the ACL Student Research Workshop, ACLstudent &apos;14</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Composition Models for Phrase Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="227" to="242" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel Methods for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
