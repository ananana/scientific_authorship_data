<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localizing Moments in Video with Temporal Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<addrLine>3 INRIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<addrLine>3 INRIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<addrLine>3 INRIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<addrLine>3 INRIA</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localizing Moments in Video with Temporal Language</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1380" to="1390"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localiza-tion offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO-Template Language) which allows for controlled studies on temporal language , and a human language dataset which consists of temporal sentences annotated by humans (TEMPO-Human Language).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider the video and natural language query in <ref type="figure">Figure 1</ref> where we seek to localize the de- sired moment in the video specified by the query. Queries like "the girl bends down" require un- derstanding objects and actions, but do not re- quire reasoning about different video moments. In contrast, queries like "the little girl talks af- ter bending down" require reasoning about the temporal relationship between different actions ("talk" and "bend down"). Localizing natural lan- guage queries in video is an important challenge, recently studied in <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> and <ref type="bibr" target="#b3">Gao et al. (2017)</ref> with applications in areas such as video search and retrieval. We argue that to * Work done at Adobe during LAH's summer internship.</p><p>Query: The little girl talks after bending down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Talk</head><p>Bend Down Talk <ref type="figure">Figure 1</ref>: We consider localizing video moments which include temporal language. To properly localize "The little girl talks after bending down" localization models must understand how the action "talks" relates to the action "bend down."</p><p>properly localize queries with temporal language, models must understand and reason about intra- video context. Reasoning about intra-video context is difficult as we do not know a priori which moments should be involved in the contextual reasoning and dif- ferent queries may require reasoning about dif- ferent contextual moments. For example, in "the little girl talks after bending down", the relevant contextual moment "bending down" occurs just before the target moment "the little girl talks". This is in contrast to the query "the little girl talks before bending down" where the relevant contextual moment occurs just after. A limita- tion of current moment-localization models <ref type="bibr" target="#b4">(Hendricks et al., 2017;</ref><ref type="bibr" target="#b3">Gao et al., 2017</ref>) is they con- sider query-independent video context when lo- calizing moments. For example, when determin- ing whether a proposed temporal region matches a natural language query, <ref type="bibr" target="#b3">Gao et al. (2017)</ref> con- siders the proposed temporal region, as well as video regions just before and after the proposed region. Similarly, <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> consid- ers video context in the form of a global-context feature which represents the entire video. While both may implicitly include the appropriate con- textual moment in their context feature, they do not explicitly determine the relevant context for the query.</p><p>To address this difficulty, we propose Moment Localization with Latent Context (MLLC) which models video context as a latent variable. The la- tent variable enables the model to attend to dif- ferent video contexts conditioned on the specific query/video pair, offering flexibility in the location and length of the contextual moment and overcom- ing the limitation of query-independent contextual reasoning. We validate the importance of latent context by showing that our model performs well both on simple queries without temporal words and more complex queries requiring temporal rea- soning. Moreover, our formulation is generic and unifies approaches in <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> and <ref type="bibr" target="#b3">Gao et al. (2017)</ref>, allowing us to ablate model component choices, as well as which kind of video context is best for localizing moments described with temporal language.</p><p>Though datasets used for moment localization in video ( <ref type="bibr" target="#b4">Hendricks et al., 2017;</ref><ref type="bibr" target="#b21">Regneri et al., 2013;</ref><ref type="bibr" target="#b22">Sigurdsson et al., 2016</ref>) include temporal language, as we will show, there is not enough temporal language to effectively train and evalu- ate models. We seek to extensively study this as- pect, particularly with respect to temporal prepo- sitions <ref type="bibr" target="#b20">(Pratt-Hartmann, 2004</ref>). Thus, we col- lect the TEMPOral reasoning in video and lan- guage (TEMPO) dataset which builds off the re- cently collected DiDeMo dataset ( <ref type="bibr" target="#b4">Hendricks et al., 2017)</ref>. The dataset consists of two parts: a dataset with real videos and sentences created with a template model (TEMPO -Template Lan- guage (TL)), and a dataset with real videos and newly collected user-provided temporal annota- tions (TEMPO -Human Language (HL)). Consid- ering template sentences allows us to create a large dataset of sentences quickly for study of temporal language in a controlled setting. The human lan- guage data then allows us to see these trends trans- fer to more complex human-language queries. For data collection, we focus on the most common temporal referring words naturally occurring in language-and-video datasets.</p><p>Our contributions are twofold. (i) We are the first to study models for temporal language in video moment retrieval with natural language queries. To this end, we introduce TEMPO which includes examples of how humans use tempo- ral language to refer to video moments. (ii) We propose MLLC for moment localization which treats video context as a latent variable and uni- fies prior approaches for moment localization. Our model outperforms prior work on TEMPO-TL and TEMPO-HL as well as the original DiDeMo dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Localizing Video Segments with Natural Lan- guage. Prior work has considered aligning natu- ral language with video, e.g., instructional videos with transcribed text ( <ref type="bibr" target="#b11">Kiddon et al., 2015;</ref><ref type="bibr" target="#b7">Huang et al., 2017;</ref><ref type="bibr" target="#b14">Malmaud et al., 2014</ref><ref type="bibr" target="#b15">Malmaud et al., , 2015</ref>. Our work is most related to recent work in video mo- ment retrieval with natural language ( <ref type="bibr" target="#b3">Gao et al., 2017;</ref><ref type="bibr" target="#b4">Hendricks et al., 2017)</ref>. Both works take a natural language query and candidate video seg- ment as input, and output a score for how well the natural language phrase aligns with the video segment. <ref type="bibr" target="#b3">Gao et al. (2017)</ref> includes an additional loss to regress to start and end-points, whereas <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> simplifies the problem by choosing from a discrete set of video seg- ments. Importantly, to represent a proposed video segment, both models consider context features around a moment: <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> uses global context by averaging features over an en- tire input video, and <ref type="bibr" target="#b3">Gao et al. (2017)</ref> incorpo- rates features adjacent to the proposed video seg- ment. We argue that to do proper temporal rea- soning, pre-determined, query independent con- text features may not cover all possible temporal relations. Thus, we propose to model the context as a latent variable, allowing our method to learn which context moments to consider as a function of the video and importantly, the query.</p><p>Both <ref type="bibr" target="#b3">Gao et al. (2017)</ref> and <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> collect data to test their models; <ref type="bibr" target="#b3">Gao et al. (2017)</ref>   <ref type="bibr">(40,000 vs. 13,000)</ref>, and is focused on general videos which we believe is an interesting and useful scenario, rather than be- ing restricted to indoor activities.</p><p>Temporal Language. Prior work on temporal lan- guage processing has considered building explicit logical frameworks to process temporal prepo- sitions like "during" or "until" <ref type="bibr" target="#b20">(Pratt-Hartmann (2004)</ref>, <ref type="bibr" target="#b12">Konur (2008)</ref>). We do not derive a partic- ular temporal logic, but rather learn to understand temporal language in a data driven fashion. Fur- thermore, we specifically consider how to under- stand temporal words commonly used when refer- ring to video content. Other work has modeled dy- namics for words which represent a change of state (e.g., "pick up") <ref type="bibr" target="#b24">( Siskind (2001)</ref>, <ref type="bibr" target="#b28">Yu et al. (2015)</ref>) in limited environments. Though we limit the se- lection of temporal words in our study, the natu- ral language in our data is open-world describing diverse events and how they relate to each other in video. Interpretation of temporal expressions in text ("The game happened on the 19 th ") is a widely studied task ( <ref type="bibr" target="#b0">Angeli et al. (2012)</ref>, <ref type="bibr" target="#b29">Zhong et al. (2017)</ref>). Our work is distinctly different from this line of work as we specifically study temporal prepositions and how they refer to video.</p><p>Modeling Visual Relationships. A variety of pa- pers have considered modeling spatial relation- ships in natural images ( <ref type="bibr" target="#b1">Dai et al., 2017;</ref><ref type="bibr" target="#b6">Hu et al., 2017;</ref><ref type="bibr" target="#b18">Peyre et al., 2017;</ref><ref type="bibr" target="#b19">Plummer et al., 2017)</ref>. Our approach is analogous to this in the temporal domain; we hope to localize moments in videos. CLEVR, a synthetic visual question answering (VQA) dataset <ref type="bibr" target="#b10">(Johnson et al., 2016)</ref>, was cre- ated to allow researchers to systematically study the ability of models to perform complex reason- ing. Our dataset is partially motivated by the suc- cess of CLEVR to enable researchers to study rea- soning abilities of different models in a controlled setting. In contrast to CLEVR we consider a more diverse visual input in the form of real videos.</p><p>In the video domain, the TGIF-QA (Jang et al., 2017) and Mario-QA ( <ref type="bibr" target="#b16">Mun et al., 2016</ref>) datasets provide opportunities to study temporal reason- ing for the task of VQA. The TGIF-QA dataset considers three types of temporal questions: be- fore/after questions, repetition count, and deter- mining a repeating action. Each question is ac- companied by multiple choice answers. Videos we consider are much longer (25-30s as opposed to an average of 3.1s) which makes the use of temporal reasoning much more important. The MarioQA dataset is an additional VQA dataset de-</p><formula xml:id="formula_0">Score Visual Feature Embedding (f V ) Similarity (f s )</formula><p>Input Query: The girl talks before she bends down. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Moment Localization with Latent Context</head><p>Given a video v and natural-language query q de- scribing a moment in the video, our goal is to output the moment τ = τ (s) , τ (e) where τ (s) and τ (e) are temporal start and end points in the video, respectively. In the following, we formulate a generic, unified model which encompasses prior approaches ( <ref type="bibr" target="#b4">Hendricks et al., 2017;</ref><ref type="bibr" target="#b3">Gao et al., 2017)</ref>. This allows us to explore and evaluate trade offs for different model components and ex- tensions which then leads to higher performance. Unlike prior work, we consider a latent context variable which enables our model to better reason about temporal language.</p><p>Let the moment τ corresponding to the text query be the base moment and the set of other video moments T τ be possible context moments for τ . We define a scoring function between the video moment and natural-language query by maximizing over all possible context moments</p><formula xml:id="formula_1">τ ∈ T τ , s φ (v, q, τ ) = max τ ∈Tτ f S f V v, τ, τ , f L (q) ,</formula><p>(1) where f V and f L are functions computing features over the video and language query, f S is a simi- larity function, and φ are model parameters. This formulation is generic and trivially encompasses the MCN and TALL formulations by letting the set of possible context moments T τ be their re- spective single-context moment. <ref type="figure">Figure 2</ref> shows the generic structure of our model.</p><p>With this formulation, we seek to answer the following questions: (i) Which combination of model components performs best for the moment- retrieval task? Though our primary goal is localiz- ing moments with temporal language, we believe a good base moment retrieval model is important for localizing moments with temporal language. (ii) How best to incorporate context for moment re- trieval with temporal language? We first detail the different terms and outline different model design choices, where design choices marked with bold- italic font is ablated in Section 5. Components which are used in our final proposed Moment Lo- calization with Latent Context (MLLC) model and prior models are summarized in <ref type="table" target="#tab_5">Table 3</ref>.</p><formula xml:id="formula_2">Video feature f V . The video feature f V = (g (v, τ ) , g (v, τ ) , f T (τ, τ )</formula><p>) is a concatenation of visual features for the base g (v, τ ) and con- text g (v, τ ) moments and endpoint features f T (τ, τ ). To compute visual features g for a temporal region τ , per-frame features are aver- aged over the temporal region. Note that if the context moment consists of more than one con- tiguous temporal region, then the visual features are computed over each contiguous temporal re- gion and then concatenated (c.f., before/after con- text in TALL, explained below). There are many choices for visual features. TALL ( <ref type="bibr" target="#b3">Gao et al., 2017</ref>) compares average f c 7 features (extracted from <ref type="bibr" target="#b23">(Simonyan and Zisserman, 2014)</ref>) to features extracted with C3D ( <ref type="bibr" target="#b26">Tran et al., 2015</ref>) and LSTM features ( <ref type="bibr" target="#b2">Donahue et al., 2015)</ref>. Surprisingly, C3D features only outperform average f c 7 features by a small margin. We use the visual features used in the MCN model ( <ref type="bibr" target="#b4">Hendricks et al., 2017)</ref>, which are similar to the f c 7 features from ( <ref type="bibr" target="#b3">Gao et al., 2017)</ref>, but included motion features as well, com- puted from optical flow (extracted with ( ). We then pass the extracted visual features through a MLP. Note that we learn sep- arate embedding functions for RGB and optical flow inputs and combine scores from different in- put modalities using a late-fusion approach <ref type="bibr" target="#b4">(Hendricks et al., 2017)</ref>.</p><p>Endpoint feature f T . Modeling temporal con- text requires understanding how different tempo- ral segments relate in time. <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> suggest including temporal endpoint fea-</p><formula xml:id="formula_3">tures (TEF) f T = τ (s) , τ (e)</formula><p>for the base mo- ment which encode when the moment starts and ends to better localize sentences which include words like "first" and "last". Note that TALL ( <ref type="bibr" target="#b3">Gao et al., 2017</ref>) does not incorporate TEFs. In order to understand temporal relationships, it is impor- tant that models also include features which indi- cate when a context moment occurs. In addition to providing TEFs for base moments, we also ex- periment with concatenating TEFs for context mo-</p><formula xml:id="formula_4">ments (conTEF) f T = τ (s) , τ (e) , τ (s) , τ (e) .</formula><p>Language feature f L . Text queries are trans- formed into a fixed-length vector with an LSTM <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref> </p><formula xml:id="formula_5">S = |f V − f L | 2 .</formula><p>Second, we con- sider a fused-feature similarity (mult) where the Hadamard product f V f L between the two fea- tures are passed to a MLP. We also explore unit normalizing features before the Hadamard prod- uct (normalized mult). Finally, we consider the similarity (TALL similarity) which consists of</p><formula xml:id="formula_6">the concatenation (f V , f L , f V f L , f V + f L )</formula><p>and then passed to a MLP.</p><p>Context moments T τ . We consider three sets of context moments. First, we consider the entire video as the context moment (global) following <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref>. Second, we consider us-ing the moments just before and after the base mo- ment (before/after). Finally, we consider using the set of all possible moments (latent context) which offers greatest flexibility in contextual reasoning.</p><p>Training loss. We consider two training losses. The first loss is the MCN ranking loss which en- courages positive moment/query pairs to have a smaller distance in a shared embedding space than negative moment/query pairs. To sample nega- tive moment/sentence pairs, they consider nega- tive moments within a specific video (called intra- video negative moments) and negative moments in different videos (called inter-video negative mo- ments). This sampling strategy leads to a small improvement in performance (approximately one point on all metrics) when compared to just us- ing intra-video negative moments. We also con- sider the alignment loss used in TALL (TALL loss) which is the sum of two log-logistic functions over positive and negative training query/moment pairs (intra-video negatives are used).</p><p>Supervising context moments. For the tempo- ral sentences in our newly collected dataset (Sec- tion 4), we have access to the ground-truth con- text moment during training. Thus, we can con- trast a weakly supervised setting in which we op- timize over the unknown latent context moments during learning and inference to a strongly super- vised setting.</p><p>Implementation details. Candidate base and con- text moments coincide to the pre-segmented five- second segments used when annotating DiDeMo. Moments may consist of any contiguous set of five-second segments. For a 30-second video par- titioned into six five-second segments, there are 21 possible moments. All models were implemented in Caffe ( <ref type="bibr" target="#b9">Jia et al., 2014</ref>) and optimized with SGD. Models were trained for ∼ 90 epochs with an ini- tial learning rate of 0.05, which decreases every 30 epochs. Code is publicly released * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The TEMPO Dataset</head><p>We collect the TEMPOral reasoning in video and language (TEMPO) dataset based off the recently released DiDeMo dataset. Our dataset consists of two parts: TEMPO -Template Language (TL) and TEMPO -Human Language (HL). We create TEMPO -TL using language templates to aug- ment the original sentences in DiDeMo with tem- * https://people.eecs.berkeley.edu/ ˜ lisa_anne/tempo.html poral words. The template allows us to generate a large number of sentences with known ground truth base and context moments. However, tem- plate language lacks the complexity of human lan- guage, so we then collect an additional fully user- constructed dataset, TEMPO -HL, consisting of sentences that contain specific temporal words.</p><p>Temporal Words in Current Datasets. We first analyze temporal words which occur in cur- rent natural language moment retrieval datasets. We consider temporal adjectives, adverbs, and prepositions found both by closely analyzing moment-localization datasets and consulting lists containing words which belong to different parts of speech. In particular, we rely on the prepo- sition project ( <ref type="bibr" target="#b13">Litkowski and Hargraves, 2005</ref>) † to scrape relevant temporal words. <ref type="table">Table 2</ref> shows example temporal words and the number of times they occur in each dataset <ref type="bibr">(TACoS (Regneri et al., 2013</ref>), Charades ( <ref type="bibr" target="#b3">Gao et al., 2017)</ref>, <ref type="bibr">DiDeMo (Hendricks et al., 2017)</ref>). Though all moment localization datasets use temporal words, they do not contain enough examples to reli- ably train and evaluate current models. Addition- ally, we observe that temporal words which are frequently used when describing video segments are different than those commonly used in text without video grounding. For example, in Pratt- Hartmann (2004), "during" is a common exam- ple, but we observe that "during" is infrequently used when describing video. Of temporal words, we focus on the four most common words, "be- fore", "after", "then", and "while" when creating our dataset.</p><p>TEMPO -Template Language. To construct sentences in TEMPO-TL, we find adjacent mo- ments in the DiDeMo dataset and fill in template sentences for "before", "after", and "then" tempo- ral words. For "before", we use two templates: "X before Y " and "Before Y , X", where X and Y are sentences from the original DiDeMo dataset. Likewise for "after", we consider the templates "X after Y " and "After Y , X". For "then" we only consider one template, "X then Y ."</p><p>TEMPO -Human Language. Though the template dataset is an interesting testbed for un- derstanding temporal language, it is difficult to replicate the interesting complexities in human language. For example, when writing long sen- † http://www.clres.com/prepositions. html  <ref type="table">Table 2</ref>: Word frequency of temporal words in natural language moment localization datasets.</p><p>The adult hands the little boy a stick.</p><p>The adult hands the little boy a stick then they begin to walk.</p><p>The boy and adult stop before adult bends over and hands child a short stick.</p><p>The girl looks at the camera and waves</p><p>The little girl turns and waves at the camera while on her skates.</p><p>After the girl waves at the camera she continues to skate. tences with temporal prepositions, humans fre- quently make use of language structure such as coreference to form more cohesive statements. To collect annotations, we follow the protocol in <ref type="bibr" target="#b4">Hendricks et al. (2017)</ref> and segment videos into 5-second temporal segments. After collecting de- scriptions, we ensure descriptions are localizable by asking other workers to localize each moment. To collect data for "before", "after", and "then", we ask annotators to describe a segment in rela- tion to a "reference" moment from the DiDeMo dataset. For example, if the DiDeMo dataset in- cludes a localized phrase like "the cat jumps", annotators write a sentence which refers to the segment "the cat jumps" using a specific tempo- ral word. We provide both the phrase ("the cat jumps") and the reference moment to annotators, and the annotators provide a sentence describing a new moment which references the reference mo- ment.</p><p>TEMPO-HL includes unique properties which are hard to replicate with template data. <ref type="figure" target="#fig_0">Figure 3</ref> depicts the base moment provided to workers, as well as descriptions from TEMPO-HL. <ref type="figure" target="#fig_0">In Fig- ure 3</ref>, the description "The adult hands the little boy the stick then they walk away" includes an example of visual coreference ("they"). We note that use of pronouns is much more prevalent in TEMPO-HL, with 28.1% of sentences in TEMPO- HL including pronouns ("he", "she", "it") in con- trast to 10.3% of sentences in the original DiDeMo dataset. Additionally, annotators will refer to the base moment with different language than orig- inally used in the base moment (e.g., "the girl waves at the camera" versus the base moment "the girl looks at the camera and waves") in order to make their sentences more fluent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Evaluation Method. We follow the evaluation protocol defined for the DiDeMo dataset <ref type="bibr" target="#b4">(Hendricks et al., 2017</ref>) over all possible combina- tions of the five-second video segments. We re- port rank at one (R@1), rank at five (R@5), and mean intersection over union (mIOU) using their aggregator over three out of the four human an- notators. We compare our models on TEMPO- TL, TEMPO-HL, and the DiDeMo dataset. When training our models, we combine the DiDeMo dataset with TEMPO-TL or TEMPO-HL. This en- ables our model to concurrently learn to localize the simpler DiDeMo sentences with more com- plex TEMPO sentences.</p><p>Baselines. We compare to the two recently pro- posed approaches for video moment localization: MCN (Hendricks et al., 2017) and TALL ( <ref type="bibr" target="#b3">Gao et al., 2017)</ref>. We adapt the implementation of TALL ( <ref type="bibr" target="#b3">Gao et al., 2017)</ref> to the DiDeMo dataset in three ways. First, we do not include the tem- poral localization loss required to regress to spe- cific start and end points as DiDeMo, and thus also TEMPO, is pre-segmented, so the model does not need to compute exact start and end points. Sec- ond, the original TALL model uses C3D features.</p><p>For a fair comparison we train both models with the same RGB and flow features extracted as was done for the original MCN model. Finally, the MCN model proposes temporal endpoint features (TEF) to indicate when a proposed moment occurs within a video. We train TALL with and without the TEF and show that TEF improves performance on the original DiDeMo dataset.</p><p>Ablations. To ablate our proposed latent context, we compare to other models which share the same MLLC base network. We consider the MLLC model with global context and before/after con- text. We also train a model with weakly supervised (WS) latent context and strongly supervised (SS) latent context. We also train models both with and without context TEF (conTEF).</p><p>The MLLC Base Model. We first ablate our MLLC base model <ref type="table" target="#tab_5">(Table 3)</ref>. We train our models on TEMPO-TL and DiDeMo and evaluate on the original DiDeMo dataset. All models are trained with global context. We find that the ranking loss is preferable on the DiDeMo dataset (com- pare lines 1 and 2) and that TALL-similarity per- forms better than the distance based similarity of the MCN model (compare lines 1 and 5). A simpler version of the TALL-similarity, in which the concatenated element wise multiplication, ele- ment wise sum, and concatenation is replaced by a single normalized elementwise multiplication, in- creases R@1 by almost one point and increases mIoU by over two points (compare lines 5-7). We call our best model the MLLC-Base model (line 7). Our MLLC-Base model performs better than previous models (MCN line 1 and TALL line 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Similarity  Results: TEMPO -TL. We first compare dif- ferent moment localization models on TEMPO - TL <ref type="table" target="#tab_7">(Table 4</ref>). In particular, our model performs well on "before" and "after" words. Additionally, our MLLC model with global context outperforms both the MCN model ( <ref type="bibr" target="#b4">Hendricks et al., 2017</ref>) and the TALL ( <ref type="bibr" target="#b3">Gao et al., 2017</ref>) model when consid- ering all sentence types, verifying the strength of our base MLLC model.</p><p>Comparing MLLC with global context and MLLC with before/after context (compare row 4 and 5), we note that before/after context is impor- tant for localizing "before" and "after" moments. However, our model with strong supervision (row 9) outperforms the model trained with before and after context, suggesting that learning to reason about which context moment is correct (as op- posed to being explicitly provided with the con- text before and after the moment) is beneficial. We note that strong supervision (SS) outperforms weak supervision (WS) (compare rows 7 and 9) and that the context TEF is important for best per- formance (compare rows 8 and 9).</p><p>We note that though the MLLC-global model outperforms our full model for "then" on TEMPO- TL, our full model performs better on then for the TEMPO-HL <ref type="table" target="#tab_9">(Table 6</ref>). One possibility is that the "then" moments in TEMPO-TL do not re- quire context to properly localize the moment. Be- cause TEMPO-TL is constructed from DiDeMo sentences, constituent sentence parts are refer- ring. For example, given an example sentence from TEMPO-TL (e.g., "The cross is seen for the first time then window is first seen in room"), the model does not need to reason about the ordering of "cross seen for the first time" and "window is seen for the first time" because both moments only happen once in the video. In contrast, when con- sidering the sentence "The adult hands the little boy a stick then they begin to walk" (from <ref type="figure" target="#fig_0">Fig- ure 3)</ref>, "begin to walk" could refer to multiple video moments. Consequently, our model must reason about the temporal ordering of reference moments to properly localize the video moment.</p><p>On TEMPO -TL, sentences differ from origi- nal DiDeMo sentences solely because of the use of temporal words. Thus, we can do a controlled study of how well models understand temporal words. If a model has good temporal reasoning, then if it can localize a reference moment "the dog jumps" it should be easier for the model to localize the moment "the dog sits after the dog jumps". To test whether models are capable of this, we look at only sentences in TEMPO -TL where the model has correctly localized the cor-   <ref type="table">Table 5</ref>: Difference between performance on full dataset and set on which reference moments are local- ized properly for different methods on TEMPO-TL.</p><p>responding context moment in DiDeMo <ref type="table">(Table 5)</ref>. We report the difference in performance when con- sidering only sentences in which temporal context was properly localized and all sentences. On our model, performance on all three temporal word types increases when the context moment can be properly localized. When considering global con- text, performance on "before" and "after" actually decreases, suggesting global context does not un- derstand temporal reasoning well. Finally, even when the context is correctly localized, there is still ample room for improvement on all three sen- tence types motivating future work on temporal reasoning for moment retrieval.</p><p>Results: TEMPO -HL. <ref type="table" target="#tab_9">Table 6</ref> compares per- formance on TEMPO -HL. We compare our best- performing model from training on the TEMPO- TL (strongly supervised MLLC and conTEF) to prior work (MCN and TALL) and to MLLC with global and before/after context. Performance on TEMPO-HL is considerably lower than TEMPO- TL suggesting that TEMPO-HL is harder than TEMPO-TL.</p><p>On TEMPO -HL, we observe similar trends as on TEMPO-TL. When considering all sentence types, MLLC has the best performance across all metrics. In particular, our model has the strongest performance for all sentence types considering the mIoU metric. In addition to performing better on temporal words, our model also performs bet- ter on the original DiDeMo dataset. As was seen in TEMPO-TL, including before/after context per- forms better than our model trained with global context for both "before" and "after" words.</p><p>The final row of <ref type="table" target="#tab_9">Table 6</ref> shows an upper bound in which the ground truth context is used at test time instead of the latent context. We note that results improve for "before", "after", and "then", suggesting that learning to better localize context will improve results for these sentence types.</p><p>Localizing Context Fragments. TEMPO-HL sentences can be broken into two parts: a base- sentence fragment (which refers to the base mo- ment), and a context-sentence fragment (which refers to the context moment). For example, for the sentence "The girl holds the ball before throw- ing it,", "the girl holds the ball" is the base frag- ment and "throwing it" is the context fragment. A majority of the "before" and "after" sentences in TEMPO-HL are of the form "X before (or after) Y ", so we can determine a list of sentence frag- ments by splitting sentences based on the tempo- ral word. Given "before" and "after" sentences, we determine the ground truth context fragment by considering which reference moment was given to annotators. We can then measure how well mod- els localize context fragments.   <ref type="table" target="#tab_8">Table 7</ref>: Comparison of different methods to localize context fragments (e.g., the text "she bends down" in the sentence "the girl talks after she bends down"). We compare localizing fragments with the MLLC model to localizing fragments with the latent context considered when localizing the whole query.</p><p>After zooming in to the dog, the dog darts across the grass and into the woods</p><p>The girl with a hat takes a drink before the girl without a hat waves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Ground truth</p><p>Getting up while holding baby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>The mother sheep leaves the babies, then the babies follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Figure 4: Moment localization predictions on TEMPO -HL using our model. In addition to the localized query, we show the selected context segment (blue line) that our model considers when localizing the query. and reporting the context used by MLLC when in- putting the entire query into our model. We find that our model reliably selects the correct context fragments, most likely because it can properly ex- ploit temporal understanding of how the context fragment relates to the base fragment. Visualizing Context. In addition to a localized query, we can also visualize which context mo- ment the temporal query refers to. <ref type="figure">Figure 4</ref> shows predicted moments and their corresponding con- text moments. For the query "The girl with a hat takes a drink before the girl without a hat waves", the little girl in the hat drinks twice, but our model correctly localizes the time she drinks before the other girl waves. Likewise, for the moment "Af- ter zooming in to the dog, the dog darts across the grass and into the woods", the dog darts towards the woods twice (at the beginning of the video and at the end). Our model properly localizes the mo- ment when the dog runs towards the forest the sec- ond time as well as the context fragment "zooming in on dog" when localizing the moment.</p><p>Discussion. We show promising results on both TEMPO-TL and TEMPO-HL, but there is po- tential improvement for building better frame- works for understanding temporal language. In <ref type="table" target="#tab_9">Table 6</ref>, strongly supervising context at test time improves overall results, suggesting that models which can better localize context text will outper- form our current model. Though TEMPO and DiDeMo have over 60,000 sentences combined, visual content is quite diverse. Integrating out- side data sources (e.g., image retrieval and cap- tioning) could possibly improve results on mo- ment localization, both with and without temporal language queries. Additionally, in <ref type="table">Table 5</ref>, even when the MLLC model can properly localize con- text, it does not always properly localize temporal sentences indicating that improved temporal rea- soning can also improve our results. We believe our dataset, analysis, and method are an important step towards better moment retrieval models that effectively reason about temporal language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example sentences in TEMPO-HL. The top sentence corresponds to the reference moment (shown in green). The bottom sentences are newly collected sentences which use temporal language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>To select our base network, we consider 
different variants on the two previously proposed mo-
ment retrieval methods, TALL (Gao et al., 2017) and 
MCN (Hendricks et al., 2017). Results reported on val. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of different model performance for different temporal words on TEMPO -TL on our test 
set. We report scores for the three temporal words in TEMPO -TL as well as on the original DiDeMo dataset. 
We find that our model performs best when considering all sentence types. B/A indicated before/after context, WS 
indicates weak context supervision, and SS indicates strong context supervision. 

Before 
After 
Then 
Context 
R@1 mIoU R@1 mIoU R@1 mIoU 

Global 
-1.07 -2.72 -7.59 -6.75 43.30 31.57 
Before/After 2.77 
2.03 11.47 12.08 42.92 29.09 
Latent 
7.78 37.55 
8.58 10.39 50.09 33.64 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 compares two approaches to localizing context fragments: inputting just the context fragment into MLLC TEMPO -Human Language (HL)R@1 mIoU R@1 mIoU R@1 mIoU R@1 mIoU R@1 R@5 mIoU</head><label>7</label><figDesc></figDesc><table>DiDeMo 
Before 
After 
Then 
While 
Average 
R@1 mIoU Frequeny Prior 
19.43 25.44 29.31 51.92 
0.00 
0.00 
0.00 
7.84 
4.74 12.27 10.69 37.56 19.50 
MCN 
26.07 39.92 26.79 51.40 14.93 34.28 18.55 47.92 10.70 35.47 
19.4 70.88 41.80 
TALL + TEF 
21.79 33.55 25.91 49.26 14.43 32.62 
2.52 31.13 
8.1 28.14 14.55 60.69 34.94 
MLLC -Global 
27.01 41.72 27.42 52.22 14.10 34.33 18.40 49.17 10.86 35.36 19.56 71.23 42.56 
MLLC -B/A 
26.47 40.39 31.95 55.89 14.93 34.78 17.36 47.52 11.32 35.52 20.40 70.97 42.82 
MLLC (Ours) 
27.38 42.45 32.33 56.91 14.43 37.33 19.58 50.39 10.39 35.95 20.82 71.68 44.57 

MLLC 
(Ours) 
Context Sup. Test 

27.39 42.25 52.58 80.37 36.48 75.79 36.05 70.51 10.39 35.87 32.58 79.86 60.96 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 : Comparison of different model performance on TEMPO -HL on the test set. "MLLC -Global" indicates our model with global context and "MLLC -B/A" indicated MLLC with before/after context.</head><label>6</label><figDesc></figDesc><table>Before 
After 
R@1 mIoU 
R@1 mIoU 

Context Fragment 25.16 32.94 23.05 27.64 
Full Sentence 
27.55 35.70 32.67 40.39 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Anna Rohrbach for helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parsing time: Learning to interpret time expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="446" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03114</idno>
		<title level="m">Detecting visual relationships with deep relational networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual-linguistic reference resolution in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De-An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tgif-qa: Toward spatiotemporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04497</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06890</idno>
		<title level="m">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mise en place: Unsupervised interpretation of instructional recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Ponnuraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2015 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An interval logic for natural language semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savas</forename><surname>Konur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Modal Logic</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="177" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The preposition project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications</title>
		<meeting>the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cooking with semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Semantic Parsing</title>
		<meeting>the ACL 2014 Workshop on Semantic Parsing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? Interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01669</idno>
		<title level="m">Marioqa: Answering questions by watching gameplay videos</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weakly-supervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09472</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Bryan A Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal prepositions and their logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Pratt-Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 11th International Symposium on</title>
		<meeting>11th International Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7" to="8" />
		</imprint>
	</monogr>
	<note>Temporal Representation and Reasoning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Grounding action descriptions in videos</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="31" to="90" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A compositional framework for grounding language inference, generation, and acquisition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="601" to="713" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Time expression analysis and recognition using syntactic token types and general heuristic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
