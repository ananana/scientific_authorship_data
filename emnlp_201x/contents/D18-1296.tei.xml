<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Session-level Language Modeling for Conversational Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2764</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft, Sunnyvale</orgName>
								<address>
									<settlement>Microsoft, Bellevue</settlement>
									<region>WA, CA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft, Sunnyvale</orgName>
								<address>
									<settlement>Microsoft, Bellevue</settlement>
									<region>WA, CA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft, Sunnyvale</orgName>
								<address>
									<settlement>Microsoft, Bellevue</settlement>
									<region>WA, CA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
							<email>anstolck@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft, Sunnyvale</orgName>
								<address>
									<settlement>Microsoft, Bellevue</settlement>
									<region>WA, CA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Session-level Language Modeling for Conversational Speech</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2764" to="2768"/>
							<date type="published">October 31-November 4, 2018. 2018. 2764</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adja-cency pairs, lexical entrainment, and topical coherence. The model consists of a long-short-term memory (LSTM) recurrent network that reads the entire word-level history of a conversation , as well as information about turn taking and speaker overlap, in order to predict each next word. The model is applied in a rescor-ing framework, where the word history prior to the current utterance is approximated with preliminary recognition results. In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past decade the state of the art in lan- guage modeling has shifted from N-gram models to feed-forward networks ( <ref type="bibr" target="#b1">Bengio et al., 2006</ref>), and then to recurrent neural networks (RNNs) that read a list of words sequentially and predict the next word at each position. Starting with stan- dard recurrent networks ( <ref type="bibr" target="#b9">Mikolov et al., 2010</ref>) the sequential modeling approach was later improved using the long-short-term memory (LSTM) archi- tecture of <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>) for further gains <ref type="bibr" target="#b18">(Sundermeyer et al., 2012;</ref><ref type="bibr" target="#b7">Medennikov et al., 2016;</ref><ref type="bibr" target="#b20">Xiong et al., 2017)</ref>. RNN mod- els give two fundamental advantages over the old N-gram framework. First, the continuous-space embedding of word identities allows word simi- larities to be exploited for generalization <ref type="bibr" target="#b1">(Bengio et al., 2006;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013)</ref>. Second, the recurrent architecture allows, in principle at least, an unlimited history to condition the prediction of next words.</p><p>The potential advantage of unlimited history, however, is not commonly used to its full benefit, since the language model (LM) is typically "re- set" at the start of each utterance in current state- of-the-art recognition systems ( <ref type="bibr" target="#b13">Saon et al., 2017;</ref><ref type="bibr" target="#b19">Xiong et al., 2018)</ref>. This presumes that each ut- terance is independent of the others, and clearly violates what we know about how language and conversation works, as discussed in the next sec- tion. Consequently, there have been many pro- posals to inject information from a longer context into standard LM architectures, going back to N- gram models <ref type="bibr" target="#b0">(Bellegarda, 2004)</ref>, or to generalize N-grams LMs to operate across utterance bound- aries and speakers ( <ref type="bibr" target="#b5">Ji and Bilmes, 2004</ref>). Based on the RNN framework, <ref type="bibr" target="#b11">(Mikolov and Zweig, 2012)</ref> proposed augmenting network inputs with a more slowly varying context vector that would encode longer-range properties of the history, such as a latent semantic indexing vector. The problem with these approaches is that the modeler has to make design decisions about how to encapsulate contex- tual information as network inputs. Therefore, our approach here is to simply provide the entire con- versation history as input to a standard LSTM-LM, and let the network learn the information that is relevant to next-word prediction.</p><p>We start by discussing linguistic phenomena that could potentially help in conversational LM (Section 2), followed by a description of the LSTM model we propose to capture them (Sec- tion 3). Section 4 describes the data and recogni- tion system we used to test our models, with re- sults reported in Section 5. We end with conclu- sions and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conversation-level Phenomena</head><p>Here we review a few of the conversation-level phenomena that could be used for predicting words from longer context. Perhaps the most widely studied effect is topical coherence, or the tendency of words that are semantically related to one or more underlying topics to appear together in the conversation. Consequently, topic-related words are bound to re-occur across utterances, or certain related words appear to trigger one another (such as "children" and "school"). This should be especially true for conversations in the Switch- board (and Fischer) corpora, which were collected by pairing up strangers to talk about a mutually agreeable topic.</p><p>Another phenomenon that could lead to words reoccurring is lexical entrainment <ref type="bibr" target="#b2">(Brennan and Clark, 1996)</ref>, or the tendency of conversants to adopt the same words and phrases. Entrainment can also apply to speaking style, so the use of com- mon discourse particles, syntactic patterns (like question tags), or even disfluencies could be trig- gered across speakers.</p><p>Other phenomena operate more locally, but across speaker turn boundaries. Linguistic conver- sation analysis has long noted that utterance types come in adjacency pairs <ref type="bibr" target="#b14">(Schegloff, 1968)</ref>, with preferences for certain pairs over others (like a statement is preferentially followed by agreement rather than disagreement). Therefore, words in an utterance should be more predicable based on the previous utterance. In the past, this has been mod- eled by conditioning utterance words on an under- lying dialog act label, which in turn is conditioned on adjacent dialog act labels via a dialog act gram- mar ( <ref type="bibr" target="#b17">Stolcke et al., 2000)</ref>.</p><p>A good part of conversational behavior has to do with how turn-taking is negotiated ( <ref type="bibr" target="#b12">Sacks et al., 1974)</ref>. Speakers use special discourse devices, such as backchannel words and pause fillers, to signal when they want to take the floor, or to signal that the other party should keep the floor. Conver- sants also anticipate the ends of turns and jump in before the other speaker is completely done, mak- ing for very efficient use of time. As a result of all of these mechanisms, a good portion of con- versations consists of overlapping (simultaneous) speaking. It was shown ( <ref type="bibr" target="#b16">Shriberg et al., 2001</ref>) that such overlap locations can be partly predicted by word-based language models. This suggests re- versing the modeling and using overlap (the tim- ing of utterances) to help predict the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>Our baseline language model is a standard LSTM that models utterances independently from one an- other, i.e., the history at the onset of each utterance is the start-of-sentence token. In fact, we used two version of this basic LSTM-LM:</p><p>• Word inputs encoded with one-hot vectors, combined with a jointly trained embedding layer</p><p>• Words encoded by multiple-hot vectors cor- responding to the letter trigrams making up the words.</p><p>Both types of LSTM-LMs use three 1000- dimensional hidden layers with recurrence. The word embedding layer is also of size 1000, and the letter-trigram encoding has size 7190 (the number of unique trigrams in our vocabulary).</p><p>The main addition for session-level modeling is that the LSTM history consists of all the utterances preceding the current utterance, followed by all words in the current utterance preceding the word to be predicted. The preceding utterances are se- rialized in the order of their onset times, so that the flow of words within an utterance is not dis- rupted. The resulting total word history and next- word prediction is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Informa- tion about utterance boundaries is encoded using a boundary tag, similar to the start-of-sentence to- ken that is commonly used in LMs.</p><p>Several of the conversational phenomena de- scribed in Section 2 refer to turn-taking between speakers; to capture this in the model we augment the word input encoding with an extra bit that indi- cates whether a speaker change occurred. This bit is turned on only for the start-of-utterance token.</p><p>We also want to capture some information about utterance overlap, since, as described earlier, speech overlap interacts with word choice. Pos- sible events to model would be overlap (exceed- ings a time threshold) at the starts and ends of ut- terances, or maybe a continuous measure of such overlaps. As a first proof of concept we chose to encode only one type of overlap, i.e., when the ut- terance in question is completely overlapped tem- porally by the other speaker's turn. This is typi- cal of backchannel acknowledgments ("uh-huh") and short utterances that attempt to grab the floor ("um", "but"). Complete utterance overlap is also encoded by an additional input bit that is turned on for the start-of-utterance token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recognition system</head><p>We used a single bidirectional LSTM acoustic model in experiments reported here, trained on the commonly used conversational telephone speech corpora (Switchboard, Fisher, CallHome English), estimating frame-level posterior probabilities for 9000 context-dependent phone units. The sys- tem decodes speech utterances using a 4-gram language model, generating lattices. These are then expanded to 500-best lists, which in turn are rescored using the various LMs.</p><p>The recognition system and the N-gram LM used in decoding have a vocabulary of 165k words, but the LSTM-LMs are trained on only the 38k words occurring at least twice in the in- domain conversational training data. Words out- side of the LSTM-LM vocabulary are penalized in rescoring with a constant weight that is empiri- cally optimized on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head><p>Language model training uses the Switchboard- 1, BBN Switchboard-2, Fisher, and English Call- Home transcripts (about 23 million words in to- tal) as well as the UW conversational Web corpus ( <ref type="bibr" target="#b3">Bulyko et al., 2003</ref>) for pre-training (see below). The N-gram LM used for N-best generation also includes the LDC Hub4 (Broadcast News) corpus. The Switchboard-1 and Switchboard-2 portions of the NIST 2002 CTS test set were used for tun- ing and development. Evaluation is carried out on the NIST 2000 CTS test set, consisting of Switch- board (SWB) and CallHome (CH) subsets.</p><p>As an expedient, we refrained from reseg- menting utterances based on forced alignments of words, and instead use utterance boundaries as given in the available transcripts (corresponding to the audio segments used in acoustic training). Similarly, in testing, we use the presegmented ut- terances provided by NIST. No doubt there are in- consistencies in how the different corpora define utterance units, and a consistent, alignment-based resegmentation of all training and test data based on the durations nonspeech regions and/or lexical tagging might give improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model training</head><p>All LSTM-LMs are trained using the Microsoft Cognitive Toolkit, or CNTK ( <ref type="bibr" target="#b8">Microsoft Research, 2016</ref>) on a Linux-based multi- GPU server farm. Training is parallelized using CNTK's distributed stochastic gradient descent (SGD) with 1-bit gradient quantization ( <ref type="bibr" target="#b15">Seide et al., 2014</ref>). We use the CNTK "FsAdaGrad" learning algorithm, which is an implementation of Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2015)</ref>.</p><p>All LSTM-LMs are pretrained for one or two epochs on a large corpus of "conversational Web" data ( <ref type="bibr" target="#b3">Bulyko et al., 2003)</ref>, followed by normal training to convergence on the in-domain data. Each utterance in the Web data is treated as a sin- gle session for purposes of session-based LM, i.e., the extra bits for speaker change and overlap are never turned on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>When evaluating the session-based LMs on speech test data, the true utterance contexts are not known, and we must use hypothesized words for word histories preceding the current utterance. In our case, the histories were obtained using the out- put of our best recognition system, which uses a combination of acoustic models ( <ref type="bibr" target="#b19">Xiong et al., 2018)</ref>, but excluding the session-based LM. 1 Per- plexity was evaluated on reference transcripts, as is customary. <ref type="table" target="#tab_0">Table 1</ref> shows the effect of session-level model- ing and of optional model elements on perplexity, based on LSTMs using letter-trigram encoding. Baseline is the standard utterance-scope LSTM- LM. We see a large perplexity reduction of 17- 21% by conditioning on session history words, with smaller incremental reductions from adding speaker change and overlap information.</p><p>The last two table rows show that some of the perplexity gain over the baseline is negated by the use of errorful recognition output for the conver- sation history. It does not make much difference whether the recognized word history is generated by just the subsystem being rescored ("single sys- tem", with 6% word error on SWB) or the full recognition system using multiple acoustic mod- els ("full system", with about 5% word error rate on SWB and 10% on CH). Using recognition out- put as history, the perplexity degrades about 6% relative for SWB, and 11% on CH, relative to us- ing the true word histories. Even with the more errorful recognition on CH, the session-based LM still gives a perplexity reduction of 14% relative to the baseline. <ref type="table" target="#tab_1">Table 2</ref> presents recognition results, compar- ing baseline LSTM-LMs to the full session-based LSTM-LMs. Both the letter-trigram and one-word word encoding versions are reported. The differ- ent models may also be used jointly, using log- linear score combination in rescoring, shown in the third section of the table. We also tried iterat- ing the session LM rescoring, after the recognized word histories were updated from the first rescor- ing pass (shown as "2nd iteration" in the <ref type="table">table)</ref>.</p><p>Results show that the session-based LM yields between 1% and 4% relative word error reduction for the two word encodings, and test sets. When the two word encoding types are combined by log- linear combination of model scores, the gain from session-based modeling is preserved. Iterating the session LM rescoring to improve the word histo- ries did not give consistent gains.</p><p>Even though the session-based LSTM sub- sumes all the information used in the standard LSTM, there is an additional gain to be had from combining those two model types (last row in the table). Thus, the overall gain from adding the session-based models to the two baseline models is 3-5% relative word error reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We have proposed a simple generalization of utterance-level LSTM language models aimed at capturing conversational phenomena that operate across utterances and speakers, such as lexical en- trainment, adjacency pairs, speech overlap, and topical coherence. To capture non-local condition- ing information, the LSTM-LM is trained to read the entire sequence of utterances making up a con- versation, along with side information encoding speaker changes and overlap of utterances. This is found to reduce perplexity by about 25%, most of which is retained when errorful recognition out- put is used to represent the word history in previ- ous utterances. The session-based LM yields up to 5% relative reduction in word error when the utterance-and session-based LMs are combined.</p><p>It would be worthwhile to investigate which conversational phenomena are actually being ex- ploited by the session LSTM model. The ease with which additional information can be input to the LSTM-LM also suggests encoding other con- ditioning information, such a more details about utterance timing, as well as semantic features that capture topical coherence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Use of conversation-level context in sessionbased LM. The utterance numbering shows how overlapping utterances are serialized (according to onset times).</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,226.78,64.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Perplexities with session-based LSTM-LMs. The last two lines reflect use of errorful recognition output for preceding utterances.</head><label>1</label><figDesc></figDesc><table>Model inputs 
devset 
test 
test 
SWB 
SWB 
CH 
Utterance words, letter-3grams 
48.90 44.56 54.57 
+ session history words 
38.86 36.81 44.31 
+ speaker change 
37.25 35.33 42.23 
+ speaker overlap 
37.09 35.12 42.02 
Using recognized word histories 
single system 
39.55 37.45 46.49 
full system (Xiong et al., 2018) 39.41 37.29 45.99 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Recognition results with standard and session-based LSTM-LMs, measured by word error rates (WER).</head><label>2</label><figDesc></figDesc><table>Word encoding Model 
WER 
WER test 
devset SWB 
CH 
Letter 3gram 
LSTM-LM 
10.01 
6.88 
12.79 
Session LSTM-LM 
9.67 
6.81 
12.54 
Session LSTM-LM, 2nd iteration 
9.66 
6.77 
12.56 
One-hot 
LSTM-LM 
9.81 
6.89 
13.02 
Session LSTM-LM 
9.47 
6.81 
12.60 
Session LSTM-LM, 2nd iteration 
9.50 
6.83 
12.73 
Letter 3gram 
LSTM-LM 
9.66 
6.63 
12.77 
+ One-hot 
Session LSTM-LM 
9.28 
6.52 
12.34 
LSTM-LM + Session LSTM-LM 
9.22 
6.45 
12.11 

</table></figure>

			<note place="foot" n="1"> We also omitted the final confusion network rescoring stage described in (Xiong et al., 2018).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical language model adaptation: review and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Fuzziness and Soft Computing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conceptual pacts and lexical choice in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1482" to="1493" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bulyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL 2003, Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>HLTNAACL 2003, Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-speaker language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2004, Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>HLT-NAACL 2004, Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 3rd International Conference for Learning Representations</title>
		<meeting>3rd International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving English conversational telephone speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Prudnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zatvornitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
		</author>
		<ptr target="https://cntk.ai" />
		<title level="m">The Microsoft Cognition Toolkit (CNTK</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simplest semantics for the organization of turntaking in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sacks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Schegloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jefferson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="696" to="735" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<editor>Lynn-Li Lim, Bergul Roomi, and Phil Hall</editor>
		<meeting>Interspeech<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequencing in conversational openings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Schegloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Anthropologist</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1075" to="1095" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1058" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Observations on overlap: Findings and implications for automatic processing of multi-party conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Speech Communication and Technology</title>
		<meeting>the 7th European Conference on Speech Communication and Technology<address><addrLine>Aalborg, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1359" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Microsoft 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP<address><addrLine>Calgary, Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5934" to="5938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward human parity in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2410" to="2423" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An introduction to computational networks and the Computational Network Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno>MSR-TR-2014-112</idno>
		<ptr target="Https://github.com/Microsoft/CNTK" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
