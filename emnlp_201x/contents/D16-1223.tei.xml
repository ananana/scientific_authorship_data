<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
							<email>gakuto@jp.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Watson</orgName>
								<orgName type="institution" key="instit3">IBM Watson</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Watson</orgName>
								<orgName type="institution" key="instit3">IBM Watson</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<email>zhou@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Watson</orgName>
								<orgName type="institution" key="instit3">IBM Watson</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Watson</orgName>
								<orgName type="institution" key="instit3">IBM Watson</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2077" to="2083"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. Explicitly modeling output label dependencies on top of RNN/LSTM is a widely-studied and effective extension. We propose another extension to incorporate the global information spanning over the whole input sequence. The proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. With this method, we can predict the label sequence while taking the whole input sequence information into consideration. In the experiments of a slot filling task, which is an essential component of natural language understanding , with using the standard ATIS corpus , we achieved the state-of-the-art F 1-score of 95.66%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language understanding (NLU) is an essen- tial component of natural human computer interac- tion and typically consists of identifying the intent of the users (intent classification) and extracting the as- sociated semantic slots (slot filling) <ref type="bibr" target="#b3">(De Mori et al., 2008)</ref>. We focus on the latter semantic slot filling task in this paper.</p><p>Slot filling can be framed as a sequential label- ing problem in which the most probable semantic slot labels are estimated for each word of the given word sequence. Slot filling is a traditional task and tremendous efforts have been done, especially since the 1980s when the Defense Advanced Research Program Agency (DARPA) Airline Travel Informa- tion System (ATIS) projects started <ref type="bibr" target="#b19">(Price, 1990)</ref>. Following the success of deep learning <ref type="bibr" target="#b7">(Hinton et al., 2006;</ref><ref type="bibr" target="#b0">Bengio, 2009)</ref>, Recurrent Neural Net- work (RNN) <ref type="bibr" target="#b5">(Elman, 1990;</ref><ref type="bibr" target="#b9">Jordan, 1997</ref>) and one of its specific architectures, Long Short-Term Mem- ory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>, have been widely used since they can capture tem- poral dependencies ( <ref type="bibr" target="#b34">Yao et al., 2013;</ref><ref type="bibr" target="#b35">Yao et al., 2014a;</ref><ref type="bibr" target="#b16">Mesnil et al., 2015)</ref>. The RNN/LSTM-based slot filling has been extended to be combined with explicit modeling of label dependencies ( <ref type="bibr" target="#b36">Yao et al., 2014b</ref>; <ref type="bibr" target="#b12">Liu and Lane, 2015)</ref>.</p><p>In this paper, we extend the LSTM-based slot filling to consider sentence-level information. In the field of machine translation, an encoder-decoder LSTM has been gaining attention , where the encoder LSTM encodes the global information spanning over the whole input sentence in its last hidden state. Inspired by this idea, we pro- pose an encoder-labeler LSTM that leverages the en- coder LSTM for slot filling. First, we encode the in- put sentence into a fixed length vector by the encoder LSTM. Then, we predict the slot label sequence by the labeler LSTM whose hidden state is initialized with the encoded vector by the encoder LSTM. With this encoder-labeler LSTM, we can predict the la- bel sequence while taking the sentence-level infor- mation into consideration.</p><p>The main contributions of this paper are two- folds:</p><p>1. Proposed an encoder-labeler LSTM to leverage sentence-level information for slot filling.</p><p>2. Achieved the state-of-the-art F 1 -score of 95.66% in the slot filling task of the standard ATIS corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>We first revisit the LSTM for slot filling and enhance this to explicitly model label dependencies. Then we explain the proposed encoder-labeler LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTM for Slot Filling</head><p>Figure 1(a) shows a typical LSTM for slot filling and we call this as labeler LSTM(W) where words are fed to the LSTM ( <ref type="bibr" target="#b35">Yao et al., 2014a)</ref>. Slot filling is a sequential labeling task to map a sequence of T words x T 1 to a sequence of T slot labels y T 1 . Each word x t is represented with a V dimensional one-hot-vector where V is the vocabu- lary size and is transferred to d e dimensional con- tinuous space by the word embedding matrix E ∈ R de×V as Ex t . Instead of simply feeding Ex t into the LSTM, Context Window is a widely used tech- nique to jointly consider k preceding and succeeding words as Ex t+k t−k ∈ R de(2k+1) . The LSTM has the architecture based on <ref type="bibr" target="#b10">Jozefowicz et al. (2015)</ref> that does not have peephole connections and yields the hidden state sequence h T 1 . For each time step t, the posterior probabilities for each slot label are calcu- lated by the softmax layer over the hidden state h t . The word embedding matrix E, LSTM parameters, and softmax layer parameters are estimated to mini- mize the negative log likelihood over the correct la- bel sequences with Back-Propagation Through Time (BPTT) <ref type="bibr" target="#b31">(Williams and Peng, 1990</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Explicit Modeling of Label Dependency</head><p>A shortcoming of the labeler LSTM(W) is that it does not consider label dependencies. To explic- itly model label dependencies, we introduce a new architecture, labeler LSTM (W+L), as shown in <ref type="figure" target="#fig_0">Fig- ure 1(b)</ref>, where the output label of previous time step is fed to the hidden state of current time step, jointly with words, as <ref type="bibr" target="#b16">Mesnil et al. (2015)</ref> and <ref type="bibr" target="#b12">Liu and Lane (2015)</ref> tried with RNN. For model training, one-hot- vector of ground truth label of previous time step is fed to the hidden state of current time step and for evaluation, left-to-right beam search is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Encoder-labeler LSTM for Slot Filling</head><p>We propose two types of the encoder-labeler LSTM that uses the labeler LSTM(W) and the labeler LSTM(W+L). This encoder-labeler LSTM is motivated by the encoder-decoder LSTM that has been applied to ma- chine translation ), grapheme- to-phoneme conversion ( <ref type="bibr" target="#b33">Yao and Zweig, 2015</ref>), text summarization ( <ref type="bibr" target="#b17">Nallapati et al., 2016</ref>) and so on. The difference is that the proposed encoder-labeler LSTM accepts the same input sequence twice while the usual encoder-decoder LSTM accepts the in- put sequence once in the encoder. Note that the LSTMs for encoding and labeling are different in the encoder-labeler LSTM, but the same word embed- ding matrix is used both for the encoder and labeler since the same input sequence is fed twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Related Work on Considering</head><p>Sentence-level Information  don't explicitly encode the whole sentence, our proposed encoder-labeler LSTM explicitly encodes whole sentence and predicts slots conditioned on the encoded information. Another method to consider the sentence-level in- formation for slot filling is the attention-based ap- proach ( <ref type="bibr" target="#b22">Simonnet et al., 2015</ref>). The attention-based approach is novel in aligning two sequences of dif- ferent length. However, in the slot filling task where the input and output sequences have the same length and the input word and the output label has strong relations, the effect of introducing "soft" attention might become smaller. Instead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and com- pare our results with the published results while dis- cussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ATIS Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Experimental Setup</head><p>We used the ATIS corpus, which has been widely used as the benchmark for NLU <ref type="bibr" target="#b19">(Price, 1990;</ref><ref type="bibr" target="#b2">Dahl et al., 1994;</ref><ref type="bibr" target="#b30">Wang et al., 2006;</ref><ref type="bibr" target="#b26">Tur et al., 2010)</ref>. <ref type="figure">Figure 2</ref> shows an example sentence and its seman- tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label se- quences from input word sequences.</p><p>The performance was measured by the F 1 -score:</p><formula xml:id="formula_0">F 1 = 2×P recision×Recall P recision+Recall ,</formula><p>where precision is the ra- tio of the correct labels in the system's output and recall is the ratio of the correct labels in the ground truth of the evaluation data <ref type="bibr" target="#b27">(van Rijsbergen, 1979)</ref>.</p><p>The ATIS corpus contains the training data of 4,978 sentences and evaluation data of 893 sen- tences. The unique number of slot labels is 127 and the vocabulary size is 572. In the following exper- iments, we randomly selected 80% of the original training data to train the model and used the remain- ing 20% as the heldout data <ref type="bibr" target="#b16">(Mesnil et al., 2015)</ref>. We reported the F 1 -score on the evaluation data with hyper-parameters that achieved the best F 1 -score on the heldout data.</p><p>For training, we randomly initialized parame- ters in accordance with the normalized initializa- tion ( <ref type="bibr" target="#b6">Glorot and Bengio, 2010)</ref>. We used ADAM for learning rate control ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) and dropout for generalization with a dropout rate of 0.5 ( <ref type="bibr" target="#b24">Srivastava et al., 2014;</ref><ref type="bibr">Zaremba et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Improvement by Encoder-labeler LSTM</head><p>We conducted experiments to compare the labeler LSTM(W) (Section 2.1), the labeler LSTM(W+L) (Section 2.2), and the encoder-labeler LSTM (Sec- tion 2.3).</p><p>As for yet another baseline, we tried the encoder-decoder LSTM as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c) <ref type="bibr">1</ref> .</p><p>For all architectures, we set the initial learn- ing rate to 0.001 ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) and the dimension of word embeddings to d e = 30. We changed the number of hidden units in the LSTM, d h ∈ {100, 200, 300} 2 , and the size of the context window, k ∈ {0, 1, 2} 3 . We used backward encoding for the encoder-decoder LSTM and the encoder-labeler LSTM as suggested in . For the encoder-decoder LSTM, labeler LSTM(W+L), and encoder-labeler LSTM(W+L), we used the left-to-right beam search decoder ) with beam sizes of 1, 2, 4, and 8 for evaluation where the best F 1 -score was reported. During 100 training epochs, we re- ported the F 1 -score on the evaluation data with the epoch when the F 1 -score for the heldout data was maximized. <ref type="table">Table 1</ref> shows the results.</p><p>The proposed encoder-labeler LSTM(W) and encoder-labeler LSTM(W+L) both outperformed the labeler LSTM(W) and labeler LSTM(W+L), which confirms the novelty of considering sentence- level information with the encoder LSTM by our proposed method.</p><p>Contrary to expectations, F 1 -score by the encoder-labeler LSTM(W+L) was not improved from that by the encoder-labeler LSTM(W). A pos- sible reason for this is the propagation of label pre- diction errors. We compared the label prediction ac- curacy for the words after the first label prediction error in the evaluation sentences and confirmed that the accuracy deteriorated from 84.0% to 82.6% by using pthe label dependencies.</p><p>For the encoder-labeler LSTM(W) which was bet- ter than the encoder-labeler LSTM(W+L), we tried the deep architecture of 2 LSTM layers (Encoder- labeler deep LSTM(W)). We also trained the cor- responding labeler deep LSTM(W). As in <ref type="table">Table 1</ref>, we obtained improvement from 94.91% to 95.47% by the proposed encoder-labeler deep LSTM(W), which was statistically significant at the 90% level.</p><p>Lastly, F 1 -score by the encoder-decoder LSTM was worse than other methods as shown in the first row of <ref type="table">Table 1</ref>. Since the slot label is closely related with the input word, the encoder-decoder LSTM was not an appropriate approach for the slot filling task. </p><note type="other">-score (c) Encoder-decoder LSTM 80.11 (a) Labeler LSTM(W) 94.80 (d) Encoder-labeler LSTM(W) 95.29 (b) Labeler LSTM(W+L) 94.91 (e) Encoder-labeler LSTM(W+L) 95.19 Labeler Deep LSTM(W) 94.91 Encoder-labeler Deep LSTM(W)</note><p>95.47 <ref type="table">Table 1</ref>: Experimental results on ATIS slot filling task. Left- most column corresponds to <ref type="figure" target="#fig_0">Figure 1</ref>. Lines with bold fonts use proposed encoder-labeler LSTM.</p><p>[%] <ref type="table">Table 2</ref> summarizes the recently published results on the ATIS slot filling task and compares them with the results from the proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Comparison with Published Results</head><p>Recent research has been focusing on RNN and its extensions. Many studies have been also conducted to explic- itly model label dependencies. <ref type="bibr" target="#b32">Xu and Sarikaya (2013)</ref> proposed CNN-CRF that explicitly models the dependencies of the output from CNN. <ref type="bibr" target="#b16">Mesnil et al. (2015)</ref> used hybrid RNN that combined Elman- type and Jordan-type RNNs. <ref type="bibr" target="#b12">Liu and Lane (2015)</ref> used the output label for the previous word to model label dependencies (RNN-SOP). <ref type="bibr" target="#b28">Vu et al. (2016)</ref> recently proposed to use ranking loss function over bi-directional RNNs with achiev- ing 95.47% (R-biRNN) and reported 95.56% by en- semble (5×R-biRNN).</p><p>By comparing with these methods, the main dif- ference of our proposed encoder-labeler LSTM is the use of encoder LSTM to leverage sentence-level information <ref type="bibr">4</ref> . <ref type="bibr" target="#b34">Yao et al., 2013)</ref> 94.11 CNN-CRF ( <ref type="bibr" target="#b32">Xu and Sarikaya, 2013)</ref> 94.35 Bi-directional <ref type="bibr">RNN (Mesnil et al., 2015)</ref> 94.73 LSTM ( <ref type="bibr" target="#b35">Yao et al., 2014a)</ref> 94.85 RNN-SOP ( <ref type="bibr" target="#b12">Liu and Lane, 2015)</ref> 94.89 Hybrid RNN <ref type="bibr" target="#b16">(Mesnil et al., 2015)</ref> 95.06 Deep LSTM ( <ref type="bibr" target="#b35">Yao et al., 2014a)</ref> 95.08 RNN-EM (  95.25 R-biRNN ( <ref type="bibr" target="#b28">Vu et al., 2016)</ref> 95.47 5×R-biRNN ( <ref type="bibr" target="#b28">Vu et al., 2016)</ref> 95.56 Encoder-labeler LSTM(W) 95.40 Encoder-labeler Deep LSTM(W) 95.66 <ref type="table">Table 2</ref>: Comparison with published results on ATIS slot filling task. F1-scores by proposed method are improved from <ref type="table">Table 1</ref> due to sophisticated hyper-parameters.</p><formula xml:id="formula_1">F 1 -score RNN (</formula><p>[%]</p><p>For our encoder-labeler LSTM(W) and encoder- labeler deep LSTM(W), we further conducted hyper-parameter search with a random search strat- egy ( <ref type="bibr" target="#b1">Bergstra and Bengio, 2012)</ref>. We tuned the di- mension of word embeddings, d e ∈ {30, 50, 75}, number of hidden states in each layer, d h ∈ {100, 150, 200, 250, 300}, size of context window, k ∈ {0, 1, 2}, and initial learning rate sampled from uniform distribution in range [0.0001, 0.01]. To the best of our knowledge, the previously published best F 1 -score was 95.56% 5 ( <ref type="bibr" target="#b28">Vu et al., 2016)</ref>. Our encoder-labeler deep LSTM(W) achieved 95.66% F 1 -score, outperforming the previously published F 1 -score as shown in <ref type="table">Table 2</ref>.</p><p>Note some of the previous results used whole training data for model training while others used randomly selected 80% of data for model training and the remaining 20% for hyper-parameter tuning. Our results are based on the latter setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Large-scale Experiment</head><p>We prepared a large-scale data set by merging the MIT Restaurant Corpus and MIT Movie Cor- <ref type="bibr">5</ref> There are other published results that achieved better F1- scores by using other information on top of word features. <ref type="bibr" target="#b29">Vukotic et al. (2015)</ref> achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. <ref type="bibr" target="#b34">Yao et al. (2013)</ref> and <ref type="bibr" target="#b35">Yao et al. (2014a)</ref> used NE features in ad- dition to word features and obtained improvement with both the RNN and LSTM upto 96.60% F1-score. <ref type="bibr" target="#b16">Mesnil et al. (2015)</ref> also used NE features and reported F1-score of 96.29% with RNN and 96.46% with Recurrent CRF. pus ( <ref type="bibr" target="#b13">Liu et al., 2013a;</ref><ref type="bibr" target="#b14">Liu et al., 2013b;</ref><ref type="bibr">Spoken Laungage Systems Group, 2013</ref>) with the ATIS cor- pus. Since users of the NLU system may pro- vide queries without explicitly specifying their do- main, building one NLU model for multiple do- mains is necessary. The merged data set contains 30,229 training and 6,810 evaluation sentences. The unique number of slot labels is 191 and the vocab- ulary size is 16,049. With this merged data set, we compared the labeler LSTM(W) and the proposed encoder-labeler LSTM(W) according to the exper- imental procedure explained in Section 3.1.2. The labeler LSTM(W) achieved the F 1 -score of 72.80% and the encoder-labeler LSTM(W) improved it to 74.41%, which confirmed the effect of the proposed method in large and realistic data set 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed an encoder-labeler LSTM that can conduct slot filling conditioned on the encoded sentence-level information. We applied this method to the standard ATIS corpus and obtained the state- of-the-art F 1 -score in a slot filling task. We also tried to explicitly model label dependencies, but it was not beneficial in our experiments, which should be further investigated in our future work.</p><p>In this paper, we focused on the slot labeling in this paper. Previous papers reported that jointly training the models for slot filling and intent classi- fication boosted the accuracy of both tasks ( <ref type="bibr" target="#b32">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b21">Shi et al., 2015;</ref>. Leveraging our encoder-labeler LSTM approach in joint training should be worth trying.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 (</head><label>1</label><figDesc>d) shows the encoder- labeler LSTM(W). The encoder LSTM, to the left of the dotted line, reads through the input sentence backward. Its last hidden state contains the en- coded information of the input sentence. The la- beler LSTM(W), to the right of the dotted line, is the same with the labeler LSTM(W) explained in Section 2.1, except that its hidden state is initialized with the last hidden state of the encoder LSTM. The labeler LSTM(W) predicts the slot label conditioned on the encoded information by the encoder LSTM, which means that slot filling is conducted with tak- ing sentence-level information into consideration. Figure 1(e) shows the encoder-labeler LSTM(W+L), which uses the labeler LSTM(W+L) and predicts the slot label considering sentence-level information and label dependencies jointly. Model training is basically the same as with the baseline labeler LSTM(W), as shown in Section 2.1, except that the error in the labeler LSTM is propa- gated to the encoder LSTM with BPTT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Neural network architectures for slot filling. Input sentence is "I need a ticket to Seattle". "B-ToCity" is slot label for specific meaning and "O"is slot label without specific meaning. "&lt;B&gt;" is beginning symbol for slot sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Yao et al. (2013) used RNN and out- performed methods that did not use neural networks, such as SVM (Raymond and Riccardi, 2007) and CRF (Deng et al., 2012). Mesnil et al. (2015) tried bi-directional RNN, but reported degradation com- paring with their single-directional RNN (94.98%). Yao et al. (2014a) introduced LSTM and deep LSTM and obtained improvement over RNN. Peng and Yao (2015) proposed RNN-EM that used an ex- ternal memory architecture to improve the memory capability of RNN.</figDesc></figure>

			<note place="foot" n="1"> Length of the output label sequence is equal to that of the input word sequence in a slot filling task. Therefore, ending symbol for slot sequence is not necessary.</note>

			<note place="foot" n="2"> When using deep architecture later in this section, d h was tuned for each layer. 3 In our preliminary experiments with using the labeler LSTM(W), F1-scores deteriorated with k ≥ 3.</note>

			<note place="foot" n="4"> Since Simonnet et al. (2015) did not report the experimental results on ATIS, we could not experimentally compare our result with their attention-based approach. Theoretical comparison is available in Section 2.4.</note>

			<note place="foot" n="6"> The purpose of this experiment is to confirm the effect of the proposed method. The absolute F1-scores can not be compared with the numbers in Liu et al. (2013b) since the capitalization policy and the data size of the training data were different.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Dr. Yuta Tsuboi, Dr. Ryuki Tachibana, and Mr. Nobuyasu Itoh of IBM Re-search-Tokyo for the fruitful discussion and their comments on this and earlier versions of the paper. We thank Dr. Ramesh M. Nallapati and Dr. Cicero Nogueira dos Santos of IBM Watson for their valu-able suggestions. We thank the anonymous review-ers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and trends R ⃝ in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Expanding the scope of the ATIS task: The ATIS-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Deborah A Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bechet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mctear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="50" to="58" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Use of kernel deep convex networks and end-to-end learning for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkanitur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="210" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in psychology</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="471" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">ADAM: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent neural network structured output prediction for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions</title>
		<meeting>NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Asgard: A portable architecture for multilingual dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8386" to="8390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Query understanding enhanced by hierarchical parsing structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep contextual language understanding in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="120" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using recurrent neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Glar Gulçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00195</idno>
		<title level="m">Recurrent neural networks with external memory for language understanding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of spoken language systems: The ATIS domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patti</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Speech and Natural Language Workshop</title>
		<meeting>DARPA Speech and Natural Language Workshop</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="91" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative and discriminative algorithms for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1605" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextual spoken language understanding using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Yuh</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5271" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the use of attentionbased recurrent neural networks for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camelin</forename><surname>Nathalie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deléglise</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estève</forename><surname>Yannick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions</title>
		<meeting>NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The MIT Restaurant Corpus and The MIT Movie Corpus</title>
		<ptr target="https://groups.csail.mit.edu/sls/downloads/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Spoken Laungage Systems Group ; MIT Computer Science and Artificial Intelligence Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What is left to be understood in ATIS?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<pubPlace>Butterworth</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bi-directional recurrent neural network with ranking loss for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6060" to="6064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is it time to switch to word embedding and recurrent neural networks for spoken language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedran</forename><surname>Vukotic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="130" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining statistical and knowledgebased spoken language understanding in conditional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="882" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient gradient-based algorithm for on-line training of recurrent network trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular CRF for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence-tosequence neural net models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3330" to="3334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Yuh</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2524" to="2528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spoken language understanding using long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent conditional random field for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4077" to="4081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
