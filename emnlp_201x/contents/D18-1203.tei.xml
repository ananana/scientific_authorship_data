<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Yokoi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Statistical Mathematics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1763" to="1775"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI). As well as deriving PMI from mutual information , we derive this new measure from the Hilbert-Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels. Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNN-based PMI while outperforming PMI in accuracy. In addition, we also demonstrate that PH-SIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natu- ral language processing (NLP). For example, in col- location extraction <ref type="bibr">(Manning and Schütze, 1999</ref>), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., "New York") are found. In dialogue response selection ( <ref type="bibr">Lowe et al., 2015)</ref>, pairs comprising a context and its response sentence are collected from dialogue cor- pora and the goal is to rank the candidate responses for each given context sentence. In either case, a set of linguistic expression pairs D = {(x i , y i )} n i=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed.  <ref type="table">Table 1</ref>: The proposed co-occurrence norm, PHSIC, eliminates the trade-off between robustness to data sparsity and learning time, which PMI has (Section 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness Learning to Sparsity Time</head><p>Pointwise mutual information (PMI) <ref type="bibr" target="#b7">(Church and Hanks, 1989)</ref> is frequently used to model the co-occurrence strength of linguistic expression pairs. There are two typical types of PMI esti- mation (computation) method. One is a counting- based estimator using maximum likelihood esti- mation, sometimes with smoothing techniques, for example, PMI MLE (x, y; D) = log n · c(x, y) y c(x, y )</p><p>x c(x , y)</p><p>,</p><p>where c(x, y) denotes the frequency of the pair (x, y) in given data D. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extraction <ref type="bibr">1</ref> ; however, when data D is sparse, i.e., when x or y is a phrase or sentence, this approach is unre- alistic. The second method uses recurrent neural networks (RNNs). <ref type="bibr">Li et al. (2016)</ref> proposed to em-ploy PMI to suppress dull responses for utterance generation in dialogue systems 2 . They estimated P(y) and P(y|x) using RNN language models and estimated PMI as follows: PMI RNN (x, y; D) = log P RNN (y|x) P RNN (y) .</p><p>This way of estimating PMI is applicable to sparse language expressions; however, learning RNN lan- guage models is computationally costly.</p><p>To eliminate this trade-off between robustness to data sparsity and learning time, in this study we propose a new kernel-based co-occurrence mea- sure, which we call the pointwise Hilbert-Schmidt independence criterion (PHSIC) (see <ref type="table">Table 1</ref>). Our contributions are as follows:</p><p>• We formalize PHSIC, which is derived from HSIC ( <ref type="bibr" target="#b16">Gretton et al., 2005</ref>), a kernel-based de- pendence measure, in the same way that PMI is derived from mutual information (Section 3).</p><p>• We give an intuitive explanation why PHSIC is robust to data sparsity. PHSIC is a "smoothed variant of PMI", which allows various similarity metrics to be plugged in as kernels (Section 4).</p><p>• We propose fast estimators of PHSIC, which are reduced to a simple and fast matrix calculation regardless of whether we use linear or nonlinear kernels (Section 5).</p><p>• We empirically confirmed the effectiveness of PHSIC, i.e., its robustness to data sparsity and learning time, in two different types of experi- ment, a dialogue response selection task and a data selection task for machine translation (Sec- tion 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setting</head><p>Let X and Y denote random variables on X and Y, respectively. In this paper, we deal with the tasks of taking a set of linguistic expression pairs</p><formula xml:id="formula_2">D = {(x i , y i )} n i=1 ∼ i.i.d. P XY ,<label>(3)</label></formula><p>which is regarded as a set of i.i.d. samples drawn from a joint distribution P XY , and then measuring the "co-occurrence strength" for each given pair (x, y) ∈ X × Y. Such tasks include collocation extraction and dialogue response selection (Sec- tion 1). <ref type="bibr">2</ref> In dialogue response selection or generation, a simple con- ditional probability P(y|x), rather than PMI, ranks dull re- sponses (e.g., "I don't know.") higher ( <ref type="bibr">Li et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pointwise HSIC</head><p>In this section, we give the formal definition of PHSIC, a new kernel-based co-occurrence measure. We show a summary of this section in <ref type="table">Table 2</ref>. Intuitively, PHSIC is a "kernelized variant of PMI."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dependence Measure</head><p>As a preliminary step, we introduce the simple con- cept of dependence (see Dependence Measure in <ref type="table">Table 2</ref>). Recall that random variables X and Y are independent if and only if the joint probabil- ity density P XY and the product of the marginals P X P Y are equivalent. Therefore, we can measure the dependence between random variables X and Y via the difference between P XY and P X P Y .</p><p>Both the mutual information and the Hilbert- Schmidt independence criterion, to be described below, are such dependence measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MI and PMI</head><p>We briefly review the well-known mutual informa- tion and PMI (see MI &amp; PMI in <ref type="table">Table 2</ref>).</p><p>The mutual information (MI) 3 between two ran- dom variables X and Y is defined by Here, by definition of the KL divergence, MI can be represented in the form of the expectation over P XY , i.e., the summation over all possible pairs (x, y) ∈ X ×Y:</p><formula xml:id="formula_3">MI(X, Y ) := KL[P XY P X P Y ]<label>(4)</label></formula><formula xml:id="formula_4">MI(X, Y ) = E (x,y) log P XY (x, y) P X (x)P Y (y) .<label>(5)</label></formula><p>The shaded part in Equation <ref type="formula" target="#formula_4">(5)</ref> is actually the pointwise mutual information (PMI) <ref type="bibr" target="#b7">(Church and Hanks, 1989)</ref>:</p><formula xml:id="formula_5">PMI(x, y; X, Y ) := log P XY (x, y) P X (x)P Y (y) .<label>(6)</label></formula><p>Therefore, PMI(x, y) can be thought of as the con- tribution of (x, y) to MI(X, Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependence Measure</head><p>Co-occurrence Measure the dependence between X and Y the contribution of (x, y) (the difference between P XY and P X P Y )</p><p>to the dependence between X and Y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MI &amp; PMI</head><formula xml:id="formula_6">MI(X, Y ) = KL[P XY P X P Y ] = E (x,y) log P XY (x, y) P X (x)P Y (y) PMI(x, y; X, Y ) = log P XY (x, y) P X (x)P Y (y) HSIC &amp; PHSIC HSIC(X, Y ; k, ) = MMD 2 k,, [P XY , P X P Y ] = E (x,y) (φ(x) − m X ) C XY (ψ(y) − m Y ) = E (x,y) E (x ,y ) [ k(x, x ) (y, y )]</formula><p>PHSIC(x, y; X, Y, k, ) <ref type="table">Table 2</ref>: Relationship between the mutual information (MI), the pointwise mutual information (PMI), the Hilbert- Schmidt independence criterion (HSIC), and the pointwise HSIC (PHSIC). As well as defining PMI as the contri- bution to MI, we define PHSIC as the contribution to HSIC. In short, PHSIC is a "kernelized PMI" (Section 3).</p><formula xml:id="formula_7">= (φ(x) − m X ) C XY (ψ(y) − m Y ) = E (x ,y ) [ k(x, x ) (y, y )]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HSIC and PHSIC</head><p>As seen in the previous section, PMI can be derived from MI. Here, we consider replacing MI with the Hilbert-Schmidt independence criterion (HSIC). Then, in analogy with the relationship between PMI and MI, we derive PHSIC from HSIC (see HSIC &amp; PHSIC in <ref type="table">Table 2</ref>). Let k : X × X → R and : Y × Y → R denote positive definite kernels on X and Y, re- spectively (intuitively, they are similarity func- tions between linguistic expressions). The Hilbert- Schmidt independence criterion (HSIC) ( <ref type="bibr" target="#b16">Gretton et al., 2005</ref>), a kernel-based dependence measure, is defined by</p><formula xml:id="formula_8">HSIC(X, Y; k, ) := MMD 2 k,, [P XY , P X P Y ], (7)</formula><p>where MMD[·, ·] denotes the maximum mean dis- crepancy (MMD) ( , which measures the difference between random vari- ables on a kernel-induced feature space. Thus, HSIC(X, Y ; k, ) is the degree of dependence be- tween X and Y measured by the MMD between P XY and P X P Y , while MI is measured by the KL divergence (Equation (4)).</p><p>Analogous to MI in Equation (5), HSIC can be represented in the form of the expectation on P XY by a simple deformation:</p><formula xml:id="formula_9">HSIC(X, Y ; k, ) = E (x,y) (φ(x)−m X ) C XY (ψ(y)−m Y ) (8) = E (x,y) E (x ,y ) [ k(x, x ) (y, y )] ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">φ(x) := k(x, ·), ψ(y) := (y, ·),<label>(10)</label></formula><formula xml:id="formula_11">m X := E x [φ(x)], m Y := E y [ψ(y)],<label>(11)</label></formula><formula xml:id="formula_12">C XY := E (x,y) (φ(x) − m X )(ψ(y) − m Y ) ,<label>(12)</label></formula><formula xml:id="formula_13">k(x, x ) := k(x, x ) − E x [k(x, x )] − E x [k(x, x )] + E x,x [k(x, x )].<label>(13)</label></formula><p>At first glance, these equations are somewhat com- plicated; however, the estimators of PHSIC we actually use are reduced to a simple matrix calcula- tion in Section 5. Unlike MI in Equation (5), HSIC has two representations: Equation <ref type="formula">(8)</ref> is the repre- sentation in feature space and Equation (9) is the representation in data space. Similar to the relationship between MI and PMI (Section 3.2), we define the pointwise Hilbert- Schmidt independence criterion (PHSIC) by the shaded parts in Equations <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_9">(9)</ref>:</p><formula xml:id="formula_14">PHSIC(x, y; X, Y, k, ) := (φ(x) − m X ) C XY (ψ(y) − m Y ) (14) = E (x ,y ) [ k(x, x ) (y, y )] .<label>(15)</label></formula><p>Namely, PHSIC(x, y) is defined as the contribu- tion of (x, y) to HSIC(X, Y ). In summary, we define PHSIC such that "MI:PMI = HSIC:PHSIC" holds (see <ref type="table">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PHSIC as Smoothed PMI</head><p>This section gives an intuitive explanation for the first feature of PHSIC, i.e., the robustness to data sparsity, using <ref type="table">Table 3</ref>. In short, we show that PHSIC is a "smoothed variant of PMI."</p><p>First, the maximum likelihood estimator of PMI add scores deduct scores <ref type="table">Table 3</ref>: Comparison of estimators of PMI and PHSIC in terms of methods of matching the given (x, y) and the observed (x i , y i ) in D. PMI matches them in an exact manner, while PHSIC smooths the matching using kernels. Therefore, PHSIC is expected to be robust to data sparsity (Section 4).</p><formula xml:id="formula_15">PMI(x, y; D) = log n · i I[x = xi ∧ y = yi] i I[x = xi] i I[y = yi] ( x , y ) = = D = {. . . , ( xi , yi ), . . . } ( x , y ) ( x , y ) = = = = {. . . , ( xi , yi ), . . . ,( xi , yi ), . . . } PHSIC(x, y; D, k, ) = 1 n i k(x, xi) (y, yi) ( x , y ) ( x , y ) ≈ ≈ ≈ ≈ {. . . , ( xi , yi ), . . . ,( xi , yi ), . . . } ( x , y ) ( x , y ) ≈ ≈ ≈ ≈ {. . . , ( xi , yi ), . . . ,( xi , yi ), . . . }</formula><p>in Equation <ref type="formula" target="#formula_0">(1)</ref> can be rewritten as</p><formula xml:id="formula_16">PMI(x, y;D) = log n · i I[x = x i ∧ y = y i ] i I[x = x i ] i I[y = y i ]</formula><p>, <ref type="formula" target="#formula_0">(16)</ref> where I[condition] = 1 if the condition is true and</p><formula xml:id="formula_17">I[condition] = 0 otherwise. According to Equa- tion (16), PMI(x, y)</formula><p>is the amount computed by repeating the following operation (see the first row in <ref type="table">Table 3</ref>):</p><p>collate the given (x, y) and the observed (x i , y i ) in D in order, and add the scores if (x, y) and (x i , y i ) match exactly or deduct the scores if either the x side or the y side (but nor both) matches.</p><p>Moreover, an estimator of PHSIC in data space (Equation <ref type="formula" target="#formula_0">(15)</ref></p><formula xml:id="formula_18">) is PHSIC(x, y; D, k, ) = 1 n i k(x, x i ) (y, y i ) ,<label>(17)</label></formula><p>where k(·, ·) and (·, ·) are similarity functions cen- tered on the data 4 . According to Equation <ref type="formula" target="#formula_0">(17)</ref>, PHSIC(x, y) is the amount computed by repeat- ing the following operation (see the second row in <ref type="table">Table 3</ref>):</p><p>collate the given (x, y) and the observed (x i , y i ) in D in order, and add the scores if the similarities on the x and y sides are both higher (both k(x, x i ) &gt; 0 and (y, y i ) &gt; 0 hold) 5 or deduct the scores if the similarities on either the x or y sides are similar but those on the other side are not similar. <ref type="bibr">4</ref> To be exact,</p><formula xml:id="formula_19">k(x, x ) := k(x, x ) − 1 n n j=1 k(x, xj) − 1 n n i=1 k(xi, x ) + 1 n 2 n i=1 n j=1 k(xi, xj)</formula><p>, which is an estimator of the centered kernel k(x, x ) in Equation <ref type="formula" target="#formula_0">(13)</ref>. <ref type="bibr">5</ref> In addition, the scores are added if the similarity on the x side and that on the y side are both lower, that is, if k(x, xi) &lt; 0 and (y, yi) &lt; 0 hold.</p><p>As described above, when comparing the esti- mators of PMI and PHSIC from the viewpoint of "methods of matching the given (x, y) and the ob- served (x i , y i )," it is understood that PMI matches them in an exact manner, while PHSIC smooths the matching using kernels (similarity functions).</p><p>With this mechanism, even for completely un- known pairs, it is possible to estimate the co- occurrence strength by referring to observed pairs through the kernels. Therefore, PHSIC is expected to be robust to data sparsity and can be applied to phrases and sentences.</p><p>Available Kernels for PHSIC In NLP, a vari- ety of similarity functions (i.e., positive definite kernels) are available. We can freely utilize such resources, such as cosine similarity between sen- tence embeddings. For a more detailed discussion, see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Estimators of PHSIC</head><p>Recall that we have two types of empirical esti- mator of PMI, the maximum likelihood estimator (Equation <ref type="formula" target="#formula_0">(1)</ref>) and the RNN-based estimator (Equa- tion <ref type="formula" target="#formula_1">(2)</ref>). In this section, we describe how to rapidly estimate PHSIC from data. When using the linear kernel or cosine similarity (e.g., cosine similarity between sentence embeddings), PHSIC can be ef- ficiently estimated in feature space (Section 5.1). When using a nonlinear kernel such as the Gaussian kernel, PHSIC can also be estimated efficiently in data space via a simple matrix decomposition (Sec- tion 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Estimation Using Linear Kernel or Cosine</head><p>When using the linear kernel or cosine similarity, the estimator of PHSIC in feature space <ref type="formula" target="#formula_0">(14)</ref> is as follows:</p><formula xml:id="formula_20">PHSIC feature (x, y; D, k, ) = (φ(x)−φ(x)) C XY (ψ(y)−ψ(y)) , (18)</formula><p>where</p><formula xml:id="formula_21">φ(x) = x (k(x, x ) = x x ) x/x (k(x, x ) = cos(x, x )) ,<label>(19)</label></formula><formula xml:id="formula_22">φ(x) := 1 n n i=1 φ(x i ), ψ(y) := 1 n n i=1 ψ(y i ), (20) C XY := 1 n n i=1 φ(x i )ψ(y i ) − φ(x) ψ(y)</formula><p>. <ref type="formula" target="#formula_0">(21)</ref> Generally in kernel methods, a feature map φ(·) induced by a kernel k(·, ·) is unknown or high- dimensional and it is difficult to compute estimated values in feature space <ref type="bibr">6</ref> . However, when we use the linear kernel or cosine similarity, feature maps can be explicitly determined (Equation <ref type="formula" target="#formula_0">(19)</ref>).</p><p>Computational Cost When learning Equa- tion (18) with feature maps φ : X → R d and ψ : Y → R d , computing the vectors φ(x), ψ(y) ∈ R d and the matrix C XY ∈ R d×d takes O(nd 2 ) time and O(nd) space (linear in the size of the input, n). When estimating PHSIC(x, y), com- puting φ(x), ψ(y) ∈ R d and Equation (18) takes O(d 2 ) time (constant; does not depend on the size of the input, n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Estimation Using Nonlinear Kernels</head><p>When using a nonlinear kernel such as the Gaussian kernel, it is necessary to estimate PHSIC in data space. Using a simple matrix decomposition, this can be achieved with the same computational cost as the estimation in feature space. See Appendix B for a detailed derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we provide empirical evidence for the greater effectiveness of PHSIC than PMI, i.e., a very short learning time and robustness to data sparsity. Among the many potential applications of PHSIC, we choose two fundamental scenarios, (re-)ranking/classification and data selection.</p><p>• In the ranking/classification scenario (measuring the co-occurrence strength of new data pairs with reference to observed pairs), PHSIC is applied as a criterion for the dialogue response selection task (Section 6.2). • In the data selection/filtering scenario (ordering the entire set of observed data pairs according to the co-occurrence strength), PHSIC is also applied as a criterion for data selection in the context of machine translation (Section 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">PHSIC Settings</head><p>To take advantage of recent developments in rep- resentation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC.</p><p>Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence ( <ref type="bibr">Mikolov et al., 2013a</ref>):</p><formula xml:id="formula_23">x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·)</formula><p>, we used the pre-trained fastText model 7 , which is a high-accuracy and popular word embedding model ( <ref type="bibr" target="#b2">Bojanowski et al., 2017)</ref>; mod- els in 157 languages are publicly distributed ( <ref type="bibr" target="#b14">Grave et al., 2018</ref>). Second, we also used a DNN-based sentence encoder, called the universal sentence en- coder ( <ref type="bibr" target="#b5">Cer et al., 2018)</ref>, which utilizes the deep averaging network (DAN) <ref type="bibr" target="#b20">(Iyyer et al., 2015</ref>). The pre-trained model for English sentences we used is publicly available 8 .</p><p>Kernels As kernels between these vectors, we used cosine similarity (cos)</p><formula xml:id="formula_24">k(x, x ) = cos(x, x )<label>(23)</label></formula><p>and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)</p><formula xml:id="formula_25">k(x, x ) = exp − x − x 2 2 2σ 2 ,<label>(24)</label></formula><p>and similarly for (y, y ). The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposi- tion (for more detail, see Section B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ranking: Dialogue Response Selection</head><p>In the first experiment, we applied PHSIC as a ranking criterion of the task of dialogue response selection ( <ref type="bibr">Lowe et al., 2015)</ref>; in the task, pairs com- prising a context (previous utterance sequence) and its response are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence. The task entails sentence sequences (very sparse linguistic expressions); moreover, <ref type="bibr">Li et al. (2016)</ref> pointed out that (RNN-based) PMI has a positive impact on suppressing dull responses (e.g., "I don't know.") in dialogue systems. Therefore, PHSIC, another co-occurrence measure, is also expected to be effective for this. With this setting, where the validity of PMI is confirmed, we investigate whether PHSIC can replace RNN-based PMI in terms of both learning time and robustness to data sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>Dataset For the training data, we gathered ap- proximately 5 × 10 5 reply chains from Twitter, fol- lowing <ref type="bibr" target="#b33">Sordoni et al. (2015)</ref>  <ref type="bibr">9</ref> . In addition, we ran- domly selected {10 3 , 10 4 , 10 5 } reply chains from that dataset. Using these small subsets, we con- firmed the effect of the difference in the size of the training set (data sparseness) on the learning time and predictive performance.</p><p>For validation and test data, we used a small (approximately 2000 pairs each) but highly reliable dataset created by <ref type="bibr" target="#b33">Sordoni et al. (2015)</ref>  <ref type="bibr">10</ref> , which consists only of conversations given high scores by human annotators. Therefore, this set was not expected to include dull responses.</p><p>For each dataset, we converted each context- message-response triple into a context-response pair by concatenating the context and message fol- lowing <ref type="bibr">Li et al. (2016)</ref>. In addition, to convert the test set (positive examples) to ten-choice multiple- choice questions, we shuffled the combinations of context and response to generate pseudo-negative examples.</p><p>Evaluation Metrics We adopted the following evaluation metrics for the task: (i) ROC-AUC (the area under the receiver operating characteristic curve), (ii) MRR (the mean reciprocal rank), and (iii) Recall@{1,2}. <ref type="bibr">9</ref> We collected tweets after 2017 for our training set to avoid duplication with the test set, which contains tweets from the year 2012. 10 https://www.microsoft.com/en-us/download/ details.aspx?id=52375</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Config</head><p>Size of Training Set n 10 3 10 4 10 5 5×10 5</p><p>Dim.</p><p>Init.  for each model and each size of training set for the dialogue response task. Each row denotes a model; each column denotes the num- ber of training data n. The text appended to each base- line model denotes the number of dimension of hidden layers (Dim.) and the method of initialization the em- bedding layer (Init.). The text appended to each pro- posed model denotes the pre-trained models used to en- code sentences into vectors (Encoder) and the kernel between these vectors (Kernel). The best result (the shortest learning time) in each column is in bold.</p><p>Experimental Procedure We used the follow- ing procedure: (i) train the model with a set of context-response pairs D = {(x i , y i )} n i=1 ; (ii) for each context sentence x in the test data, rank the candidate responses {y j } 10 j=1 by the model; and (iii) report three evaluation metrics. ) were trained, and (3) PMI based on these language mod- els, RNN-PMI, was also used for experiments (see Equation <ref type="formula" target="#formula_1">(2)</ref>). We trained these models with all combinations of the following settings: (a) the num- ber of dimensions of the hidden layers being 300 or 1200 and (b) the initialization of the embed- ding layer being random (uniform on [−0.1, 0.1]) or fastText. For more detailed settings, see Ap- pendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Learning Time <ref type="table" target="#tab_2">Table 4</ref> shows the experimental results of the learning time <ref type="bibr">11</ref> . Regardless of the size of the training set n, the learning time for PHSIC is much shorter than that of the RNN-based method. For example, even when the size of the training set n is 5 × 10 5 , PHSIC is approximately 1400-4000 times faster than RNN-based PMI. This is because the estimators of PHSIC are reduced to a deterministic and efficient matrix calculation (Section 5), whereas neural network-based models involve the sequential optimization of parameters via gradient descent methods. <ref type="table">Table 5</ref> shows the experimental results of the predictive perfor- mance. When the size of the training data is small (n = 10 3 , 10 4 ), that is, when the data is extremely sparse, the predictive performance of PHSIC hardly deteriorates while that of PMI rapidly decays as the number of data decreases. This indicates that PH- SIC is more robust to data sparsity than RNN-based PMI owing to the effect of kernels. Moreover, PH- SIC with the simple cosine kernel outperforms the RNN-based model regardless of the number of data, while the learning time of PHSIC is thousands of times shorter than those of the baseline methods (Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to Data Sparsity</head><p>Additionally we report Spearman's rank correla- tion coefficient between models to verify whether PHSIC shows similar behavior to PMI. See Ap- pendix D for more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Data Selection for Machine Translation</head><p>The aim of our second experiment was to demon- strate that PHSIC is also beneficial as a criterion of data selection. To achieve this, we attempted to apply PHSIC to a parallel corpus filtering task that has been intensively discussed in recent (neu- ral) machine translation (MT, NMT) studies. This task was first adopted as a shared task in the third conference on machine translation <ref type="bibr">(WMT 2018)</ref>  <ref type="bibr">12</ref> .</p><p>Several existing parallel corpora, especially those automatically gathered from large-scale text data, such as the Web, contain unacceptable amounts of noisy (low-quality) sentence pairs that greatly affect the translation quality. Therefore, the development of an effective method for paral- lel corpus filtering would potentially have a large influence on the MT community; discarding such noisy pairs may improve the translation quality and shorten the training time.</p><p>We expect PHSIC to give low scores to excep- tional sentence pairs (misalignments or missing translations) during the selection process because PHSIC assigns low scores to pairs that are highly inconsistent with other pairs (see Section 4). Note that applying RNN-based PMI to a parallel corpus selection task is unprofitable since obtaining RNN- based PMI also has an identical computational cost for training a sequence-to-sequence model for MT, and thus, we cannot expect a reduction of the total training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>Dataset We used the ASPEC-JE corpus <ref type="bibr">13</ref> , which is an official dataset used for the MT-evaluation shared task held in the fourth workshop on Asian translation <ref type="bibr">(WAT 2017)</ref>  <ref type="bibr">14</ref> ( <ref type="bibr" target="#b26">Nakazawa et al., 2017)</ref>. ASPEC-JE consists of approximately three million (3M) Japanese-English parallel sentences from sci- entific paper abstracts. As discussed by <ref type="bibr">Kocmi et al. (2017)</ref>, ASPEC-JE contains many low-quality parallel sentences that have the potential to signifi- cantly degrade the MT quality. In fact, they empir- ically revealed that using only the reliable part of the training parallel corpus significantly improved the translation quality. Therefore, ASPEC-JE is a suitable dataset for evaluating the data selection ability.</p><p>Model For our data selection evaluation, we se- lected the Transformer architecture ( <ref type="bibr">Vaswani et al., 2017)</ref> as our baseline NMT model, which is widely- used in the NMT community and known as one of the current state-of-the-art architectures. We uti- lized fairseq 15 , a publicly available tool for neu- ral sequence-to-sequence models, for building our models.</p><p>Experimental Procedure We used the follow- ing procedure for this evaluation: (1) rank all paral- lel sentences in a given parallel corpus according to each criterion, (2) extract the top K ranked par- allel sentences, (3) train the NMT model using the extracted parallel sentences, and (4) evaluate the translation quality of the test data using a typical MT automatic evaluation measure, i.e., BLEU <ref type="bibr" target="#b27">(Papineni et al., 2002)</ref>  <ref type="bibr">16</ref> . In our experiments we evalu- ated PHSIC with K = 0.5M and 1M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Config Size of Training Set n 10 3 10 4 10 5 5 × 10 5</p><p>Chance <ref type="bibr">Level .50; .29; .10, .20 .50; .29; .10, .20 .50; .29; .10, .20 .50; .29; .10, .20</ref> Dim.</p><p>Init  <ref type="table">Table 5</ref>: Predictive performance for each model and each training set size for the dialogue response selection task: ROC-AUC; MRR; Recall@1,2. The best result in each column is in bold. The other notation is the same as in <ref type="table" target="#tab_2">Table 4</ref>.  <ref type="table">Table 6</ref>: BLEU scores with the Transformer for each data selection criterion and each size of selected data K for the parallel corpus filtering task."Random" rep- resents the baseline method of selecting sentences at random.</p><p>Baseline Measure As a baseline measure, we utilize a publicly available script 17 of fast align <ref type="bibr" target="#b11">(Dyer et al., 2013)</ref>, which is one of the state-of-the- art word aligner. We firstly used the fast align for the training set D = {(x i , y i )} i to obtain the word alignment between each sentence pair (x i , y i ), i.e., a set of aligned word pairs with its probabilities. We then computed the co-occurrence score of (x i , y i ) with sentence-length normaliza- tion, i.e., the average log probability of aligned word pairs. <ref type="table">Table 6</ref> shows the results of our data selection eval- uation. It is common knowledge in NMT that more data gives better performance in general. However, we observed that PHSIC successfully extracted ben- eficial parallel sentences from the noisy parallel <ref type="bibr">17</ref> https://github.com/clab/fast align corpus; the result using 1M data extracted from the 3M corpus by PHSIC was almost the same as that using 3M data (the decrease in the BLEU score was only 0.07), whereas that by random extraction reduced the BLEU score by 1.20. This was actually a surprising result because PH- SIC utilizes only monolingual similarity measures (kernels) without any other language resources. This indicates that PHSIC can be applied to a lan- guage pair poor in parallel resources. In addition, the surface form and grammatical characteristics between English and Japanese are extremely differ- ent <ref type="bibr">18</ref> ; therefore, we expect that PHSIC will work well regardless of the similarity of the language pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Dependence Measures Measuring indepen- dence or dependence (correlation) between two random variables, i.e., estimating dependence from a set of paired data, is a fundamental task in statistics and a very wide area of data science. To measure the complex nonlinear dependence that real data has, we have several choices.</p><p>First, information-theoretic MI <ref type="bibr" target="#b9">(Cover and Thomas, 2006</ref>) and its variants ( <ref type="bibr" target="#b35">Suzuki et al., 2009;</ref><ref type="bibr" target="#b30">Reshef et al., 2011</ref>) are the most commonly used dependence measures. However, to the best of our knowledge, there is no practical method of com- puting MIs for large-multi class high-dimensional (having a complex generative model) discrete data, such as sparse linguistic data.</p><p>Second, several kernel-based dependence mea- sures have been proposed for measuring nonlin- ear dependence <ref type="bibr" target="#b0">(Akaho, 2001;</ref><ref type="bibr" target="#b1">Bach and Jordan, 2002;</ref><ref type="bibr" target="#b16">Gretton et al., 2005</ref>). The reason why kernel- based dependence measures work well for real data is that they do not explicitly estimate den- sities, which is difficult for high-dimensional data. Among them, HSIC ( <ref type="bibr" target="#b16">Gretton et al., 2005</ref>) is pop- ular because it has a simple estimation method, which is used for various tasks such as feature se- lection <ref type="figure">(Song et al., 2012)</ref>, dimensionality reduc- tion ( <ref type="bibr" target="#b13">Fukumizu et al., 2009)</ref>, and unsupervised ob- ject matching ( <ref type="bibr" target="#b29">Quadrianto et al., 2009;</ref><ref type="bibr" target="#b21">Jagarlamudi et al., 2010)</ref>. We follow this line.</p><p>Co-occurrence Measures First, In NLP, PMI <ref type="bibr" target="#b7">(Church and Hanks, 1989)</ref> and its variants <ref type="bibr" target="#b3">(Bouma, 2009)</ref> are the de facto co-occurrence measures be- tween dense linguistic expressions, such as words <ref type="bibr" target="#b3">(Bouma, 2009)</ref> and simple narrative-event expres- sions <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2008)</ref>. In recent years, positive PMI (PPMI) has played an impor- tant role as a component of word vectors <ref type="bibr">(Levy and Goldberg, 2014)</ref>.</p><p>Second, there are several studies in which the pairwise ranking problem has been solved by us- ing deep neural networks (DNNs) in NLP. <ref type="bibr">Li et al. (2016)</ref> proposed a PMI estimation using RNN lan- guage models; this was used as a baseline model in our experiments (see Section 6.2). Several studies have used DNN-based binary classifiers modeling P(C = positive | (x, y)) to solve the given ranking problem directly ( <ref type="bibr" target="#b19">Hu et al., 2014;</ref><ref type="bibr" target="#b40">Yin et al., 2016;</ref><ref type="bibr" target="#b25">Mueller and Thyagarajan, 2016</ref>) (these networks are sometimes called Siamese neu- ral networks). Our study focuses on comparing co-occurrence measures. It is unknown whether Siamese NNs capture the co-occurrence strength; therefore we did not deal with Siamese NNs in this paper.</p><p>Finally, to the best of our knowledge, <ref type="bibr" target="#b41">Yokoi et al. (2017)</ref>'s paper is the first study that suggested con- verting HSIC to a pointwise measure. The present study was inspired by their suggestion; here, we have (i) provided a formal definition (population) of PHSIC; (ii) analyzed the relationship between PHSIC and PMI; (iii) proposed linear-time estima- tion methods; and (iv) experimentally verified the computation speed and robustness to data sparsity of PHSIC for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The NLP community has commonly employed PMI to estimate the co-occurrence strength be- tween linguistic expressions; however, existing PMI estimators have a high computational cost when applied to sparse linguistic expressions (Section 1). We proposed a new kernel-based co-occurrence measure, the pointwise Hilbert- Schmidt independent criterion (PHSIC). As well as defining PMI as the contribution to mutual in- formation, PHSIC is defined as the contribution to HSIC; PHSIC is intuitively a "kernelized variant of PMI" (Section 3). PHSIC can be applied to sparse linguistic expressions owing to the mechanism of smoothing by kernels. Comparing the estimators of PMI and PHSIC, PHSIC can be interpreted as a smoothed variant of PMI, which allows various similarity metrics to be plugged in as kernels (Sec- tion 4). In addition, PHSIC can be estimated in linear time owing to the efficient matrix calculation, regardless of whether we use linear or nonlinear kernels (Section 5). We conducted a ranking task for dialogue systems and a data selection task for machine translation (Section 6). The experimen- tal results show that (i) the learning of PHSIC was completed thousands of times faster than that of the RNN-based PMI while outperforming it in ranking accuracy (Section 6.2); and (ii) even when using a nonlinear kernel, PHSIC can be applied to a large dataset. Moreover, PHSIC reduces the amount of training data to one third without sacrificing the output translation quality (Section 6.3).</p><p>Future Work Using the PHSIC estimator in fea- ture space (Equation <ref type="formula" target="#formula_0">(18)</ref>), we can generate the most appropriate ψ(y) for a given φ(x) (uniquely, up to scale). That is, if a DNN-based sentence de- coder is used, y (a sentence) can be restored from ψ(y) (a feature vector) so that generative models of strong co-occurring sentences can be realized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Available Kernels for PHSIC</head><p>Similarity between Sentence Vectors A variety of vector representations of phrases and sentences based on the distributional hypothesis have recently been proposed, including sentence encoders ( <ref type="bibr">Kiros et al., 2015;</ref><ref type="bibr" target="#b10">Dai and Le, 2015;</ref><ref type="bibr" target="#b20">Iyyer et al., 2015;</ref><ref type="bibr" target="#b17">Hill et al., 2016;</ref><ref type="bibr" target="#b5">Cer et al., 2018)</ref> and the sum of word embeddings; it is known as additive composi- tionality ( <ref type="bibr" target="#b23">Mitchell and Lapata, 2010;</ref><ref type="bibr">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b39">Wieting et al., 2015</ref>) that we can express the meaning of phrases and sentences well with the sum of word vectors (e.g., <ref type="bibr">word2vec (Mikolov et al., 2013b</ref>), <ref type="bibr">GloVe (Pennington et al., 2014</ref>), and fastText ( <ref type="bibr" target="#b2">Bojanowski et al., 2017)</ref>). Note that var- ious pre-trained models of sentence encoders and word embeddings have also been made available.</p><p>The cosine of these vectors, which is a positive definite kernel, can be used as a convenient and highly accurate similarity function between phrases or sentences. Other major kernels can also be used, such as the RBF kernel, the Laplacian kernel, and polynomial kernels.</p><p>Structured Kernels Various structured kernels for NLP, such as tree kernels, which capture fine structure of sentences such as syntax, were devised in the support vector machine era <ref type="bibr" target="#b8">(Collins and Duffy, 2002;</ref><ref type="bibr" target="#b4">Bunescu and Mooney, 2006;</ref><ref type="bibr" target="#b24">Moschitti, 2006</ref>).</p><p>Combinations We can freely combine the previ- ously mentioned kernels because the sum and the product of positive definite kernels are also posi- tive definite kernels <ref type="bibr">(Shawe-Taylor and Cristianini, 2004, Proposition 3.22</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation of Fast PHSIC Estimation in Data Space</head><p>Although estimators of HSIC and PHSIC depend on kernels k, and data D, hereinafter, we use the following notation for the sake of simplicity:</p><formula xml:id="formula_26">HSIC(X, Y ) := HSIC(X, Y ; D, k, ),<label>(25)</label></formula><formula xml:id="formula_27">PHSIC(x, y) := PHSIC(x, y; D, k, ). (26)</formula><p>Na¨ıveNa¨ıve Estimation Fist, an estimator of PHSIC in the data space <ref type="formula" target="#formula_0">(15)</ref> is</p><formula xml:id="formula_28">PHSIC kernel (x, y) = (k − k) ( 1 n H)( − ),<label>(27)</label></formula><p>where k := (k(x, x 1 ), . . . , k(x, x n )) ∈ R n , so as ; and vector k := 1 n K1 denotes empirical mean of {k i } n i=1 , so as . This estimation has a large computational cost. When learning, comput- ing the vectors k, takes O(n 2 ) time and O(n) space. When estimating PHSIC, computing k, and multiplying the matrix 1 n H takes O(n) time. Fast Estimation via Incomplete Cholesky De- composition Equation <ref type="formula" target="#formula_1">(27)</ref> has a large compu- tational cost because it is necessary to construct the Gram matrices K and L ∈ R n×n . In kernel methods, several methods have been proposed for approximating Gram matrices at low cost with- out constructing them explicitly, such as incom- plete Cholesky decomposition <ref type="bibr" target="#b12">(Fine and Scheinberg, 2001</ref>).</p><p>By incomplete Cholesky decomposition, from data points {x 1 , . . . , x n } ⊆ X and a positive def- inite kernel k : X × X → R, a matrix A = (a 1 , . . . , a n ) ∈ R n×d (d n) can be obtained with O(nd 2 ) time complexity. This makes it possi- ble to approximate the Gram matrix K by vectors a i ∈ R d without configuring the entire of K:</p><formula xml:id="formula_29">a i a j ≈ k(x i , x j )<label>(28)</label></formula><p>AA ≈ K.</p><p>Also, for HSIC, an efficient approximation method utilizing incomplete Cholesky decompo- sition has been proposed ( <ref type="bibr" target="#b16">Gretton et al., 2005</ref>, Lemma 2):</p><formula xml:id="formula_31">HSIC ICD (X, Y ) = 1 n 2 (HA) B 2 F ,<label>(30)</label></formula><p>where A = (a 1 , . . . , a n ) ∈ R n×d is a matrix satisfying AA ≈ K computed via incomplete Cholesky decomposition, so as B (BB ≈ L). Equation <ref type="formula" target="#formula_2">(30)</ref> can be represented in the form of the expectation on data points:</p><formula xml:id="formula_32">HSIC ICD (X, Y ) = 1 n n i=1 (a i −a) C ICD (b i −b) (31) C ICD := 1 n (HA) B ∈ R d×d ,<label>(32)</label></formula><p>where vector a := 1 n A 1 ∈ R d denotes empirical mean of {a i } n i=1 , so as b := 1 n B 1. Recall that PHSIC(x, y) is the contribution of (x, y) to HSIC(X, Y ) (see Section 3.3); PHSIC then can be efficiently estimated by the shaded part of Equation <ref type="formula" target="#formula_0">(31)</ref>  <ref type="formula" target="#formula_2">(33)</ref> Here, the vector a ∈ R d corresponding to the new x can be calculated by "performing from halfway"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Config (A) (B) (C) (D)</head><p>Dim. Init.  <ref type="table">Table 7</ref>: Spearman's ρ between the co-occurrence scores computed by the models in the dialogue re- sponse selection task (Section 6.2). The size of training set n is 5 × 10 5 . The other notation is the same as in <ref type="table" target="#tab_2">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-PMI</head><p>on the incomplete Cholesky decomposition algo- rithm. Let x (1) , . . . , x (d) denote the dominant x i s adopted during decomposition algorithm. The jth element of a can be computed as follows:</p><formula xml:id="formula_33">a[j] = k(x, x (j) ) − j−1 m=1 a[m]A jm / A jj ,<label>(34)</label></formula><p>so as b ∈ R d corresponding to the new y. The estimation via incomplete Cholesky decomposi- tion (33) is extremely efficient compared to the naive estimation <ref type="bibr">(27)</ref>; Equation (33)'s computa- tional complexity is equivalent to the estimation in the feature space (18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Settings for Learning RNNs</head><p>Detailed settings for learning RNNs used in this research are as follows.</p><p>• Hidden layers: single layer LSTMs <ref type="bibr" target="#b18">(Hochreiter and Schmidhuber, 1997</ref>  <ref type="table">Task   Table D</ref> shows Spearman's rank correlation coef- ficient (Spearman's ρ) between the co-occurrence scores on the test set computed by the models in the dialogue response selection task (Section 6.2). This shows that the behavior of RNN-based PMI and PHSIC are considerably different. Furthermore, in- terestingly, the behavior of PHSICs using different kernels is also different. Possible reasons for these observations are as follows: (1) the difference in the dependence measures (MI or HSIC) on which each model is based; (2) the validity or numerical stability of estimating PMI with RNN language models; and (3) differences in the behavior of PH- SIC originating from differences in the plugged in kernels. A more detailed analysis of the compati- bility between tasks and measures (or kernels) is attractive future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Baseline Measures As baseline measures, both (1) an RNN language model P RNN (y) (Mikolov et al., 2010) and (2) a conditional RNN language model P RNN (y|x) (Sutskever et al., 2014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>: PHSIC ICD (x, y) = (a−a) C ICD (b−b) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) • Vocabulary: words with a frequency: 10 or more (n = 5 × 10 5 ), 2 or more (otherwise) • Dropout rate: 0.1 (300-dim), 0.3 (1200-dim) • Batch size: 64 • Max epoch number: 5 (n = 5 × 10 5 ), 30 (other- wise) • Deep learning framework: Chainer (Tokui et al., 2015) D Correlation Between Models in Dialogue Response Selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 : Learning time [s]</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> In collocation extraction, simple counting c(x, y) ∝ P(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., &quot;of the&quot;) higher (Manning and Schütze, 1999).</note>

			<note place="foot" n="3"> Conventionally, mutual information is denoted by I(X; Y ); in this paper, however, for notational consistency, mutual information is denoted by MI(X, Y ).</note>

			<note place="foot" n="6"> One of the characteristics of kernel methods is that an intractable estimation in feature space is replaced with an efficient estimation in data space.</note>

			<note place="foot" n="7"> https://fasttext.cc/docs/en/english-vectors. html, https://fasttext.cc/docs/en/crawl-vectors. html 8 https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder/1</note>

			<note place="foot" n="11"> The computing environment was as follows: (i) CPU: Xeon E5-1650-v3 (3.5 GHz, 6 Cores); (ii) GPU: GTX 1080 (8 GB).</note>

			<note place="foot" n="12"> http://www.statmt.org/wmt18/ parallel-corpus-filtering.html</note>

			<note place="foot" n="13"> http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 14 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/ 15 https://github.com/pytorch/fairseq 16 We used multi-bleu.perl in the Moses tool (https:// github.com/moses-smt/mosesdecoder).</note>

			<note place="foot" n="18"> For example, word order; English is an SVO (subject-verbobject) language and Japanese is an SOV (subject-object-verb) language.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to anonymous reviewers for their helpful comments. We also thank Weihua Hu for useful discussions, Kenshi Yamaguchi for collect-ing data, and Paul Reisert for proofreading. This work was supported in part by JSPS KAKENHI Grant Number JP15H01702 and JST CREST Grant Number JPMJCR1513, Japan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A kernel method for canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Akaho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kernel Independent Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalized (Pointwise) Mutual Information in Collocation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GSCL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Subsequence Kernels for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<idno>abs/1803.1</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Universal Sentence Encoder. CoRR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Event Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word Association Norms, Mutual Information, and Lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolution Kernels for Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Simple, Fast, and Effective Reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katya</forename><surname>Scheinberg</surname></persName>
		</author>
		<title level="m">Efficient SVM Training Using Low-Rank Kernel Representations. JMLR</title>
		<imprint>
			<date type="published" when="2001-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="243" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel dimension reduction in regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1871" to="1905" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Word Vectors for 157 Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3483" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring Statistical Dependence with Hilbert-Schmidt Norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALT</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Sentences from Unlabelled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network Architectures for Matching Natural Language Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernelized Sorting for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Juarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composition in Distributional Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making Tree Kernels practical for Natural Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Siamese Recurrent Architectures for Learning Sentence Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Higashiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<title level="m">Overview of the 4th workshop on asian translation. In WAT</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kernelized sorting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novi</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1289" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting Novel Associations in Large Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">N</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakir</forename><forename type="middle">A</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilary</forename><forename type="middle">K</forename><surname>Finucane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilean</forename><surname>Mcvean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Turnbaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pardis</forename><forename type="middle">C</forename><surname>Sabeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="issue">6062</biblScope>
			<biblScope unit="page" from="1518" to="1524" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature Selection via Dependence Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1393" to="1434" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mutual information estimation reveals global associations between stimuli and biological processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takafumi</forename><surname>Kanamori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Suppl 1</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chainer: a Next-Generation Open Source Framework for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LearningSys</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<pubPlace>Llion Jones, Aidan N. Gomez, ukasz</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards Universal Paraphrastic Sentence Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning CoSubstructures by Kernel Dependence Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3329" to="3335" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
