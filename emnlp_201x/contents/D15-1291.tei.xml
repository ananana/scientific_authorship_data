<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntactic Dependencies and Distributed Word Representations for Chinese Analogy Detection and Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
							<email>qiulikun@pku.edu.cn, yue zhang@sutd.edu.sg, luyanan@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Chinese Language and Literature</orgName>
								<orgName type="institution">Ludong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Syntactic Dependencies and Distributed Word Representations for Chinese Analogy Detection and Mining</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributed word representations capture relational similarities by means of vector arithmetics, giving high accuracies on analogy detection. We empirically investigate the use of syntactic dependencies on improving Chinese analogy detection based on distributed word representations , showing that a dependency-based em-beddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by filtering candidates. In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relational similarity measures the correspondence between word-word relations ( <ref type="bibr" target="#b13">Medin et al., 1990)</ref>. It is relevant to many tasks in NLP <ref type="bibr" target="#b24">(Turney, 2006</ref>), such as word sense disambiguation, information extraction, question answering, information re- trieval, semantic role identification and metaphor detection. Typical tasks on relational similarity in- clude analogy detection, which measures the de- gree of relational similarities, and analogy mining, which extracts analogous word pairs from unstruc- tured text.</p><p>Recently, distributed word representations (i.e. embeddings) ( <ref type="bibr" target="#b14">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b12">Levy and Goldberg, 2014b</ref>) have been used for unsupervised analogy detection. <ref type="bibr">Mikolov et al. use</ref> attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best sys- tem in the SemEval 2012 shared task on analo- gy detection. <ref type="bibr" target="#b12">Levy and Goldberg (2014b)</ref> fur- ther improve Mikolov's relational similarity mea- sure method using novel arithmetic combination- s of attributional similarities. For simplicity, we call the method of Mikolov et al. embedding- based analogy detection, without stressing the dif- ference between distributed and distributional (i.e. counting-based) word representations.</p><p>Most work on embedding-based analogy detec- tion uses relational similarities as a measure of the quality of embeddings. However, relatively little has been done in the opposite direction, exploring how to leverage embeddings for improving rela- tional similarity algorithms. We empirically study the use of word embeddings for Chinese analogy detection and mining, leveraging syntactic depen- dencies, which has been shown to be closely asso- ciated with semantic relations <ref type="bibr" target="#b10">(Levin, 1993;</ref><ref type="bibr" target="#b4">Chiu et al., 2007</ref>). Compared with many other lan- guages, this association is particularly strong for Chinese, which is fully configurational and lack- s morphology. To our knowledge, relatively little work has been reported on Chinese relational sim- ilarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation.</p><p>We work on three specific problems. First, we study the effect of dependency-based word em- beddings for analogy detection. There are two variations of Mikolov et al's skip-gram embed- ding model, one training the distributed word rep- resentation of a word using its context words in local ngram window ( <ref type="bibr" target="#b14">Mikolov et al., 2013a)</ref>, and the other training the distributed representation of a word using words in a syntactic dependency context ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014b;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>). The latter has attracted much recent atten-tion due to its potential in capturing more syntac- tic regularities. It has been shown to outperfor- m the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our experiments on both English and Chinese show that the dependency-context embeddings consis- tently under-perform ngram-context embeddings. We give some theoretical justifications to the find- ings.</p><p>Second, we propose to use syntactic depen- dencies as a context for improving embedding- based analogy detection, pruning the search space and filtering noise using syntactic dependencies. While highly useful for measuring relational sim- ilarities, attributional similarities between words are not the only source of information for analo- gy detection. Traditional methods, such as Tur- ney and <ref type="bibr" target="#b23">Littman (2005)</ref>, Turney (2006), <ref type="bibr">Chiu et al. (2007) and´Oand´ and´O Séaghdha and</ref><ref type="bibr" target="#b17">Copestake (2009)</ref>, also leverage context between word pairs in a corpus for better accuracies, which the current embedding-based methods ignore. Results show that our proposed method achieves significant im- provements for this task.</p><p>Third, we show that a novel distributed repre- sentation of syntactic dependencies between word pairs can be used to mine analogous dependencies from a large Chinese corpus. Inspired by the fact that distributed word representations can be used to measure word similarities, we use our distribut- ed dependency representations to measure relation similarities. We propose a bootstrapping algorith- m for analogy mining using dependency embed- dings, and experiments on a large Chinese corpus show that the method can achieve a precision of 95.2% at a recall of 56.8%.</p><p>Our automatically-parsed corpus, trained em- beddings and evaluation datasets are released publicly at http://people.sutd.edu.sg/ ˜ yue_zhang/publication.html. To our knowledge, we are the first to present results on Chinese analogy detection and to release large- scale Chinese word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relational Similarity Tasks</head><p>There are three main tasks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 <ref type="bibr" target="#b9">(Jurgens et al., 2012)</ref>. In this task, all four words in two word pairs are given, and one needs to judge whether <ref type="figure">Figure 1</ref>: Dependency tree of the sentence "1991c (in 1991) § (,) cnê (Obama) oÚ (President) .' (graduate) u (from) MÃ (Har- vard) {AE (Law School)". they belong to a same relation type. In order to address this task, various supervised methods have been used <ref type="bibr" target="#b2">(Bollegala et al., 2008;</ref><ref type="bibr" target="#b6">HerdaˇgdelenHerdaˇgdelen and Baroni, 2009;</ref><ref type="bibr" target="#b25">Turney, 2013)</ref>.</p><p>The second task is analogy detection <ref type="bibr" target="#b15">(Mikolov et al., 2013b)</ref>, which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embed- dings ( <ref type="bibr" target="#b15">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b12">Levy and Goldberg, 2014b)</ref>.</p><p>The third task is analogy mining ( <ref type="bibr" target="#b4">Chiu et al., 2007)</ref>, which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classifi- cation and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Skip-gram Word Embeddings</head><p>As a by-product of neural language models <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mnih and Hinton, 2007)</ref>, word embeddings are distributed vector representations of words, trained using local contexts. They cap- ture linguistic regularities in languages ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>) and have been used in various tasks <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b22">Turian et al., 2010;</ref><ref type="bibr" target="#b21">Socher et al., 2011)</ref>.</p><p>In this paper, we apply the Skip-gram method of <ref type="bibr" target="#b14">Mikolov et al. (2013a)</ref> for training embed- dings, which works by maximizing the probabil- ity of a word given a context of multiple words. <ref type="bibr" target="#b15">Mikolov et al. (2013b)</ref> use an ngram window as the context, and observe that the resulting embed- dings are highly useful for unsupervised analogy detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Embedding-based Analogy Detection</head><p>Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. <ref type="bibr" target="#b15">Mikolov et al. (2013b)</ref> show that the task can be solved by finding a word that maximizes:</p><formula xml:id="formula_0">score = sim(b * , b − a + a * ) (1)</formula><p>where sim is a similarity measure, typically the cosine function. <ref type="bibr" target="#b12">Levy and Goldberg (2014b)</ref> show that the Equation 1 is equivalent to:</p><formula xml:id="formula_1">score = cos(b * , b)−cos(b * , a)+cos(b * , a * ) (2)</formula><p>As a result, the goal of analogy detection is to find a word b* which is similar to b and a* but differ- ent from a. <ref type="bibr" target="#b12">Levy and Goldberg (2014b)</ref> further propose to substitute the addictive functions in E- quation 2 with multiplicative functions:</p><formula xml:id="formula_2">score = cos(b * , b)cos(b * , a * )/(cos(b * , a) + ε)<label>(3)</label></formula><p>Here ε = 0.001 is used to prevent division by zero. Their experiments show that the use of Equation 3 can improve the state-of-the-art. Following <ref type="bibr" target="#b12">Levy and Goldberg (2014b)</ref>, we refer to Equation 1 and 2 as 3COSADD and Equation 3 as 3COSMUL, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Chinese Relational Similarity</head><p>There are various types of relational similarities. Syntactically, inflections can be treated as a type of word-word relation ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>). For example, the comparative pairs "good:better" and "rough:rougher" are analogous, and the past tense inflections "see:saw" and "return:returned" are analogous. However, such inflectional rela- tions do not apply to Chinese, which is fully con- figurational and lacks morphology. Consequent- ly, our main focus is semantic similarities, which include antonymy (e.g.  <ref type="formula">(2007)</ref> show that English semantic relations are also reflected by syntactic dependen- cies. Their finding coincides with <ref type="bibr" target="#b10">Levin (1993)</ref>, who study English verbs. We find that this obser- vation is even more prevalent for Chinese. In our automatically-parsed Chinese corpus of 3.4 bil- lion words (Section 5.1), 86.4% word pairs from the analogy test dataset (Section 5.2) have corre- sponding dependencies, each of which appearing at least ten times.</p><p>The frequent correlation between semantic re- lations and syntactic dependencies can be due to the lack of morphology and function words in Chinese. In fact, Chinese syntactic ambigui- ties often need to be resolved by leveraging se- mantic information ( <ref type="bibr" target="#b26">Xiong et al., 2005;</ref>. Although not all occurrences of semantically-related word pairs must also form a syntactic dependency in a corpus, we show that syntactic dependencies can effectively improve analogy detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dependency-context Word Embeddings for Analogy Detection</head><p>A first use of syntactic dependencies for embedding-based analogy detection is to use them directly for embeddings. Recently, a depen- dency context has been used for the skip-gram method, for capturing more syntactic regularities. Taking the sentence in <ref type="figure">Figure 1</ref> for example, a bi-gram context for the word ".' (graduate)" can be "c n ê (Obama), o Ú (President), u (from), M Ã (Harvard)", while a dependency context of the same word can be "1991c/ADV, o Ú/SBV, u/CMP, { AE /POB u" 1 , where "ADV, SBV, CMP, POB" indicate adverbial modifier, subject, complement and prepositional object, respectively.</p><p>It has been shown that a dependency context leads to embeddings that better help parsing ( <ref type="bibr" target="#b0">Bansal et al., 2014</ref>) and measuring word sim- ilarity ( <ref type="bibr" target="#b11">Levy and Goldberg, 2014a</ref>), compared with ngram contexts. However, little previous work has systematically compared dependency contexts with ngram contexts in analogy detec- tion. We empirically study this problem (c.f Sec- tion 6.3), finding that dependency context lead- s to significantly worse analogy detection results for both Chinese and English using state-of-the-art embedding-based methods ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014b)</ref>. We give analysis in Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Search Space Pruning Using Syntactic Dependencies</head><p>We study an alternative way of making use of syn- tactic dependencies, by using them to prune the vocabulary-sized search space of analogy detec- tion. Given two word pairs a:b and a*:b*, where b* is hidden and a is the head word, we search for dependencies, taking a* as the head word. The dependent words in the search candidates need to share the POS tag of b. If there are several type- s of dependencies between a and b, only the one with highest frequency is used. We rank all result- ing dependencies using the 3COSMUL objective, and take the word b* in the highest-scored depen- dencies as the answer. For example, given the word pair (i . 9 ¶ (Sarajevo):Å ç (Bosnia and Herzegovina)), whose most frequency dependency is &lt;i . 9 ¶ (Sarajevo), Å ç (Bosnia and Herzegovina), ATT&gt;, and the unknown pair (Ôí (London):b*), we acquire a list of dependencies, including &lt;Ô í (London), {I (USA), ATT&gt;, &lt;Ôí (Lon- don), ni (Paris), COO&gt;, &lt;Ôí (London), \ &lt;OE (Canada), ATT&gt; and &lt;Ôí (London), =I (England), ATT&gt;. Some of these dependencies, such as &lt;Ôí (London), ni (Paris), COO&gt;, are parsed as the coordinate relation (COO), and thus pruned because the target syntactic relation is ATT. From the resulting list, the 3COSMUL ob- jective successfully ranks the triple &lt;Ôí (Lon- don), =I (England), ATT&gt; as the top candidate. In contrast, Levy and Goldberg's method takes "H š (South Africa)" as the answer, which does not form an attributive-head phrase with "Ôí (Lon- don)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analogy Mining Using Dependency Embeddings</head><p>Formally, analogy mining is the task of mining analogous dependencies &lt;x 1 , y 1 , r&gt;, &lt;x 2 , y 2 , r&gt; ...&lt;x n , y n , r&gt; that share the same relation r with a given dependency &lt;a, b, r&gt;. We mine analo- gous dependencies by considering relational sim- ilarity and attributional similarity simultaneously using the skip-gram model for embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dependency Embedding</head><p>Inspired by the fact that word similarities can be measured by using distributed word representa- tions, we hypothesize that relation similarities can Input : dependency embedding DT, word embedding DW, seed dependency s, threshold α and β. Output: set of ranked dependencies WP. be measured by distributed relation representa- tions. Based on the observation in Section 2.4, semantically analogous word pairs typically have syntactic dependencies. We use the skip-gram al- gorithm to train distributed representations of syn- tactic dependencies, and use them for mining anal- ogous word pairs.</p><p>With respect to the skip-gram model, words are the most common target for embeddings ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014b;</ref><ref type="bibr" target="#b11">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013a</ref>), although continuous vec- tor representations can be trained for other struc- tures. For example, <ref type="bibr" target="#b14">Mikolov et al. (2013a)</ref> take idiomatic phrases as embedding targets. Depen- dencies, which consist of a modifier word, a head word and a syntactic relation between them, can also be represented by continuous embeddings us- ing the same algorithm.</p><p>To induce dependency embeddings, we take the union of the dependency context of both the dependent and the head of a dependency as the context. For instance, in the example sentence, the context of the dependency &lt;oÚ (Presiden-t), . ' (graduate), SBV&gt; consists of four to- kens: "1991c/ADV", "cnê/ATT", "u/CMP" and "{AE/POB u". The same skip-gram algo- rithm is used to train embeddings for dependency structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analogy Mining by Bootstrapping</head><p>A bootstrapping algorithm is used to mine anal- ogous word pairs based on dependency-context word embeddings and dependency embeddings. Algorithm 1 shows pseudocode of the recursive bootstrapping algorithm.</p><p>The recursive function Mine (Algorithm 1) contains three steps with six parameters, includ- ing the dependency embeddings DT, word embed- dings DW, a seed dependency s, and two thresh- olds α and β. Step 1 (lines 3 to 5) is an initial- ization process, where the dependency embedding is used to return up to 100 most similar dependen- cies for the given seed s. These dependencies are stored in SimDT, and the candidate analogous de- pendency set DTSet is initialized to an empty set.</p><p>In Step 2 (lines 6 to 16), an analogous score S- coreXY is computed for each dependency Triple in SimDT by multiplying the similarity scores be- tween the two dependents and the two heads in Triple and s, respectively. Triple is stored into the set DTSet if ScoreXY is ranked top α. The top 1 score in DTSet is referred to as MScore. In Step 3 (lines 17 to 24), if the score of a dependency Triple in DTSet is larger than β×MScore, it is used as a new seed for mining more analogous dependen- cies, by calling the function Mine recursively.</p><p>We take the seed dependency &lt; (play), g OE (piano), VOB&gt; as an example to illustrate the work-flow of the Mine function. In Step 1, a set of similar dependencies (e.g., &lt; (play),3¦ (gui- tar), VOB&gt;, &lt; (play), OE (lyra), VOB&gt;), is cal- culated using the dependency embeddings DT and stored in SimDT. Each dependency in SimDT is s- cored in Step 2, and the top α scores are put into the set DTSet. Finally, a dependency is used as seed to mine new analogous dependencies if its s- core is larger than a threshold (β×MScore). For instance, the dependency &lt; (play), OE (lyra), VOB&gt; is used to mine the new dependency &lt; (play), 8 (zheng), VOB&gt;, which is then used to mine other dependencies such as &lt;N (blow), ¨j (cucurbit flute), VOB&gt; and &lt;N (blow), iŽd (sax), VOB&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Word Embeddings</head><p>We train three sets of word embeddings: NG5 (n- gram context with 5 words to the left of the target word and 5 words to the right), NG2 (2 words to the left and right) and DEP (dependency context), and one set of dependency embeddings DT (de- pendency context), using the Skip-Gram model. WORD2VEC 2 is used to train NG5 and NG2, and WORD2VECF 3 is used to train DEP and DT. The negative-sampling parameter is set to 15 in all the training processes.</p><p>All embeddings are trained on a free Chinese news archive 4 that contains about 170 million- s sentences and 3.4 billions words. We segment and parse these sentences using the MVT imple- mentation of ZPar 0.7 5 ( <ref type="bibr" target="#b27">Zhang and Clark, 2011)</ref>, which is trained on a large-scale annotated cor- pus and achieves state-of-the-art analyzing accu- racy on contemporary Chinese ( <ref type="bibr" target="#b19">Qiu et al., 2014)</ref>  <ref type="bibr">6</ref> . Targets and contexts for word and dependency em- beddings were filtered with a minimum frequency of 100 and 10, respectively, and all the four types of embeddings are trained with 200 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Datasets and Evaluation Metrics</head><p>Three datasets are used for evaluating Chinese embeddings. First, we construct a set of se- mantic analogy questions. This set contains five types of semantic analogy questions, including capital-country (136 word pairs, and 18354 anal- ogy questions), provincial capital-province <ref type="bibr">(28,</ref><ref type="bibr">756)</ref>, city-province (637, 386262), family mem- ber (male-female) <ref type="bibr">(18,</ref><ref type="bibr">306)</ref> and currency-country <ref type="bibr">(62,</ref><ref type="bibr">3782)</ref>. We collect the five types of word pairs and then produce analogy questions automatical- ly by concatenating two word pairs. The resulting analogy dataset contains 400K analogy question- s. We refer to this dataset as the Chinese Analogy Question Set (CAQS).   . This dataset consists of 297 word pairs.</p><p>The second one is the Chinese thesaurus Tongyicicilin (Cilin) ( <ref type="bibr" target="#b3">Che et al., 2010)</ref>, which groups 74,000 Chinese words into five-layer hi- erarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models ( <ref type="bibr" target="#b18">Qiu et al., 2011;</ref>). The third level of Cilin, which contain- s 1428 classes, is used to evaluate whether two words are semantically similar.</p><p>For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset 7 ( <ref type="bibr" target="#b14">Mikolov et al., 2013a)</ref>, to e- valuate the English word embeddings of <ref type="bibr" target="#b11">Levy and Goldberg (2014a)</ref>  <ref type="bibr">8</ref> on analogy detection.</p><p>On both the CAQS and the Google dataset- s, the 3COSMUL method ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014b</ref>) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics for the task, namely Spearman's ρ and K- endall's τ rank correlation coefficients. The re- sults on Cilin are evaluated using Precision@K: the percentage of words from the top-K candidates that belong to the Cilin category of the target word. If one of the top-K candidates belongs to the same third-level category in Cilin as the target word, the candidate word is taken as correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dependency-based and Word-based</head><p>Word Similarity and Analogy Detection Word Similarity   <ref type="table">Table 3</ref>: English results on the Google set. <ref type="table" target="#tab_0">Table 1</ref> shows the results of the three Chinese embedding on Cilin and CWS, where NG2 per- forms much better than NG5 on both datasets. This demonstrates that one does not need to use large window sizes in training word-based embed- dings for capturing word similarities. The result is similar to the finding of <ref type="bibr" target="#b20">Shi et al. (2010)</ref>, which indicates that a window size of 2 is better than a window size of 4 for capturing word similarity by using distributional word representations. DEP performs slightly worse than NG2 on CWS and Cilin in P@1 and P@5. However, it achieves better results on Cilin in P@10 to P@100 when more candidate similar words are evaluated. In contrast, NG5 and NG2 mix more semantically related words. This finding is consistent with that of Levy and Goldberg (2014a). <ref type="table" target="#tab_2">Table 2</ref> shows the results of the three Chinese embeddings on CAQS. Unlike on Cilin and CWS, NG5 outperforms DEP, and is also slightly better than NG2. Similar tendency is shown in <ref type="table">Table 3</ref> for the three English embeddings evaluated on the Google dataset. These results show that dependen- cy embeddings are relatively weak for answering analogy questions. On the other hand, the perfor- mance also varies across different relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analogy Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NG5</head><p>NG2 DEP B (wear) á¦ (shorts), ; (slim-fit), B (wear), @ (coat), * f (skirt) B (wear), X (wear), á¦ (shorts), • (wear), ; (slim-fit) B (wear), X (wear), • (wear), UB (change cloths), B (wear outside) ' ‹ (Guan Yu; P) ë (Zhao Yun; P), 4 (Liu Bei; P), Ã• (Zhuge Liang; P), Üoe (Zhang Fei; P), ùö (Cao Cao; P) ë (Zhao Yun; P), 4 (Liu Bei; P), Üoe (Zhang Fei; P), ù ö (Cao Cao; P), gû (Xiahou Yuan; P) ë (Zhao Yun; P), ¸&amp; (Han Xin; P), ùö (Cao Cao; P), 4 (Liu Bei; P), C?Û (Asura; P)  <ref type="table">Table 4</ref>: Comparison between NG2, NG5 and DEP Embeddings. (P: personal name, C: city name)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Analysis</head><p>To analyze the difference between the three Chi- nese embeddings methods qualitatively, we man- ually inspect the words "B (wear)", "'‹ (Guan Yu, a person name in the novel 'n I ü Â (Romance of the three kingdoms)')", and "x² (Zhengzhou, a city)". Their most similar words are shown in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Similarity</head><p>For the word "B (wear)", both NG5 and NG2 yield similar words such as "B (wear)", " X (wear)", "• (wear)" and related words such as "á¦ (shorts)", "; (slim-fit)", "@ (coat)", "*f (skirt)", although NG5 gives more related words. In contrast, DEP gives only words that are similar both syntactically and semantically. This observation holds for other verbs and nouns, and can be explained by the context extraction method- s. For instance, the word "B (wear)" usually takes one of the words "á¦ (shorts)", "@ (coat)", "*f (skirt)" as its object, and thus shares sim- ilar contexts with them in NG5 and NG2. The context extraction method in DEP, on the oth- er hand, yields different context across syntactic roles, such as verbs (e.g. "B (wear)") and their objects (e.g. "á¦ (shorts)" and "@ (coat)").</p><p>Observations on the person name "'‹ (Guan Yu)" and location "x² (Zhengzhou)" are simi- lar. For "'‹ (Guan Yu)", NG5 and NG2 can yield more person names in the same novel, while DEP yields person names from other novels (i.e. "¸&amp; (Hanxin)" and "C?Û (Asura)"). For "x ² (Zhengzhou)", the provincial capital of "àH (Henan)", NG5 and NG2 give more cities in the same province "àH (Henan)", while DEP yields capitals of other provinces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analogy Detection</head><p>As mentioned in Section 2.3, both 3COSADD and 3COSMUL seek a word b * that is similar to b and a * but dissimilar to a. Ideally, the two word pairs b:b * and a:a * should be semantically similar while the two word pairs a:b and a * :b * should be semantically related. Therefore, 3COSADD and 3COSMUL require the embeddings to give high- er cosine scores for both semantically similar and related words.</p><p>Our analysis above shows that word-context embeddings tend to mix semantically related and similar words, but dependency-context embed- dings only capture semantic similarity. This partly explains the reason that dependency-context word embeddings are weak for analogy detection.</p><p>It has also been shown in Section 6.3 that the performances of analogy detection vary across dif- ferent types of relations, which indicates that there are more sophisticated underlying factors. One in- tuitive explanation is that different semantic rela- tions correspond to different syntactic dependency structures. For example, the male-female family member relation is expected to stand less frequent- ly in a syntactic dependency relation, compared with geographic relations such as city-country, which stand frequently in attributional syntactic relations (e.g. "London, England"). As a result, where the coupling between syntactic and seman- tic relations is weak, our analysis in Section 6.3 and other work based on syntactic relations can find limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Syntactic Dependencies for Improved Analogy Detection</head><p>The results on CAQS using the method in Section 4 are shown in the IMP rows of <ref type="table" target="#tab_2">Table 2</ref>. The method achieves significant improvements (from 80.0% to 90.9% using NG5) compared with Levy and Goldberg's method. In addition, DEP al- so performs significantly better than with MUL, with an increase from 22.0% to 89.8%. The main reason for this improvement is that the filtering process using syntactic dependencies successfully prunes noisy words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed</head><p>Count Prec ¯ (eat), °J (apple), VOB 572 84.70% (play), gOE (piano), VOB 142 40.49% B (wear), ŸÑ (clothing), VOB 452 67.37% (write), ` (novel), VOB 441 53.40% ¥I (China), ® (Beijing), ATT <ref type="bibr">2224</ref> 95.23% (Hubei), ÉÇ (Wuhan), ATT 3201 96.34% <ref type="table">Table 5</ref>: Main results of Analogy Mining.</p><p>Error analysis shows that the main errors by the improved method are quite different from those by the baseline. For instance, the main errors of Levy and Goldberg's method for the city-province relation are caused by giving another province as the answer, while the improved method gives the name of the country as answer. This is because ir- relevant provinces do not co-occur frequently with the city in syntactic dependencies, and hence can be filtered by our method. On the other hand, both the country name and province name co-occur fre- quently with the city name in syntactic dependen- cies, and our method cannot make a choice be- tween them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Dependency Structure Embeddings for Analogy Mining</head><p>Shown in <ref type="table">Table 5</ref>, we use six seeds to mine anal- ogous dependencies. The first seed is used for de- velopment and the others for test. The first three seeds, the fourth seed and the last two seeds be- long to the Use:Thing, Produce:Thing and Sub- Location:Location relations, respectively. α and β are set to 20, and 0.6, respectively. Each set of mined dependencies together with the seed depen- dency and relation type is shown to two human evaluators, who are required to give a Yes/No an- swer to each dependency in the set. We take the average scores of the two evaluators (the average inter-annotator agreement is 0.95) as the final pre- cision scores. As shown in the table, the precisions using dif- ferent seeds are quite different, ranging from 40% to 96%. One possible reason is that different rela- tions have different numbers of analogous depen- dencies, ranging from dozens to thousands, and thus the fixed thresholds tuned on a development seed does not apply as effectively to all test cas- es. For instance, " (play)" and its analogous ac- tions, "N (blow)" and ". (play)", are all human actions on musical instruments, while the action- s "¯ (eat)" and " (write)" can apply to many patients. For the seed &lt; (play), gOE (piano), VOB&gt;, irrelevant results such as &lt;&lt; (use), } f (scissors), VOB&gt; and &lt;&lt; (use), &gt;Ù (flash- light), VOB&gt;, have the verb "&lt; (use)", which is also a human action, yet cannot be considered as usage of the patients "}f (scissors)" and "&gt; Ù (flashlight)". Because of the stricter selectional preference of " (play)", its precision of analogy mining is lower.</p><p>We tentatively measure the recall of the algo- rithm by taking the first three types of word pairs in CAQS as the gold set, which contains 801 word pairs. All the three types of word pairs belong to the relation Sub-Location:Location. The recall is computed as the percentage of the gold word pairs covered by the mined dependencies. When us- ing the two seeds &lt;ÉÇ (Wuhan), (Hubei), ATT&gt; and &lt;® (Beijing), ¥I (China), ATT&gt; for analogy mining, the recalls are 50.2% and 11.3%, respectively. Their union recall is 56.8%. When the precision of each seed is similar, we can achieve better recall without precision loss by us- ing more seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Turney <ref type="formula">(2006)</ref> introduces a latent relational anal- ysis (LRA) model to measure relational similari- ty, and apply a novel co-occurrence-based method for analogy filtering. The model can be used for both analogy detection and relation classifi- cation, yet cannot scale up well to large dataset- s due to the complexity of Singular Value De- composition. Recently, distributed word repre- sentations using the skip-gram model <ref type="bibr" target="#b14">(Mikolov et al., 2013a</ref>) has been shown to give competi- tive results on analogy detection. <ref type="bibr" target="#b11">Levy and Goldberg (2014a)</ref> extends the skip-gram method with dependency-context embeddings. We study the ef- fect of Levy and Goldberg's embeddings on analo- gy detection, and further extend their embeddings to dependency-context dependency structure em- beddings for analogy mining. <ref type="bibr" target="#b4">Chiu et al. (2007)</ref> presents a similarity graph tranversal (SGT) method to mine analogous re- lations from raw English text automatically, us- ing syntactic dependencies to find candidate rela- tions. The method is unsupervised, and can scale up well to large data sets. However, <ref type="bibr" target="#b4">Chiu et al. (2007)</ref> mainly focuses on relations between sub- jects and objects because of its word-pair extrac- tion method. ´ O Séaghdha and Copestake (2009) is a supervised method, which combines lexical similarity and relational similarity to classify se-mantic relations. These methods are based on dis- tributional word representation models and fit for classifying noun-noun word pairs. In contrast, our methods are based on distributed word representa- tion models, and can mine noun-noun word pairs as well as verb-noun word pairs. In addition, our analogy mining method is unsupervised, while the methods of both Turney (2006) and´Oand´ and´O Séaghdha and Copestake (2009) are supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We studied several Chinese relational similarity tasks to train embeddings under the context of dis- tributed word representations using the skip-gram model and syntactic dependencies. For Chinese analogy detection, we compared word-context and dependency-context embeddings, finding that the former results in much better accuracies. Observ- ing that common relations in Chinese are frequent- ly represented by syntactic dependencies, we im- proved Chinese analogy detection using a depen- dency context. Further, we empirically studied Chinese analogy mining by proposing a bootstrap- ping algorithm using a novel distributed represen- tation of syntactic dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 9 (</head><label>9</label><figDesc>hot):e (cold)) VS (¯ (fast):ú (slow))), meronymy (e.g. (• (car):Ó f (wheel)) VS (= (bear):Ý (paw))), gender (e.g. (I&lt; (man):å&lt; (woman)) VS (I (king):å (queen))) and function relations (e.g. (Ÿ Ñ (clothing):B (wear)) VS (lf (hat):• (wear))), etc. Chiu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>Function Mine (DT,DW,s,WP,α,β): 2 begin 3 DTSet =∅; 4 MScore =0; 5 SimDT =GetSimDT (DT,s); 6 for each Triple ∈ SimDT do 7 MWS =GetMWord (s); 8 HWS =GetHWord (s); 9 MWD =GetMWord (Triple); 10 HWD =GetHWord (Triple); 11 ScoreX =Sim (MWS,MWD,DW); 12 ScoreY =Sim (HWS,HWD,DW); 13 ScoreXY =ScoreX × ScoreY; 14 MScore =Max (ScoreXY,MScore); 15 TopK (ScoreXY,Triple,DTSet,α) 16 end 17 MScore =MScore × β ; 18 for each Triple, ScoreXY ∈ DTSet do 19 if ScoreXY &gt; MScore and Triple / ∈ WP then 20 AddToSet (Triple,WP); 21 s =Triple; 22 Mine (DT,DW,s,WP,α,β); 23 end 24 end 25 end 26 WP =∅; 27 Mine (DT,DW,s,WP,α,β); Algorithm 1: Bootstrapping for analogy mining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Data</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ÜS (Xian; C), N (Xuchang; C), (Taiyuan) oe[B (Shijiazhuang; C), (Taiyuan; C), LH (Ji-nan; C), Ü • (Hefei; C), ÜS (Xi-an; C) Ü• (Hefei; C), LH (Jinan; C), ÉÇ (Wuhan; C), oe[B (Shiji- azhuang; C), Hw (Nanning; C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Results on Cilin and CWS. Because embeddings are central for analogy de- tection, yet there is little large-scale evaluation results on Chinese embeddings in the literature, we perform embedding evaluation on two dataset- s. The first one is the Chinese WordSim (CWS), translated from the English WordSim-353 Set and re-scored by native Chinese speakers</figDesc><table>Metrics 

NG5 
NG2 
DEP 
Cilin P@1 
43.3% 45.9% 43.6% 
P@5 
31.1% 33.3% 32.6% 
P@10 
25.5% 27.5% 27.5% 
P@20 
20.5% 22.2% 
22.7% 
P@50 
15.0% 16.2% 
17.0% 
P@100 
11.5% 12.2% 
12.8% 
CWS Kendall's τ 
38.6% 44.1% 42.4% 
Spearman's ρ 54.5% 62.2% 60.7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on CAQS. MUL and IMP indi-
cate 3COSMUL and our improved method, re-
spectively. 

Relation 
NG5 
NG2 
DEP 
capital-country 94.6% 84.5% 38.5% 
capital-world 
71.5% 64.7% 14.2% 
city-in-state 
53.2% 42.5% 13.1% 
family 
82.0% 81.2% 81.0% 
currency 
10.5% 10.7% 6.0% 
All 
63.7% 60.7% 38.8% 

</table></figure>

			<note place="foot" n="1"> The last token is a grand-child of &quot;.&apos; (graduate)&quot;, via the preposition &quot;u (at)&quot; (Levy and Goldberg, 2014a).</note>

			<note place="foot" n="2"> http://code.google.com/p/word2vec/ 3 https://bitbucket.org/yoavgo/ word2vecf 4 This dataset contains news articles in 2014 from various news websites, and can be downloaded from http://pan. baidu.com/s/1o6wRjp4 5 http://people.sutd.edu.sg/\%7Eyue_ zhang/doc/doc/multiview.html 6 The system achieves 96.1% , 92.6% and 83.28% F1score for words segmentation, joint POS-tagging and dependency parsing, respectively, on 1493 manually annotated sentences.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their con-structive comments, and gratefully acknowledge the support of the Singapore Ministry of Educa-tion (MOE) AcRF Tier 2 grant T2MOE201301, the National Natural Science Foundation of <ref type="bibr">China (No. 61572245, 61170144, 61103089</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Www sits the sat: Measuring relational similarity on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="333" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ltp: A chinese language technology platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating lexical analogies using dependency relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrysanne</forename><surname>Dimarco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bagpack: A general framework to represent semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaç</forename><surname>Herdaˇgdelenherdaˇgdelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Geometrical Models of Natural Language Semantics</title>
		<meeting>the Workshop on Geometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 4: evaluating chinese word similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="374" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distributional similarity for chinese: Exploiting characters and radicals. Mathematical Problems in Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David A Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">English verb classes and alternations: a preliminary investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CONLL</title>
		<meeting>CONLL<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Similarity involving attributes and relations: Judgments of similarity and difference are not inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas L Medin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedre</forename><surname>Robert L Goldstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="69" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using lexical and relational similarity to classify semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copestake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2009</title>
		<meeting>EACL 2009<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-03" />
			<biblScope unit="page" from="621" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining contextual and structural information for supersense tagging of chinese unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view chinese treebanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpus-based semantic class mining: Distributional vs. pattern-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpusbased learning of analogies and semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael L Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="251" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity of semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="416" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Distributional semantics beyond words: Supervised learning of analogy and paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.5042</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsing the penn chinese treebank with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuanglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueliang</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing-IJCNLP 2005</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="70" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A semantics oriented grammar for chinese treebanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="366" to="378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
