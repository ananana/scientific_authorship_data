<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1829" to="1838"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Network embeddings, which learn low-dimensional representations for each vertex in a large-scale network, have received considerable attention in recent years. For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. We propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices. We introduce a word-byword alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggrega-tion function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. Results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Networks are ubiquitous, with prominent exam- ples including social networks (e.g., Facebook, Twitter) or citation networks of research papers (e.g., arXiv). When analyzing data from these real-world networks, traditional methods often represent vertices (nodes) as one-hot representa- tions (containing the connectivity information of each vertex with respect to all other vertices), usu- ally suffering from issues related to the inherent sparsity of large-scale networks. This results in models that are not able to fully capture the re- lationships between vertices of the network <ref type="bibr" target="#b12">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b24">Tu et al., 2016)</ref>. Alternatively, network embedding (i.e., network representation learning) has been considered, representing each vertex of a network with a low-dimensional vec- tor that preserves information on its similarity rel- This paper investigates random walk graphs in high dimensional space.</p><p>We propose an algorithm for multidimensional random walk problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>citation</head><p>Figure 1: Example of the text information (abstracts) associated to two papers in a citation network. Key words for matching are highlighted.</p><p>ative to other vertices. This approach has attracted considerable attention in recent years ( <ref type="bibr" target="#b20">Tang and Liu, 2009;</ref><ref type="bibr" target="#b12">Perozzi et al., 2014;</ref><ref type="bibr" target="#b19">Tang et al., 2015;</ref><ref type="bibr" target="#b4">Grover and Leskovec, 2016;</ref><ref type="bibr" target="#b25">Wang et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b27">Wang et al., 2017a;</ref><ref type="bibr" target="#b30">Zhang et al., 2018)</ref>.</p><p>Traditional network embedding approaches fo- cus primarily on learning representations of ver- tices that preserve local structure, as well as inter- nal structural properties of the network. For in- stance, Isomap ( <ref type="bibr" target="#b22">Tenenbaum et al., 2000</ref>), LINE ( <ref type="bibr" target="#b19">Tang et al., 2015)</ref>, and <ref type="bibr">Grarep (Cao et al., 2015)</ref> were proposed to preserve first-, second-, and higher-order proximity between nodes, respec- tively. DeepWalk ( <ref type="bibr" target="#b12">Perozzi et al., 2014</ref>), which learns vertex representations from random-walk sequences, similarly, only takes into account struc- tural information of the network. However, in real- world networks, vertices usually contain rich tex- tual information (e.g., user profiles in Facebook, paper abstracts in arXiv, user-generated content on Twitter, etc.), which may be leveraged effectively for learning more informative embeddings.</p><p>To address this opportunity, <ref type="bibr" target="#b29">Yang et al. (2015)</ref> proposed text-associated DeepWalk, to incorpo- rate textual information into the vectorial rep- resentations of vertices (embeddings).  employed deep recurrent neural net- works to integrate the information from vertex-associated text into network representations. Fur- ther, <ref type="bibr" target="#b23">Tu et al. (2017)</ref> proposed to more effectively model the semantic relationships between vertices using a mutual attention mechanism.</p><p>Although these methods have demonstrated per- formance gains over structure-only network em- beddings, the relationship between text sequences for a pair of vertices is accounted for solely by comparing their sentence embeddings. How- ever, as shown in <ref type="figure">Figure 1</ref>, to assess the simi- larity between two research papers, a more effec- tive strategy would compare and align (via local- weighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (down- weighted). This alignment mechanism is diffi- cult to accomplish in models where text sequences are first embedded into a common space and then compared in pairs <ref type="bibr" target="#b6">(He and Lin, 2016;</ref><ref type="bibr" target="#b11">Parikh et al., 2016;</ref><ref type="bibr" target="#b26">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b28">Wang et al., 2017b;</ref><ref type="bibr" target="#b14">Shen et al., 2018a</ref>).</p><p>We propose to learn a semantic-aware Net- work Embedding (NE) that incorporates word- level alignment features abstracted from text se- quences associated with vertex pairs. Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an atten- tion mechanism), producing a set of fine-grained matching vectors. These features are then ac- cumulated via a simple but efficient aggregation function, obtaining the final representation for the sentence. As a result, the word-by-word alignment features (as illustrated in <ref type="figure">Figure 1</ref>) are explicitly and effectively captured by our model. Further, the learned network embeddings under our framework are adaptive to the specific (local) vertices that are considered, and thus are context-aware and espe- cially suitable for downstream tasks, such as link prediction. Moreover, since the word-by-word matching procedure introduced here is highly par- allelizable and does not require any complex en- coding networks, such as Long Short-Term Mem- ory (LSTM) or Convolutional Neural Networks (CNNs), our framework requires significantly less time for training, which is attractive for large-scale network applications.</p><p>We evaluate our approach on three real-world datasets spanning distinct network-embedding- based applications: link prediction, vertex classi- fication and visualization. We show that the pro- posed word-by-word alignment mechanism effi- ciently incorporates textual information into the network embedding, and consistently exhibits su- perior performance relative to several competi- tive baselines. Analyses considering the extracted word-by-word pairs further validate the effective- ness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>A network (graph) is defined as G = {V , E}, where V and E denote the set of N vertices (nodes) and edges, respectively, where elements of E are two-element subsets of V . Here we only consider undirected networks, however, our approach (introduced below) can be readily ex- tended to the directed case. We also define W , the symmetric R N ×N matrix whose elements, w ij , denote the weights associated with edges in V , and T , the set of text sequences assigned to each vertex. Edges and weights contain the structural information of the network, while the text can be used to characterize the semantic properties of each vertex. Given network G, with the network embedding we seek to encode each vertex into a low-dimensional vector h (with dimension much smaller than N ), while preserving structural and semantic features of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework Overview</head><p>To incorporate both structural and semantic infor- mation into the network embeddings, we specify two types of (latent) embeddings: (i) h s , the struc- tural embedding; and (ii) h t , the textual embed- ding.</p><formula xml:id="formula_0">Specifically, each vertex in G is encoded into a low-dimensional embedding h = [h s ; h t ].</formula><p>To learn these embeddings, we specify an objec- tive that leverages the information from both W and T , denoted as</p><formula xml:id="formula_1">L = e∈E L struct (e) + L text (e) + L joint (e) , (1)</formula><p>where L struct , L text and L joint denote structure, text, and joint structure-text training losses, re- spectively. For a vertex pair {v i , v j } weighted by</p><formula xml:id="formula_2">w ij , L struct (v i , v j ) in (1) is defined as (Tang et al., 2015) L struct (v i , v j ) = w ij log p(h i s |h j s ) ,<label>(2)</label></formula><p>where p(h i s |h j s ) denotes the conditional proba- bility between structural embeddings for vertices {v i , v j }. To leverage the textual information in T , similar text-specific and joint structure-text train- ing objectives are also defined</p><formula xml:id="formula_3">L text (v i , v j ) = w ij α 1 log p(h i t |h j t ) ,<label>(3)</label></formula><formula xml:id="formula_4">L joint (v i , v j ) = w ij α 2 log p(h i t |h j s ) (4) + w ij α 3 log p(h i s |h j t ) ,<label>(5)</label></formula><p>where p(h i t |h j t ) and p(h i t |h j s ) (or p(h i s |h j t )) de- note the conditional probability for a pair of text embeddings and text embedding given structure embedding (or vice versa), respectively, for ver- tices {v i , v j }. Further, α 1 , α 2 and α 3 are hyper- parameters that balance the impact of the different training-loss components. Note that structural em- beddings, h s , are treated directly as parameters, while the text embeddings h t are learned based on the text sequences associated with vertices.</p><p>For all conditional probability terms, we follow <ref type="bibr" target="#b19">Tang et al. (2015)</ref> and consider the second-order proximity between vertex pairs. Thus, for vertices {v i , v j }, the probability of generating h i condi- tioned on h j may be written as</p><formula xml:id="formula_5">p(h i |h j ) = exp h j T h i N k=1 exp h j T h k .<label>(6)</label></formula><p>Note that (6) can be applied to both structural and text embeddings in (2) and (3). Inspired by <ref type="bibr" target="#b23">Tu et al. (2017)</ref>, we further as- sume that vertices in the network play different roles depending on the vertex with which they interact. Thus, for a given vertex, the text em- bedding, h t , is adaptive (specific) to the vertex it is being conditioned on. This type of context- aware textual embedding has demonstrated supe- rior performance relative to context-free embed- dings ( <ref type="bibr" target="#b23">Tu et al., 2017</ref>). In the following two sections, we describe our strategy for encoding the text sequence associated with an edge into its adaptive textual embedding, via word-by-context and word-by-word alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word-by-Context Alignment</head><p>We first introduce our base model, which re- weights the importance of individual words within a text sequence in the context of the edge be- ing considered. Consider text sequences associ- ated with two vertices connected by an edge, de-  <ref type="figure">Figure 2</ref>: Schematic of the proposed fine-grained word alignment module for incorporating textual information into a network embedding. In this setup, word-by-word matching features are explicitly abstracted to infer the relationship between vertices.</p><p>noted t a and t b and contained in T . Text se- quences t a and t b are of lengths M a and M b , re- spectively, and are represented by X a ∈ R d×Ma and X b ∈ R d×M b , respectively, where d is the di- mension of the word embedding. Further, x (i) a de- notes the embedding of the i-th word in sequence t a .</p><p>Our goal is to encode text sequences t a and t b into counterpart-aware vectorial representations h a and h b . Thus, while inferring the adaptive tex- tual embedding for sentence t a , we propose re- weighting the importance of each word in t a to explicitly account for its alignment with sentence t b . The weight α i , corresponding to the i-th word in t a , is generated as:</p><formula xml:id="formula_6">α i = exp(tanh(W 1 c b + W 2 x (i) a )) Ma j=1 exp(tanh(W 1 c b + W 2 x (j) a ))</formula><p>, <ref type="formula">(7)</ref> where W 1 and W 2 are model parameters and</p><formula xml:id="formula_7">c b = M b i=1 x b i</formula><p>is the context vector of sequence t b , obtained by simply averaging over all the word embeddings in the sequence, similar to fastText <ref type="bibr" target="#b8">(Joulin et al., 2016)</ref>. Further, the word-by-context embedding for sequence t a is obtained by taking the weighted average over all word embeddings</p><formula xml:id="formula_8">h a = Ma i=1 α i x (i) a .<label>(8)</label></formula><p>Intuitively, α i may be understood as the relevance score between the ith word in t a and sequence t b . Specifically, keywords within t a , in the context of t b , should be assigned larger weights, while less important words will be correspondingly down- weighted. Similarly, h b is encoded as a weighted embedding using <ref type="formula">(7)</ref> and <ref type="formula" target="#formula_8">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fine-Grained Word-by-Word Alignment</head><p>With the alignment in the previous section, word- by-context matching features α i are modeled; however, the word-by-word alignment information (fine-grained), which is key to characterize the re- lationship between two vertices (as discussed in the above), is not explicitly captured. So moti- vated, we further propose an architecture to explic- itly abstract word-by-word alignment information from t a and t b , to learn the relationship between the two vertices. This is inspired by the recent success of Relation Networks (RNs) for relational reasoning ( <ref type="bibr" target="#b13">Santoro et al., 2017)</ref>. As illustrated in <ref type="figure">Figure 2</ref>, given two input em- bedding matrices X a and X b , we first compute the affinity matrix A ∈ R M b ×Ma , whose elements represent the affinity scores corresponding to all word pairs between sequences t a and t b</p><formula xml:id="formula_9">A = X T b X a .<label>(9)</label></formula><p>Subsequently, we compute the context-aware ma- trix for sequence t b as</p><formula xml:id="formula_10">A b = softmax(A) , X b = X b A b ,<label>(10)</label></formula><p>where the softmax(·) function is applied column- wise to A, and thus A b contains the attention weights (importance scores) across sequence t b (columns), which account for each word in se- quence t a (rows). Thus, matrix X b ∈ R d×Ma in (10) constitutes an attention-weighted embed- ding for X b . Specifically, the i-th column of</p><formula xml:id="formula_11">X b , denoted as x (i)</formula><p>b , can be understood as a weighted average over all the words in t b , where higher at- tention weights indicate better alignment (match) with the i-th word in t a .</p><p>To abstract the word-by-word alignments, we compare x</p><formula xml:id="formula_12">(i) a with x (i)</formula><p>b , for i = 1, 2, ..., M a , to obtain the corresponding matching vector</p><formula xml:id="formula_13">m (i) a = f align x (i) a , x (i) b ,<label>(11)</label></formula><p>where f align (·) represents the alignment function. Inspired by the observation in <ref type="bibr" target="#b26">Wang and Jiang (2017)</ref> that simple comparison/alignment func- tions based on element-wise operations exhibit ex- cellent performance in matching text sequences, here we use a combination of element-wise sub- traction and multiplication as</p><formula xml:id="formula_14">f align (x (i) a , x (i) a ) = [x (i) a − x (i) a ; x (i) a x (i) a ] ,</formula><p>where denotes the element-wise Hadamard product, then these two operations are concate- nated to produce the matching vector m</p><formula xml:id="formula_15">(i)</formula><p>a . Note these operators may be used individually or com- bined as we will investigate in our experiments.</p><p>Subsequently, matching vectors from (11) are aggregated to produce the final textual embedding h a t for sequence t a as</p><formula xml:id="formula_16">h a t = f aggregate m (1) a , m (2) a , ..., m (Ma) a ,<label>(12)</label></formula><p>where f aggregate denotes the aggregation function, which we specify as the max-pooling pooling op- eration. Notably, other commutative operators, such as summation or average pooling, can be otherwise employed. Although these aggregation functions are simple and invariant to the order of words in input sentences, they have been demon- strated to be highly effective in relational reason- ing ( <ref type="bibr" target="#b11">Parikh et al., 2016;</ref><ref type="bibr" target="#b13">Santoro et al., 2017)</ref>. To further explore this, in Section 5.3, we conduct an ablation study comparing different choices of alignment and aggregation functions. The representation h b can be obtained in a simi- lar manner through (9), (10), <ref type="formula" target="#formula_13">(11)</ref> and <ref type="formula" target="#formula_2">(12)</ref>, but re- placing (9) with A = X T a X b (its transpose). Note that this word-by-word alignment is more com- putationally involved than word-by-context; how- ever, the former has substantially fewer parame- ters to learn, provided we no longer have to esti- mate the parameters in (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training and Inference</head><p>For large-scale networks, computing and optimiz- ing the conditional probabilities in (1) using (6) is computationally prohibitive, since it requires the summation over all vertices V in G. To address this limitation, we leverage the negative sampling strategy introduced by <ref type="bibr" target="#b10">Mikolov et al. (2013)</ref>, i.e., we perform computations by sampling a subset of negative edges. As a result, the conditional in (6) can be rewritten as: v , where d v is the out-degree of vertex v ∈ V . The number of nega- tive samples K is treated as a hyperparameter. We use Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2014</ref>) to update the model parameters while minimizing the objective in (1).</p><formula xml:id="formula_17">p(h i |h j ) = log σ h j T h i + K i=1 E h i ∼P (v) log σ −h j T h i ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Network embedding methods can be divided into two categories: (i) methods that solely rely on the structure, e.g., vertex information; and (ii) meth- ods that leverage both the structure the network and the information associated with its vertices.</p><p>For the first type of models, DeepWalk ( <ref type="bibr" target="#b12">Perozzi et al., 2014</ref>) has been proposed to learn node rep- resentations by generating node contexts via trun- cated random walks; it is similar to the concept of Skip-Gram ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>), originally introduced for learning word embeddings. LINE ( <ref type="bibr" target="#b19">Tang et al., 2015)</ref> proposed a principled objective to explicitly capture first-order and second-order proximity information from the vertices of a net- work. Further, <ref type="bibr" target="#b4">Grover and Leskovec (2016)</ref> in- troduced a biased random walk procedure to gen- erate the neighborhood for a vertex, which infers the node representations by maximizing the like- lihood of preserving the local context information of vertices. However, these algorithms generally ignore rich heterogeneous information associated with vertices. Here, we focus on incorporating tex- tual information into network embeddings.</p><p>To learn semantic-aware network embeddings, Text-Associated DeepWalk (TADW) ( <ref type="bibr" target="#b29">Yang et al., 2015)</ref> proposed to integrate textual features into network representations with matrix factoriza- tion, by leveraging the equivalence between Deep- Walk and matrix factorization. CENE (Content- Enhanced Network Embedding) (  used bidirectional recurrent neural networks to ab- stract the semantic information associated with vertices, which further demonstrated the advan- tages of employing textual information. To cap- ture the interaction between sentences of vertex pairs, <ref type="bibr" target="#b23">Tu et al. (2017)</ref> further proposed Context- Aware Network Embedding (CANE), that em- ploys a mutual attention mechanism to adaptively account for the textual information from neigh- boring vertices. Despite showing improvement over structure-only models, these semantic-aware methods cannot capture word-level alignment in- formation, which is important for inferring the re- lationship between node pairs, as previously dis- cussed. In this work, we introduce a Word- Alignment-based Network Embedding (WANE) framework, which aligns and aggregates word-by- word matching features in an explicit manner, to obtain more informative network representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets We investigate the effectiveness of the proposed WANE model on two standard network- embedding-based tasks, i.e., link prediction and multi-label vertex classification. The following three real-world datasets are employed for quan- titative evaluation: (i) Cora, a standard paper ci- tation network that contains 2,277 machine learn- ing papers (vertices) grouped into 7 categories and connected by 5,214 citations (edges) (ii) HepTh, another citation network of 1,038 papers with ab- stract information and 1,990 citations; (iii) Zhihu, a network of 10,000 active users from Zhihu, the largest Q&amp;A website in China, where 43,894 ver- tices and descriptions of the Q&amp;A topics are avail- able. The average lengths of the text in the three datasets are 90, 54, and 190, respectively. To make direct comparison with existing work, we employed the same preprocessing procedure 1 of <ref type="bibr" target="#b23">Tu et al. (2017)</ref>.</p><p>Training Details For fair comparison with CANE ( <ref type="bibr" target="#b23">Tu et al., 2017)</ref>, we set the dimension of network embedding for our model to 200. The number of negative samples K is selected from {1, 3, 5} according to performance on the valida- tion set. We set the batch size as 128, and the model is trained using Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2014)</ref>, with a learning rate of 1 × 10 −3 for all pa- rameters. Dropout regularization is employed on the word embedding layer, with rate selected from {0.5, 0.7, 0.9}, also on the validation set. Our code will be released to encourage future research.</p><p>Baselines To evaluate the effectiveness of our framework, we consider several strong baseline methods for comparisons, which can be catego- rized into two types: (i) models that only ex- ploit structural information: MMB ( <ref type="bibr" target="#b0">Airoldi et al., 2008</ref>), DeepWalk ( <ref type="bibr" target="#b12">Perozzi et al., 2014</ref>), LINE ( <ref type="bibr" target="#b19">Tang et al., 2015)</ref>, and node2vec <ref type="bibr" target="#b4">(Grover and Leskovec, 2016)</ref>. (ii) Models that take both structural and textual information into account: Naive combination (which simply concatenates the structure-based embedding with CNN-based text embeddings, as explored in ( <ref type="bibr" target="#b23">Tu et al., 2017</ref>   2016), and CANE ( <ref type="bibr" target="#b23">Tu et al., 2017)</ref>. It is worth not- ing that unlike all these baselines, WANE explic- itly captures word-by-word interactions between text sequence pairs.</p><p>Evaluation Metrics We employ AUC <ref type="bibr" target="#b5">(Hanley and McNeil, 1982)</ref> as the evaluation metric for link prediction, which measures the probability that vertices within an existing edge, randomly sampled from the test set, are more similar than those from a random pair of non-existing vertices, in terms of the inner product between their corre- sponding embeddings. For multi-label vertex classification and to en- sure fair comparison, we follow <ref type="bibr" target="#b29">Yang et al. (2015)</ref> and employ a linear SVM on top of the learned network representations, and evaluate classifica- tion accuracy with different training ratios (vary- ing from 10% to 50%). The experiments for each setting are repeated 10 times and the average test accuracy is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We experiment with three variants for our WANE model: (i) WANE: where the word embeddings of each text sequence are simply average to ob- tain the sentence representations, similar to ( <ref type="bibr" target="#b8">Joulin et al., 2016;</ref><ref type="bibr" target="#b16">Shen et al., 2018c</ref>). (ii) WANE- wc: where the textual embeddings are inferred with word-by-context alignment. (iii) WANE-ww: where the word-by-word alignment mechanism is leveraged to capture word-by-word matching fea- tures between available sequence pairs. <ref type="table" target="#tab_2">Table 1</ref> presents link prediction results for all mod- els on Cora dataset, where different ratios of edges are used for training. It can be observed that when only a small number of edges are available, e.g., 15%, the performances of structure-only methods is much worse than semantic-aware models that have taken textual information into consideration The perfromance gap tends to be smaller when a larger proportion of edges are employed for train- ing. This highlights the importance of incorporat- ing associated text sequences into network embed- dings, especially in the case of representing a rela- tively sparse network. More importantly, the pro- posed WANE-ww model consistently outperforms other semantic-aware NE models by a substantial margin, indicating that our model better abstracts word-by-word alignment features from the text se- quences available, thus yields more informative network representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Link Prediction</head><p>Further, WANE-ww also outperforms WANE or WANE-wc on a wide range of edge training pro-%Training Edges 15% 25% 35% 45% 55% 65% 75% 85% 95% MMB 51.0 51.5 53.7 58.6 61.   portions. This suggests that: (i) adaptively as- signing different weights to each word within a text sequence (according to its paired sequence) tends to be a better strategy than treating each word equally (as in WANE). (ii) Solely consid- ering the context-by-word alignment features (as in WANE-wc) is not as efficient as abstracting word-by-word matching information from text se- quences. We observe the same trend and the supe- riority of our WANE-ww models on the other two datasets, HepTh and Zhihu datasets, as shown in <ref type="table" target="#tab_3">Table 2</ref> and 3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-label Vertex Classification</head><p>We further evaluate the effectiveness of proposed framework on vertex classification tasks with the Cora dataset. Similar to <ref type="bibr" target="#b23">Tu et al. (2017)</ref>, we gen- erate the global embedding for each vertex by tak- ing the average over its context-aware embeddings with all other connected vertices. As shown in <ref type="figure" target="#fig_1">Fig- ure 3(c)</ref>, semantic-aware NE methods (including naive combination, TADW, CENE, CANE) ex- hibit higher test accuracies than semantic-agnostic models, demonstrating the advantages of incor- porating textual information. Moreover, WANE- ww consistently outperforms other competitive semantic-aware models on a wide range of labeled proportions, suggesting that explicitly capturing word-by-word alignment features is not only use- ful for vertex-pair-based tasks, such as link pre- diction, but also results in better global embed- dings which are required for vertex classification tasks. These observations further demonstrate that WANE-ww is an effective and robust framework to extract informative network representations.</p><p>Semi-supervised classification We further con- sider the case where the training ratio is less than 10%, and evaluate the learned network embedding with a semi-supervised classifier. Following <ref type="bibr" target="#b29">Yang et al. (2015)</ref>, we employ a Transductive SVM (TSVM) classifier with a linear kernel (Joachims, 1998) for fairness. As illustrated in <ref type="table" target="#tab_7">Table 4</ref>, the proposed WANE-ww model exhibits superior per- formances in most cases. This may be due to the fact that WANE-ww extracts information from the vertices and text sequences jointly, thus the ob- tained vertex embeddings are less noisy and per- form more consistently with relatively small train- ing ratios (Yang et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Motivated by the observation in <ref type="bibr" target="#b26">Wang and Jiang (2017)</ref> that the advantages of different functions to match two vectors vary from task to task, we further explore the choice of alignment and ag- gregation functions in our WANE-ww model. To match the word pairs between two sequences, we experimented with three types of operations: sub-  <ref type="bibr" target="#b26">Wang and Jiang (2017)</ref>, where they found that simple comparison functions based on element- wise operations work very well on matching text sequences.</p><p>In terms of the aggregation functions, we com- pare (one-layer) CNN, mean-pooling, and max- pooling operations to accumulate the matching vectors. As shown in <ref type="figure" target="#fig_1">Figure 3(b)</ref>, max-pooling has the best empirical results on all three datasets. This may be attributed to the fact that the max- pooling operation is better at selecting impor- tant word-by-word alignment features, among all matching vectors available, to infer the relation- ship between vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>Embedding visualization To visualize the learned network representations, we further em- ploy t-SNE to map the low-dimensional vectors of the vertices to a 2-D embedding space. We use the Cora dataset because there are labels associated with each vertex and WANE-ww to obtain the network embeddings.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 4</ref> where each point indicates one paper (vertex), and the color of each point in- dicates the category it belongs to, the embeddings of the same label are indeed very close in the 2-D plot, while those with different labels are relatively farther from each other. Note that the model is not trained with any label information, indicating that WANE-ww has extracted meaningful patterns from the text and vertex information available.  Case study The proposed word-by-word align- ment mechanism can be used to highlight the most informative words (and the corresponding match- ing features) wrt the relationship between ver- tices. We visualize the norm of matching vec- tor obtained in (11) in <ref type="figure" target="#fig_3">Figure 5</ref> for the Cora dataset. It can be observed that matched key words, e.g., 'MCMC', 'convergence', between the text sequences are indeed assigned higher values in the matching vectors. These words would be se- lected preferentially by the final max-pooling ag- gregation operation. This indicates that WANE- ww is able to abstract important word-by-word alignment features from paired text sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented a novel framework to in- corporate the semantic information from vertex- associated text sequences into network embed- dings. An align-aggregate framework is intro- duced, which first aligns a sentence pair by captur- ing the word-by-word matching features, and then adaptively aggregating these word-level alignment information with an efficient max-pooling func- tion. The semantic features abstracted are fur- ther encoded, along with the structural informa- tion, into a shared space to obtain the final net- work embedding. Compelling experimental re- sults on several tasks demonstrated the advantages of our approach. In future work, we aim to lever- age abundant unlabeled text data to abstract more informative sentence representations <ref type="bibr" target="#b3">(Dai and Le, 2015;</ref><ref type="bibr" target="#b21">Tang and de Sa, 2018)</ref> . Another interesting direction is to learn binary and compact network embed- ding, which could be more efficient in terms of both computation and memory, relative to its con- tinuous counterpart <ref type="bibr" target="#b15">(Shen et al., 2018b</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>where</head><label></label><figDesc>σ(x) = 1/(1 + exp(−x)) is the sigmoid function. Following Mikolov et al. (2013), we set the noise distribution P (v) ∝ d 3/4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a, b) Ablation study on the choice of different alignment and aggregation functions. (c) Test accuracy of supervised vertex classification on the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE visualization of the learned network embeddings on the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the word-level matching vectors. Darker shades represent larger values of the norm of m (i) at each word position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>AUC scores for link prediction on the Cora dataset. 

%Training Edges 
15% 25% 35% 45% 55% 65% 75% 85% 95% 
MMB 
54.6 
57.9 
57.3 
61.6 
66.2 
68.4 
73.6 
76.0 
80.3 
DeepWalk 
55.2 
66.0 
70.0 
75.7 
81.3 
83.3 
87.6 
88.9 
88.0 
LINE 
53.7 
60.4 
66.5 
73.9 
78.5 
83.8 
87.5 
87.7 
87.6 
node2vec 
57.1 
63.6 
69.9 
76.2 
84.3 
87.3 
88.4 
89.2 
89.2 
Naive combination 
78.7 
82.1 
84.7 
88.7 
88.7 
91.8 
92.1 
92.0 
92.7 
TADW 
87.0 
89.5 
91.8 
90.8 
91.1 
92.6 
93.5 
91.9 
91.7 
CENE 
86.2 
84.6 
89.8 
91.2 
92.3 
91.8 
93.2 
92.9 
93.2 
CANE 
90.0 
91.2 
92.0 
93.0 
94.2 
94.6 
95.4 
95.7 
96.3 
WANE 
88.5 
90.7 
91.1 
92.6 
93.5 
94.2 
94.9 
95.3 
95.8 
WANE-wc 
90.1 
91.4 
91.9 
94.1 
95.3 
95.9 
96.5 
96.9 
97.2 
WANE-ww 
92.3 
94.1 
95.7 
96.7 
97.5 
97.5 
97.7 
98.2 
98.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>AUC scores for link prediction on the HepTh dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>AUC scores for link prediction on the Zhihu dataset. 

Cora 
HepTh 
Zhihu 

Dataset 

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

AUC 

Subtraction 
Multiplication 
Sub &amp; Multi 

Cora 
HepTh 
Zhihu 

Dataset 

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

AUC 

CNN 
Mean-Pooling 
Max-Pooling 

10 
20 
30 
40 
50 
Proportion of Labeled Vertices (%) 

0.7 

0.8 

0.9 

Test Accuracy (%) 

LINE 
Deep Walk 
Naive Combination 
TADW 
CENE 
CANE 
WANE-FG 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Semi-supervised vertex classification results 
on the Cora dataset. 

</table></figure>

			<note place="foot" n="1"> https://github.com/thunlp/CANE</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporate group information to enhance network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1901" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (roc) curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making large-scale svm learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SFB 475: Komplexitätsreduktion in Multivariaten Datenstrukturen</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Universität Dortmund</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning context-sensitive convolutional filters for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Renqiang Min Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nash: Toward end-to-end neural architecture for generative semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paidamoyo</forename><surname>Chapfuwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05361</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07109</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A general framework for content-enhanced network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02906</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-view sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia R De</forename><surname>Sa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07443</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cane: Context-aware network embedding for relation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1722" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Max-margin deepwalk: Discriminative learning of network representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3889" to="3895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Diffusion maps for textual network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09906</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deconvolutional paragraph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4169" to="4179" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
