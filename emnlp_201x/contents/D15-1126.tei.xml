<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Phrase Table Triangulation with Neural Word Embeddings for Low-Resource Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Levinboim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Phrase Table Triangulation with Neural Word Embeddings for Low-Resource Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we develop a supervised learning technique that improves noisy phrase translation scores obtained by phrase table triangulation. In particular, we extract word translation distributions from small amounts of source-target bilingual data (a dictionary or a parallel corpus) with which we learn to assign better scores to translation candidates obtained by trian-gulation. Our method is able to gain improvement in translation quality on two tasks: (1) On Malagasy-to-French translation via English, we use only 1k dictionary entries to gain +0.5 Bleu over triangula-tion. (2) On Spanish-to-French via English we use only 4k sentence pairs to gain +0.7 Bleu over triangulation interpolated with a phrase table extracted from the same 4k sentence pairs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phrase-based statistical machine translation sys- tems require considerable amounts of source- target parallel data to produce good quality trans- lation. However, large amounts of parallel data are available for only a fraction of language pairs, and mostly when one of the languages is English.</p><p>Phrase table triangulation <ref type="bibr" target="#b7">(Utiyama and Isahara, 2007;</ref><ref type="bibr" target="#b0">Cohn and Lapata, 2007;</ref><ref type="bibr" target="#b8">Wu and Wang, 2007</ref>) is a method for generating source- target phrase tables without having access to any source-target parallel data. The intuition behind triangulation (and pivoting techniques in general) is the transitivity of translation: if a source lan- guage phrase s translates to a pivot language phrase p which in turn translates to a target lan- guage phrase t, then s should likely translate to t. Following this intuition, a triangulated source- target phrase tablê T can be composed from a source-pivot and pivot-target phrase table ( §2).</p><p>However, the resulting triangulated phrase tablêtablê T contains many spurious phrase pairs and noisy probability estimates. Therefore, early triangula- tion work ( <ref type="bibr" target="#b8">Wu and Wang, 2007</ref>) already realis- tically assumed access to a limited source-target parallel data from which a relatively high-quality source-target phrase table T can be directly esti- mated. The two phrase tables were then combined, resulting in a higher quality phrase table that pro- poses translations for many source phrases not found in T . <ref type="bibr" target="#b8">Wu and Wang (2007)</ref> report that in- terpolation of the two phrase tables T andˆTandˆ andˆT leads to higher quality translations. However, the trian- gulated phrase tablê T is obtained without using the source-target bilingual data, which suggests that the source-target data is not used as fully as it could be.</p><p>In this paper, we develop a supervised learning algorithm that corrects triangulated word transla- tion probabilities by relying on word translation distributions w sup derived from the limited source- target data. In particular, we represent source and target words using word embeddings <ref type="bibr" target="#b6">(Mikolov et al., 2013)</ref> and learn a transformation between the two embedding spaces in order to approxi- mate w sup , thus down-weighting incorrect trans- lation candidates proposed by triangulation ( §3). By representing words as embeddings, our model can generalize the information contained in the source-target data (as encoded in the distributions w sup ) to a much larger vocabulary, and can as- sign lexical-weighting probabilities to most of the phrase pairs inˆTinˆ inˆT .</p><p>Fixing English as the pivot language (the most realistic pivot language choice), on a low- resource Spanish-to-French translation task our model gains +0.7 Bleu on top of standard phrase table interpolation. On Malagasy-to-French trans- lation, our model gains +0.5 Bleu on top of tri- angulation when using only 1k Malagasy-French dictionary entries ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let s, p, t denote words and s, p, t denote phrases in the source, pivot, and target languages, respec- tively. Also, let T denote a phrase table estimated over a parallel corpus andˆTandˆ andˆT denote a triangu- lated phrase table. We use similar notation for their respective phrase translation features φ, lexical- weighting features lex, and the word translation probabilities w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Triangulation (weak baseline)</head><p>In phrase table triangulation, a source-target phrase table T st is constructed by combining a source-pivot and pivot-target phrase table T sp , T pt , each estimated on its respective parallel data. For each resulting phrase pair (s, t), we can also com- pute an alignmentâalignmentˆalignmentâ as the most frequent align- ment obtained by combining source-pivot and pivot-target alignments a sp and a pt across all pivot phrases p as follows:</p><formula xml:id="formula_0">{(s, t) | ∃p : (s, p) ∈ a sp ∧ (p, t) ∈ a pt }.</formula><p>The triangulated source-to-target lexical weights, denoted lex st , are approximated in two steps: First, word translation scoresˆwscoresˆ scoresˆw st are ap- proximated by marginalizing over the pivot words:</p><formula xml:id="formula_1">ˆ w st (t | s) = p w sp (p | s) · w pt (t | p).<label>(1)</label></formula><p>Next, given a (triangulated) phrase pair (s, t) with alignmentâalignmentˆalignmentâ, letâletˆletâ s,: = {t | (s, t) ∈ ˆ a}; the lexical- weighting probability is ( <ref type="bibr" target="#b2">Koehn et al., 2003)</ref>:</p><formula xml:id="formula_2">lex st (t | s, ˆ a) = s∈s 1 |â s,: | t∈â s,: ˆ w st (t | s). (2)</formula><p>The triangulated phrase translation scores, de- notedˆφnotedˆ notedˆφ st , are computed by analogy with Eq. 1.</p><p>We also compute these scores in the reverse direction by swapping the source and target lan- guages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interpolation (strong baseline)</head><p>Given access to source-target data, an ordinary source-target phrase table T st can be estimated di- rectly. <ref type="bibr" target="#b8">Wu and Wang (2007)</ref> suggest interpolating phrase pairs entries that occur in both tables:</p><formula xml:id="formula_3">T interp = αT st + (1 − α) ˆ T st .<label>(3)</label></formula><p>Phrase pairs appearing in only one phrase table are added as-is. We refer to the resulting table as the interpolated phrase table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supervised Word Translations</head><p>While interpolation (Eq. 3) may help correct some of the noisy triangulated scores, its effect is lim- ited to phrase pairs appearing in both phrase ta- bles. Here, we suggest a discriminative supervised learning method that can affect all phrase pairs. Our idea is to regard word translation distri- butions derived from source-target bilingual data (through word alignments or dictionary entries) as the correct translation distributions, and use them to learn discriminately: correct target words should become likely translations, and incorrect ones should be down-weighted. To generalize be- yond the vocabulary of the source-target data, we appeal to word embeddings.</p><p>We present our formulation in the source-to- target direction. The target-to-source direction is obtained simply by swapping the source and tar- get languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Let c sup st denote the number of times source word s was aligned to target word t (in word alignment, or in the dictionary). We define the word transla- Clearly, the solution q(· | s) := w sup (· | s) maxi- mizes L. However, we would like a solution that generalizes to source words s beyond those ob- served in the source-target corpus -in particular, those source words that appear in the triangulated phrase tablê T , but not in T . In order to generalize, we abstract from words to vector representations of words. Specifically, we constrain q to the following parameterization:</p><formula xml:id="formula_4">q(t | s) = 1 Z s exp v T s Av t + f T st h Z s = t∈T (s) exp v T s Av t + f T st h .</formula><p>Here, the vectors v s and v t represent monolingual features and the vector f st represents bilingual fea- tures. The parameters A and h are to be learned.</p><p>In this work, we use monolingual word embed- dings for v s and v t , and set the vector f st to con- tain only the value of the triangulated score, such that f st := ˆ w st . Therefore, the matrix A is a lin- ear transformation between the source and target embedding spaces, and h (now a scalar) quantifies how the triangulated scoresˆwscoresˆ scoresˆw are to be trusted.</p><p>In the normalization factor Z s , we let t range only over possible translations of s suggested by either w sup or the triangulated word probabilities. That is:</p><formula xml:id="formula_5">T (s) = {t | w sup (t | s) &gt; 0 ∨ ˆ w(t | s) &gt; 0}.</formula><p>This restriction makes efficient computation pos- sible, as otherwise the normalization term would have to be computed over the entire target vocab- ulary. Under this parameterization, our goal is to solve the following maximization problem:</p><formula xml:id="formula_6">max A,h L(A, h) = max A,h s,t c sup st log q(t | s). (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization</head><p>The objective function in Eq. 4 is concave in both A and h. This is because after taking the log, we are left with a weighted sum of linear and concave (negative log-sum-exp) terms in A and h. We can therefore reach the global solution of the problem using gradient descent.</p><p>Taking derivatives, the gradient is For quick results, we limited the number of gra- dient steps to 200 and selected the iteration that minimized the total variation distance to w sup over a held out dev set:</p><formula xml:id="formula_7">s ||q(· | s) − w sup (· | s)|| 1 .<label>(5)</label></formula><p>We obtained better convergence rate by us- ing a batch version of the effective and easy- to-implement Adagrad technique ( <ref type="bibr" target="#b1">Duchi et al., 2011)</ref>. See <ref type="figure" target="#fig_3">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Re-estimating lexical weights</head><p>Having learned the model (A and h), we can now use q(t | s) to estimate the lexical weights (Eq. 2) of any aligned phrase pairs (s, t, ˆ a), assuming it is composed of embeddable words. French to Spanish train, with adagrad dev, with adagrad train, no adagrad dev, no adagrad However, we found the supervised word trans- lation scores q to be too sharp, sometimes assign- ing all probability mass to a single target word. We therefore interpolated q with the triangulated word translation scoresˆwscoresˆ scoresˆw:</p><formula xml:id="formula_8">q β = βq + (1 − β) ˆ w.<label>(6)</label></formula><p>To integrate the lexical weights induced by q β (Eq. 2), we simply appended them as new features in the phrase table in addition to the existing lexi- cal weights. Following this, we can search for a β value that maximizes Bleu on a tuning set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary of method</head><p>In summary, to improve upon a triangulated or in- terpolated phrase table, we:</p><p>1. Learn word translation distributions q by super- vision against distributions w sup derived from the source-target bilingual data ( §3.1).</p><p>2. Smooth the learned distributions q by interpo- lating with triangulated word translation scoresˆw scoresˆ scoresˆw ( §3.3).</p><p>3. Compute new lexical weights and append them to the phrase table ( §3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To test our method, we conducted two low- resource translation experiments using the phrase-based MT system Moses ( <ref type="bibr" target="#b3">Koehn et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>Fixing the pivot language to English, we applied our method on two data scenarios:</p><p>1. Spanish-to-French: two related languages used to simulate a low-resource setting. The baseline is phrase table interpolation (Eq. 3).</p><p>2. Malagasy-to-French: two unrelated languages for which we have a small dictionary, but no parallel corpus (aside from tuning and testing data). The baseline is triangulation alone (there is no source-target model to interpolate with). <ref type="table">Table 1</ref> lists some statistics of the bilin- gual data we used. European-language bitexts were extracted from Europarl ( <ref type="bibr" target="#b5">Koehn, 2005</ref>). For Malagasy-English, we used the Global Voices par- allel data available online. <ref type="bibr">1</ref> The Malagasy-French dictionary was extracted from online resources 2 and the small Malagasy-French tune/test sets were extracted 3 from Global Voices.</p><p>lines of data language pair train tune test sp-fr 4k 1.5k 1.5k mg-fr 1.1k 1.2k 1.2k sp-en 50k - - mg-en 100k - - en-fr 50k - - <ref type="table">Table 1</ref>: Bilingual datasets. Legend: sp=Spanish, fr=French, en=English, mg=Malagasy. <ref type="table">Table 2</ref> lists token statistics of the monolin- gual data used. We used word2vec 4 to generate French, Spanish and Malagasy word embeddings. The French and Spanish embeddings were (in- dependently) estimated over their combined to- kenized and lowercased Gigaword <ref type="bibr">5</ref> and Leipzig news corpora. <ref type="bibr">6</ref> The Malagasy embeddings were similarly estimated over data form Global Voices, <ref type="bibr">7</ref> the Malagasy Wikipedia and the Malagasy Com- mon Crawl. 8 In addition, we estimated a 5-gram French language model over the French monolin- gual data.  1.5G Spanish 1.4G Malagasy 58M <ref type="table">Table 2</ref>: Size of monolingual corpus per language as measured in number of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spanish-French Results</head><p>To produce w sup , we aligned the small Spanish- French parallel corpus in both directions, and symmetrized using the intersection heuristic. This was done to obtain high precision alignments (the often-used grow-diag-final-and heuristic is opti- mized for phrase extraction, not precision).</p><p>We used the skip-gram model to estimate the Spanish and French word embeddings and set the dimension to d = 200 and context window to w = 5 (default). Subsequently, to run our method, we filtered out source and target words that either did not appear in the triangulation, or, did not have an embedding. We took words that appeared more than 10 times in the parallel corpus for the training set (∼690 words), and between 5-9 times for the held out dev set (∼530 words). This was done in both source-target and target-source directions.</p><p>In <ref type="table">Table 3</ref> we show that the distributions learned by our method are much better approximations of w sup compared to those obtained by triangulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>source→target target→source triangulation 71.6% 72.0% our scores 30.2% 33.8% <ref type="table">Table 3</ref>: Average total variation distance (Eq. 5) to the dev set portion of w sup (computed only over words whose translations in w sup appear in the tri- angulation). Using word embeddings, our method is able to better generalize on the dev set.</p><p>We then examined the effect of appending our supervised lexical weights. We fixed the word level interpolation β := 0.95 (effectively assigning very little mass to triangulated word translationsˆw translationsˆ translationsˆw) and searched for α ∈ {0.9, 0.8, 0.7, 0.6} in Eq. 3 to maximize Bleu on the tuning set.</p><p>Our MT results are reported in <ref type="table" target="#tab_1">Table 4</ref>. While interpolation improves over triangulation alone by +0.8 Bleu, our method adds another +0.7 Bleu on top of interpolation, a statistically significant gain (p &lt; 0.01) according to a bootstrap resampling significance test <ref type="bibr" target="#b4">(Koehn, 2004</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Malagasy-French Results</head><p>For Malagasy-French, the w sup distributions used for supervision were taken to be uniform distri- butions over the dictionary translations. For each training direction, we used a 70%/30% split of the dictionary to form the train and dev sets.</p><p>Having significantly less Malagasy monolin- gual data, we used d = 100 dimensional embed- dings and a w = 3 context window to estimate both Malagasy and French words.</p><p>As before, we added our supervised lexical weights as new features in the phrase table. How- ever, instead of fixing β = 0.95 as above, we searched for β ∈ {0.9, 0.8, 0.7, 0.6} in Eq. 6 to max- imize Bleu on a small tune set. We report our re- sults in <ref type="table">Table 5</ref>. Using only a dictionary, we are able to improve over triangulation by +0.5 Bleu, a statistically significant difference (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>β tune test triangulation -12.2 11.1 triangulation+our scores 0.6 12.4 11.6 <ref type="table">Table 5</ref>: Malagasy-French Bleu. Supervision with a dictionary significantly improves upon simple triangulation by +0.5 Bleu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we argued that constructing a trian- gulated phrase table independently from even very limited source-target data (a small dictionary or parallel corpus) underutilizes that parallel data.</p><p>Following this argument, we designed a super- vised learning algorithm that relies on word trans- lation distributions derived from the parallel data as well as a distributed representation of words (embeddings). The latter enables our algorithm to assign translation probabilities to word pairs that do not appear in the source-target bilingual data.</p><p>We then used our model to generate new lexi- cal weights for phrase pairs appearing in a trian- gulated or interpolated phrase table and demon- strated improvements in MT quality on two tasks. This is despite the fact that the distributions (w sup ) we fit our model to were estimated automatically, or even na¨ıvelyna¨ıvely as uniform distributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>tion distributions w sup (t | s) = c sup st /c sup s , where c sup s = t c sup st . Furthermore, let q(t | s) denote the word translation probabilities we wish to learn and consider maximizing the log-likelihood function: arg max q L(q) = arg max q (s,t) c sup st log q(t | s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>m</head><label></label><figDesc>st f st where the scalar m st = c sup st − c sup s q(t | s) for the current value of q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The (target-to-source) objective function per iteration. Applying batch Adagrad (blue) significantly accelerates convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>French</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Method 

α tune test 
source-target 
-26.8 25.3 
triangulation 
-29.2 28.4 
interpolation 
0.7 30.2 29.2 
interpolation+our scores 0.6 30.8 29.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 : Spanish-French Bleu scores.</head><label>4</label><figDesc></figDesc><table>Append-
ing lexical weights obtained by supervision over 
a small source-target corpus significantly out-
performs phrase table interpolation (Eq. 3) by 
+0.7 Bleu. 

</table></figure>

			<note place="foot" n="1"> http://www.ark.cs.cmu.edu/global-voices 2 http://motmalgache.org/bins/homePage 3 https://github.com/vchahun/gv-crawl 4 https://radimrehurek.com/gensim/models/word2vec.html 5 http://catalog.ldc.upenn.edu 6 http://corpora.uni-leipzig.de/download.html 7 http://www.isi.edu/˜qdou/downloads.html 8 https://commoncrawl.org/the-data/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Daniel Marcu and Kevin Knight for initial discussions and a sup-portive research environment at ISI, as well as the anonymous reviewers for their helpful comments. This research was supported in part by a Google Faculty Research Award to Chiang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine translation by triangulation: Making effective use of multi-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="728" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL, Interactive Poster and Demonstration Sessions</title>
		<meeting>ACL, Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MT Summit</title>
		<meeting>MT Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR, Workshop Track</title>
		<meeting>ICLR, Workshop Track</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison of pivot methods for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pivot language approach for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
