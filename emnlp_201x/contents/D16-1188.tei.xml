<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularizing Text Categorization with Clusters of Words</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Skianis</surname></persName>
							<email>kskianis@lix.polytechnique.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularizing Text Categorization with Clusters of Words</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1827" to="1837"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Regularization is a critical step in supervised learning to not only address overfitting, but also to take into account any prior knowledge we may have on the features and their dependence. In this paper, we explore state-of-the-art structured regularizers and we propose novel ones based on clusters of words from LSI topics, word2vec embeddings and graph-of-words document representation. We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy. Code and data are available online 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Harnessing the full potential in text data has always been a key task for the NLP and ML communities. The properties hidden under the inherent high di- mensionality of text are of major importance in tasks such as text categorization and opinion mining.</p><p>Although simple models like bag-of-words man- age to perform well, the problem of overfitting still remains. Regularization as proven in <ref type="bibr" target="#b6">Chen and Rosenfeld (2000)</ref> is of paramount importance in Natural Language Processing and more specifically language modeling, structured prediction, and clas- sification. In this paper we build upon the work of <ref type="bibr" target="#b41">Yogatama and Smith (2014b)</ref> who introduce prior knowledge of data as a regularization term. One of the most popular structured regularizers, the group lasso ( <ref type="bibr" target="#b42">Yuan and Lin, 2006</ref>), was proposed to avoid large L2 norms for groups of weights.</p><p>In this paper, we propose novel linguistic struc- tured regularizers that capitalize on the clusters learned from texts using the word2vec and graph-of- words document representation, which can be seen as group lasso variants. The extensive experiments we conducted demonstrate these regularizers can boost standard bag-of-words models on most cases tested in the task of text categorization, by imposing additional unused information as bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Notation</head><p>We place ourselves in the scenario where we con- sider a prediction problem, in our case text catego- rization, as a loss minimization problem, i. e. we define a loss function L(x, θ, y) that quantifies the loss between the prediction h θ,b (x) of a classifier parametrized by a vector of feature weights θ and a bias b, and the true class label y ∈ Y associated with the example x ∈ X . Given a training set of N data points {(x i , y i )} i=1...N , we want to find the optimal set of feature weights θ * such that:</p><formula xml:id="formula_0">θ * = argmin θ N i=1 L(x i , θ, y i ) empirical risk (1)</formula><p>In the case of logistic regression with binary predic- tions (Y = {−1, +1}), h θ,b (x) = θ x + b and L(x, θ, y) = e −yh θ,b (x) (log loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Regularization</head><p>Only minimizing the empirical risk can lead to over- fitting, that is, the model no longer learns the un- derlying pattern we are trying to capture but fits the noise contained in the training data and thus results in poorer generalization (e. g., lower performances on the test set). For instance, along with some fea- ture space transformations to obtain non-linear de- cision boundaries in the original feature space, one could imagine a decision boundary that follows ev- ery quirk of the training data. Additionally, if two hypothesis lead to similar low empirical risks, one should select the "simpler" model for better general- ization power, simplicity assessed using some mea- sure of model complexity.</p><p>Loss+Penalty Regularization takes the form of additional constraints to the minimization problem, i. e. a budget on the feature weights, which are of- ten relaxed into a penalty term Ω(θ) controlled via a Lagrange multiplier λ. We refer to the book of <ref type="bibr" target="#b5">Boyd and Vandenberghe (2004)</ref> for the theory be- hind convex optimization. Therefore, the overall expected risk <ref type="bibr" target="#b38">(Vapnik, 1991)</ref> is the weighted sum of two components: the empirical risk and a reg- ularization penalty term, expression referred to as "Loss+Penalty" by <ref type="bibr" target="#b11">Hastie et al. (2009)</ref>. Given a training set of N data points {(x i , y i )} i=1...N , we now want to find the optimal set of feature weights θ * such that:</p><formula xml:id="formula_1">θ * = argmin θ N i=1 L(x i , θ, y i ) empirical risk + λΩ(θ) penalty term expected risk (2)</formula><p>L 1 and L2 regularization The two most used penalty terms are known as L 1 regularization, a. k. a. lasso <ref type="bibr" target="#b37">(Tibshirani, 1996)</ref>, and L2 regularization, a. k. a. ridge <ref type="bibr" target="#b13">(Hoerl and Kennard, 1970)</ref> as they cor- respond to penalizing the model with respectively the L 1 and L2 norm of the feature weight vector θ:</p><formula xml:id="formula_2">θ * = argmin θ N i=1 L(x i , θ, y i ) + λ p j=1 |θ j | (3) θ * = argmin θ N i=1 L(x i , θ, y i ) + λ p j=1 θ j 2 (4)</formula><p>Prior on the feature weights L 1 (resp. L2) reg- ularization can be interpreted as adding a Laplacian (resp. Gaussian) prior on the feature weight vector. Indeed, given the training set, we want to find the most likely hypothesis h * ∈ H, i. e. the one with maximum a posteriori probability:</p><formula xml:id="formula_3">h * = argmax h∈H P(h|{(x i , y i )} i=1...N ) = argmax h∈H P({y i } i |{x i } i , h) P(h|{x i } i ) P({y i } i |{x i } i ) = argmax h∈H P({y i } i |{x i } i , h) P(h|{x i } i ) = argmax h∈H P({y i } i |{x i } i , h) P(h) (5) = argmax h∈H N i=1 P(y i |x i , h) P(h) (6) = argmax h∈H N i=1 log P(y i |x i , h) + log P(h) = argmin h∈H       N i=1 − log P(y i |x i , h) empirical risk − log P(h) penalty term      </formula><p>For the derivation, we assumed that the hypothesis h does not depend on the examples alone (Eq. 5) and that the N training labeled examples are drawn from an i.i.d. sample (Eq. 6). In that last form, we see that the loss function can be interpreted as a neg- ative log-likelihood and the regularization penalty term as a negative log-prior over the hypothesis. Therefore, if we assume a multivariate Gaussian prior on the feature weight vector of mean vector 0 and covariance matrix Σ = σ 2 I (i. e. independent features of same prior standard deviation σ), we do obtain the L2 regularization:</p><formula xml:id="formula_4">P(h) = 1 (2π) p |Σ| e − 1 2 θ Σ −1 θ (7) ⇒ − log P(h) = 1 2σ 2 θ Iθ + p 2 log(2πσ) argmax = λθ 2 2 , λ = 1 2σ 2<label>(8)</label></formula><p>And similarly, if we assume a multivariate Lapla- cian prior on the feature weight vector (i. e. θ i ∼ Laplace(0, 1 λ )), we obtain L 1 -regularization. In practice, in both cases, the priors basically mean that we expect weights around 0 on average. The main difference between L 1 and L2 regularization is that the Laplacian prior will result in explicitly setting some feature weights to 0 (feature sparsity) while the Gaussian prior will only result in reducing their values (shrinkage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structured regularization</head><p>In L 1 and L2 regularizations, features are considered as independent, which makes sense without any ad- ditional prior knowledge. However, similar features have similar weights in the case of linear classifiers -equal weights for redundant features in the ex- treme case -and therefore, if we have some prior knowledge on the relationships between features, we should include that information for better general- ization, i. e. include it in the regularization penalty term. Depending on how the similarity between fea- tures is encoded, e. g., through sets, trees <ref type="bibr" target="#b17">(Kim and Xing, 2010;</ref><ref type="bibr" target="#b19">Liu and Ye, 2010;</ref>) or graphs ( , the penalization term varies but in any case, we take into account the struc- ture between features, hence the "structured regular- ization" terminology. It should not be confused with "structured prediction" where this time the outcome is a structured object as opposed to a scalar (e. g., a class label) classically.</p><p>Group lasso Bakin (1999) and later <ref type="bibr" target="#b42">Yuan and Lin (2006)</ref> proposed an extension of L 1 regularization to encourage groups of features to either go to zero (as a group) or not (as a group), introducing group sparsity in the model. To do so, they proposed to regularize with the L 1,2 norm of the feature weight vector:</p><formula xml:id="formula_5">Ω(θ) = λ g λ g θ g 2 (9)</formula><p>where θ g is the subset of feature weights restricted to group g. Note that the groups can be overlapping ( <ref type="bibr" target="#b14">Jacob et al., 2009;</ref><ref type="bibr" target="#b34">Schmidt and Murphy, 2010;</ref><ref type="bibr" target="#b16">Jenatton et al., 2011;</ref><ref type="bibr" target="#b43">Yuan et al., 2011</ref>) even though it makes the optimization harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning</head><p>In our case we use a logistic regression loss function in order to integrate our regularization terms easily.</p><formula xml:id="formula_6">L(x, θ, y) = log(1 + exp(−yθ T x))<label>(10)</label></formula><p>It is obvious that the framework can be extended to other loss functions (e. g., hinge loss). For the case of structured regularizers, there exist a plethora of optimization methods such group lasso. Since our tasks involves overlapping groups, we se- lect the method of Yogatama and Smith (2014b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ADMM for overlapping group-lasso</head><p>Require: augmented Lagrangian variable ρ, regulariza- tion strengths λ glas and λ las 1: while update in weights not small do 2:</p><formula xml:id="formula_7">θ = argmin θ Ω las (θ) + L(θ) + ρ 2 V i=1 N i (θ i − µ i ) 2 3: for g = 1 to G do 4: v g = prox Ω glas , λg ρ (z g ) 5:</formula><p>end for</p><formula xml:id="formula_8">6: u = u + ρ(v − M θ) 7: end while</formula><p>Their method uses the alternating directions method of multipliers <ref type="bibr" target="#b12">(Hestenes, 1969;</ref><ref type="bibr" target="#b29">Powell, 1969)</ref>. Now given the lasso penalty for each feature and the group lasso regularizer, the problem becomes: <ref type="formula">(11)</ref> so that v = M θ, where v is a copy-vector of θ. The copy-vector v is needed because the group-lasso reg- ularizer contains overlaps between the used groups. M is an indicator matrix of size L × V , where L is the sum of the total sizes of all groups, and its ones show the link between the actual weights θ and their copies v. Following Yogatama and Smith (2014b) a constrained optimization problem is formed, that can be transformed to an augmented Lagrangian problem:</p><formula xml:id="formula_9">min θ,v Ω las (θ) + Ω glas (v) + D d=1 L(x d , θ, y d )</formula><formula xml:id="formula_10">Ω las (θ) + Ω glas (v) + L(θ) + u (v − M θ) + ρ 2 v − M θ 2 2 (12)</formula><p>Essentially, the problem becomes the iterative up- date of θ, v and u:</p><formula xml:id="formula_11">min θ Ω las (θ) + L(θ) + u M θ + ρ 2 v − M θ 2 2 (13) min v Ω glas (v) + u v + ρ 2 v − M θ 2 2 (14) u = u + ρ(v − M θ)<label>(15)</label></formula><p>Convergence Yogatama and Smith (2014b) proved that ADMM for sparse overlapping group lasso converges. It is also shown that a good approximate solution is reached in a few tens of iterations. Our experiments confirm this as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structured Regularization in NLP</head><p>In recent efforts there are results to identify useful structures in text that can be used to enhance the ef- fectiveness of the text categorization in a NLP con- text. Since the main regularization approach we are going to use are variants of the group lasso, we are interested on prior knowledge in terms of groups/clusters that can be found in the training text data. These groups could capture either semantic, or syntactic structures that affiliate words to communi- ties. In our work, we study both semantic and syn- tactic properties of text data, and incorporate them in structured regularizer. The grouping of terms is pro- duced by either LSI or clustering in the word2vec or graph-of-words space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Statistical regularizers</head><p>In this section, we present statistical regularizers, i. e. with groups of words based on co-occurrences, as opposed to syntactic ones ( <ref type="bibr" target="#b27">Mitra et al., 1997</ref>).  <ref type="bibr">, 2007)</ref> shows such sim- ilarities can be inferred from prior domain knowl- edge and statistics computed on unlabeled data. The weights of G are mapped in a matrix P , where P ij ≥ 0 gives the weight of the directed edge from vertex i to vertex j. The out-degree of each vertex is constrained to sum to one, j P ij = 1, so that no feature "dominates" the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network of features</head><formula xml:id="formula_12">Ω network (θ) = λ net θ k M θ k<label>(16)</label></formula><p>where M = α(I − P ) (I − P ) + βI. The matrix M is symmetric positive definite, and therefore it pos- sesses a Bayesian interpretation in which the weight vector θ, is a priori normally distributed with mean zero and covariance matrix 2M −1 . However, pre- liminary results show poorer performance compared to structured regularizers in larger datasets.</p><p>Sentence regularizer <ref type="bibr" target="#b41">Yogatama and Smith (2014b)</ref> proposed to define groups as the sentences in the training dataset. The main idea is to define a group d d,s for every sentence s in every training document d so that each group holds weights for occurring words in its sentence. Thus a word can be a member of one group for every distinct (training) sentence it occurs in. The regularizer is:</p><formula xml:id="formula_13">Ω sen (θ) = D d=1 S d s=1 λ d,s θ d,s 2<label>(17)</label></formula><p>where S d is the number of sentences in document d.</p><p>Since modern text datasets typically contain thou- sands of sentences and many words appear in more than one sentence, the sentence regularizer could potentially lead to thousands heavily overlapping groups. As stated in the work of <ref type="bibr" target="#b41">Yogatama and Smith (2014b)</ref>, a rather important fact is that the reg- ularizer will force all the weights of a sentence, if it is recognized as irrelevant. Respectively, it will keep all the weights of a relevant sentence, even though the group contains unimportant words. Fortunately, the problem can be resolved by adding a lasso regu- larization (Friedman et al., 2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic regularizers</head><p>In this section, we present semantic regularizers that define groups based on how semantically close words are.</p><p>LDA regularizer Yogatama and Smith (2014a) considered topics as another type of structure. It is obvious that textual data can contain a huge num- ber of topics and especially topics that overlap each other. Again the main idea is to penalize weights for words that co-occur in the same topic, instead of treating the weight of each word separately.</p><p>Having a training corpus, topics can be easily ex- tracted with the help of the latent Dirichlet allocation (LDA) model ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>. In our experiments, we form a group by extracting the n most probable words in a topic. We note that the extracted topics can vary depending the text preprocessing methods we apply on the data.</p><p>LSI regularizer Latent Semantic Indexing (LSI) can also be used in order to identify topics or groups and thus discover correlation between terms <ref type="bibr" target="#b7">(Deerwester et al., 1990</ref>). LSI uses singular value de- composition (SVD) on the document-term matrix to </p><note type="other">propos numer special kind A method for solution of systems of linear algebraic</note><p>equations with m-dimensional lambda matrices. A system of linear algebraic equations with m-dimensional lambda matrices is considered. The proposed method of searching for the solution of this system lies in reducing it to a numerical system of a special kind. identify latent variables that link co-occurring terms with documents. The main basis behind LSI is that words being used in the same contexts (i. e. the doc- uments) tend to have similar meanings. We used LSI as a baseline and compare it with other stan- dard baselines as well as other proposed structured regularizers. In our work we keep the top 10 words which contribute the most in a topic.</p><p>The regularizer for both LDA and LSI is:</p><formula xml:id="formula_14">Ω LDA,LSI (θ) = K k=1 λθ k 2<label>(18)</label></formula><p>where K is the number of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graphical regularizers</head><p>In this section we present our proposed regularizers based on graph-of-words and word2vec. Essentially the word2vec space can be seen as a large graph where nodes represent terms and edges similarities between them.</p><p>Graph-of-words regularizer Following the idea of the network of features, we introduce a simpler and faster technique to identify relationships be- tween features. We create a big collection graph from the training documents, where the nodes cor- respond to terms and edges correspond to co- occurrence of terms in a sliding window. We present a toy example of a graph-of-words in <ref type="figure" target="#fig_1">Figure 1</ref>. A critical advantage of graph-of-words is that it easily encodes term dependency and term order (via edge direction). The strength of the dependence be- tween two words can also be captured by assigning a weight to the edge that links them.</p><p>Graph-of-words was originally an idea of Mihal- cea and Tarau (2004) and <ref type="bibr" target="#b8">Erkan and Radev (2004)</ref> who applied it to the tasks of unsupervised keyword extraction and extractive single document summa- rization. <ref type="bibr" target="#b32">Rousseau and Vazirgiannis (2013)</ref> and <ref type="bibr" target="#b23">Malliaros and Skianis (2015)</ref> showed it performs well in the tasks of information retrieval and text cat- egorization. Notably, the former effort ranked nodes based on a modified version of the PageRank algo- rithm.</p><p>Community detection on graph-of-words Our goal is to identify groups or communities of words. Having constructed the collection-level graph-of- words, we can now apply community detection al- gorithms <ref type="bibr" target="#b9">(Fortunato, 2010)</ref>.</p><p>In our case we use the Louvain method, a commu- nity detection algorithm for non-overlapping groups described in the work of <ref type="bibr" target="#b4">Blondel et al. (2008)</ref>. Es- sentially it is a fast modularity maximization ap- proach, which iteratively optimizes local communi- ties until we reach optimal global modularity given some perturbations to the current community state. The regularizer becomes:</p><formula xml:id="formula_15">Ω gow (θ) = C c=1 λθ c 2<label>(19)</label></formula><p>where c ranges over the C communities. Thus θ c corresponds to the sub-vector of θ such that the cor- responding features are present in the community c.</p><p>Note that in this case we do not have overlapping groups, since we use a non-overlapping version of the algorithm. As we observe that the collection-level graph-of- words does not create well separated communities of terms, overlapping community detection algorithms, like the work of <ref type="bibr" target="#b39">Xie et al. (2013)</ref> fail to identify "good" groups and do not offer better results.</p><p>Word2vec regularizer <ref type="bibr" target="#b26">Mikolov et al. (2013)</ref> pro- posed the word2vec method for learning continu- ous vector representations of words from large text datasets. Word2vec manages to capture the ac- tual meaning of words and map them to a multi- dimensional vector space, giving the possibility of applying vector operations on them. We introduce another novel regularizer method, by applying un- supervised clustering algorithms on the word2vec space.</p><p>Clustering on word2vec Word2vec contains mil- lions of words represented as vectors.</p><p>Since word2vec succeeds in capturing semantic similarity between words, semantically related words tend to group together and create large clusters that can be interpreted as "topics".</p><p>In order to extract these groups, we use a fast clustering algorithm such as K-Means <ref type="bibr" target="#b21">(Macqueen, 1967)</ref> and especially Minibatch K-means. The reg- ularizer is:</p><formula xml:id="formula_16">Ω word2vec (θ) = K k=1 λθ k 2 (20)</formula><p>where K is the number of clusters we extracted from the word2vec space. Clustering these semantic vectors is a very inter- esting area to study and could be a research topic by itself. The actual clustering output could vary as we change the number of clusters we are trying to iden- tify. In this paper we do not focus on optimizing the clustering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated our structured regularizers on several well-known datasets for the text categorization task. <ref type="table" target="#tab_2">Table 1</ref> summarizes statistics about the ten datasets we used in our experiments.  <ref type="table" target="#tab_2">science  949 238 790  25787  16411  sports  957 240 796  21938  14997  religion 863 216 717  18822  18853  comp.  934 234 777  16282  10772   Sentiment   vote  1175 257 860  19813  43563  movie  1600 200 200  43800  49433  books  1440 360 200  21545  13806  dvd  1440 360 200  21086  13794  electr.  1440 360 200  10961  10227  kitch.</ref> 1440 360 200 9248 8998 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>As features we use unigram frequency concatenated with an additional unregularized bias term. We re- produce standard regularizers like lasso, ridge, elas- tic and state-of-the-art structured regularizers like sentence, LDA as baselines and compare them with our proposed methods. For LSI, LDA and word2vec we use the gensim package <ref type="bibr">( ˇ Rehůřek and Sojka, 2010</ref>) in Python. For the learning part we used Matlab and specifically code by <ref type="bibr" target="#b35">Schmidt et al. (2007)</ref>.</p><p>We split the training set in a stratified manner to retain the percentage of classes. We use 80% of the data for training and 20% for validation.</p><p>All the hyperparameters are tuned on the develop- ment dataset, using accuracy as the evaluation crite- rion. For lasso and ridge regularization, we choose λ from {10 −2 , 10 −1 , 1, 10, 10 2 }. For elastic net, we perform grid search on the same set of values as ridge and lasso experiments for λ rid and λ las . For the LDA, LSI, sentence, graph-of-words (GoW), word2vec regularizers, we perform grid search on the same set of values as ridge and lasso experi- ments for the ρ, λ glas , λ las parameters. In the case we get the same accuracy on the development data, the model with the highest sparsity is selected. For    LDA we set the number of topics to 1000 and we keep the 10 most probable words of each topic as a group. For LSI we keep 1000 latent dimensions and we select the 10 most significant words per topic. For the clustering process on word2vec we ran Minibatch-Kmeans for max 2000 clusters. For each word belonging to a cluster, we also keep the top 5 or 10 nearest words so that we introduce overlapping groups. The intuition behind this is that words can be part of multiple "concepts" or topics, thus they can belong to many clusters.</p><note type="other">dataset no reg. lasso ridge elastic group lasso LDA</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In <ref type="table" target="#tab_4">Table 2</ref> we report the results of our experiments on the aforementioned datasets, and we distinguish our proposed regularizers LSI, GoW, word2vec with underlining. Our results are inline and confirm that of ( <ref type="bibr" target="#b40">Yogatama and Smith, 2014a)</ref> showing the advan- tages of using structured regularizers in the text cat- egorization task. The group based regularizers per- form systematically better than the baseline ones.</p><p>We observe that the word2vec clustering based regularizers performs very well -achieving best per- formance for three out of the ten data sets while it is quite fast with regards to execution time as it appears in <ref type="table" target="#tab_5">Table 3</ref> (i. e. it is four to ten times faster than the sentence based one).</p><p>The LSI based regularization, proposed for the first time in this paper, performs surprisingly well as it achieves the best performance for three of the ten datasets. This is somehow interpreted by the fact that this method extracts the inherent dimen- sions that best represent the different semantics of the documents -as we see as well in the anecdotal <ref type="table" target="#tab_2">dataset   GoW word2vec   20NG   science  79  691  sports  137  630  religion  35  639  computer</ref> 95 594  <ref type="table" target="#tab_2">6  15  11  76  12  19  sports  12  3  3  7  20  67  5  9  religion  12  3  7  10  4  248  6  20  computer  7</ref> 1.4 0.8 8 6 43 5 10 and space the launch health for use that medical you space cancer and nasa hiv health shuttle for tobacco that cancer that research center space hiv aids are use theory keyboard data telescope available are from system information space ftp  examples in <ref type="table" target="#tab_8">Table 6</ref>, 7, 8. This method proves as well very fast as it appears in <ref type="table" target="#tab_7">Table 5</ref> (i.e. it is three to sixty times faster than the sentence based one).</p><p>The GoW based regularization although very fast, did not outperform the other methods (while it has a very good performance in general). It remains to be seen whether a more thorough parameter tuning and community detection algorithm selection would improve further the accuracy of the method.</p><p>In <ref type="table" target="#tab_5">Table 3</ref> we present the feature space sizes re- tained by each of the regularizers for each dataset. As expected the lasso regularizer sets the vast major- ity of the features' weights to zero, and thus a very sparse feature space is generated. This fact has as a consequence the significant decrease in accuracy performance. Our proposed structured regularizers = 0 islands inta spain galapagos canary originated anodise advertises jewelry mercedes benzes diamond trendy octave chanute lillienthal = 0 vibrational broiled relieving succumb spacewalks dna nf-psychiatry itself commented usenet golded insects alternate self-consistent retrospect managed to perform better in most of the cases, in- troducing more sparse models compared to the state- of-the-art regularizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Time complexity</head><p>Although certain types of structured regularizers improve significantly the accuracy and address the problem of overfitting, they require a notable amount of time in the learning process. As seen in Yogatama and Smith (2014b), a con- siderable disadvantage is the need of search for the optimal hyperparameters: λ glas , λ lasso , and ρ, whereas standard baselines like lasso and ridge only have one hyperparameter and elastic net has two.</p><p>Parallel grid search can be critical for finding the optimal set of hyperparameters, since there is no de- pendency on each other, but again the process can be very expensive. Especially for the case of the sentence regularizer, the process can be extremely slow due to two factors. First, the high number of sentences in text data. Second, sentences consist of heavily overlapping groups, that include words reap- pearing in one or more sentences. On the contrary, as it appears on <ref type="table" target="#tab_6">Table 4</ref>, the number of clusters in the clustering based regularizers is significantly smaller than that of the sentences -and definitely controlled by the designer -thus resulting in much faster com- putation. The update of v still remains time consum- ing for small datasets, even with parallelization.</p><p>Our proposed structured regularizers are consid- erably faster in reaching convergence, since they of-fer a smaller number of groups with less overlapping between words. For example, on the computer sub- set of the 20NG dataset, learning models with the best hyperparameter value(s) for lasso, ridge, and elastic net took 7, 1.4, and 0.8 seconds, respectively, on an Intel Xeon CPU E5-1607 3.00 GHz machine with 4 cores and 128GB RAM. Given the best hyper- parameter values the LSI regularizer takes 6 seconds to converge, the word2vec regularizer takes 10 sec- onds to reach convergence, the graph-of-words takes 4 seconds while the sentence regularizer requires 43 seconds. <ref type="table" target="#tab_7">Table 5</ref> summarizes required learning time on 20NG datasets.</p><p>We also need to consider the time needed to ex- tract the groups. For word2vec, Minibatch K-means requires 15 minutes to cluster the pre-trained vectors by Google. The clustering is executed only once. Getting the clusters of words that belong to the vo- cabulary of each dataset requires 20 minutes, but can be further optimized. Finding also the communities in the graph-of-words approach with the Louvain al- gorithm, is very fast and requires a few minutes de- pending on the size and structure of the graph.</p><p>In <ref type="table" target="#tab_8">Tables 6, 7</ref>, 8 we show examples of our pro- posed regularizers-removed and -selected groups (in v) in the science subset of the 20NG dataset. Words with weights (in w) of magnitude greater than 10 −3 are highlighted in red (sci.med) and blue (sci.space).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion &amp; Future Work</head><p>This paper proposes new types of structured regular- izers to improve not only the accuracy but also the efficiency of the text categorization task. We mainly focused on how to find and extract semantic and syn- tactic structures that lead to sparser feature spaces and therefore to faster learning times. Overall, our results demonstrate that linguistic prior knowledge in the data can be used to improve categorization performance for baseline bag-of-words models, by mining inherent structures. We only considered lo- gistic regression because of its interpretation for L2 regularizers as Gaussian prior on the feature weights and following <ref type="bibr" target="#b33">Sandler et al. (2009)</ref>, we considered a non-diagonal covariance matrix for L2 based on word similarity before moving to group lasso as pre- sented in the paper. We are not expecting a signif- icant change in results with different loss functions as the proposed regularizers are not log loss specific.</p><p>Future work could involve a more thorough in- vestigation on how to create and cluster graphs, i. e. covering weighted and/or signed cases. Finding bet- ter clusters in the word2vec space is also a criti- cal part. This is not only restricted in finding the best number of clusters but what type of clusters we are trying to extract. Gaussian Mixture Models <ref type="bibr" target="#b24">(McLachlan and Basford, 1988</ref>) could be applied in order to capture overlapping groups at the cost of high complexity. Furthermore, topical word embed- dings ( <ref type="bibr" target="#b20">Liu et al., 2015)</ref> can be considered for reg- ularization. This approach could enhance the reg- ularization on topic specific datasets. Additionally, we plan on exploring alternative regularization algo- rithms diverging from the group-lasso method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Graph-of-words example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Topic categorization. From the 20 Newsgroups 2 dataset, we examine four binary classification tasks. We end up with binary classification problems, where we classify a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space and alt.atheism vs. soc.religion.christian. We use the 20NG dataset from Python. Sentiment analysis. The sentiment analysis datasets we examined include movie reviews 2 http://qwone.com/~jason/20Newsgroups/ dataset train dev test # words # sents 20NG</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Descriptive statistics of the datasets 

(Pang and Lee, 2004; Zaidan and Eisner, 2008) 3 , 
floor speeches by U.S. Congressmen deciding 
"yea"/"nay" votes on the bill under discussion 
(Thomas et al., 2006) 3 and product reviews from 
Amazon (Blitzer et al., 2007) 4 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy results on the test sets. Bold font marks the best performance for a dataset. * indicates statistical significance 

of improvement over lasso at p &lt; 0.05 using micro sign test for one of our models LSI, GoW and word2vec (underlined). 

dataset 
no reg. lasso ridge elastic 
group lasso 
LDA LSI sentence GoW word2vec 

20NG 

science 
100 
1 
100 
63 
19 
20 
86 
19 
21 
sports 
100 
1 
100 
5 
60 
11 
6.4 
55 
44 
religion 
100 
1 
100 
3 
94 
31 
99 
10 
85 
computer 
100 
2 
100 
7 
40 
35 
77 
38 
18 

Sentiment 

vote 
100 
1 
100 
8 
15 
16 
13 
97 
13 
movie 
100 
1 
100 
59 
72 
81 
55 
90 
62 
books 
100 
3 
100 
14 
41 
74 
72 
90 
99 
dvd 
100 
2 
100 
28 
64 
8 
8 
58 
64 
electr. 
100 
4 
100 
6 
10 
8 
43 
8 
9 
kitch. 
100 
5 
100 
79 
73 
44 
27 
75 
46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Fraction (in %) of non-zero feature weights in each model for each dataset: the smaller, the more compact the model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 : Number of groups.</head><label>4</label><figDesc></figDesc><table>dataset 
lasso ridge elastic 
group lasso 
LDA LSI sentence GoW word2vec 

20NG 

science 
10 
1.6 
1.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 5 : Time (in seconds) for learning with best hyperparameters.</head><label>5</label><figDesc></figDesc><table>= 0 

piscataway combination jil@donuts0.uucp 
jamie reading/seeing chambliss 
left-handedness abilities lubin 
acad sci obesity page erythromycin bottom 

= 0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Examples with LSI regularizer. 

= 0 

village town 
edc fashionable trendy trendy fashionable 
points guard guarding 
crown title champion champions 

= 0 

numbness tingling dizziness fevers 
laryngitis bronchitis undergo undergoing 
undergoes undergone healed 
mankind humanity civilization planet 
nasa kunin lang tao kay kong 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 : Examples with word2vec regularizer.</head><label>7</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Examples with graph-of-words regularizer. 

</table></figure>

			<note place="foot" n="1"> https://goo.gl/mKqvro</note>

			<note place="foot" n="3"> http://www.cs.cornell.edu/~ainur/data. html 4 http://www.cs.jhu.edu/~mdredze/ datasets/sentiment/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive regression and model selection in data mining problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey Bakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D</title>
		<imprint>
			<date type="published" when="1999-05" />
		</imprint>
	</monogr>
	<note>The Australian National University</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics, ACL &apos;07</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Loup</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of smoothing techniques for ME models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LexRank: graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A note on the group lasso and a sparse group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Friedman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The elements of statistical learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiplier and gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Magnus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hestenes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="303" to="320" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Group Lasso with Overlap and Graph Lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th International Conference on Machine Learning, ICML &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proximal methods for sparse hierarchical dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning, ICML &apos;10</title>
		<meeting>the 27th International Conference on Machine Learning, ICML &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured variable selection with sparsity-inducing norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2777" to="2824" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tree-guided group lasso for multi-task regression with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning, ICML &apos;10</title>
		<meeting>the 27th International Conference on Machine Learning, ICML &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incorporating Prior Knowledge on Features into Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Krupka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 11th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
	<note>of AISTATS &apos;07</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moreau-Yosida Regularization for Grouped Tree Structure Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23, NIPS &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th national conference on Artificial intelligence</title>
		<meeting>the 29th national conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
	<note>Topical word embeddings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5-th Berkeley Symposium on Mathematical Statistics and Probability</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Network flow algorithms for structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23, NIPS &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph-based term weighting for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fragkiskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skianis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting>the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mixture Models: Inference and Applications to Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Basford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;04</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at International Conference on Learning Representations, ICLR &apos;13</title>
		<meeting>Workshop at International Conference on Learning Representations, ICLR &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Analysis of Statistical and Syntactic Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Computer-Assisted Information Retrieval, volume 97 of RIAO &apos;97</title>
		<meeting>the 5th International Conference on Computer-Assisted Information Retrieval, volume 97 of RIAO &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="200" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A method for nonlinear constraints in minimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
		<editor>R. Fletcher editor, Optimization</editor>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constructing Informative Priors Using Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="713" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph-of-word and tw-idf: New approach to ad hoc ir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information and knowledge management, CIKM &apos;13</title>
		<meeting>the 22nd ACM international conference on Information and knowledge management, CIKM &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularized learning with networks of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22, NIPS &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1401" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convex structure learning in log-linear models: Beyond pairwise potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, AISTATS &apos;10</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics, AISTATS &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast optimization methods for L1 regularization: A comparative study and two new approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th European Conference on Machine Learning, ECML &apos;07</title>
		<meeting>the 18th European Conference on Machine Learning, ECML &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Get out the vote: Determining support or opposition from Congressional floor-debate transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Principles of Risk Minimization for Learning Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">Naumovich</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 4, NIPS &apos;91</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Overlapping community detection in networks: The state-of-the-art and comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jierui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boleslaw</forename><forename type="middle">K</forename><surname>Szymanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL &apos;14</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="786" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Making the most of bag of words: Sentence regularization with alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient methods for overlapping group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24, NIPS &apos;11</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling annotators: A generative approach to learning from annotator rationales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
