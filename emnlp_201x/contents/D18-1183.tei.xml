<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Multitask Learning for Simile Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Engineering</orgName>
								<orgName type="department" key="dep2">‡ iFLYTEK Research</orgName>
								<orgName type="institution">Capital Normal University</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Multitask Learning for Simile Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1543" to="1553"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1543</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Simile is a special type of metaphor, where comparators such as like and as are used to compare two objects. Simile recognition is to recognize simile sentences and extract simile components, i.e., the tenor and the vehicle. This paper presents a study of simile recognition in Chinese. We construct an annotated corpus for this research, which consists of 11.3k sentences that contain a compara-tor. We propose a neural network framework for jointly optimizing three tasks: simile sentence classification, simile component extraction and language modeling. The experimental results show that the neural network based approaches can outperform all rule-based and feature-based baselines. Both simile sentence classification and simile component extraction can benefit from multitask learning. The former can be solved very well, while the latter is more difficult.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A metaphor is a figure of speech that describes an object or action in a way that isn't literally true. Metaphors are common in human lan- guage. <ref type="bibr" target="#b29">Shutova and Teufel (2010)</ref> reported that 241 among 760 sentences in an annotated corpus contain a metaphor. The use of metaphors helps to explain an idea or realize rhetorical effects through an analogical procedure. Metaphor analysis has been drawn more attention for expanding current natural language processing (NLP) to high-level semantic tasks <ref type="bibr" target="#b2">(Carbonell, 1980)</ref>. Metaphors reflect creative thought of humans. On the other hand, inferring the meaning of a metaphor has to integrate background knowledge, which makes it difficult to automatically recog- nize metaphors in language. Previous work on ⇤ corresponding author metaphor recognition mainly depends on linguis- tic cues <ref type="bibr" target="#b6">(Goatly, 2011</ref>) and selectional preference violation on a pair of concepts <ref type="bibr" target="#b4">(Fass, 1991)</ref> or their domains <ref type="bibr" target="#b16">(Mason, 2004</ref>). The domains can be created by knowledge bases such as WordNet <ref type="bibr" target="#b16">(Mason, 2004</ref>) or based on automatic clustering ( .</p><p>In this paper, we focus on a special type of metaphor-simile. A simile is a figure of speech that directly compares two things using connect- ing words such as like, as, than in English and "oe" or "πÇ" in Chinese. Due to the use of such comparators, it is much easier to locate similes compared with locating other types of metaphors. As a result, it is possible to collect and annotate large scale of simile sentences and investigate data driven simile recognition. This task is to find sim- ile sentences and extract simile components, i.e., the tenor and the vehicle. The mined simile struc- tures can potentially be used to support general metaphor analysis, where large scale training data is lacking.</p><p>However, simile recognition is still challenging due to the diversity of syntactic roles of a word and the distinction between metaphorical and lit- eral comparisons. As shown in <ref type="table">Table 1</ref>, a sentence containing a comparator may not trigger a simile. It is necessary to analyze the relationship between meanings of concepts. And It is also difficult to define a complete set of rules to extract the objects to be compared with high accuracy and coverage.</p><p>This paper presents an end-to-end neural net- work framework for simile sentence recognition. Specifically, we make following contributions:</p><p>• We build a dataset consisting of 11.3k sen- tences containing a frequently used compara- tor "oe" for simile recognition in Chinese, which can support data-driven approaches. In contrast to English, datasets on simile or</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Ÿ*[iP]tenor Óóoe oe oe 4[[] vehicle</head><p>This <ref type="bibr">[boy]</ref>tenor is as strong as a <ref type="bibr">[bull]</ref> vehicle Simile 2. Ÿ*iPóoe oe oe 88 The boy looks like his father Literal 3. ÷ÕÕ''Ñ©Ä,oe oe oe /J…÷÷Åae« He patted his uncle as if telling him not to be sad Literal 4. oe oe oe ÷Ÿ7ÑfîÂÙ †™õ The students like him should work even harder Literal <ref type="table">Table 1</ref>: Sentences that contain the comparator "oe".</p><p>metaphor analysis are relatively less in Chi- nese. This dataset provides a new resource for related research. 1</p><p>• We propose a neural multitask learning framework jointly optimizing three tasks: simile sentence classification, simile compo- nent extraction and language modeling. Sim- ile classification is to determine whether a sentence with a comparator contains a sim- ile, without knowing exactly what the tenor and the vehicle are. Simile component ex- traction aims to locate the tenor and the vehi- cle in a simile sentence. Intuitively, the two tasks should benefit each other. We design our model to enhance interactions between the two tasks. We also borrow the idea of Rei (2017) by incorporating a language mod- eling task, which attempts to predict neigh- bor words. All three tasks consider the whole sentence so that rich context information is involved.</p><p>• We conduct comprehensive experiments.</p><p>The results demonstrate that the neural end- to-end framework is superior to feature- based and rule-based baselines and every sin- gle model can benefit from multitask learn- ing. Simile sentence classification can be solved very well, while simile component ex- traction is more chellenging. With multi- task learning enhanced classifier and extrac- tor, a classification-then-extraction method achieves the best performance for simile component extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metaphor/Simile Analysis</head><p>Metaphor analysis becomes active in recent years. The tasks include metaphor recognition, metaphor explanation and metaphor generation <ref type="bibr" target="#b30">(Shutova et al., 2013;</ref><ref type="bibr" target="#b33">Veale, 1995;</ref><ref type="bibr" target="#b9">Jang et al., 2016)</ref>. Simile is a special type of metaphor with the comparator and it is relatively easier to lo- cate metaphorical parts. Niculae and Danescu- Niculescu-Mizil (2014) aimed to distinguish a comparison from figurative or literal in product re- views using a series of linguistic cues as features. It is similar to simile sentence classification. So we take it as a baseline. In their work, they as- sumed that the components can be correctly rec- ognized. In our work, we use an automated com- ponent extractor instead.</p><p>Syntactic patterns are often used for extracting potential simile components and semantic analy- sis is then used to distinguish similes from literal comparisons <ref type="bibr" target="#b21">(Niculae and Yaneva, 2013;</ref><ref type="bibr" target="#b19">Niculae, 2013)</ref>. The main limitation is that such pattern based method is difficult to deal with sentences with complex structures. As a result, the coverage is relatively small. <ref type="bibr" target="#b24">Qadir et al. (2016)</ref> used syntactic structures, dictionary definitions, statistical cooccurrence, and word embedding vectors to infer implicit properties in similes. <ref type="bibr" target="#b23">Qadir et al. (2015)</ref> also built a classifier with lexical features, semantic features, and sentiment features to infer the affective polar- ity of simile in twitters. <ref type="bibr" target="#b36">Veale and Hao (2007)</ref> and Veale (2012a) utilized knowledge generated by similes to deal with metaphor and irony, and Veale (2012b) built a lexical stereotype model from sim- iles. These work demonstrates the wide applica- tions of simile recognition.</p><p>In Chinese, <ref type="bibr" target="#b13">Li et al. (2008)</ref> proposed a feature- based method for simile recognition. Their evalu- ation was done on a small dataset. The annotated data in this work is much larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multitask Learning for NLP</head><p>Many researchers have proposed to jointly learn multiple tasks with shared representations <ref type="bibr" target="#b3">(Collobert and Weston, 2008)</ref>. Improvements are reported on joint models between closely re- lated tasks, such as text classification ( <ref type="bibr" target="#b14">Liu et al., 2016)</ref>, POS tagging and parsing <ref type="bibr" target="#b38">(Zhang and Weiss, 2016)</ref>, parsing and named entity recogni- tion (NER) ( <ref type="bibr" target="#b5">Finkel and Manning, 2010)</ref>, NER and linking ( <ref type="bibr" target="#b15">Luo et al., 2015)</ref>, extraction of entities and relations <ref type="bibr" target="#b18">(Miwa and Bansal, 2016)</ref>. <ref type="bibr" target="#b1">Bingel and Søgaard (2017)</ref> offered a systematic view of re- lations between different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task and Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>A metaphor is as a matter of cross-domain map- pings in conceptual structure which are expressed in language. <ref type="bibr" target="#b11">Lakoff and Johnson (2008)</ref> explains it as a mapping between the target and the source, corresponding to the terms tenor and vehicle. The tenor is the subject to which attributes are as- cribed, while the vehicle is the object whose at- tributes are borrowed.</p><p>Simile can be seen as a special type of metaphor, which is signaled by explicit markers such as like or as in English and "oe" or "πÇ" in Chinese. We call such words comparators <ref type="bibr" target="#b7">(Hanks, 2012)</ref>. Notice that a sentence containing a comparator doesn't guarantee that it is a simile sentence. Consider the examples in <ref type="table">Table 1</ref>. All sentences contain the comparator "oe". The first two sentences form comparison structures, but the first one triggers a cross-domain concept mapping, while the second one is a literal comparison. The word "oe" in the third sentence means as if, rather than forming a comparison, and the comparator in the fourth sentence is to give examples.</p><p>Simile Recognition task can be defined as: Given a sentence containing a comparator, deter- mine whether it is a simile sentence, if so, extract the tenor and the vehicle from it.</p><p>Simile recognition involves two subtasks.</p><p>Simile Sentence Classification (SC). For a sen- tence containing a comparator, determine whether the comparator triggers a metaphorical compari- son, in another word, whether the sentence is a simile sentence.</p><p>Simile Component Extraction (CE). For a simile sentence, extract text spans that are corre- sponding to the tenor and the vehicle objects re- spectively.</p><p>Both tasks have a realistic significance. Sim- ile can be seen as a rhetorical device for making thoughts or expressions more vivid. Simile sen- tence classification could be used to provide a sig- nal to evaluate rhetorical effects of writings. Sim- ile component extraction is potentially useful for building cognitive knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>We construct a dataset from Chinese student es- says written in Mandarin Chinese. We focus on the comparator "oe", which is the most often used simile comparator in Chinese. The data annotation involves the following steps: (1) we sampled sen- tences that contain the word "oe" from a sentence index built on more than 20,000 student essays; (2) we asked two annotators to label every sentence as a simile sentence or not; (3) the annotators fur- ther annotated boundaries of simile components, the tenor and the vehicle, in simile sentences.</p><p>Two points are important for annotation: (1) the definition of simile and (2) the boundary of simile components.</p><p>We desire that a simile sentence should satisfy at least two standards. First, there exists explicit tenor and vehicle, which are from different seman- tic domains. Second, the tenor and vehicle should have similar properties. We provided a manual and positive/negative examples to annotators. Even so, there are still fuzzy cases. The two annotators first labeled a sample of 200 sentences independently and then we measured their agreement and asked them to discuss the disagreements. After discus- sion, they labeled another set of sampled sentences to check whether they really reached a consen- sus. This process iterated several times. Their fi- nal inner-annotator agreement on simile sentence classification can reach to 91%. Finally, they la- beled all sentences in the whole dataset.</p><p>Next, the annotators should further label simile components in simile sentences, which are usually noun phrases. Boundaries of simile components are required to be annotated as compact as pos- sible until they can't be simplified any more. In most cases, we asked the annotators to label the head noun phrase without a modifier. A modi- fier often plays a role as a shared property. For example, in the sentence 7iÑ8oe*¢˘ ú(The boy's face is like a red apple), 8(face) and ˘ú(apple) would be annotated as the tenor and vehicle respectively, while 7iÑ(boy's) and ¢(red) are not included. In contrast, in the sen- tence )oeiPÑ8Ùÿ1ÿ(The weather is like a child's face, which changes unpredictably), iPÑ8(child's face) is preferred to 8(face) as a component, because 8(face) alone can't capture the property well. We used one annotator's anno- tation of 200 sentences as the gold answer and the other's annotation as the prediction to computer the F 1 score, which is 93.57% on all components and 90.7% on tenors and 96.47% on vehicles.  Intuitively, the two subtasks in simile recognition can benefit each other and the interactions between them should not be ignored. If the component ex- tractor knows that a sentence contains a simile, it would be more confident to extract the tenor and the vehicle. On the other hand, if the component extractor tells the classifier that the tenor and the vehicle likely exist, the classifier gets additional information for decision.</p><p>Therefore, we propose a multitask learning ap- proach to combine them. Our approach jointly op- timizes three tasks: simile sentence classification, simile component extraction and language model- ing. Language modeling is used as auxiliary task, which can help capture local information. <ref type="figure">Figure 1</ref> illustrates the main framework, which is based on neural networks. We will first explain the repre- sentation layers that are shared by multiple tasks, and then introduce separate prediction layers for individual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shared Representation</head><p>Word embedding layer. We first map words to dense distributed word embeddings. Since our dataset is not so large, we make use of pre-trained word embeddings, which are trained on a much larger corpus with Word2Vec toolkit ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref>. Sentence representation layer. Recurrent neu- ral networks (RNNs) have become the natural choice for handling sequential data to capture long-range dependencies. Given a sentence X = (x 1 , x 2 , ..., x n ) containing n words as an input, the RNNs produce H = (h 1 , h 2 , ..., h n ) as the hid- den states to represent the semantic of partial se- quence so far. Recently, Long Short Term Mem- ory (LSTM) model <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) has been proved more effective in various NLP tasks. Therefore, we use LSTM as the basic memory cell. At time step t, LSTM takes the hid- den state from the previous time step and the word embedding from the current step as input, and pro- duces a new hidden state, as shown in Formula 1.</p><formula xml:id="formula_0">! h t = LST M (x t , LST M( ! h t1 ))<label>(1)</label></formula><p>The LSTM architecture is sensitive to word or-der, and the bidirectional LSTM ( <ref type="bibr" target="#b27">Schuster and Paliwal, 2002</ref>) allows model to look arbitrarily far at both the past and the future for the sake of grasping the whole sentence. Noted the forward LSTM as ! h , and the backward as h . Bidirec- tional LSTM concatenates the forward and back- ward states as the representation at the tth time step, i.e., h t = h ! h t ; h t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task 1: Simile Component Extraction</head><p>We view simile component extraction as a se- quence labeling problem. We convert the anno- tated dataset to IOBES scheme (indicating Inside, Outside, Beginning, Ending, Single) (Ratinov and Roth, 2009). We use different prefixes to distin- guish the tenor and the vehicle components. For example, tb and vb indicate the beginning of a tenor and a vehicle respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Neural Sequence Labeling Model</head><p>Conditional Random Field (CRF) ( <ref type="bibr" target="#b10">Lafferty et al., 2001</ref>) is a standard solution in such scenario to exploit the dependency among labels.To further make use of the dense representation of words, we build a CRF layer on the shared representation lay- ers following <ref type="bibr" target="#b12">(Lample et al., 2016)</ref>.</p><p>Formally, H = (h 1 , h 2 , ..., h n ) is a sequence of hidden states produced by the bidirectional LSTM for a sentence X and y = (y 1 , y 2 , ..., y n ) is the tag sequence, y i 2 L and |L| = k. Define (H, y) as the score of the sequence.</p><formula xml:id="formula_1">(H, y) = n X t=0 A yt,y t+1 + n X t=1 P t,yt<label>(2)</label></formula><p>where A 2 R k⇥k is a transition matrix and A yt,y t+1 records the score of a transition from cur- rent label y t to next label y t+1 ; P = (p 1 , ..., p n ) 2 R n⇥k is the emission matrix and P t,yt represents the score of assigning tag y t to x t . Here, h t is the tth hidden state that is as assigned by the bidirectional LSTM. It is first mapped to a hidden layer through a feedforward layer. After a non-linear activation transition tanh, the output of the hidden layer is mapped to a k-dimension vector p t , through another feedfor- ward layer.</p><formula xml:id="formula_2">p t = W p · tanh (W t h t )<label>(3)</label></formula><p>where W t and W p are parameter matrixes. p t can be seen as a tag score vector given the current word without considering context words. Taking the whole state sequence into account, the probability of tag sequence y given sentence X is:</p><formula xml:id="formula_3">p(y|H) = (H, y) P e y2Y e (H,e y)<label>(4)</label></formula><p>where Y indicates all possible sequences. Learn- ing algorithm attempts to optimize the model by maximizing the log-likelihood of correct tag se- quence. Thus, the loss function is</p><formula xml:id="formula_4">E ce = log(p(y|H)) = (H, y) + log X e y2Y e (H,e y)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task 2: Simile Sentence Classification</head><p>The second task is simile sentence classification.</p><p>To fully exploit contextual information, we con- sider all words in a sentence. For each word, in- stead of using hidden state h t only, we combine h t and its score vector p t as a representation s t :</p><formula xml:id="formula_5">s t = [h t ; p t ]</formula><p>Since p t is directly related to the component extraction task, this labeling connection operation increases the interaction between the two tasks.</p><p>However, words in a sentence should not con- tribute the same for classification. Intuitively, the words corresponding to the tenor or the vehicle or near comparators should be more important. Therefore, we introduce the attention mechanism, which was firstly proposed for neural machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>.</p><p>Given the sequence of expanded word represen- tations S = (s 1 , s 2 , ..., s n ), the attention vector is computed via:</p><formula xml:id="formula_6">↵ = sof tmax (tanh (W ↵ S))<label>(6)</label></formula><p>where W ↵ is a parameter matrix. The semantic representation of the sentence is:</p><formula xml:id="formula_7">r = ↵ T · S<label>(7)</label></formula><p>This representation is fed into an activation and a softmax layer to generate the probability dis- tribution. The loss function is the negative log- likelihood of the correct classification tag:</p><formula xml:id="formula_8">E sc = log (p (y|s)) .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Task 3: Language Modeling</head><p>Although LSTM can capture long dependencies, simile structure may be more related to local con- texts. In many cases, the comparator and the ve- hicle are near, and similes often have some collo- cations involving the comparator such as "oe... 7(the same as)" or "1oe(just like)". As a result, we attempt to emphasize such local information. Inspired by <ref type="bibr" target="#b26">(Rei, 2017)</ref>, we also incorporate language modeling as an auxiliary task. For each word, we let the model predict the next word. In our case, the representation of each word h t is firstly mapped into a low dimension vector space through a nonlinear transform.</p><formula xml:id="formula_9">! m t = tanh ⇣ ! W m · ! h t ⌘<label>(9)</label></formula><p>And the vehicle word is predicted by maximizing the probability of the specific next word, which is generated by a softmax layer.</p><formula xml:id="formula_10">P (w t+1 | ! m t ) = sof tmax ⇣ ! W q · ! m t ⌘<label>(10)</label></formula><p>where, ! m t indicates the forward language model- ing specific features. W m , W q are trainable pa- rameters.</p><p>The loss function for a sequence is defined as the sum of the negative log-likelihood of the pre- dicted words.</p><formula xml:id="formula_11">! E lm = n1 X t=1 log (P (w t+1 | ! m t ))<label>(11)</label></formula><p>We can also predict the previous words in the same way and get another loss function noted as E lm . The losses in double direction are summed to be the loss function for language modeling task.</p><formula xml:id="formula_12">E lm = ! E lm + E lm<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Final Loss Function</head><p>The final loss function for each sentence is a weighted sum of task-specific loss functions.</p><formula xml:id="formula_13">E = · E lm + · E ce + ✏ · E sc (13)</formula><p>where , , ✏ are non-negative weights, which are used to control the importance of three tasks. In experiments, they are hyper-parameters assigned beforehand, and we constrain the sum of , and ✏ to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>The dataset is randomly divided into 5 folds, 4 of which are used as training set and validation set (80% for training, 20% for validation), and the rest one fold is used as test set. All models were trained on the training set. The best hyper- parameters were gained based on the results on the validation set. The results reported were all evalu- ated on the test set. We conduct word segmentation, part-of-speech (POS) tagging and dependency parsing with HIT- LTP 2 .The word embeddings were pre-trained us- ing Word2Vec ( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>), on a large essay corpus crawled from the web. We adopt the Theano framework <ref type="bibr">(Theano Development Team, 2016)</ref> to implement neural network models.</p><p>The dimension of word embeddings is 50. The hidden size of LSTM is 128 for each direction. The dimension of activation layers for component extraction, simile sentence classification and lan- guage modeling are set to 64, 32 and 64 respec- tively. A dropout layer <ref type="bibr" target="#b31">(Srivastava et al., 2014)</ref> is used between the word embedding layer and the bidirectional LSTM layer with the probabil- ity of 0.5. Moreover, early stopping <ref type="bibr" target="#b22">(Prechelt, 1998</ref>) is adopted to finish the learning process. The AdaDelta <ref type="bibr" target="#b37">(Zeiler, 2012)</ref> strategy is used for parameter optimization with a learning rate of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating Simile Sentence Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Comparisons</head><p>We compare the following systems.</p><p>Feature based approaches. With manually de- signed features, we build two Random Forest clas- sifiers to determine whether a sentence contains a simile. Baseline1 follows (Niculae and Danescu- Niculescu-Mizil, 2014). It first extracts candidate simile components and then uses a classifier to de- termine whether they form a simile. We adopt our best neural component extractor (will be in- troduced in Section 5.3.1) to extract components. The classifier uses features including: (1) bag-of- words; (2) corresponding occurrence within con- stituents; (3) word embeddings of extracted com- ponents. Niculae and Danescu-Niculescu-Mizil (2014) also used domain specific information and lexicon knowledge, which our data lacks. Base- line2 is based on ( <ref type="bibr" target="#b13">Li et al., 2008)</ref>, which doesn't  need to identify components beforehand. The fea- tures include: (1) the tokens and POS tags of the words around the comparator within a fixed win- dow (set to 5 in experiments); (2) the tokens, POS tags and dependency relation tags of the words that have dependency relations with the comparator.</p><p>Singletask(SC). This system is a simplified ver- sion of our proposed model in Section 4 by consid- ering the simile sentence classification task only.</p><p>Multitask learning approaches. The full ar- chitecture is described in Section 4. To see their contributions, we add simile component extrac- tion, language modeling and their combination in- crementally. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of the systems. The results are reported with the precision (P), re- call (R), and their harmonic mean F 1 score (F 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>The two feature based methods perform differ- ently. Baseline1 performs poorly. The reason may be that the classification depends on the perfor- mance of component extraction, while even our best component extractor performs far from per- fect, which brings error propagation. In addition, classifying with component related features only ignores much context, which further decreases the performance. Baseline2 considers context win- dows and outperforms baseline1 largely. This con- firms our intuition that context information im- plies the semantic of simile expression.</p><p>Furthermore, we have other observations: (1) neural network based approaches largely outper- form feature-based classifiers;(2) multitask learn- ing approaches outperform every single task ap- proach and other baselines. Both the component extraction and the language modeling task con- tribute for simile sentence classification. Com- ponent extraction improves the precision and lan- guage modeling improves both the precision and the recall. Combining them together can achieve the best performance. The improvement of F 1 score can reach to 3.3% compared with the best single task model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluating Simile Component Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparisons</head><p>We compare the following systems for simile com- ponent extraction.</p><p>Rule based approach. We follow <ref type="bibr" target="#b21">(Niculae and Yaneva, 2013</ref>) to design syntactic patterns for ex- traction. We convert the original patterns to fit the outputs of the parser we used.</p><p>CRF model. Since we view component extrac- tion as a sequence labeling problem, a CRF model with manually designed feature templates is used as a baseline. Feature template is designed for ev- ery word. The features include the tokens and their POS tags within a fixed context window (set to 5 in experiments). We also use dependency parsing based features to capture dependencies between words.</p><p>Singletask(CE). We remove the simile classi- fication and language modeling modules from the multitask learning framework introduced in Sec- tion 4 to build an end-to-end single task compo- nent extractor.</p><p>Pipeline approaches. Pipeline approaches first classify a sentence as simile or not, and then extract components from simile sen- tences.</p><p>We investigate two combinations: RandomForest!CRF, we use baseline2 for sen- tence classification and CRF for component ex- traction; SingleSC!SingleCE, we use neural net- work based single task sentence classifier and component extractor.</p><p>Multitask learning approaches. We exploit simile sentence classification and language mod- eling modules to enhance component extraction in our multitask learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results and Discussion</head><p>A component (i.e., the tenor or the vehicle) is judged to be correct only if both the boundary and the tag exactly match the gold answer. For a sim- ile sentence, we should extract both the tenor and the vehicle rather than only partial components. Therefore, we use pair-wise level precision (P), re- call (R) and F 1 score (F 1 ) for evaluation. A tenor- vehicle pair is viewed as correct only if both com- ponents are correct.  of all manually labeled simile sentences in the test set and the second dataset is the whole test set. We want to compare how component extraction sys- tems work when they know whether a sentence contains a simile or not. We report and discuss the results from the following aspects.</p><p>The effect of simile sentence classification. First, we can compare the results in the middle column and the rightmost column in <ref type="table" target="#tab_4">Table 4</ref>. It is clear that the component extraction systems work much better when they know whether a sentence contains a simile or not. Second, we can see that both pipelines (the feature-based and the neural network based) achieve a better performance com- pared with extracting components directly using either the CRF model or the neural single task model. Third, Multitask(CE+SC) doesn't bring significant improvements compared with the sin- gle task neural model. These observations indi- cate that simile sentence classification is suitable to be a pre-processing for simile component clas- sification. It is necessary to further study how to use high level predictions (sentence classification) to learn better representations for consistently im- proving local predictions (simile component ex- traction).</p><p>Rule based, feature-based and neural mod- els. We can see that even on gold simile sentences, the rule based method doesn't work well. The poor performance of the rule based approach is due to the following reasons. First, the rule-based method is difficult to deal with complex sentence structures. It often fails when there are multiple subordinate clauses. Second, the comparator "oe" in Chinese has multiple syntactic roles, sometimes is used as a verb, sometimes is used as a preposi- tion. Third, the accuracy of Chinese dependency parser still has room to be improved.</p><p>The CRF method performs significantly bet- ter, because it considers more contextual signals. Our neural single task model achieves large im- provements on both datasets. This verifies the effectiveness of the end-to-end approach. Neu- ral models can see a long range of context and learn features automatically. The word embed- dings learned on external resources implicitly have semantic domain information, which is not only useful for generalization but also important for fig- urative language processing.</p><p>The effect of language modeling. Surpris- ingly, using language modeling as an auxiliary task is very useful, especially when dealing with noisy sentences. It gains a 1.3% F 1 improvement on the gold simile sentences due to the improve- ment on the precision and a 3% F 1 improvement on the whole test set due to a large improvement on the recall. Generally, language modeling may help learn better task specific representations, es- pecially when data size is limited <ref type="bibr" target="#b26">(Rei, 2017)</ref>. An- other reason may be that language modeling aims to make local predictions, the same as simile com- ponent extraction. Additional information from the same level may be more useful.</p><p>Some observations help understand the effect of language modeling. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the rel- ative distance of tenors and vehicles to the com- parator. Both tenors and vehicles tend to occur near the comparator and have a clear preference on which side of the comparator. Tenors are more dispersed compared with vehicles, which may in- crease the difficulty. As shown in <ref type="table">Table 5</ref>, the per- formance on identifying tenors is obviously worse than identifying vehicles in both settings. <ref type="figure" target="#fig_1">Figure 3</ref> shows the distribution of extracted components by two settings on the whole test set. We can see that with language modeling, Mul-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Tenor Vehicle P R F1 P R F1 Singletask(CE) 0.7156 0.6935 0.7044 0.7789 0.8313 0.8043 Multitask (CE+LM) 0.6792 0.7881 0.7296 0.7393 0.9026 0.8128 <ref type="table">Table 5</ref>: Experimental results on identifying individual types of components. titask(CE+LM) makes predictions more aggres- sively compared with Singletask(CE) and tends to recognize more nearby components. We also ob- serve that Multitask(CE+LM) identifies more cor- rect UNK components, which are out of vocabu- lary or low frequency concepts. This means that language modeling leads the model to consider more local contextual patterns.  Optimized Pipeline. According to the results and analysis, we can summarize that (1) simile sen- tence classification can achieve good performance by jointly optimizing three tasks; (2) simile com- ponents can be improved with language modeling as an auxiliary task; (3) simile sentence classifi- cation is suitable to be used as a pre-precessing for simile component classification. Therefore, we could build an optimized pipeline. We first use the enhanced simile sentence classifier to filter simile sentences and then use the enhanced component extractor to extract tenors and vehicles. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the optimized pipeline performs better than the strongest multitask learning setting.</p><p>However, in all settings, the precision scores are lower compared with the recall scores. This indicates that compared with identifying surface patterns, distinguishing metaphorical from lit- eral meanings is much harder and more external knowledge should be incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented a study on simile recogni- tion by exploiting neural networks. We construct a manually annotated dataset for advancing the re- search on simile analysis in Chinese. We propose a multitask learning framework, which jointly op- timizes three tasks: simile sentence classification, simile component extraction and language model- ing. The experimental results demonstrate the ef- fectiveness of proposed approaches. It shows that simile sentence classification and simile compo- nent extraction both benefit from multitask learn- ing. Simile sentence classification can achieve a high performance and simile component extrac- tion still has a lot of room to improve.</p><p>In future, we plan to extend this work in several aspects: (1) enrich the simile component struc- ture by adding shared properties or events so that the extracted structures would be more useful for metaphor processing; (2) improve representation learning for recognition by incorporating external knowledge; (3) apply simile recognition to study the use of figurative language in writings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relative distance of tenors and vehicles to the comparator in the dataset.</figDesc><graphic url="image-1.png" coords="9,71.97,269.88,226.67,136.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of extracted simile components on the whole test set.</figDesc><graphic url="image-2.png" coords="9,71.97,465.32,226.67,119.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 shows the basic statistics of our dataset.</head><label>2</label><figDesc></figDesc><table>Word embeddings 

树叶 
(the leaf) 

像 
(is like) 

蝴蝶 
(a butterfly) 

ts 
o 
vs 

Bi-LSTM 

Hidden layer 

Label representation 

CRF layer 

Labels 

Words 

Shared representation 

Simile or not 

+ 

&lt;s&gt; 
&lt;/s&gt; 
像 
树叶 蝴蝶 
像 

Attention 
vector 

Bi-LSTM 

Sentence representation 

Labeling 
connection 

Bi-LSTM 

Backward 
Forward 

Softmax 

Simile component extraction 

Simile sentence classification 

Language modeling 

Predicting 
the next 
words 

Figure 1: The proposed multitask learning framework, which jointly optimizes three tasks. 

#Sentence 
11337 
#Simile sentence 
5088 
#Literal sentence 
6249 
#Token 
334k 
#Tenor 
5183 
#Vehicle 
5119 
#Unique tenor concept 
1680 
#Unique vehicle concept 
1972 
#Tenor-vehicle pair 
5214 
#Unique tenor-vehicle pair 
4521 
Avg. #token per tenor 
1.033 
Avg. #token per vehicle 
1.056 
Avg. #token per sentence 
29.47 
Avg. #pair per simile sentence 1.024 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the annotated simile dataset 

4 Multitask Learning Approach 

4.1 Motivation 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results on simile sentence clas-
sification. SC: simile sentence classification; CE: com-
ponent extraction; LM: language modeling. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 shows the results of various systems and settings on two test sets. The first dataset consists</head><label>4</label><figDesc></figDesc><table>Model 
Gold simile sentences 
Whole test set 
P 
R 
F1 
P 
R 
F1 
Rule based 
0.4094 0.1805 0.2505 
-
CRF 
0.5619 0.5907 0.5760 0.3157 0.3698 0.3406 
Singletask (CE) 
0.7297 0.7854 0.7564 0.5580 0.6489 0.5998 
RandomForest ! CRF 
-
0.4591 0.4980 0.4778 
SingleSC ! SingleCE 
-
0.5720 0.7074 0.6325 
Multitask (CE+SC) 
-
0.5409 0.6400 0.5861 
Multitask (CE+LM) 
0.7530 0.7876 0.7699 0.5741 0.7015 0.6306 
Multitask (CE+SC+LM) 
-
0.5599 0.6989 0.6211 
Optimized pipeline 
-
0.6160 0.7361 0.6707 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results on component extraction. Experiments on dataset of simile sentences assume that 
the sentence classifier is perfect. CE: component extraction; SC: simile sentence classification; LM: language 
modeling. 

</table></figure>

			<note place="foot" n="1"> The dataset is at https://github.com/cnunlp/ Chinese-Simile-Recognition-Dataset</note>

			<note place="foot" n="2"> https://github.com/HIT-SCIR/pyltp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research work is funded by the National Nat-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying beneficial task relations for multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Metaphor: A key to extensible semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 18th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">met*: A method for discriminating metonymy and metaphor by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Fass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="90" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The language of metaphors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Goatly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The roles and structure of comparisons, similes, and metaphors in natural language (an analogical system). Prose (in honor of the Dickens Bicentennial)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Metaphor detection with topic transition, emotion and cognition in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeju</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">951</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Metaphors we live by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computation of chinese simile with &quot;xiang</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Guang</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Chinese Information Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cormet: a computational, corpus-based conventional metaphor extraction system. Computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mason</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="23" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison pattern matching and creative simile recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</title>
		<meeting>the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="110" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Brighter than gold: Figurative language in user generated comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2008" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computational considerations of comparisons and similes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Yaneva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic early stopping using cross validation: quantifying the criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to recognize affective polarity in similes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatically inferring implicit properties in similes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1223" to="1232" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conll &apos;09 design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL &apos;09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metaphor identification using verb and noun clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1002" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metaphor corpus annotated for source-target domain mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh conference on International Language Resources and Evaluation (LREC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3255" to="3261" />
		</imprint>
	</monogr>
	<note>European Languages Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical metaphor processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="353" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Metaphor, memory and meaning: Symbolic and connectionist issues in metaphor interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A computational exploration of creative similes. Metaphor in Use: Context, culture, and communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="329" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A context-sensitive, multi-faceted model of lexico-conceptual affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to understand figurative language: from similes to metaphors to irony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfen</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="683" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stackpropagation: Improved representation learning for syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1557" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
