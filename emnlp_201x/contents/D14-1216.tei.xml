<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classifying Idiomatic and Literal Expressions Using Topic Models and Intensity of Emotions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science/Linguistics</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution">Montclair State University Montclair</orgName>
								<address>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Feldman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science/Linguistics</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution">Montclair State University Montclair</orgName>
								<address>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
							<email>evylomova@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Bauman State Technical University Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classifying Idiomatic and Literal Expressions Using Topic Models and Intensity of Emotions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2019" to="2027"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe an algorithm for automatic classification of idiomatic and literal expressions. Our starting point is that words in a given text segment, such as a paragraph, that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression. Our additional hypothesis is that contexts in which idioms occur, typically , are more affective and therefore, we incorporate a simple analysis of the intensity of the emotions expressed by the contexts. We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal (a target phrase). We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used in the local topic. We treat idioms as semantic outliers, and the identification of a semantic shift as outlier detection. Thus, this topic representation allows us to differentiate idioms from literals using local semantic contexts. Our results are encouraging.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The definition of what is literal and figurative is still object of debate. <ref type="bibr" target="#b0">Ariel (2002)</ref> demonstrates that lit- eral and non-literal meanings cannot always be distin- guished from each other. Literal meaning is originally assumed to be conventional, compositional, relatively context independent, and truth conditional. The prob- lem is that the boundary is not clear-cut, some figu- rative expressions are compositional -metaphors and many idioms; others are conventional -most of the id- ioms. Idioms present great challenges for many Natu- ral Language Processing (NLP) applications. They can violate selection restrictions <ref type="bibr" target="#b22">(Sporleder and Li, 2009)</ref> as in push one's luck under the assumption that only concrete things can normally be pushed. Idioms can disobey typical subcategorization constraints (e.g., in line without a determiner before line), or change the default assignments of semantic roles to syntactic cate- gories (e.g., in X breaks something with Y, Y typically is an instrument but for the idiom break the ice, it is more likely to fill a patient role as in How to break the ice with a stranger). In addition, many potentially id- iomatic expressions can be used either literally or fig- uratively, depending on the context. This presents a great challenge for machine translation. For example, a machine translation system must translate held fire differently in Now, now, hold your fire until I've had a chance to explain. Hold your fire, Bill. You're too quick to complain. and The sergeant told the soldiers to hold their fire. Please hold your fire until I get out of the way. In fact, we tested the last two examples using the Google Translate engine and we got proper translations of the two neither into Russian nor into Hebrew, Span- ish, or Chinese. Most current translation systems rely on large repositories of idioms. Unfortunately, these systems are not capable to tell apart literal from figura- tive usage of the same expression in context. Despite the common perception that phrases that can be idioms are mainly used in their idiomatic sense, <ref type="bibr" target="#b9">Fazly et al. (2009)</ref>'s analysis of 60 idioms has shown that close to half of these also have a clear literal meaning; and of those with a literal meaning, on average around 40% of their usages are literal.</p><p>In this paper we describe an algorithm for automatic classification of idiomatic and literal expressions. Our starting point is that words in a given text segment, such as a paragraph, that are high-ranking representa- tives of a common topic of discussion are less likely to be a part of an idiomatic expression. Our additional hypothesis is that contexts in which idioms occur, typ- ically, are more affective and therefore, we incorpo- rate a simple analysis of the intensity of the emotions expressed by the contexts. We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal (a target phrase). We extract top- ics from paragraphs containing idioms and from para- graphs containing literals using an unsupervised clus- tering method, Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>. Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used in the local topic. We treat idioms as semantic outliers, and the identification of semantic shift as outlier detec- tion. Thus, this topic representation allows us to differ- entiate idioms from literals using the local semantics.</p><p>The paper is organized as follows. Section 2 briefly describes previous approaches to idiom recognition or classification. In Section 3 we describe our approach in detail, including the hypothesis, the topic space repre- sentation, and the proposed algorithm. After describing the preprocessing procedure in Section 4, we turn to the actual experiments in Sections 5 and 6. We then com- pare our approach to other approaches (Section 7) and discuss the results (Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Previous approaches to idiom detection can be classi- fied into two groups: 1) Type-based extraction, i.e., de- tecting idioms at the type level; 2) token-based detec- tion, i.e., detecting idioms in context. Type-based ex- traction is based on the idea that idiomatic expressions exhibit certain linguistic properties that can distinguish them from literal expressions ( <ref type="bibr" target="#b20">Sag et al. (2002);</ref><ref type="bibr" target="#b9">Fazly et al. (2009)</ref>), among many others, discuss various properties of idioms. Some examples of such proper- ties include 1) lexical fixedness: e.g., neither 'shoot the wind' nor 'hit the breeze' are valid variations of the idiom shoot the breeze and 2) syntactic fixedness: e.g., The guy kicked the bucket is potentially idiomatic whereas The bucket was kicked is not idiomatic any- more; and of course, 3) non-compositionality. Thus, some approaches look at the tendency for words to oc- cur in one particular order, or a fixed pattern. Hearst (1992) identifies lexico-syntactic patterns that occur frequently, are recognizable with little or no precoded knowledge, and indicate the lexical relation of interest. <ref type="bibr" target="#b24">Widdows and Dorow (2005)</ref> use Hearst's concept of lexicosyntactic patterns to extract idioms that consist of fixed patterns between two nouns. Basically, their technique works by finding patterns such as "thrills and spills", whose reversals (such as "spills and thrills") are never encountered.</p><p>While many idioms do have these properties, many idioms fall on the continuum from being composi- tional to being partly unanalyzable to completely non- compositional ( <ref type="bibr" target="#b5">Cook et al. (2007)</ref>). <ref type="bibr" target="#b9">Fazly et al. (2009)</ref>; <ref type="bibr" target="#b15">Li and Sporleder (2010)</ref>, among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers to- kens in context is more appropriate for the task of idiom recognition.</p><p>A number of token-based approaches have been discussed in the literature, both supervised ( <ref type="bibr" target="#b14">Katz and Giesbrech (2006)</ref>), weakly supervised ( <ref type="bibr" target="#b1">Birke and Sarkar (2006)</ref>) and unsupervised <ref type="bibr" target="#b22">(Sporleder and Li (2009);</ref><ref type="bibr" target="#b9">Fazly et al. (2009)</ref>). <ref type="bibr" target="#b9">Fazly et al. (2009)</ref> de- velop statistical measures for each linguistic property of idiomatic expressions and use them both in a type- based classification task and in a token identification task, in which they distinguish idiomatic and literal us- ages of potentially idiomatic expressions in context. <ref type="bibr" target="#b22">Sporleder and Li (2009)</ref> present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are con- nected by edges whose value is determined by a seman- tic relatedness function. They experiment with two dif- ferent approaches to semantic relatedness: 1) Depen- dency vectors, as described in <ref type="bibr" target="#b18">Pado and Lapata (2007)</ref>; 2) Normalized Google Distance ( <ref type="bibr" target="#b4">Cilibrasi and Vitányi (2007)</ref>). <ref type="bibr" target="#b22">Sporleder and Li (2009)</ref> show that this method works better for larger contexts (greater than five para- graphs). <ref type="bibr" target="#b15">Li and Sporleder (2010)</ref> assume that literal and figurative data are generated by two different Gaus- sians, literal and non-literal and the detection is done by comparing which Gaussian model has a higher prob- ability to generate a specific instance. The approach assumes that the target expressions are already known and the goal is to determine whether this expression is literal or figurative in a particular context. The impor- tant insight of this method is that figurative language in general exhibits less semantic cohesive ties with the context than literal language.</p><p>Feldman and Peng (2013) describe several ap- proaches to automatic idiom identification. One of them is idiom recognition as outlier detection. They apply principal component analysis for outlier detec- tion -an approach that does not rely on costly an- notated training data and is not limited to a specific type of a syntactic construction, and is generally lan- guage independent. The quantitative analysis provided in their work shows that the outlier detection algorithm performs better and seems promising. The qualitative analysis also shows that their algorithm has to incor- porate several important properties of the idioms: (1) Idioms are relatively non-compositional, comparing to literal expressions or other types of collocations. <ref type="formula" target="#formula_5">(2)</ref> Idioms violate local cohesive ties, as a result, they are semantically distant from the local topics. (3) While not all semantic outliers are idioms, non-compositional semantic outliers are likely to be idiomatic. (4) Id- iomaticity is not a binary property. Idioms fall on the continuum from being compositional to being partly unanalyzable to completely non-compositional.</p><p>The approach described below is taking Feldman and Peng (2013)'s original idea and is trying to address (2) directly and (1) indirectly. Our approach is also somewhat similar to <ref type="bibr" target="#b15">Li and Sporleder (2010)</ref> because it also relies on a list of potentially idiomatic expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Hypothesis</head><p>Similarly to <ref type="bibr" target="#b10">Feldman and Peng (2013)</ref>, out starting point is that idioms are semantic outliers that violate cohesive structure, especially in local contexts. How- ever, our task is framed as supervised classification and we rely on data annotated for idiomatic and literal ex- pressions. We hypothesize that words in a given text segment, such as a paragraph, that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Space Representation</head><p>Instead of the simple bag of words representation of a target document (segment of three paragraphs that con- tains a target phrase), we investigate the bag of words topic representation for target documents. That is, we extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsuper- vised clustering method, Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>). The idea is that if the LDA model is able to capture the semantics of a target docu- ment, an idiomatic phrase will be a "semantic" outlier of the themes. Thus, this topic representation will al- low us to differentiate idioms from literals using the semantics of the local context.</p><p>Let d = {w 1 , · · · , w N } t be a segment (document) containing a target phrase, where N denotes the num- ber of terms in a given corpus, and t represents trans- pose. We first compute a set of m topics from d. We denote this set by</p><formula xml:id="formula_0">T (d) = {t 1 , · · · , t m },</formula><formula xml:id="formula_1">where t i = (w 1 , · · · , w k ) t .</formula><p>Here w j represents a word from a vocabulary of W words. Thus, we have two representations for d: (1) d, represented by its original terms, and (2) ˆ d, represented by its topic terms. Two corresponding term by document matrices will be de- noted by M D and M ˆ D , respectively, where D denotes a set of documents. That is, M D represents the original "text" term by document matrix, while M ˆ D represents the "topic" term by document matrix. <ref type="figure" target="#fig_1">Figure 1</ref> shows the potential benefit of topic space representation. In the figure, text segments containing target phrase "blow whistle" are projected on a two di- mensional subspace. The left figure shows the projec- tion in the "text" space, represented by the term by doc- ument matrix M D . The middle figure shows the projec- tion in the topic space, represented by M ˆ D . The topic space representation seems to provide a better separa- tion.</p><p>We note that when learning topics from a small data sample, learned topics can be less coherent and inter- pretable, thus less useful. To address this issue, regu- larized LDA has been proposed in the literature (New- man et al., 2011). A key feature is to favor words that exhibit short range dependencies for a given topic. We can achieve a similar effect by placing restrictions on the vocabulary. For example, when extracting topics from segments containing idioms, we may restrict the vocabulary to contain words from these segments only. The middle and right figures in <ref type="figure" target="#fig_1">Figure 1</ref> illustrate a case in point. The middle figure shows a projection onto the topic space that is computed with a restricted vocabu- lary, while the right figure shows a projection when we place no restriction on the vocabulary. That is, the vo- cabulary includes terms from documents that contain both idioms and literals.</p><p>Note that by computing M ˆ D , the topic term by doc- ument matrix, from the training data, we have created a vocabulary, or a set of "features" (i.e., topic terms) that is used to directly describe a query or test segment. The main advantage is that topics are more accurate when computed by LDA from a large collection of id- iomatic or literal contexts. Thus, these topics capture more accurately the semantic contexts in which the tar- get idiomatic and literal expressions typically occur. If a target query appears in a similar semantic context, the topics will be able to describe this query as well. On the other hand, one might similarly apply LDA to a given query to extract query topics, and create the query vec- tor from the query topics. The main disadvantage is that LDA may not be able to extract topic terms that match well with those in the training corpus, when ap- plied to the query in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm</head><p>The main steps of the proposed algorithm, called TopSpace, are shown below.</p><formula xml:id="formula_2">Input: D = {d 1 , · · · , d k , d k+1 , · · · , d n }: training documents of k idioms and n − k literals. Q = {q 1 , · · · , q l }: l query documents.</formula><p>1. Let DicI be the vocabulary determined solely from idioms {d 1 , · · · , d k }. Similarly, let DicL be the vocabulary obtained from literals {d k+1 , · · · , d n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For a document</head><formula xml:id="formula_3">d i in {d 1 , · · · , d k }, apply LDA to extract a set of m topics T (d i ) = {t 1 , · · · , t m } using DicI. For d i ∈ {d k+1 , · · · , d n }, DicL is used. 3. LetˆDLetˆ LetˆD = { ˆ d 1 , · · · , ˆ d k , ˆ d k+1 , · · · , ˆ d n } be the resulting topic representation of D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Compute the term by document matrix M ˆ</head><p>D fromˆD fromˆ fromˆD, and let DicT and gw be the resulting dictionary and global weight (idf ), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Compute the term by document matrix M Q from</head><p>Q, using DicT and gw from the previous step.</p><p>Output: M ˆ D and M Q To summarize, after splitting our corpus (see section 4) into paragraphs and preprocessing it, we extract top- ics from paragraphs containing idioms and from para- graphs containing literals. We then compute a term by document matrix, where terms are topic terms and doc- uments are topics extracted from the paragraphs. Our test data are represented as a term-by-document matrix as well (See the details in section 5).  </p><note type="other">2D Text Space: Blow Whistle Idioms Literals −20 −15 −10 −5 0 5 10 15 −5 0 5 10 15 20 2D Topic Space: Blow Whistle Idioms Literals −12 −10 −8 −6 −4 −2 0 2 4 6 8 −10 −5 0 5 10 15 20 25 2D Topic Space: Blow Whistle Idioms Literals</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fisher Linear Discriminant Analysis</head><p>Once M ˆ D and M Q are obtained, a classification rule can be applied to predict idioms vs. literals. The ap- proach we are taking in this work for classifying id- ioms vs. literals is based on Fisher's discriminant anal- ysis (FDA) <ref type="bibr" target="#b11">(Fukunaga, 1990)</ref>. FDA often significantly simplifies tasks such as regression and classification by computing low-dimensional subspaces having statisti- cally uncorrelated or discriminant variables. In lan- guage analysis, statistically uncorrelate or discriminant variables are extracted and utilized for description, de- tection, and classification. <ref type="bibr" target="#b25">Woods et al. (1986)</ref>, for ex- ample, use statistically uncorrelated variables for lan- guage test scores. A group of subjects is scored on a battery of language tests, where the subtests measure different abilities such as vocabulary, grammar or read- ing comprehension. <ref type="bibr" target="#b13">Horvath (1985)</ref> analyzes speech samples of Sydney speakers to determine the relative occurrence of five different variants of each of five vowels sounds. Using this data, the speakers cluster according to such factors as gender, age, ethnicity and socio-economic class.</p><p>A similar approach has been discussed in <ref type="bibr" target="#b19">Peng et al. (2010)</ref>. FDA is a class of methods used in machine learning to find the linear combination of features that best separate two classes of events. FDA is closely related to principal component analysis (PCA), where a linear combination of features that best explains the data. Discriminant analysis explicitly exploits class in- formation in the data, while PCA does not.</p><p>Idiom classification based on discriminant analysis has several advantages. First, as has been mentioned, it does not make any assumption regarding data distri- butions. Many statistical detection methods assume a Gaussian distribution of normal data, which is far from reality. Second, by using a few discriminants to de- scribe data, discriminant analysis provides a compact representation of the data, resulting in increased com- putational efficiency and real time performance.</p><p>In FDA, within-class, between-class, and mixture scatter matrices are used to formulate the criteria of class separability. Consider a J class problem, where m 0 is the mean vector of all data, and m j is the mean vector of jth class data. A within-class scatter ma- trix characterizes the scatter of samples around their respective class mean vector, and it is expressed by</p><formula xml:id="formula_4">S w = J j=1 p j lj i=1 (x j i − m j )(x j i − m j ) t ,<label>(1)</label></formula><p>where l j is the size of the data in the jth class, p j ( j p j = 1) represents the proportion of the jth class contribution, and t denotes the transpose operator. A between-class scatter matrix characterizes the scatter of the class means around the mixture mean m 0 . It is ex- pressed by</p><formula xml:id="formula_5">S b = J j=1 p j (m j − m 0 )(m j − m 0 ) t .<label>(2)</label></formula><p>The mixture scatter matrix is the covariance matrix of all samples, regardless of their class assignment, and it is given by</p><formula xml:id="formula_6">S m = l i=1 (x i − m 0 )(x i − m 0 ) t = S w + S b . (3)</formula><p>The Fisher criterion is used to find a projection matrix W ∈ q×d that maximizes</p><formula xml:id="formula_7">J(W ) = |W t S b W | |W t S w W | .<label>(4)</label></formula><p>In order to determine the matrix W that maximizes J(W ), one can solve the generalized eigenvalue prob- lem: S b w i = λ i S w w i . The eigenvectors corresponding to the largest eigenvalues form the columns of W . For a two class problem, it can be written in a simpler form: S w w = m = m 1 − m 2 , where m 1 and m 2 are the means of the two classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data preprocessing 4.1 Verb-noun constructions</head><p>For our experiments we use the British National Cor- pus (BNC, Burnard <ref type="formula" target="#formula_5">(2000)</ref>) and a list of verb-noun con- structions (VNCs) extracted from BNC by Fazly et al.</p><p>(2009); <ref type="bibr" target="#b6">Cook et al. (2008)</ref> and labeled as L (Literal), I (Idioms), or Q (Unknown). The list contains only those VNCs whose frequency was greater than 20 and that occurred at least in one of two idiom dictionaries ( <ref type="bibr" target="#b7">Cowie et al., 1983;</ref><ref type="bibr">Seaton and Macaulay, 2002</ref>). The dataset consists of 2,984 VNC tokens. For our experi- ments we only use VNCs that are annotated as I or L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lemmatization</head><p>Instead of dealing with various forms of the same root, we use lemmas provided by the BNC XML annotation, so our corpus is lemmatized. We also apply the (modi- fied) Google stop list before extracting the topics. The reason we modified the stop list is that some function words can potentially be idiom components (e.g., cer- tain prepositions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Paragraphs</head><p>We use the original SGML annotation to extract para- graghs from BNC. We only kept the paragraphs that contained VNCs for our experiments. We experi- mented with texts of one paragraph length (single para- graph contexts) and of three-paragraph length (multi- paragraph contexts). An example of multi-paragraph contexts is shown below:</p><p>So, reluctantly, I joined Jack Hobbs in not rocking the boat, reporting the play and the general uproar with perhaps too much impartiality. My reports went to all British newspapers, with special direct services by me to India, South Africa and West Indies; even to King George V in Buckingham Palace, who loved his cricket. In other words, I was to some extent leading the British public astray.</p><p>I regret I can shed little new light on the mystery of who blew the whistle on the celebrated dressing-room scene after Woodfull was hit. while he was lying on the massage table after his innings waiting for a doctor, Warner and Palairet called to express sympathy.</p><p>Most versions of Woodfull's reply seem to agree that he said. There are two teams out there on the oval. One is playing cricket, the other is not. This game is too good to be spoilt. It is time some people got out of it. Warner and Palairet were too taken aback to reply. They left the room in embarrassment.</p><p>Single paragraph contexts simply consist of the mid- dle paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>We have carried out an empirical study evaluating the performance of the proposed algorithm. For compar- ison, the following methods are evaluated. <ref type="formula" target="#formula_4">(1)</ref> The proposed algorithm TopSpace (1), where the data are represented in topic space. <ref type="formula" target="#formula_5">(2)</ref> TexSpace algorithm, where the data are represented in original text space. For each representation, two classification schemes are applied: a) FDA (Eq. 4), followed by the nearest neigh- bor rule. b) SVMs with Gaussian kernels <ref type="bibr" target="#b8">(Cristianini and Shawe-Taylor (2000)</ref>). For the nearest neighbor rule, the number of nearest neighbors is set to n/5, where n denotes the number of training examples. For SVMs, kernel width and soft margin parameters are set to default values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Sets</head><p>The following data sets are used to evaluate the perfor- mance of the proposed technique. These data sets have enough examples from both idioms and literals to make our results meaningful. On average, the training data is 6K word tokens. Our test data is of a similar size.</p><p>BlowWhistle: This data set has 78 examples, 27 of which are idioms and the remaining 51 are literals. The training data for BlowWhistle consist of 40 randomly chosen examples (20 paragraphs containing idioms and 20 paragraphs containing literals). The remaining 38 examples (7 idiomatic and 31 literals) are used as test data.</p><p>MakeScene: This data set has 50 examples, 30 of which are paragraphs containing idioms and the re- maining 20 are paragraphs containing literals. The training data for MakeScene consist of 30 randomly chosen examples, 15 of which are paragraphs contain- ing make scene as an idiom and the rest 15 are para- graphs containing make scene as a literal. The remain- ing 20 examples (15 idiomatic paragraphs and 5 liter- als) are used as test data.</p><p>LoseHead: This data set has 40 examples, 21 of which are idioms and the remaining 19 are literals. The training data for LoseHead consist of 30 randomly chosen examples (15 idiomatic and 15 literal). The remaining 10 examples (6 idiomatic and 4 literal) are used as test data.</p><p>TakeHeart: This data set has 81 examples, 61 of which are idioms and the remaining 20 are literals. The training data for TakeHeart consist of 30 randomly chosen examples (15 idiomatic and 15 literals). The remaining 51 examples (46 idiomatic and 5 literals) are used as test data. <ref type="bibr" target="#b17">Nunberg et al. (1994)</ref> notice that "idioms are typically used to imply a certain evaluation or affective stance toward the things they denote". Language users usu- ally choose an idiom in non-neutral contexts. The situ- ations that idioms describe can be positive or negative; however, the polarity of the context is not as impor- tant as the strength of the emotion expressed. So, we decided to incorporate the knowledge about the emo- tion strength into our algorithm. We use a database of word norms collected by <ref type="bibr" target="#b23">Warriner et al. (2013)</ref>. This database contains almost 14,000 English lemmas an- notated with three components of emotions: valence (the pleasantness of a stimulus), arousal (the intensity of emotion provoked by a stimulus), and dominance <ref type="table">Table 1</ref>: Average accuracy of competing methods on four datasets in single paragraph contexts: A = Arousal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adding affect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>BlowWhistle LoseHead MakeScene TakeHeart Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc FDA-Topics 0.44 0.40 0.79 0.70 0.90 0.70 0.82 0.97 0.81 0.91 0.97 0.89 FDA-Topics+A 0.51 0.51 0.75 0.78 0.68 0.66 0.80 0.99 0.80 0.93 0.84 0.80 FDA-Text 0.37 0.81 0.63 0.60 0.88 0.58 0.82 0.89 0.77 0.36 0.38 0.41 FDA-Text+A 0.42 0.49 0.76 0.64 0.92 0.63 0.83 0.95 0.82 0.75 0.53 0.53 SVMs-Topics 0.08 0.39 0.59 0.28 0.25 0.45 0.59 0.74 0.61 0.91 1.00 0.91 SVMs-Topics+A 0.06 0.21 0.69 0.38 0.18 0.44 0.53 0.40 0.44 0.91 1.00 0.91 SVMs-Text 0.08 0.39 0.59 0.36 0.60 0.52 0.23 0.30 0.40 0.42 0.16 0.22 SVMs-Text+A 0.15 0.51 0.60 0.31 0.38 0.48 0.37 0.40 0.45 0.95 0.48 0.50</p><p>(the degree of control exerted by a stimulus). These components were elicited from human subjects via an Amazon Mechanical Turk crowdsourced experiment. We only used the arousal feature in our experiments because we were interested in the intensity of the emo- tion rather than its valence. For a document d = {w 1 , · · · , w N } t , we calculate the corresponding arousal value a i for each w i , ob-</p><formula xml:id="formula_8">taining d A = {a 1 , · · · , a N } t .</formula><p>Let m A be the aver- age arousal value calculated over the entire training data. The centered arousal value for a training docu- ment is obtained by subtracting m A from d A , i.e.,</p><formula xml:id="formula_9">¯ d A = d A −m A = {a 1 −m A , · · · , a N −m A } t .</formula><p>Similarly, the centered arousal value for a query is computed accord- ing to</p><formula xml:id="formula_10">¯ q A = q A − m A = {q 1 − m A , · · · , q N − m A } t .</formula><p>That is, the training arousal mean is used to center both training and query arousal values. The corresponding arousal matrices for D, ˆ D, and Q are A D , A ˆ D , A Q , re- spectively. To incorporate the arousal feature, we sim- ply compute</p><formula xml:id="formula_11">Θ D = M D + A D ,<label>(5)</label></formula><p>and</p><formula xml:id="formula_12">Θ ˆ D = M ˆ D + A ˆ D .<label>(6)</label></formula><p>The arousal feature can be similarly incoporated into query Θ Q = M Q + A Q . <ref type="table">Table 1</ref> shows the average precision, recall, and ac- curacy of the competing methods on the four data sets over 10 runs in simple paragraph contexts. <ref type="table" target="#tab_0">Table  2</ref> shows the results for the multi-paragraph contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Note that for single paragraph contexts, we chose two topics, each having 10 terms. For multi-paragrah con- texts, we had four topics, with 10 terms per topic. No optimization was made for selecting the number of top- ics as well as the number of terms per topic. In the tables, the best performance in terms of the sum of pre- cision, recall and accuracy is given in boldface.</p><p>The results show that the topic representation achieved the best performance in 6 out of 8 cases. The arousal feature (Eqs 5 and 6) also improved the overall performance, particularly in text representation (Eq. 5). This can be seen in the top panel in <ref type="figure" target="#fig_5">Figure 3</ref>. In fact, in 2/8 cases, text representation coupled with the arousal feature achieved the best performance. One possible explanation is that the LDA model already per- formed "feature" selection (choosing topic terms), to the extent possible. Thus, any additional information such as arousal only provides marginal improvement at the best (bottom panel in <ref type="figure" target="#fig_5">Figure 3</ref>). On the other hand, original text represents "raw" features, whereby arousal information helps provide better contexts, thus improving overall performance.  <ref type="table" target="#tab_0">Tables 1 and 2</ref>, where the arousal feature generally improves text representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Comparisons with other approaches</head><p>Even though we used <ref type="bibr" target="#b9">Fazly et al. (2009)</ref>'s dataset for these experiments, the direct comparison with their method is impossible here because our task is formu- lated differently and we do not use the full dataset for the experiments.   model that relies on the so-called canonical forms gives 72.4% (macro-)accuracy on the extraction of idiomatic tokens when evaluated on their test data.</p><p>We cannot compare our method directly with the other methods discussed in section 2 either because each uses a different dataset or formulates the task differently (detection vs. recognition vs. identifica- tion). However, we can compare the method presented here with Feldman and Peng (2013) who also experi- ment with LDA, use similar data, and frame the prob- lem as classification. Their goal, however, is to clas- sify sentences as either idiomatic or literal. To obtain a discriminant subspace, they train their model on a small number of randomly selected idiomatic and non- idiomatic sentences. They then project both the train- ing and the test data on the chosen subspace and use the three nearest neighbor (3NN) classifier to obtain accuracy. The average accuracy they report is 80%. Our method clearly outperforms the Feldman and Peng (2013) approach (at least on the dataset we use).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Conclusion</head><p>We have described an algorithm for automatic classi- fication of idiomatic and literal expressions. We have investigated the bag of words topic representation for target documents (segments of one or three paragraphs that contains a target phrase). The approach definitely outperforms the baseline model that is based on the simple bag of words representation, but it also outper- forms approaches previously discussed in the literature. Our model captures the local semantics and thus is ca- pable to identify semantic outliers (=idioms).</p><p>While we realize that the data set we use is small, the results are encouraging. We notice that using 3 para- graphs for local contexts improves the performance of the classifiers. The reason is that some paragraphs are relatively short. A larger context provides more related terms, which gives LDA more opportunities to sample these terms.</p><p>Idioms are also relatively non-compositional. While we do not measure their non-compositionality in this approach, we indirectly touch upon this property by hy- pothesizing that non-compositional idiomatic expres- sions are likely to be far from the local topics.</p><p>We feel that incorporating the intensity of emotion expressed by the context into our model improves per- formance, in particular, in text representation. When we performed a qualitative analysis of the results try- ing to determine the causes of false positives and neg- atives, we noticed that there were quite a number of cases that improved after incorporating the arousal fea- ture into the model. For example, the FDA:topic classi- fier labels "blow the whistle" as literal in the following context, but FDA:topics+A marks this expression as id- iomatic (italicized words indicate words with relatively high arousal values):</p><p>Peter thought it all out very carefully. He decided the wis- est course was to pool all he had made over the last two years, enabling Julian to purchase the lease of a high street property. This would enable them to set up a business on a more set- tled and permanent trading basis. Before long they opened a grocery-cum-delicatessen in a good position as far as passing trade was concerned. Peter's investment was not misplaced. The business did very well with the two lads greatly appreci- ated locally for their hard work and quality of service. The range of goods they were able to carry was welcomed in the area, as well as lunchtime sandwich facilities which had pre- viously been missing in the neighbourhood.</p><p>Success was the fruit of some three years' strenuous work. But it was more than a shock when Julian admitted to Pe- ter that he had been running up huge debts with their bank. Peter knew that Julian gambled, but he hadn't expected him to gamble to that level, and certainly not to use the shop as security. With continual borrowing over two years, the bank had blown the whistle. Everything was gone. Julian was bankrupt. Even if they'd had a formal partnership, which they didn't, it would have made no difference. Peter lost all he'd made, and with it his chance to help his parents and his younger brother and sister, Toby and Laura.</p><p>Peter was heartbroken. His father had said all along: nei- ther a lender nor a borrower. Peter had found out the hard way. But as his mother observed, he was the same Peter, he'd pick himself up somehow. Once again, Peter was resolute. He made up his mind he'd never make the same mistake twice. It wasn't just the money or the hard work, though the waste of that was difficult enough to accept. Peter had been working a debt of love. He'd done all this for his parents, particularly for his father, whose dedication to his children had always impressed Peter and moved him deeply. And now it had all come to nothing.</p><p>Therefore, we think that idioms have the tendency to appear in more affective contexts; and we think that in- corporating more sophisticated sentiment analysis into our model will improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 2D projection of text segments containing "blow whistle." Left panel: Original text space. Middle panel: Topic space with restricted vocabulary. Right panel: Topic space with enlarged vocabulary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Aggregated performance: Topic vs text representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 shows</head><label>4</label><figDesc>a case in point: the average (sorted) arousal values of idioms and literals of the target phrase "lose head." The upper panel plots arousal values in the text space, while lower panel plots arousal values in the topic space. The plot supports the results shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fazly et al. (2009)'s unsupervised</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Aggregated performance: Text vs. text+Arousal representations (top) and Topics vs. Topics+Arousal representations (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average arousal values-Upper panel: Text space. Lower panel: Topic space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Average accuracy of competing methods on four datasets in multiple paragraph contexts: A = Arousal</head><label>2</label><figDesc></figDesc><table>Model 
BlowWhistle 
LoseHead 
MakeScene 
TakeHeart 
Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc 
FDA-Topics 
0.62 0.60 0.83 0.76 0.97 0.78 0.79 0.95 0.77 0.93 0.99 0.92 
FDA-Topics+A 0.47 0.44 0.79 0.74 0.93 0.74 0.82 0.69 0.65 0.92 0.98 0.91 
FDA-Text 
0.65 0.43 0.84 0.72 0.73 0.65 0.79 0.95 0.77 0.46 0.40 0.42 
FDA-Text+A 
0.45 0.49 0.78 0.67 0.88 0.65 0.80 0.99 0.80 0.47 0.29 0.33 
SVMs-Topics 
0.07 0.40 0.56 0.60 0.83 0.61 0.46 0.57 0.55 0.90 1.00 0.90 
SVMs-Topics+A 0.21 0.54 0.55 0.66 0.77 0.64 0.42 0.29 0.41 0.91 1.00 0.91 
SVMs-Text 
0.17 0.90 0.25 0.30 0.50 0.50 0.10 0.01 0.26 0.65 0.21 0.26 
SVMs-Text+A 
0.24 0.87 0.41 0.66 0.85 0.61 0.07 0.01 0.26 0.74 0.</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the Na-tional Science Foundation under Grant No. 1319846. We also thank the anonymous reviewers for useful comments. The third author thanks the Fulbright Foun-dation for giving her an opportunity to conduct this re-search at Montclair State University (MSU).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The demise of unique concept of literal meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ariel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="361" to="402" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A clustering approach to the nearly unsupervised recognition of nonliteral language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Birke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;06)</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;06)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="329" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The British National Corpus Users Reference Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford University Computing Services</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The google similarity distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M B</forename><surname>Vitányi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="383" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 07 Workshop on A Broader Perspective on Multiword Expressions</title>
		<meeting>the ACL 07 Workshop on A Broader Perspective on Multiword Expressions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The VNC-Tokens Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC Workshop: Towards a Shared Task for Multiword Expressions</title>
		<meeting>the LREC Workshop: Towards a Shared Task for Multiword Expressions<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Oxford Dictionary of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Mccaig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Idiomatic English</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1983" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and other kernel-based learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised Type and Token Identification of Idiomatic Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="103" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic detection of idiomatic clauses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Computational Linguistics</title>
		<meeting>the 14th Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
	<note>COLING &apos;92. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Variation in Australian English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Horvath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Cambridge University PRess</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic Identification of Non-compositional Multiword Expressions using Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Giesbrech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties</title>
		<meeting>the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using Gaussian Mixture Models to Detect Figurative Language in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
		<meeting>NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving topic coherence with regularized topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nunberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wasow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Idioms. Language</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="538" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computing linear discriminants for idiomatic sentence detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Street</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in Computing Science, Special issue: Natural Language Processing and its Applications</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="17" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiword expressions: A Pain in the Neck for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Intelligence Text Processing and Computational Linguistics</title>
		<meeting>the 3rd International Conference on Intelligence Text Processing and Computational Linguistics<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Collins COBUILD Idioms Dictionary</title>
		<editor>Seaton, M. and A. Macaulay</editor>
		<imprint>
			<publisher>HarperCollins Publishers</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Recognition of Literal and Non-literal Use of Idiomatic Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL &apos;09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 english lemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic extraction of idioms using graph analysis and asymmetric lexicosyntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, DeepLA &apos;05</title>
		<meeting>the ACL-SIGLEX Workshop on Deep Lexical Acquisition, DeepLA &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hughes</surname></persName>
		</author>
		<title level="m">Statistics in Language Studies</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
