<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Parsing with Relaxed Hybrid Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Information Systems Technology and Design</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Parsing with Relaxed Hybrid Trees</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1308" to="1318"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel model for parsing natural language sentences into their formal semantic representations. The model is able to perform integrated lexicon acquisition and semantic parsing, mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a re-cursive manner, where certain overlap-pings amongst such word sequences are allowed. It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics. Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding. Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner. We demonstrate through experiments that by exploiting a large collection of simple features, our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages. The system and code can be downloaded from http://statnlp.org/research/sp/.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing, the task of transforming natu- ral language sentences into formal representations of their underlying semantics, is one of the clas- sic goals for natural language processing and ar- tificial intelligence. This area of research recently has received a significant amount of attention. Var- ious models have been proposed over the past few years ( <ref type="bibr" target="#b24">Zettlemoyer and Collins, 2005</ref> Following previous research efforts, we perform semantic parsing under a setting where the seman- tics for complete sentences are provided as train- ing data, but detailed word-level semantic infor- mation is not explicitly given during the training phase. As one example, consider the following natural language sentence paired with its corre- sponding semantic representation:</p><p>What rivers do not run through <ref type="bibr">Tennessee ? answer(exclude(river(all)</ref>, traverse(stateid( tn ))))</p><p>The training data consists of a set of sentences paired with semantic representations. Our goal is to learn from such pairs a model, which can be effectively used for parsing novel sentences into their semantic representations.</p><p>Certain assumptions about the semantics are typically made. One common assumption is that the semantics can be represented as certain re- cursive structures such as trees, which consist of atomic semantic units as tree nodes. For exam- ple, the above semantics can be converted into an equivalent tree structure as illustrated in <ref type="figure">Figure 1</ref>. We will provide more details about such tree struc- tured semantic representations in Section 2.1.</p><p>Currently, most state-of-the-art approaches that deal with such tree structured semantic represen- tations either cast the semantic parsing problem as a statistical string-to-string transformation prob- lem ( <ref type="bibr" target="#b22">Wong and Mooney, 2006</ref>), which ignores the potentially useful structural information of the tree, or employ latent-variable models to cap- ture the correspondences between words and tree nodes using a generative approach ( <ref type="bibr" target="#b15">Lu et al., 2008;</ref><ref type="bibr" target="#b7">Jones et al., 2012)</ref>. While generative models can be used to flexibly model the correspondences be- tween individual words and semantic nodes of the tree, such an approach is limited to modeling local dependencies and is unable to flexibly incorporate a large set of potentially useful features.</p><p>In this work, we propose a novel model for parsing natural language into tree structured se- mantic representations. Specifically, we propose a novel relaxed hybrid tree representation which jointly encodes both natural language sentences and semantics; such representations can be effec- tively learned with a latent-variable discriminative model where long-distance dependencies can be captured. We present dynamic programming al- gorithms for efficient learning and decoding. With a large collection of simple features, our model reports state-of-the-art results on benchmark data annotated with four different languages.</p><p>Furthermore, although we focus our discussions on semantic parsing in this work, our proposed model is a general. Essentially our model is a dis- criminative string-to-tree model which recursively maps overlapping contiguous word sequences to tree nodes at different levels, where efficient dy- namic programming algorithms can be used. Such a model may find applications in other areas of natural language processing, such as statistical machine translation and information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantics</head><p>Various semantic formalisms have been consid- ered for semantic parsing. Examples include the tree-structured semantic representations <ref type="bibr" target="#b22">(Wong and Mooney, 2006</ref>), the lambda calculus expres- sions ( <ref type="bibr" target="#b24">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b23">Wong and Mooney, 2007)</ref>, and dependency-based compo- sitional semantic representations ( <ref type="bibr" target="#b11">Liang et al., 2013)</ref>. In this work, we specifically focus on the tree-structured representations for semantics.</p><p>Each semantic representation consists of se- mantic units as its tree nodes, where each semantic unit is of the following form:</p><formula xml:id="formula_0">m a ≡ τ a : p α (τ b * )<label>(1)</label></formula><p>Here m a is used to denote a complete seman- tic unit, which consists of its semantic type τ a , its function symbol p α , as well as an argument list τ b * (we assume there are at most two arguments for each semantic unit). In other words, each seman- tic unit can be regarded as a function which takes in other semantics of specific types as arguments, and returns new semantics of a particular type. For example, in <ref type="figure">Figure 1</ref>, the semantic unit at the root has a type QUERY, a function name answer, and a single argument type RIVER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Representations</head><p>Semantic parsing models transform sentences into their corresponding semantics. It is therefore es- sential to make proper assumptions about joint representations for language and semantics that capture how individual words and atomic seman- tic units connect to each other. Typically, differ- ent existing models employ different assumptions for establishing such connections, leading to very different definitions of joint representations. We survey in this section various representations pro- posed by previous works.</p><p>The WASP semantic parser ( <ref type="bibr" target="#b22">Wong and Mooney, 2006</ref>) essentially casts the semantic parsing prob- lem as a string-to-string transformation problem by employing a statistical phrase-based machine translation approach with synchronous gram- mars <ref type="bibr" target="#b1">(Chiang, 2007)</ref>. Therefore, one can think of the joint representation for both language and se- mantics as a synchronous derivation tree consist- ing of those derivation steps for transforming sen- tences into target semantic representation strings. While this joint representation is flexible, allow- ing blocks of semantic structures to map to word sequences, it does not fully exploit the structural information (tree) as conveyed by the semantics.</p><p>The KRISP semantic parser ( <ref type="bibr" target="#b8">Kate and Mooney, 2006</ref>) makes use of Support Vector Machines with string kernels ( <ref type="bibr" target="#b13">Lodhi et al., 2002</ref>) to recursively map contiguous word sequences into semantic units to construct a tree structure. Our relaxed hybrid tree structures also allow input word se- quences to map to semantic units in a recursive manner. One key distinction, as we will see, is that our structure distinguishes words which are imme-diately associated with a particular semantic unit, from words which are remotely associated.</p><p>The SCISSOR model <ref type="bibr" target="#b4">(Ge and Mooney, 2005</ref>) performs integrated semantic and syntactic pars- ing. The model parses natural language sentences into semantically augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds semantic representations based on such augmented trees. Such a joint representation conveys more information, but requires language- specific syntactic analysis.</p><p>The hybrid tree model ( <ref type="bibr" target="#b15">Lu et al., 2008</ref>) is based on the assumption that there exists an underlying generative process which jointly produces both the sentence and the semantic tree in a top-down re- cursive manner. The generative process results in a hybrid tree structure which consists of words as leaves and semantic units as nodes. An example hybrid tree structure is shown in <ref type="figure">Figure 2</ref> (a). Such a representation allows each semantic unit to map to a possibly discontiguous sequence of words. The model was shown to be effective empirically, but it implicitly assumes that both the sentence and semantics exhibit certain degree of structural sim- ilarity that allows the hybrid tree structures to be constructed.</p><p>UBL ( <ref type="bibr" target="#b9">Kwiatkowski et al., 2010</ref>) is a semantic parser based on restricted higher-order unification with CCG <ref type="bibr" target="#b20">(Steedman, 1996)</ref>. The model can be used to handle both tree structured semantic rep- resentations and lambda calculus expressions, and assumes there exist CCG derivations as joint rep- resentations in which each semantic unit is associ- ated with a contiguous word sequence where over- lappings amongst word sequences are not allowed. <ref type="bibr" target="#b7">Jones et al. (2012)</ref> recently proposed a frame- work that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian setting. Thus, their representations also potentially present simi- lar issues as the ones mentioned above.</p><p>Besides these supervised approaches, recently there are also several works that take alternative learning approaches to (mostly task-dependent) semantic parsing. <ref type="bibr" target="#b18">Poon and Domingos (2009)</ref> pro- posed a model for unsupervised semantic pars- ing that transforms dependency trees into seman- tic representations using Markov logic <ref type="bibr" target="#b19">(Richardson and Domingos, 2006</ref>). <ref type="bibr" target="#b2">Clarke et al. (2010)</ref> proposed a model that learns a semantic parser Symbol Description n A complete natural language sentence m A complete semantic representation h A complete latent joint representation (or, a relaxed hybrid tree for our work) H(n, m) A complete set of latent joint representations that contain the (n, m) pair exactly n, na A contiguous sequence of words w, w k A natural language word m, ma A semantic unit h, ha A node in the relaxed hybrid tree Φ The feature vector φ k The k-th feature Λ The weight vector (model parameters) λ k The weight for the k-th feature φ k  <ref type="formula" target="#formula_0">(2013)</ref> proposed a model for learning the dependency-based compositional se- mantics (DCS) which can be used for optimiz- ing the end-task performance. Artzi and Zettle- moyer (2013) proposed a model for mapping in- structions to actions with weak supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We discuss our approach to semantic parsing in this section. The notation that we use in this paper is summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>In standard supervised syntactic parsing, one typi- cally has access to a complete syntactic parse tree for each sentence in the training phase, which ex- actly tells the correct associations between words and syntactic labels. In our problem, however, each sentence is only paired with a complete se- mantic representation where the correct associa- tions between words and semantic units are un- available. We thus need to model such information with latent variables. For a given n-m pair (where n is a complete natural language sentence, and m is a complete semantic representation), we assume there exists a latent joint representation h that consists of both n and m which tells the correct associations between words and semantic units in such a pair. We use H(n, m) 1 to denote the set of all such possible (a) ma (w1 w2 w3) w4 w5 w6 w7 w8 w9 w10 <ref type="figure">Figure 2</ref>: Two different ways of jointly representing sentences and their semantics. The hybrid tree representation of <ref type="bibr">Lu et al. (2008) (left)</ref>, and our novel relaxed hybrid tree representation (right). In our representation, a word w can be either immediately associated with its parent m (the words which appear inside the parenthesis), or remotely associated with m (the words that do not appear inside the parenthesis, and will also appear under a subtree rooted by one of m's children). latent joint representations that contain both n and m exactly.</p><formula xml:id="formula_1">m b (w4 w5) w6 w7 (w8) w9 (w10) m d (w9) mc (w6 w7) (b)</formula><p>Given the joint representations, to model how the data is generated, one can either take a gener- ative approach which models the joint probability distribution over (n, m, h) tuples, or a discrimina- tive approach which models the distribution over (m, h) tuples given the observation n. Following several previous research efforts <ref type="bibr" target="#b24">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b9">Kwiatkowski et al., 2010;</ref><ref type="bibr" target="#b11">Liang et al., 2013)</ref>, in this work we define a discriminative model using a log-linear approach:</p><formula xml:id="formula_2">P (m, h|n; Λ) = e Λ·Φ(n,m,h) m ,h ∈H(n,m ) e Λ·Φ(n,m ,h ) (2)</formula><p>Here Φ(n, m, h) is a function defined over the tuple (n, m, h) that returns a vector consisting of counts of features associated with the tuple, and Λ is a vector consisting of feature weights, which are the parameters of the model.</p><p>In practice, we are only given the n-m pairs but the latent structures are not observed. We there- fore consider the following marginal probability:</p><formula xml:id="formula_3">P (m|n; Λ) = h∈H(n,m) P (m, h|n; Λ) = h∈H(n,m) e Λ·Φ(n,m,h) m ,h ∈H(n,m ) e Λ·Φ(n,m ,h )<label>(3)</label></formula><p>The above probability is defined for a particular n-m pair. The complete log-likelihood objective for the training set is:</p><formula xml:id="formula_4">L(Λ) = i log P (m i |n i ; Λ) − κ||Λ|| 2 = i log h∈H(n i ,m i ) P (m i , h|n i ; Λ) − κ||Λ|| 2 (4)</formula><p>where (n i , m i ) refers to the i-th instance in the training set. Note that here we introduce the ad- ditional regularization term −κ · ||Λ|| 2 to control over-fitting, where κ is a positive scalar.</p><p>Our goal is to maximize this objective function by tuning the model parameters Λ. Let's assume Λ = λ 1 , λ 2 , . . . , λ N , where N is the total num- ber of features (or the total number of parameters). Differentiating with respect to λ k , the weight as- sociated with the k-th feature φ k , yields:</p><formula xml:id="formula_5">∂L(Λ) ∂λ k = i h E P (h|n i ,m i ;Λ) [φ k (n i , m i , h)] − i m,h E P (m,h|n i ;Λ) [φ k (n i , m, h)] − 2κλ k (5)</formula><p>where φ k (n, m, h) refers to the number of occur- rences for the k-th feature in the tuple (n, m, h). Given the objective value (4) and gradients (5), standard methods such as stochastic gradient de- scent or L-BFGS ( <ref type="bibr" target="#b12">Liu and Nocedal, 1989)</ref> can be employed to optimize the objective function. We will discuss the computation of the objective func- tion and gradients next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relaxed Hybrid Trees</head><p>To allow tractable computation of the values for the objective function (4) and the gradients (5),    Figure 3: An example hybrid tree and an example relaxed hybrid tree representation. When the correct latent structure can not be found, the dependency between the words "how many" and the semantic unit "NUM : count(STATE)" can not be captured if the hybrid tree is used, whereas with our relaxed hybrid tree representation, such a dependency can still be captured.</p><p>certain restrictions on the latent structures (h) will need to be imposed. We define in this section the set of all valid latent structures H(n, m) for the (n, m) pair so that some efficient dynamic pro- gramming algorithms can be deployed.</p><p>We introduce our novel relaxed hybrid tree rep- resentations which jointly encode both natural lan- guage sentences and the tree-structured semantics. A relaxed hybrid tree h defined over (n, m) is a tree whose nodes are (n, m) pairs, where each n is a contiguous sequence of words from n, and each m is a semantic unit (a tree node) from m. For any two nodes h a ≡ (n a , m a ) and h b ≡ (n b , m b ) that appear in the relaxed hybrid tree h, if h a is the parent of h b in h, then m a must also be the parent of m b in m, and n a must contain n b . If the low- est common ancestor of h a and h b in h is neither h a nor h b , then n a and n b do not share any com- mon word. Note that words that appear at different positions in n are regarded as different words, re- gardless of their string forms. <ref type="figure">Figure 2</ref> (b) shows an example relaxed hybrid tree structure that we consider. Assume we would like to jointly represent both the natural language sentence n ≡ w 1 w 2 . . . w 10 and its corresponding semantic representation m ≡ m a <ref type="figure">(m b (m c , m d )</ref>). In the given example, the semantic unit m a maps to the complete sentence, m b maps to the sequence w 4 w 5 . . . w 10 , m c maps to w 6 w 7 , and m d maps to w 9 . Certain words such as w 4 and w 10 that ap- pear directly below the semantic unit m b but do not map to any of m b 's child semantic units are highlighted with parentheses "()", indicating they are immediately associated with m b . These words play unique roles in the sub-tree rooted by m b and are expected to be semantically closely related to m b . Note that each word is immediately associ- ated with exactly one semantic unit.</p><p>As a comparison, we also show an example hy- brid tree representation ( <ref type="bibr" target="#b15">Lu et al., 2008</ref>) in <ref type="figure">Fig- ure 2 (a)</ref> that has similar words-semantics cor- respondences. Different from our representation, the hybrid tree representation assumes each natu- ral language word only maps to a single seman- tic unit (which is its immediate parent), and each semantic unit maps to a possibly discontiguous sequence of words. We believe that such a rep- resentation is overly restrictive, which might ex- hibit problems in cases where natural language sentences are highly non-isomorphic to their se- mantic tree structures. Under our relaxed hybrid tree representations, words that are immediately associated with a particular semantic unit now can also be remotely associated with all its parent se- mantic units as well. Essentially, our representa- tion allows us to capture certain unbounded depen- dencies -for any word, as long as it appears be- low a certain semantic unit (in the relaxed hybrid tree), we can always capture the dependency be- tween the two, regardless of which actual seman- tic unit that word is immediately associated with. Such an important relaxation allows some long- distance dependencies to be captured, which can potentially alleviate the sentence-semantics non- isomorphism issue reported in several earlier se- mantic parsing works ( <ref type="bibr" target="#b8">Kate and Mooney, 2006;</ref><ref type="bibr" target="#b23">Wong and Mooney, 2007)</ref>.</p><p>To better illustrate the differences, we show a concrete example in <ref type="figure">Figure 3</ref>, where the correct latent structure showing the correspondences be- tween words and semantic units can not be found with the hybrid tree model. As a result, the hy- brid tree model will fail to capture the correct de- pendency between the words "how many" and the semantic unit "NUM : count(STATE)". On the other hand, with our relaxed hybrid tree represen- tation, such a dependency can still be captured, since these words will still be (remotely) associ- ated with the semantic unit.</p><p>Such a relaxed hybrid tree representation, when further constrained with the word association pat- terns that we will introduce next, allows both the objective function (4) and the gradients of <ref type="formula">(5)</ref> to be computed through the dynamic programming algorithms to be presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Association Patterns</head><p>As we have mentioned above, in the relaxed hy- brid tree structures, each word w under a certain semantic unit m can either appear directly below m only (immediately associated with m), or can also appear in a subtree rooted by one of m's child semantic unit (remotely associated with m).</p><p>We allow several different ways for word asso- ciations and define the allowable patterns for se- mantic units with different number of arguments in <ref type="table" target="#tab_2">Table 2</ref>. Such patterns are defined so that our model is amendable to dynamic programming al- gorithms to be discussed in Sec 4. In this table, w refers to a contiguous sequence of natural lan- guage words that are immediately associated with the current semantic unit, while X and Y refers to a sequence of natural language words that the first and second child semantic unit will map to, respectively.</p><p>For example, in <ref type="figure">Figure 2</ref> (b), the word sequence directly below the semantic unit m a follows the pattern wX (since the word sequence w 1 w 2 w 3 is immediately associated with m a , and the remain- ing words are remotely associated with m a ), and the word sequence below m b follows wXwYw 2 .</p><p>The word association patterns are similar to those hybrid patterns used in hybrid trees. One key difference is that we disallow the unary pat-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head><p>The features are defined over the (n, m, h) tuples. In practice, we define features at each level of the relaxed hybrid tree structure h. In other words, features are defined over (n, m) tuples where n is a contiguous sequence of natural language words (immediately or remotely) associated with the se- mantic unit m (recall that h contains both n and m, and each level of h simply consists of a seman- tic unit and a contiguous sequence of words). Each feature over (n, m) is then further decomposed as a product between two indicator feature func- tions, defined over the natural language words (n) and semantic unit (m) respectively: φ(n, m) = φ i (n) × φ o (m). For each φ i (n) we define two types of features: the local features, which are de- fined over immediately associated words only, and the span features, which are defined over all (im- mediately or remotely) associated words to cap- ture long range dependencies. The local features include word unigrams and bigrams, the word association patterns, as well as character-level features <ref type="bibr">3</ref> which perform implicit morphological analysis. The span features include word unigrams, bigrams, as well as trigrams. Al- though our model allows certain more sophisti- cated features to be exploited, such as word POS features, word similarity features based on the WordNet ( <ref type="bibr" target="#b17">Pedersen et al., 2004</ref>), we deliberately choose to only include these simple features so as to make a fair comparison with previous works which also did not make use of external resources. For the features defined on m (i.e., φ o (m)), we in- clude only the string form of m, as well as m's function name as features.</p><p>Finally, we also define features over m only. Such features are defined over semantic unit pairs such as (m a , m b ) where m a is the parent node of m b as in m. They include: 1) concatenation of the string forms of m a and m b , 2) concatenation of the string form of m a and m b 's type, and 3) concate- nation of the function names of m a and m b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithms</head><p>In this section we describe the efficient algorithms used for learning and decoding. The algorithms are inspired by the inside-outside style algorithms used for the generative hybrid tree models ( <ref type="bibr" target="#b15">Lu et al., 2008)</ref>, but are different in the following ways: 1) we need to handle features, including long-distance features, 2) we need to additionally handle the computation of the partition function of Equation (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning</head><p>The training process involves the computation of the objective function (4) as well as the gradient terms (5).</p><p>The objective function (4) (excluding the regu- larization term which can be trivially computed) is equivalent to the following:</p><formula xml:id="formula_6">L(Λ) = i log h∈H(n i ,m i ) e Λ·Φ(n i ,m i ,h) − i log m ,h ∈H(n i ,m ) e Λ·Φ(n i ,m ,h )<label>(6)</label></formula><p>In the first term, h∈H(n i ,m i ) e Λ·Φ(n i ,m i ,h) is in fact the sum of the scores (as defined by Φ and Λ) associated with all such latent structures that contain both m i and n i exactly. The second term is the sum of the scores associated with all the la- tent structures that contain n i exactly. We focus our discussions on the computation of the first part first.</p><p>We use m (p) w i . . . w j to denote the combined score of all such latent relaxed hybrid tree structures that contain both the semantic tree rooted by m and the natural language word sequence w i . . . w j that forms the word association pattern p with respect to m. For example, the score of the relaxed hybrid tree in <ref type="figure">Figure 2</ref> (b) is contained by m a(wX) w 1 . . . w 10 (here p = wX because only w 1 w 2 w 3 are imme- diately associated with m a ).</p><p>We give an illustrative example that shows how these scores can be computed efficiently using dy- namic programming. Consider the following case when m has at least one child semantic unit:</p><formula xml:id="formula_7">m (wXw) w i . . . w j = m (w) w i ⊗ m (wXw) w i+1 . . . w j + m (w) w i ⊗ m (Xw)</formula><p>w i+1 . . . w j Here the symbol ⊗ means extract and compute, a process that involves 1) extraction of additional features when the two structures on the right-hand side are put together (for example, the local bi- gram feature "w i w i+1 " can be extracted in the above case), and 2) computation of the score for the new structure when the two structures from both sides of ⊗ are combined, based on the scores of these structures and newly extracted features.</p><p>The above equation holds because for any re- laxed hybrid tree contained by the left-hand side, the left-most word w i is always immediately as- w i . . . w j can also be computed based on similar equations. In other words, such terms can be computed from even smaller similar terms in a recursive manner. A bottom-up dynamic programming algorithm is used for computing such terms. When the semantic unit m has two child nodes, similar equations can also be established. Here we give an illustrative example:</p><formula xml:id="formula_8">m (wXwYw) w i . . . w j = j−1 k=i m (wX) w i . . . w k ⊗ m (wYw)</formula><p>w k+1 . . . w j Finally, we have the following equation: m</p><formula xml:id="formula_9">w i . . . w j = p m (p)</formula><p>w i . . . w j The left-hand side simply means the combined score for all such relaxed hybrid trees that have (n, m) as the root, where n ≡ w i . . . w j . Once the computation for a certain (n, m) pair is done, we can move up to process such pairs that involve m's parent node.</p><p>The above process essentially computes the in- side score associated with the (n, m) pair, which gives the sum of the scores of all such (incomplete) relaxed hybrid trees that can be constructed with (n, m) as the root. Similar to <ref type="figure">(Lu et al., 2008)</ref>, we can also define and compute the outside scores for (n, m) (the combined score of such incomplete re- laxed hybrid trees that contain (n, m) as one of its leave nodes) in an analogous manner, where the computation of the gradient functions can be effi- ciently integrated in this process.</p><p>Computation of the second part of the objective function (6) involves dynamic programming over a packed forest representation rather than a single tree, which requires an extension to the algorithm described in ( <ref type="bibr" target="#b15">Lu et al., 2008</ref>). The resulting al- gorithm is similar to the one used in ( <ref type="bibr" target="#b14">Lu and Ng, 2011)</ref>, which has been used for language gener- ation from packed forest representations of typed λ-calculus expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoding</head><p>The decoding phase involves finding the optimal semantic tree m * given a new input sentence n:</p><formula xml:id="formula_10">m * = arg max m P (m|n)<label>(7)</label></formula><p>This in fact is equivalent to finding the follow- ing optimal semantic tree m * :</p><formula xml:id="formula_11">m * = arg max m h∈H(n,m) e Λ·Φ(n,m,h) (8)</formula><p>Unfortunately, the summation operation inside the arg max prevents us from employing a simi- lar version of the dynamic programming algorithm we developed for learning in Section 4.1. To over- come this difficulty, we instead find the optimal semantic tree using the following equation:</p><formula xml:id="formula_12">m * = arg max m,h∈H(n,m) e Λ·Φ(n,m,h)<label>(9)</label></formula><p>We essentially replace the operation by the max operation inside the arg max. In other words, we first find the best latent relaxed hybrid tree h * that contains the input sentence n, and next we ex- tract the optimal semantic tree m * from h * . This decoding algorithm is similar to the dy- namic programming algorithm used for comput- ing the inside score for a given natural language sentence n (i.e., the algorithm for computing the second term of Equation <ref type="formula" target="#formula_6">(6)</ref>). The difference here is, at each intermediate step, instead of computing the combined score for all possible relaxed hybrid tree structures (i.e., performing sum), we find the single-best relaxed hybrid tree structure (i.e., per- forming max).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We present evaluations on the standard GeoQuery dataset which is publicly available. This dataset has been used for evaluations in various seman- tic parsing works ( <ref type="bibr" target="#b22">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b8">Kate and Mooney, 2006;</ref><ref type="bibr" target="#b15">Lu et al., 2008;</ref><ref type="bibr" target="#b7">Jones et al., 2012)</ref>. It consists of 880 natural language sen- tences paired with their corresponding formal se- mantic representations. Each semantic represen- tation is a tree structured representation derived from a Prolog query that can be used to interact with a database of U.S. geography facts for retriev- ing answers. The original dataset was fully anno- tated in English, and recently Jones et al. <ref type="formula" target="#formula_0">(2012)</ref> released a new version of this dataset with three additional language annotations <ref type="bibr">(German, Greek and Thai)</ref>. For all the experiments, we used the identical experimental setup as described in <ref type="bibr" target="#b7">Jones et al. (2012)</ref>. Specifically, we trained on 600 in- stances, and evaluated on the remaining 280.</p><p>We note that there exist two different versions of the GeoQuery dataset annotated with completely different semantic representations. Besides the version that we use in this work, which is an- notated with tree structured semantic representa- tions, the other version is annotated with lambda calculus expressions <ref type="bibr" target="#b24">(Zettlemoyer and Collins, 2005)</ref>. Results obtained from these two versions are not comparable. <ref type="bibr">4</ref> Like many previous works, we focus on tree structured semantic representa- tions for evaluations in this work since our model is designed for handling the class of semantic rep- resentations with recursive tree structures.</p><p>We used the standard evaluation criteria for judging the correctness of the outputs. Specifi- cally, our system constructs Prolog queries from the output parses, and uses such queries to retrieve answers from the GeoQuery database. An output is considered correct if and only if it retrieves the  <ref type="table">Table 3</ref>: Performance on the benchmark data, using four different languages as inputs. RHT: relaxed hybrid tree (this work).</p><p>same answers as the gold standard ( <ref type="bibr" target="#b7">Jones et al., 2012</ref>). We report accuracy scores -the percentage of inputs with correct answers, and F1 measures - the harmonic mean of precision (the proportion of correct answers out of inputs with an answer) and recall (the proportion of correct answers out of all inputs). By adopting such an evaluation method we will be able to directly compare our model's performance against those of the previous works.</p><p>The evaluations were conducted under such a setting in order to make comparisons to previous works. We would like to stress that our model is designed for general-purpose semantic parsing that is not only natural language-independent, but also task-independent. We thus distinguish our work from several previous works in the literature which focused on semantic parsing under other as- sumptions. Specifically, for example, works such as ( <ref type="bibr" target="#b11">Liang et al., 2013;</ref><ref type="bibr" target="#b18">Poon and Domingos, 2009;</ref><ref type="bibr" target="#b2">Clarke et al., 2010</ref>) essentially performed seman- tic parsing under different settings where the goal was to optimize the performance of certain down- stream NLP tasks such as answering questions, and different semantic formalisms and language- specific features were usually involved.</p><p>For all our experiments, we used the L-BFGS algorithm for learning the feature weights, where feature weights were all initialized to zeros and the regularization hyper-parameter κ was set to 0.01. We set the maximum number of L-BFGS steps to 100. When all the features are considered, our model creates over 2 million features for each lan- guage on the dataset (English: 2.1M, Thai: 2.3M, German: 2.7M, Greek: 2.6M). Our model re- quires (on average) a per-instance learning time of 0.428 seconds and a per-instance decoding time of 0.235 seconds, on an Intel machine with a 2.2 GHz CPU. Our implementation is in Java. Here the per-instance learning time refers to the time spent on computing the instance-level log-likelihood as well as the expected feature counts (needed for the gradients). <ref type="table">Table 3</ref> shows the evaluation results of our sys- tem as well as those of several other comparable previous works which share the same experimen- tal setup as ours. UBL-S is the system presented in <ref type="bibr" target="#b9">Kwiatkowski et al. (2010)</ref> which performs se- mantic parsing with the CCG based on mapping between graphs, and is the only non-tree based top-performing system. Their system, similar to ours, also uses a discriminative log-linear model where two types of features are defined. WASP is a model based on statistical phrase-based machine translation as we have described earlier. The hy- brid tree model (HYBRIDTREE+) performs learn- ing using a generative process which is augmented with an additional discriminative-reranking stage, where certain global features are incorporated ( <ref type="bibr" target="#b15">Lu et al., 2008</ref>). The Bayesian tree transducer model (TREETRANS) learns under a Bayesian genera- tive framework, using hyper-parameters manually tuned on the German training data.</p><p>We can observe from <ref type="table">Table 3</ref> that the semantic parser based on relaxed hybrid tree gives compet- itive performance when all the features (described in Sec 3.4) are used. It significantly outperforms the hybrid tree model that is augmented with a dis- criminative reranking step. The model reports the best accuracy and F1 scores on English and Thai and best accuracy score on Greek. The scores on German are lower than those of UBL-S and TREETRANS, mainly because the span features appear not to be effective for this language, as we will discuss next.</p><p>We report in <ref type="table">Table 4</ref> the test set performance when certain types of features are excluded from our system. Such results can help us understand the effectiveness of features of different types. As we can see from the table, in general, all fea- tures play essential roles, though their effective-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head><p>Thai German Greek Acc. F1 Acc. F1 Acc. F1 Acc. F1 RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2 RHT (no local features) 81.4 81.4 78.2 78.2 74.3 74.3 75.7 75.7 RHT (no span features) 81.1 81.1 77.9 77.9 78.2 78.2 78.9 78.9 RHT (no char features) 79.6 79.6 82.1 82.1 73.6 73.6 76.1 76.1 <ref type="table">Table 4</ref>: Results when certain types of features (local features, span features and character-level features) are excluded.</p><p>ness vary across different languages. The local features, which capture local dependencies, are of particular importance. Performance on three languages (English, Thai, and Greek) will drop when such features are excluded. Character-level features are very helpful for the three European languages (English, German, and Greek), but ap- pear to be harmful for Thai. This indicates the character-level features that we propose do not perform effective morphological analysis for this Asian language. <ref type="bibr">5</ref> The span features, which are able to capture certain long-distance dependen- cies, also play important roles. Specifically, if such features are excluded, our model's perfor- mance on three languages (Greek, English, Thai) will drop. Such features do not appear to be help- ful for Thai and appear to be harmful for Ger- man. Clearly, such long-distance features are not contributing useful information to the model when these two languages are considered. This is espe- cially the case for German, where we believe such features are contributing substantial noisy infor- mation to the model. What underlying language- specific, syntactic properties are generally caus- ing these gaps in the performances? We believe this is an important question that needs to be ad- dressed in future research. As we have mentioned, to make an appropriate comparison with previ- ous works, only simple features are used. We be- lieve that our system's performance can be further improved when additional informative language- specific features can be extracted from effective language tools and incorporated into our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we present a new discriminative model for semantic parsing which extends the hy-brid tree model. Such an extension is similar to the extension of the generative syntactic parser based on probabilistic context-free grammars (PCFG) to the feature-based CRF parser ( <ref type="bibr" target="#b3">Finkel et al., 2008)</ref>, but is slightly more complex due to latent struc- tures. Developed on top of our novel relaxed hy- brid tree representations, our model allows cer- tain long-distance dependencies to be captured. We also present efficient algorithms for learn- ing and decoding. Experiments on benchmark data show that our model is competitive to previ- ous works and achieves the state-of-the-art perfor- mance across several different languages.</p><p>Future works include development of efficient algorithms for feature-based semantic parsing with alternative loss functions ( <ref type="bibr" target="#b25">Zhou et al., 2013</ref>), development of feature-based language generation models ( <ref type="bibr" target="#b16">Lu et al., 2009;</ref><ref type="bibr" target="#b14">Lu and Ng, 2011</ref>) and multilingual semantic parsers <ref type="bibr" target="#b6">(Jie and Lu, 2014)</ref>, as well as the development of efficient semantic parsing algorithms for optimizing the performance of certain downstream NLP tasks with less super- vision ( <ref type="bibr" target="#b2">Clarke et al., 2010;</ref><ref type="bibr" target="#b11">Liang et al., 2013)</ref>.</p><p>Being able to efficiently exploit features defined over individual words, our model also opens up the possibility for us to exploit alternative representa- tions of words for learning ( <ref type="bibr" target="#b21">Turian et al., 2010)</ref>, or to perform joint learning of both distributional and logical semantics ( <ref type="bibr" target="#b10">Lewis and Steedman, 2013)</ref>. Furthermore, as a general string-to-tree structured prediction model, this work may find applications in other areas within NLP.</p><p>The system and code can be downloaded from http://statnlp.org/research/sp/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>NUM</head><label></label><figDesc>: count(STATE) STATE : state(STATE) states STATE : next to(CITY) borders how many . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>NUM</head><label></label><figDesc>: count(STATE) . . . borders how many states (. . . ) STATE : state(STATE) . . . borders how many (states) STATE : next to(CITY) . . . (borders how many) . . . (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>sociated with m.</head><label></label><figDesc>The term m (wXw) w i+1 . . . w j is present- ing a similar but smaller structure to the term on the left-hand side. The other term m (wX)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Notation Table 

for answering questions without relying on seman-
tic annotations. Goldwasser et al. (2011) took 
an unsupervised approach for semantic parsing 
based on self-training driven by confidence esti-
mation. Liang et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The complete list of word association pat-
terns. Here #Args means the number of arguments 
for a semantic unit. 

tern X. The reason is, when computing the parti-
tion function in Equation 3, inclusion of pattern X 
will result in relaxed hybrid trees consisting of an 
infinite number of nodes. However, this issue does 
not come up in the original hybrid tree models due 
to their generative setting, where the training pro-
cess does not involve such a partition function. 

</table></figure>

			<note place="foot" n="1"> We will give a concrete definition of H(n, m) used for this work, which is the complete set of all possible relaxed hybrid tree structures for the n-m pair, when we discuss our own joint representations later in Section 3.2.</note>

			<note place="foot" n="2"> This is based on the assumption that mc and m d are the first and second child of m b in the semantic representation, respectively. If m d is the first child in the semantic representation and mc is the second, the pattern should be wYwXw.</note>

			<note place="foot" n="3"> For each word, we used all its prefixes (not necessarily linguistically meaningful ones) whose length are greater than 2 as features, for all languages.</note>

			<note place="foot" n="4"> Kwiatkowski et al. (2010) showed in Table 3 of their work that the version with tree-structured representations appeared to be more challenging-their semantic parser&apos;s performance on this version was substantially lower than that on the lambda calculus version.</note>

			<note place="foot" n="5"> The character-level features that we introduced are indeed very general. We have conducted several additional experiments, which show that our model&apos;s performance for each language can be further improved when certain languagespecific character-level features are introduced.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknoledgments</head><p>The author would like to thank the anonymous re-viewers for their helpful comments. This work was supported by SUTD grant SRG ISTD 2013 064.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CONLL &apos;10</title>
		<meeting>of CONLL &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient, feature-based, conditional random field parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL/HLT</title>
		<meeting>of ACL/HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A statistical semantic parser that integrates syntax and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CONLL &apos;05</title>
		<meeting>of CONLL &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Confidence driven unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL &apos;11</title>
		<meeting>of ACL &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1486" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual semantic parsing: Parsing multiple languages into semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1291" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic parsing with bayesian tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeley</forename><surname>Bevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL &apos;12</title>
		<meeting>of ACL &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="488" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using string-kernels for learning semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING/ACL</title>
		<meeting>of COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="913" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inducing probabilistic ccg grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP&apos;10</title>
		<meeting>EMNLP&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huma</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A probabilistic forest-to-string model for language generation from typed lambda calculus expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP &apos;11</title>
		<meeting>of EMNLP &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1611" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generative model for parsing natural language to meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP &apos;08</title>
		<meeting>of EMNLP &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural language generation with tree conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Sun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordnet:: Similarity: measuring the relatedness of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL &apos;04 (Demonstration)</title>
		<meeting>of HLT-NAACL &apos;04 (Demonstration)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="38" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP &apos;09</title>
		<meeting>of EMNLP &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Surface structure and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL &apos;10</title>
		<meeting>of ACL &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT/NAACL &apos;06</title>
		<meeting>of HLT/NAACL &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL &apos;07</title>
		<meeting>of ACL &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI &apos;05</title>
		<meeting>of UAI &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient latent structural perceptron with hybrid trees for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2246" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
