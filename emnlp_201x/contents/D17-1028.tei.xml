<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Morphological Regularities in Distributional Word Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarfaraz</forename><surname>Syed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akhtar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arihant</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Vajpayee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjit</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madan</forename><surname>Gopal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jhanwar</forename><forename type="middle">Manish</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Center(LTRC) Kohli Center On Intelligent Systems (KCIS) International Institute of Information Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Morphological Regularities in Distributional Word Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a simple, fast and unsuper-vised approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set (Mikolov et al., 2013d) with an accuracy of 85% which is 12% higher than the previous best known system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vector representation of words are presently be- ing used to solve a variety of problems like doc- ument classification <ref type="bibr" target="#b11">(Sebastiani, 2002</ref>), question answering ( <ref type="bibr" target="#b14">Tellex et al., 2003</ref>) and chunking <ref type="bibr" target="#b17">(Turian et al., 2010)</ref>.</p><p>Word representations capture both syntactic and semantic properties <ref type="bibr" target="#b10">(Mikolov et al., 2013d</ref>) of nat- ural language. <ref type="bibr" target="#b13">Soricut and Och (2015)</ref> exploited these regularities to generate prefix/suffix based morphological transformation rules in an unsuper- vised manner. These morphological transforma- tions were represented as vectors in the same em- bedding space as the vocabulary. They used a graph based approach and represented transforma- tions as "type:from:to" triples and a direction vec- tor: for example "suffix:ion:e:â†‘ creation " implies a suffix change just like in the case "creation" to "create".</p><p>Using Soricut's transformation rules, the major problem is identifying the correct direction vector to use for a given case, i.e. if we have to generate an embedding for "runs", which rule to apply on "run". Experimental results showed that "walk - * These authors contributed equally to this <ref type="bibr">work.</ref> walks" gives better results than rules like "invent -invents" or "object -objects" in generating word embedding for "runs". In this paper, we try to ex- plore if we can harness this morphological regu- larity in a much better way, than applying a single direction using vector arithmetic.</p><p>Hence, we tried to come up with a global trans- formation operator, which aligns itself with the source word, to give best possible word embed- ding for target word. We will have a single trans- formation operator for each rule, irrespective of the form of root word (like verb or a noun). Our transformation operator is in the form of a ma- trix, which when applied on a word embedding (cross product of vector representation of word with transformation matrix) gives us a word em- bedding for target word.</p><p>The intuition is not to solve for "invent is to in- vents as run is to ?" or "object is to objects as run is to ?", but instead we are solving for "walk is to walks, object is to objects, invent is to invents, .... as run is to ?". A transformation operator aims to be a unified transition function for different forms of the same transition. Learning a representation of this operator would allow us to capture the se- mantic changes associated with the transition. As word embeddings for rare and out-of-vocabulary words are poorly trained or not trained at all, learn- ing this operator will be beneficial to reducing the sparsity in corpus.</p><p>The idea of projection learning has been applied to a multitude of tasks such as in the learning of cross lingual mappings for translation of English to <ref type="bibr">Spanish (Mikolov et al., 2013b</ref>) and for unsu- pervised mapping between vector spaces (Akhtar et al., 2017a). Our approach has its basis on the same lines but with a different formulation and end goal to learn morphological rules rather than se- mantic associations and translational constraints.</p><p>In summary, we present a new method to har- ness morphological regularities present in high di- mensional word embeddings and learn its repre- sentation in the form of a matrix. Using this method, we present state of the art results on MSR word analogy dataset. This paper is structured as follows. We first dis- cuss the corpus used for training the transforma- tion operators in section 2. In section 3, we discuss how these transformation operators are trained. Later in sections 4, we analyze and discuss the re- sults of our experiments. We finish this paper with future scope of our work in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>We are using word embeddings trained on Google News corpus ( <ref type="bibr" target="#b9">Mikolov et al., 2013c</ref>) for our ex- periments. For the model trained in this paper, we have used the Skip-gram ( <ref type="bibr" target="#b7">Mikolov et al., 2013a)</ref> algorithm. The dimensionality has been fixed at 300 with a minimum count of 5 along with nega- tive sampling. As training set and for estimating the frequencies of words, we use the Wikipedia data <ref type="bibr" target="#b12">(Shaoul, 2010)</ref>. The corpus contains about 1 billion tokens.</p><p>The MSR dataset (Mikolov et al., 2013d) con- tains 8000 analogy questions. This data set has been used by us for testing our model. The re- lations portrayed by these questions are morpho- syntactic, and can be categorized according to parts of speech -adjectives, nouns and verbs. Adjective relations include comparative and su- perlative (good is to best as smart is to smartest). Noun relations include singular and plural, pos- sessive and non-possessive (dog is to dog's as cat is to cat's). Verb relations are tense modifications (work is to worked as accept is to accepted).</p><p>For all the experiments, we have calculated the fraction of answers correctly answered by the sys- tem on MSR word analogy dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our approach has two steps - Note that all the thresholds mentioned in following sub-sections were determined by empirical fine tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformation Extraction</head><p>For unsupervised extraction of candidate rules and corresponding word pairs for that rule, we fol- low the approach used by <ref type="bibr" target="#b1">Akhtar et al. (2017b)</ref>. For example, in case of the rule &lt;null,s&gt;, we find word pairs such as &lt;boy,boys&gt;, &lt;object,objects&gt; and &lt;invent,invents&gt;. We re- strict the scope of our work to dealing with only prefix and suffix based morphology. To extract candidate suffixes / prefixes, we maintain two TRIE data structures (one where inverted words are inserted for suffixes and another where words are inserted in original order for prefixes). By thresholding on the basis of branching factor of a node (bf = 10), we obtain candidate suffixes / prefixes and stems associated with them.</p><p>Defining two types of transitions - For extracting null transitions, we take the inter- section of stems associated with candidate suf- fixes/prefixes with the vocabulary of our training corpus.</p><p>For extracting cross transitions, we take the intersection between stems of different suf- fixes/prefixes. For e.g. the stem "talk" would be associated with both suffixes "ed" and "ing".</p><p>We prune the candidate rules and associated pairs thus extracted based on both cosine sim- ilarity and frequency. For e.g. &lt;hat,hated&gt; is a co-incidental example of the transition &lt;null,ed&gt;. We lower bound the cosine similar- ity at theta sim = 0.2 for word vectors of the pair. Since our transformation matrix is derived from all the word pairs following a particular transition rule, we carefully use only those word pairs which are of high frequency (as they have better trained embeddings). We lower bound the frequency of both words of pair at theta f req = 1000.</p><p>We could have relied on an external morph ana- lyzer such as Morfessor ( <ref type="bibr" target="#b3">Creutz and Lagus, 2007)</ref> to extract candidate rules and word pairs, but we wished to keep the approach completely unsuper- vised. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Transformation Matrices</head><p>Previous works that handle morphology using vec- tor space representations involved complex neural network architectures such as recursive neural net- works ( <ref type="bibr" target="#b6">Luong et al., 2013</ref>) and log-bilinear mod- els ( <ref type="bibr" target="#b2">Botha and Blunsom, 2014)</ref>. Both the referred works treat morph-analysis as a pre-processing step using Morfessor <ref type="bibr" target="#b3">(Creutz and Lagus, 2007)</ref>. In contrast, we propose a simple yet effective linear approach to learn the representations of transfor- mations without depending on external segmenta- tion tools.</p><p>Suppose we get "N" highly frequent word pairs following the same regularity(transition rule). For our experiments, the lower bound of "N" is set at 50. Dimensions of word embedding of a word in our model is "D". Using first word of our "N" chosen word pairs, we create a matrix "A" of di- mensions N*D, where each row is vector repre- sentation of a word. Similarly, we create another matrix B, of similar dimensions as A, using second word of our chosen word pairs.</p><p>We now propose that a matrix "X" (our trans- formation matrix) exists such that,</p><formula xml:id="formula_0">A * X = B or, X = A âˆ’1 * B<label>(1)</label></formula><p>(all instances of A that we encountered were non- singular). Our matrix "X" will be of dimensions "D*D" and when applied to a word embedding While testing, we extract the lexical transition using the first two words of the analogy question. For example, for pairs like &lt;reach, reached&gt;, &lt; walk, walked&gt;, we are able to extract that they follow &lt;null, ed&gt; rule. But, for &lt;go, went&gt;, we are not able to find any transformation operator after lexical analysis, and for such cases, we fall back on CosSum/CosMul ( <ref type="bibr" target="#b5">Levy et al., 2014</ref>) ap- proaches as our backup. <ref type="bibr">Mikolov et al. showed</ref> that relations between words are reflected to a large extent in the offsets between their vector em- beddings (queen -king = woman -man), and thus the vector of the hidden word b * will be similar to the vector b âˆ’ a + a * , suggesting that the analogy question can be solved by optimizing:</p><formula xml:id="formula_1">arg max b * âˆˆV (sim(b * , b âˆ’ a + a * ))<label>(2)</label></formula><p>where V is the vocabulary and sim is a simi- larity measure. Specifically, they used the cosine similarity measure, defined as:</p><formula xml:id="formula_2">cos(u, v) = u . v ||u|| . ||v||<label>(3)</label></formula><p>resulting in:</p><p>arg max</p><formula xml:id="formula_3">b * âˆˆV (cos(b * , b âˆ’ a + a * ))<label>(4)</label></formula><p>Equation 4 has been referred to as CosAdd model.</p><p>While experimenting, Omer Levy ( <ref type="bibr" target="#b5">Levy et al., 2014)</ref> found that for an analogy question "London is to England as Baghdad is to -?", using CosAdd model, they got Mosul -a large Iraqi city, instead of Iraq which is a country, as an answer. They were seeking for Iraq because of its similarity to England (both are countries), similarity to Bagh- dad (similar geography/culture) and dissimilarity to London (different geography/culture). While Iraq was much more similar to England than Mo- sul was (because both Iraq and England are coun- tries), the sums were dominated by the geographic and cultural aspect of the analogy.</p><p>Hence to achieve better balancing among dif- ferent aspects of similarity, they proposed a new model, where they moved from additive to multi- plicative approach:</p><formula xml:id="formula_4">arg max b * âˆˆV cos(b * , b) . cos(b * , a * )</formula><p>cos(b * , a) + ( = 0.001 to prevent division by zero)</p><p>This was equivalent to taking the logarithm of each term before summation, thus amplifying the differences between small quantities and reducing the differences between larger ones. This model has been referred to as CosMul model. Even though our transformation operator can handle any sort of transformation, but if we are not able to detect the rule from lexical analysis, we are not able to determine which transformation operator to use, and hence, we fall back on Cos- Sum/CosMul. Like for the above mentioned ex- amples, we will use transformation operator (if ex- isting) for transformations like &lt;reach, reached&gt;, since we can find the rule, but for &lt;go, went&gt;, we can not, since we can not extract the correspond- ing rule itself -even if the matrix can handle such transitions.</p><p>If a transformation matrix exists for a transition rule, we apply the corresponding transformation matrix on the word embedding of the third word and search the whole vocabulary for the word with an embedding most similar to the transformed em- bedding (ignoring the third word itself). If the similarity of the resultant word's embedding with our transformed embedding is less than 0.68 (de- termined empirically) or the transformation ma- trix itself does not exist, we fall back on the Cos- Sum/CosMul techniques. <ref type="bibr" target="#b4">Levy et. al. (2015)</ref> proposed the systems Cos- Sum and CosMul in which they showed that tun- ing the hyperparameters has a significant impact on the performance. <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of how we train the transformation matrix and 2 shows how target word embeddings are generated using transforma- tion operators and our backup models.    <ref type="table" target="#tab_1">Table 4</ref>: Example results of transformation oper- ators for irregular transformations.</p><p>In table 2, GN denotes the scores of Google- News word embeddings on the test set. SGNS- L and Glove-L ( <ref type="bibr" target="#b4">Levy et al., 2015</ref>) denote the re- sults of Skip-gram with negative sampling and Glove word embeddings respectively, both trained on large datasets. SG denotes the scores of our word2vec trained model (on 1B tokens). "w/ M" implies that we have used matrix arithmetic (along with CosSum/CosMul as backup) for word analogy answering questions. Our model uses  <ref type="table">Table 5</ref>: Example results of transformation oper- ators for complete change of word form.</p><p>"CosSum" and "CosMul" as backup transforma- tion method in case a transformation operator (ma- trix) does not exist. We see that the results of GN+Matrix are better than the previously used models.</p><p>However, one thing we noticed was that the model trained on Google-News did not contain words with apostrophe sign(s) and 1000 out of 8000 words in MSR word analogy test set con- tained apostrophe sign(s). Also, we noticed that in SG, the matrix approach was able to answer word analogy queries where words contained apostro- phe sign(s), with an accuracy of 93.7% since it is a very common transformation -which resulted in well trained transformation matrix. So, we used SG as a backup for words which were not found in GN. The results of this hybrid model are denoted by GN-SG Hybrid. We see that this model per- forms considerably better than the existing state of the art system.</p><p>As we can see in table 3, our approach works re- ally well for analogy questions where target word experiences regular transformation, i.e. the trans- formation type is simple addition/subtraction of suffix/prefix.</p><p>In <ref type="table" target="#tab_1">table 4 and table 5</ref> we observe that trans- formations are irregular transformations i.e there is slight change in word form while addi- tion/subtraction of suffix/prefix or there is com- plete change in word form in the target word of our analogy question. This is an interesting ob- servation, because even though our rule extrac- tion (as explained above) is syntactic in nature, our method still learns and can apply transforma- tion rules on words which undergo such irregu- lar/complete transformations.</p><p>In operator "&lt;null,s&gt;", we see that our trans- formation matrix works pretty well irrespective of the form the word. For example, it works for "school-schools" and "reduce-reduces" which are noun and verb word pairs respectively. Our ap- proach works by statistically creating global trans- formation operators and is agnostic in applying them (i.e. applied on a verb or a noun). Our trans- formation rules learn from both noun transitions and verb transitions and hence, even though we agree that linguistically there is a difference be- tween noun and verb transitions, our approach per- formed better than previously existing systems We also observed that in some cases, cosine similarity score is 1. This is mostly because "stricter-strictest" was used for training transfor- mation matrix of "&lt;r,st&gt;" operator.</p><p>Although our cosine scores for irregu- lar/complete transformations are not that high with respect to scores for regular transformations, our system still performs at par or better than previous known systems. It is still able to pre- dict words with high accuracy using its limited training corpora.</p><p>These observations can also help us ana- lyze how certain complex transformations (irreg- ular/complete) still behave similar to their regular counterpart computationally, as is apparent from our transformation matrix -which has learnt it- self from rules that were extracted via all possible prefix and suffix substitutions from w1 to w2, and thus irregular/complete transformations would not be present in training our transformation matrix (where w1 and w2 belong to our vocabulary V - the size of our corpus).</p><p>The main application of our approach lies in its ability to generate representations for un- seen/unreliable words on the go. If we encounter a word such as "preparedness" for which we do not have a representation or our representation is not reliable, we can identify any reliable form of the word, say "prepared" and apply &lt;null,ness&gt; operator on it, resulting in a representation for "preparedness". In a similar case, we can gener- ate embeddings for words such as "unprepared- ness" from "prepared" by sequentially applying &lt;null,ness&gt; and a prefix operator trained in a sim- ilar manner -&lt;null,un&gt;. Overall, this results in a much larger vocabulary than of the model initially being used. Sequential application for such learned operators would also be beneficial for morphologically rich languages.</p><p>We conclude that our matrix is able to harness morphological regularities present in word pairs used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>One of the major drawbacks to our system is that the rule extraction process is designed towards prefix/suffix based morphology only. Improve- ments will be required in that step to handle com- plex morphological phenomena such as affixation.</p><p>To solve the word analogy task, we currently employ a simple lexical analysis to determine which transformation operator to apply. We thus require a backup model for pairs that do not con- form to any known operator. A more complicated scheme involving comparisons between multiple outputs (after applying different rules) could help remove the dependency on a backup model.</p><p>Although our work is a general way to gener- ate morphologically informed embeddings in an unsupervised manner, we have designed the pre- diction approach to deal with the word analogy task. Recent trends ( <ref type="bibr" target="#b16">Tsvetkov et al., 2015)</ref> and <ref type="bibr" target="#b15">(Tsvetkov et al., 2016)</ref> have suggested that eval- uation methodologies such as word analogy and word similarity tasks may not be holistic. Thus, embeddings generated by our approach should be evaluated by plugging into end-level tasks such as machine translation, POS tagging etc. This would also help in analysing which tasks bene- fit from having morphologically informed word embeddings and which would suffice with simple orthographic features such as presence of certain suffixes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Extraction of candidate rules and word pairs 2. Training of a transformation matrix per rule</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Null transitions -involve a prefix/suffix going to null for e.g. the transition &lt;suffix:null:ed&gt; would involve pairs &lt;talk, talked&gt; , &lt;walk, walked&gt; etc. 2. Cross transitions -involve both addition and deletion of characters for e.g. the tran- sition &lt;suffix:ed:ing&gt; would involve pairs &lt;talked, talking&gt;, &lt;walked, walking&gt; etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training WorkFlow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Prediction WorkFlow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>4 Result and Analysis</head><label>4</label><figDesc></figDesc><table>Model 
CosSum CosSum w/ M CosMul CosMul w/ M 
SGNS-L 
0.69 
-
0.729 
-
Glove-L 
0.628 
-
0.685 
-
SG 
0.269 
0.554 
0.282 
0.566 
GN 
0.646 
0.718 
0.67 
0.733 
GN-SG Hybrid 0.674 
0.835 
0.698 
0.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Scores on MSR word analogy test set.</head><label>2</label><figDesc></figDesc><table>Word1 
Word2 
Word3 Operator 
Word4 Cosine 

decides 
decided studies 
&lt;s , d&gt; 
studied 
0.89 
reach 
reaches 
go 
&lt;null , es&gt; 
goes 
1.0 
member members school 
&lt;null , s&gt; schools 
0.88 
ask 
asks 
reduce &lt;null , s&gt; reduces 
0.91 
resident residents 
rate 
&lt;null , s&gt; 
rates 
0.86 
get 
gets 
show 
&lt;null , s&gt; 
shows 
0.83 
higher 
highest 
stricter 
&lt;r , st&gt; 
strictest 
1.0 
wild 
wilder 
harsh 
&lt;null , er&gt; harsher 
0.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Example results of transformation oper-
ators for regular transformations. 

Word1 
Word2 
Word3 
Operator 
Word4 Cosine 

joined 
joins 
became 
&lt;ed , s&gt; 
becomes 
0.68 
turned 
turns 
said 
&lt;ed , s&gt; 
says 
0.74 
learn 
learned 
build 
&lt;null , ed&gt; 
built 
0.80 
support supported 
see 
&lt;null , ed&gt; 
saw 
0.72 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An unsupervised approach for mapping between vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arihant</forename><surname>Syed Sarfaraz Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjit</forename><surname>Vajpayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Madan Gopal Jhanwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Research in Computer Science, forthcoming</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised morphological expansion of small datasets for improving word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arihant</forename><surname>Syed Sarfaraz Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Avijit Vajpayee, Arjit Srivastava, and Manish Shrivastava. forthcoming</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised models for morpheme segmentation and morphology learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">hlt-Naacl</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The westbury lab wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shaoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Edmonton, AB</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised morphology induction using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Correlation-based intrinsic evaluation of word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">Evaluation of word vector representations by subspace alignment</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
