<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Ordering Without Syntax</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
							<email>{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word Ordering Without Syntax</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2319" to="2324"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work on word ordering has argued that syntactic structure is important, or even required , for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outper-forming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We address the task of recovering the original word order of a shuffled sentence, referred to as bag gen- eration ( <ref type="bibr" target="#b0">Brown et al., 1990)</ref>, shake-and-bake genera- tion <ref type="bibr">(Brew, 1992)</ref>, or more recently, linearization, as standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models ( <ref type="bibr" target="#b7">Zhang and Clark, 2011;</ref><ref type="bibr" target="#b5">Liu et al., 2015</ref>; <ref type="bibr" target="#b8">Zhang and Clark, 2015)</ref>. The predominant argument of the more re- cent works is that jointly recovering explicit syn- tactic structure is crucial for determining the correct word order of the original sentence. As such, these methods either generate or rely on given parse struc- ture to reproduce the order.</p><p>Independently, Elman (1990) explored lineariza- tion in his seminal work on recurrent neural net- works. Elman judged the capacity of early recurrent neural networks via, in part, the network's ability to predict word order in simple sentences. He notes,</p><p>The order of words in sentences reflects a num- ber of constraints. . . Syntactic structure, selective restrictions, subcategorization, and discourse con- siderations are among the many factors which join together to fix the order in which words oc- cur. . . <ref type="bibr">[T]</ref>here is an abstract structure which un- derlies the surface strings and it is this structure which provides a more insightful basis for under- standing the constraints on word order. . . . It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure <ref type="bibr" target="#b1">(Elman, 1990)</ref>.</p><p>Recently, recurrent neural networks have reemerged as a powerful tool for learning the latent structure of language. In particular, work on long short-term memory (LSTM) networks for language modeling has provided improvements in perplexity.</p><p>We revisit Elman's question by applying LSTMs to the word-ordering task, without any explicit syn- tactic modeling. We find that language models are in general effective for linearization relative to ex- isting syntactic approaches, with LSTMs in particu- lar outperforming the state-of-the-art by 11.5 BLEU points, with further gains observed when training with additional text and decoding with larger beams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Linearization</head><p>The task of linearization is to recover the original or- der of a shuffled sentence. We assume a vocabulary V and are given a sequence of out-of-order phrases x 1 , . . . , x N , with x n ∈ V + for 1 ≤ n ≤ N . Define M as the total number of tokens (i.e., the sum of the lengths of the phrases). We consider two varieties of the task: (1) WORDS, where each x n consists of a single word and M = N , and (2) WORDS+BNPS, where base noun phrases (noun phrases not con- taining inner noun phrases) are also provided and M ≥ N . The second has become a standard for- mulation in recent literature.</p><p>Given input x, we define the output set Y to be all possible permutations over the N elements of x, wherê y ∈ Y is the permutation generating the true order. We aim to findˆyfindˆ findˆy, or a permutation close to it. We produce a linearization by (approximately) optimizing a learned scoring function f over the set of permutations, y * = arg max y∈Y f (x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work: Syntactic Linearization</head><p>Recent approaches to linearization have been based on reconstructing the syntactic structure to produce the word order. Let Z represent all projective de- pendency parse trees over M words. The objective is to find y * , z * = arg max y∈Y,z∈Z f (x, y, z) where f is now over both the syntactic structure and the lin- earization. The current state of the art on the Penn Treebank (PTB) <ref type="bibr">(Marcus et al., 1993)</ref>, without ex- ternal data, of  uses a transition- based parser with beam search to construct a sen- tence and a parse tree. The scoring function is a linear model f (x, y) = θ Φ(x, y, z) and is trained with an early update structured perceptron to match both a given order and syntactic tree. The feature function Φ includes features on the syntactic tree. This work improves upon past work which used best-first search over a similar objective ( <ref type="bibr" target="#b7">Zhang and Clark, 2011)</ref>.</p><p>In follow-up work,  argue that syntactic models yield improvements over pure surface n-gram models for the WORDS+BNPS case. This result holds particularly on longer sentences and even when the syntactic trees used in training are of low quality. The n-gram decoder of this work utilizes a single beam, discarding the probabilities of internal, non-boundary words in the BNPs when comparing hypotheses. We revisit this comparison between syntactic models and surface-level models, utilizing a surface-level decoder with heuristic fu- ture costs and an alternative approach for scoring partial hypotheses for the WORDS+BNPS case.</p><p>Additional previous work has also explored n- gram models for the word ordering task. The work of de <ref type="bibr">Gispert et al. (2014)</ref> demonstrates improve- ments over the earlier syntactic model of  by applying an n-gram language model over the space of word permutations restricted to concate- nations of phrases seen in a large corpus. Horvat and Byrne (2014) models the search for the highest prob- ability permutation of words under an n-gram model as a Travelling Salesman Problem; however, direct comparisons to existing works are not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LM-Based Linearization</head><p>In contrast to the recent syntax-based approaches, we use an LM directly for word ordering. We consider two types of language models: an n- gram model and a long short-term memory network <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>). For the pur- pose of this work, we define a common abstraction for both models. Let h ∈ H be the current state of the model, with h 0 as the initial state. Upon seeing a word w i ∈ V, the LM advances to a new state h i = δ(w i , h i−1 ). At any time, the LM can be queried to produce an estimate of the probability of the next word q(w i , h i−1 ) ≈ p(w i | w 1 , . . . , w i−1 ). For n-gram language models, H, δ, and q can natu- rally be defined respectively as the state space, tran- sition model, and edge costs of a finite-state ma- chine.</p><p>LSTMs are a type of recurrent neural network (RNN) that are conducive to learning long-distance dependencies through the use of an internal memory cell. Existing work with LSTMs has generated state- of-the-art results in language modeling ( <ref type="bibr" target="#b7">Zaremba et al., 2014</ref>), along with a variety of other NLP tasks.</p><p>In our notation we define H as the hidden states and cell states of a multi-layer LSTM, δ as the LSTM update function, and q as a final affine trans- formation and softmax given as q( * ,</p><formula xml:id="formula_0">h i−1 ; θ q ) = softmax(W h (L) i−1 + b) where h (L)</formula><p>i−1 is the top hid- den layer and θ q = (W , b) are parameters. We di- rect readers to the work of Graves (2013) for a full description of the LSTM update.</p><p>For both models, we simply define the scoring function as</p><formula xml:id="formula_1">f (x, y) = N n=1 log p(x y(n) | x y(1) , . . . , x y(n−1) )</formula><p>where the phrase probabilities are calculated word- by-word by our language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 LM beam-search word ordering</head><p>1: procedure ORDER(x 1 . . . x N , K, g)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>B 0 ← (, {1, . . . , N }, 0, h 0 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for m = 0, . . . , M − 1 do 4:</p><formula xml:id="formula_2">for k = 1, . . . , |B m | do 5: (y, R, s, h) ← B (k) m 6:</formula><p>for i ∈ R do</p><note type="other">7: (s , h ) ← (s, h) 8: for word w in phrase x i do 9: s ← s + log q(w, h ) 10: h ← δ(w, h ) 11: j ← m + |x i | 12:</note><formula xml:id="formula_3">B j ← B j + (y + x i , R − i, s , h ) 13: keep top-K of B j by f (x, y) + g(R) 14:</formula><p>return B M Searching over all permutations Y is intractable, so we instead follow past work on linearization (  and LSTM generation <ref type="bibr" target="#b7">(Sutskever et al., 2014</ref>) in adapting beam search for our genera- tion step. Our work differs from the beam search approach for the WORDS+BNPS case of previ- ous work in that we maintain multiple beams, as in stack decoding for phrase-based machine trans- lation <ref type="bibr" target="#b3">(Koehn, 2010)</ref>, allowing us to incorporate the probabilities of internal, non-boundary words in the BNPs. Additionally, for both WORDS and WORDS+BNPS, we also include an estimate of fu- ture cost in order to improve search accuracy.</p><p>Beam search maintains M + 1 beams, B 0 , . . . , B M , each containing at most the top- K partial hypotheses of that length. A partial hypothesis is a 4-tuple (y, R, s, h), where y is a partial ordering, R is the set of remaining indices to be ordered, s is the score of the partial linearization f (x, y), and h is the current LM state. Each step consists of expanding all next possible phrases and adding the next hypothesis to a later beam. The full beam search is given in Algorithm 1.</p><p>As part of the beam search scoring function we also include a future cost g, an estimate of the score contribution of the remaining elements in R. To- gether, f (x, y) + g(R) gives a noisy estimate of the total score, which is used to determine the K best elements in the beam. In our experiments we use a very simple unigram future cost estimate, g(R) = i∈R w∈x i log p(w).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Setup Experiments are on PTB with sections 2- 21 as training, 22 as validation, and 23 as test <ref type="bibr">1</ref> . We utilize two UNK types, one for initial upper- case tokens and one for all other low-frequency to- kens; end sentence tokens; and start/end tokens, which are treated as words, to mark BNPs for the WORDS+BNPS task. We also use a special symbol to replace tokens that contain at least one numeric character. We otherwise train with punctuation and the original case of each token, resulting in a vocab- ulary containing around 16K types from around 1M training tokens. For experiments marked GW we augment the PTB with a subset of the Annotated Gigaword cor- pus ( <ref type="bibr">Napoles et al., 2012</ref>). We follow  and train on a sample of 900k Agence France-Presse sentences combined with the full PTB training set. The GW models benefit from both ad- ditional data and a larger vocabulary of around 25K types, which reduces unknowns in the validation and test sets.</p><p>We compare the models of <ref type="bibr">Liu</ref>   (known as ZGEN), a 5-gram LM using Kneser-Ney smoothing (NGRAM) 2 , and an LSTM. We experi- ment on the WORDS and WORDS+BNPS tasks, and we also experiment with including future costs (g), the Gigaword data (GW), and varying beam size. We retrain ZGEN using publicly available code 3 to replicate published results. The LSTM model is similar in size and architec- ture to the medium LSTM setup of <ref type="bibr" target="#b7">Zaremba et al. (2014)</ref>  <ref type="bibr">4</ref> . Our implementation uses the Torch 5 frame- work and is publicly available <ref type="bibr">6</ref> .</p><p>We compare the performance of the models using the BLEU metric ( <ref type="bibr" target="#b6">Papineni et al., 2002</ref>). In gener- ation if there are multiple tokens of identical UNK type, we randomly replace each with possible un- used tokens in the original source before calculating BLEU. For comparison purposes, we use the BLEU script distributed with the publicly available ZGEN code.  NGRAM-64 by more than 5 BLEU points. Differ- ences on the WORDS task are smaller, but show a similar pattern. Incorporating Gigaword further in- creases the result another 2 points. Notably, the NGRAM model outperforms the combined result of ZGEN-64+LM+GW+POS from , which uses a 4-gram model trained on Gi- gaword. We believe this is because the combined ZGEN model incorporates the n-gram scores as dis- cretized indicator features instead of using the prob- ability directly. 7 A beam of 512 yields a further im- provement at the cost of search time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To further explore the impact of search accuracy, <ref type="table" target="#tab_2">Table 2</ref> shows the results of various models with beam widths ranging from 1 (greedy search) to 512, and also with and without future costs g. We see that for the better models there is a steady increase in ac- curacy even with large beams, indicating that search errors are made even with relatively large beams.   <ref type="table">Table 3</ref>: Unlabeled attachment scores (UAS) on the PTB validation set after parsing and aligning the output. For ZGEN we also include a result us- ing the tree z * produced directly by the system. For WORDS+BNPS, internal BNP arcs are always counted as correct.</p><p>One proposed advantage of syntax in lineariza- tion models is that it can better capture long-distance relationships. <ref type="figure" target="#fig_1">Figure 1</ref> shows results by sentence length and distortion, which is defined as the abso- lute difference between a token's index position in y * andˆyandˆ andˆy, normalized by M . The LSTM model ex- hibits consistently better performance than existing syntax models across sentence lengths and generates fewer long-range distortions than the ZGEN model. Finally, <ref type="table">Table 3</ref> compares the syntactic fluency of the output. As a lightweight test, we parse the output with the Yara Parser ( <ref type="bibr">Rasooli and Tetreault, 2015)</ref> and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system. We first align the gold head to each output token. (In cases where the alignment is not one-to-one, we ran- domly sample among the possibilities.) The models with no knowledge of syntax are able to recover a higher proportion of gold arcs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Strong surface-level language models recover word order more accurately than the models trained with explicit syntactic annotations appearing in a recent series of papers. This has implications for the utility of costly syntactic annotations in generation mod- els, for both high-and low-resource languages and domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Experiments on the PTB validation on the WORDS+BNPS models: (a) Performance on the set by length on sentences from length 2 to length 40. (b) The distribution of token distortion, binned at 0.1 intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU results on the validation set for 
the LSTM and NGRAM model with varying beam 
sizes, future costs, additional data, and use of base 
noun phrases. 

</table></figure>

			<note place="foot" n="1"> In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author correspondence).</note>

			<note place="foot" n="7"> In work of Liu and Zhang (2015), with the given decoder, N-grams only yielded a small further improvement over the syntactic models when discretized versions of the LM probabilities were incorporated as indicator features in the syntactic models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yue Zhang and Jiangming Liu for assis-tance in using ZGen, as well as verification of the task setup for a valid comparison. Jiangming Liu also assisted in pointing out a discrepancy in the im-plementation of an earlier version of our NGRAM decoder, the resolution of which improved BLEU performance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrick</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul S Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<editor>Adrì a de Gispert, Marcus Tomalin, and Bill Byrne</editor>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA; Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
	<note>Gothenburg</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<editor>Matic Horvat and William Byrne</editor>
		<meeting>the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
	<note>Sepp Hochreiter and Jürgen Schmidhuber</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical comparison between n-gram and syntactic language models for word ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang2015] Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme</editor>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado; Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993-06" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
	<note>AKBC-WEKEX. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Yara parser: A fast and accurate dependency parser. CoRR, abs/1503.06733</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<editor>Rasooli and Tetreault2015] Mohammad Sadegh Rasooli and Joel R. Tetreault</editor>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntax-based grammaticality improvement using ccg and guided search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1147" to="1157" />
		</imprint>
	</monogr>
	<note>Recurrent neural network regularization. CoRR, abs/1409.2329. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Syntax-based word ordering incorporating a large-scale language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark2015] Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark ; Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<idno>Zhang et al.2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="736" to="746" />
		</imprint>
	</monogr>
	<note>Discriminative syntax-based word ordering for text generation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
