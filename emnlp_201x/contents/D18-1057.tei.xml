<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantifying Context Overlap for Training Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimeng</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research Institute China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research Institute China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research Institute China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research Institute China -Beijing (SRC-B)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quantifying Context Overlap for Training Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="587" to="593"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>587</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most models for learning word embeddings are trained based on the context information of words, more precisely first order co-occurrence relations. In this paper, a metric is designed to estimate second order co-occurrence relations based on context overlap. The estimated values are further used as the augmented data to enhance the learning of word embeddings by joint training with existing neural word embedding models. Experimental results show that better word vectors can be obtained for word similarity tasks and some downstream NLP tasks by the enhanced approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last decade, the distributed word representa- tion (a.k.a word embedding) has attracted tremen- dous attention in the field of natural language pro- cessing (NLP). Instead of large vectors, such as the one-hot representation, the distributed word rep- resentation embeds semantic and syntactic char- acteristics of words into a low-dimensional space, which makes it popular in NLP applications.</p><p>The main idea of most word embedding mod- els follows the distributional hypothesis <ref type="bibr">(Harris, 1954)</ref>, i.e., the embedding of each word may be in- ferred using its context. An important model fam- ily for distributional word representation learning is built based on the global matrix factorization approach <ref type="bibr">(Deerwester et al., 1990;</ref><ref type="bibr">Lee and Seung, 2001;</ref><ref type="bibr" target="#b20">Srebro et al., 2005;</ref><ref type="bibr" target="#b7">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b0">Li et al., 2015;</ref><ref type="bibr" target="#b22">Wang and Cohen, 2016)</ref>, in which a dimensionality reduction over a sparse matrix is performed to capture the statistical in- formation about a corpus in low-dimensional vec- tors. Another model family is neural word em- beddings ( <ref type="bibr">Levy and Goldberg, 2014b</ref>), some at- tempts include the famous Neural Probabilistic Language Model ( <ref type="bibr">Bengio et al., 2003</ref>), SGNS and CBOW ( <ref type="bibr">Mikolov et al., 2013a,b)</ref>, GloVe <ref type="bibr" target="#b11">(Pennington et al., 2014</ref>) and their variants <ref type="bibr" target="#b18">(Shazeer et al., 2016;</ref><ref type="bibr">Kenter et al., 2016;</ref><ref type="bibr" target="#b1">Ling et al., 2017;</ref><ref type="bibr" target="#b10">Patel et al., 2017</ref>).</p><p>Most of these models capture the context in- formation of each word using the co-occurrence matrix. However, the co-occurrence matrix only represents relatively local information, i.e., it de- scribes context associations based on word pairs' co-occurrence counts without considering global context perspective. Besides, the co-occurrence matrix is only an estimation of a corpus, which is only a sample of a language. A mass of re- lated word pairs may not be observed in the cor- pus, and the latent relations between unobserved word pairs may not be modeled well due to the missing knowledge.</p><p>Few attempts are carried out to indirectly deal with unobserved co-occurrence for dense neural word embeddings.</p><p>SGNS <ref type="bibr">(Mikolov et al., 2013a,b)</ref> indirectly addresses this prob- lem through negative sampling. <ref type="bibr">Swivel (Shazeer et al., 2016)</ref> improves GloVe by using a "soft hinge" loss to prevent from over-estimating zero co-occurrences. However, the latent relations be- tween unobserved word pairs are not explicitly represented. There are also some works around semantic composition and distributional inference <ref type="bibr" target="#b6">(Mitchell and Lapata, 2008;</ref><ref type="bibr">Erk and</ref><ref type="bibr">Padó, 2008, 2010;</ref><ref type="bibr" target="#b13">Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b21">Thater et al., 2011;</ref><ref type="bibr">Kartsaklis et al., 2013;</ref><ref type="bibr">Kober et al., 2016</ref>) that are explored to address the sparseness prob- lem, but they are not designed for training neural word embeddings.</p><p>In this paper, we explore an approach that uti- lizes context overlap information to dig up more effective co-occurrence relations and propose ex- tensions for GloVe and Swivel to validate the pos- itive impact of introducing context overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quantify Context Overlap</head><p>In this work, we explore quantifying context over- lap based on the observation that to a certain ex- tent the overlap of Point-wise Mutual Information (PMI) <ref type="bibr">(Church and Hanks, 1990</ref>) reflects context overlap.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, two separate words may exhibit a particular aspect of interest or be seman- tically related when the overlap area between their PMI is relatively large.</p><p>The calculation of complete PMI-weighted con- text overlap may be time-consuming when the number of words is large. To make the time com- plexity affordable, only the context words that have strong lexical association with a target word i are considered:</p><formula xml:id="formula_0">S i = {k ∈ V |PMI(i, k) &gt; h PMI } (1)</formula><p>in which V is the vocabulary, h PMI is a thresh- old which acts as a magnitude to shift PMI, and S i denotes the set that consists of the context words that have enough large PMI values with the target word i. It is expected that most context informa- tion associated with the word i can be captured by its PMI values over S i . Then, we measure the de- gree of context overlap (CO) between two target words i, j as a function of their PMI values over the intersection of S i and S j , i.e.,</p><formula xml:id="formula_1">CO(i, j) = k∈S i ∩S j min(f (PMI(i, k)), f (PMI(j, k)))<label>(2)</label></formula><p>where f is a monotonic mapping function to rec- tify the data characteristics for certain objective function in word embedding training.</p><p>Compared to identity function f (x) = x, we find exponential function f (x) = exp(x) works much better in our experiments. For the quantized context overlap, the exponential mapping func- tion results in a similar data distribution as the co- occurrence counts, i.e., few word pairs have ex- tremely large values while most word pairs' values are distributed in a relatively small range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extend to Existing Models</head><p>We consider the original co-occurrence matrix as a description of first order co-occurrence rela- tions, while the quantized context overlap as a de- scription of second order co-occurrence relations <ref type="bibr" target="#b16">(Schütze, 1998)</ref>, i.e., co-co-occurrences, which is represented by "non-logarithmic PMI-weighted context overlap" in this work. The context overlap between two words can be inferred even when they never co-occur in the corpus. According to our statistics, more than 84% word pairs in the second order co-occurrence matrix are not included in the first order co-occurrence matrix. We expect intro- ducing second order co-occurrence relations may enhance the quality of the word embedding that is originally trained on first order co-occurrence rela- tions. <ref type="bibr">GloVe (Pennington et al., 2014</ref>) and Swivel ( <ref type="bibr" target="#b18">Shazeer et al., 2016</ref>) are extended by joint train- ing with context overlap information in this paper.</p><p>GloVe The logarithmic co-occurrence matrix is factorized in GloVe with bias terms, and a weighted least squares loss function is optimized:</p><formula xml:id="formula_2">J GloV e = i,j λ ij (w T i ˜ w j +b i + ˜ b j −log X ij ) 2 (3)</formula><p>where X ij denotes the word-context co- occurrence count between a target word i and a context word j. The model parameters to be learned include w i ∈ R d , ˜ w j ∈ R d , b i and˜band˜and˜b j , which correspond to target word vector, context word vector, bias terms associated with the target word and the context word, respec- tively. λ ij is a weight whose value equals to (min(X ij , x max )/x max ) α .</p><p>To extend GloVe, two tasks are trained in par- allel during the training process: One is the main task that follows the original GloVe training pro-cess as above; Another one is an auxiliary task that tunes word embeddings using context overlap. The parameters of word embeddings are shared in both tasks.</p><p>Following GloVe-style loss function, in the aux- iliary task, the dot products of word vectors are pushed to estimate logarithmic second order co- occurrence.</p><formula xml:id="formula_3">J (2) GloV e = i,j λ (2) ij (Aw T i w j +b (2) i +b (2) j −log X (2) ij ) 2</formula><p>(4) where the superscripts <ref type="bibr">(2)</ref> are used to differenti- ate with the terms in the original GloVe. X (2) ij = CO(i, j) represents context overlap, a word inde- pendent learnable scale A is adopted to relieve the potential inconformity between first order and sec- ond order co-occurrences. The weight λ <ref type="bibr">(2)</ref> ij is sim- ilar to the original λ ij , but using a different hyper- parameter x <ref type="bibr">(2)</ref> max . The multi-task <ref type="bibr" target="#b15">(Ruder, 2017)</ref> loss function is the weighted sum of the two tasks, i.e., J = J GloV e +β·J <ref type="bibr">(</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>GloV e , where the weight β is a hyper- parameter.</p><p>Swivel As pointed out by ( <ref type="bibr">Levy et al., 2015)</ref> , if the bias terms in GloVe are fixed to the log- arithmic count of the corresponding word, the dot products of target word vectors and context word vectors are almost equivalent to the approx- imation of logarithmic PMI matrix with a shift of log i,j X ij . Submatrix-wise Vector Embed- ding Learner (Swivel) directly reconstructs the PMI matrix by dot product between target vec- tors and context vectors and deals with unobserved co-occurrences using a "soft hinge" loss function. ( <ref type="bibr" target="#b18">Shazeer et al., 2016</ref>) details its loss functions and training process. In our extended version, we add a supplementary loss function to handle second or- der co-occurrences. When the second order co- occurrence X <ref type="bibr">(2)</ref> ij is more than zero, the PMI of context overlap is approximated.</p><formula xml:id="formula_4">1 2 λ (2) ij (Aw T i w j + B − PMI (2) (i, j)) 2 (5)</formula><p>in which A, B are word independent learnable scale parameters, and PMI (2) (i, j) is the Point- wise Mutual Information computed on the second order co-occurrence matrix [X</p><p>ij ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Corpus The training dataset contains 6 billion tokens collected from diversified corpora, includ- ing the News Crawl corpus ( <ref type="bibr">Chelba et al., 2013)</ref>, the April 2010 Wikipedia dump <ref type="bibr" target="#b17">(Shaoul, 2010;</ref><ref type="bibr">Lee and Chen, 2017)</ref>, and a year-2012 subset of the Reddit comment datasets <ref type="bibr">1</ref> .</p><p>Preprocessing Following ( <ref type="bibr">Lee and Chen, 2017)</ref>, the Stanford tokenizer is used to process the training corpus, which are split into sentences with characters converted to lower cases. Punctu- ations are removed.</p><p>Parameter Configuration The vocabularies are limited to the 200K most frequent words. Fol- lowing ( <ref type="bibr" target="#b11">Pennington et al., 2014</ref>), a decreasing weighting function is adopted to construct the co- occurrence matrix. We use symmetric context window of five words to the left and five words to the right.</p><p>For GloVe, recommended parameters in <ref type="bibr" target="#b11">(Pennington et al., 2014</ref>) are used. Specifically, we set α = 3 4 , x max = 100, initial learning rate as 0.05, 100 iterations. For Swivel, recommended parame- ters in <ref type="bibr" target="#b18">(Shazeer et al., 2016</ref>) are used. The weight- ing function is 0.1 + 0.25x 0.5 ij , each shard is sam- pled about 100 times. But we set the block size as 4000 so that the vocabulary size can be divided exactly.</p><p>For the auxiliary tasks, we tune the hyper- parameters on the small News Crawl corpus. And we find that in an appropriate range, the threshold h PMI is not sensitive to the performance. In this paper, h PMI , x (2) max and β are set to log 100, 10000 and 0.2 respectively. Since there is no difference between target vectors and context vectors (except random initialization), in order to keep symmetry, we not only approximate context overlap between target vectors, but also approximate context over- lap between context vectors simultaneously. Final vectors are the sum of w and˜wand˜ and˜w in both GloVe and Swivel. <ref type="table">Table 1</ref> shows the evaluation results of word sim- ilarity tasks and word analogy tasks. Word sim- ilarity is measured as the Spearman's rank corre- lation ρ between human-judged similarity and co- sine distance of word vectors. In word analogy  task, the questions are answered over the whole vocabulary through 3CosMul ( <ref type="bibr">Levy and Goldberg, 2014a</ref>). In addition to GloVe and Swivel, the evaluations of SGNS are also reported for refer- ence. We train SGNS with the word2vec tool, us- ing symmetric context window of five words to the left and five words to the right, and 5 negative sam- ples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intrinsic Evaluation</head><p>As can be seen from the table, the context over- lap information enhanced word embeddings per- form better in most word similarity tasks and get higher analogy accuracy in semantic aspect at the cost of syntactic score. The improved semantics performance, to a certain extent, reflects second order co-occurrence relations are more semantic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text Classification</head><p>Text classification tasks are conducted on five shared benchmark datasets from <ref type="bibr">(Kim, 2014</ref>) in- cluding binary classification tasks CR ( <ref type="bibr">Hu and Liu, 2004</ref>), MR (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>), Subj ( <ref type="bibr" target="#b8">Pang and Lee, 2004</ref>) and multiple classification tasks TREC ( <ref type="bibr">Li and Roth, 2002</ref>), SST1 . Texts are preprocessed following the de- scription of Section 4.1. We train Convolutional Neural Networks (CNN) on top of our static pre- trained word vectors following <ref type="bibr">(Kim, 2014)</ref>. To avoid the high-risk of single-run estimate being false ( <ref type="bibr" target="#b3">Melis et al., 2017;</ref><ref type="bibr" target="#b12">Reimers and Gurevych, 2017)</ref>, average classification accuracies of 20 runs are reported as the final scores. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. As can be seen from the re- sults that the enhanced word embeddings outper- form the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Analysis</head><p>As it is known to all, word frequency plays an important role in the computation of word em- beddings ( <ref type="bibr">Gittens et al., 2017</ref>  the graph in <ref type="bibr" target="#b18">(Shazeer et al., 2016)</ref>, relations be- tween word analogy accuracy and the log mean frequency of the words in analogy questions and answers are plotted on <ref type="figure" target="#fig_2">Figure 2</ref>. The word em- beddings trained by GloVe with or without context overlap information are used here. An obvious semantic performance improve- ment is observed in the range of low frequency. Our observation of second order co-occurrences may explain this fact. We randomly sample 1 mil- lion word pairs, and rank these word pairs in de- scending order by their quantized context overlap. In all the word pairs, average word frequency is 13934.4. However, it is only 1676.1 in the top 0.1% word pairs, it is 3984.8 in the top 1%, and it is 7904.9 in the top 10%. This may be caused by PMI's bias towards infrequent words, but it il- lustrates infrequent words carry more information in second order co-occurrence relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an empirical metric to enhance the word embeddings through estimating second order co-occurrence relations using con- text overlap. Instead of only local statistical infor- mation, context overlap leverages global associa- tion distribution to measure word pairs correlation.</p><p>The proposed method is easy to extend to ex- isting models, such as GloVe and Swivel, by an auxiliary objective function. The improvement in experimental results helps to validate the positive impact of introducing quantized context overlap.</p><p>We have considered the feasibility of enriching SGNS and CBOW with information from context- overlap. However, because of their training mode, we can't remake them in a straightforward way following their "original spirit". When training SGNS and CBOW, the program scans the training text. The target and context words are chosen us- ing a slide window and negative sampling is used. In this process, no co-occurrence matrix is explic- itly computed, and we fail to extend it in a united form as we extend GloVe and Swivel. The exten- sions for GloVe and Swivel can also be used for reference for extending other word embedding ap- proaches that are trained on co-occurrence matrix. The exploration for second order co-occurrence can be traced back to 1990s. We think it is help- ful to revive the classical method in a modern, embedding driven way. How to integrate second order co-occurrence information for approaches like SGNS, CBOW should be an interesting future work.</p><p>As future works, we suggest further investigat- ing the characteristics of context overlap in diver- sified ways. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PMI of different words. The x-axis represents a series of context words in a subset of the whole vocabulary, the y-axis denotes PMI values between the target word and context words. (a) The upper part of this figure illustrates the large overlap between semantics related words. (b) The lower part, on the contrary, is an example of relatively unrelated word pair, in which the overlap is relatively small.</figDesc><graphic url="image-1.png" coords="2,313.00,62.81,204.09,163.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relations between word analogy accuracy and the log mean frequency.</figDesc><graphic url="image-2.png" coords="5,77.72,62.81,204.09,144.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>). Inspired from</head><label></label><figDesc></figDesc><table>Method 
CR 
MR SST1 Subj TREC 
GloVe 
80.9 76.5 46.9 
90.9 
89.7 
+ CO 
81.7  † 76.4 47.6  † 91.4  † 90.2  † 
Swivel 81.7 76.7 47.9 
91.4 
90.4 
+ CO 
82.4  † 76.7 48.3  † 91.7  † 
90.5 
CBOW 80.6 75.3 46.5 
89.8 
89.6 
SGNS 
81.6 77.0 48.0 
91.2 
90.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Text classification results (Acc.%). Pre-
trained word vectors with 300 dimensions are reported 
here. Enhanced runs statistically significantly (t-test, 
p-value &lt; 0.05) different from the GloVe/Swivel base-
line runs are marked with a  †. The results of CBOW 
and SGNS are also given for reference. 

</table></figure>

			<note place="foot" n="1"> Available at https://files.pushshift.io/ reddit/comments/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word embedding revisited: A new representation learning and explicit matrix factorization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3650" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating extra knowledge into word embedding models for biomedical nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yetian</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="968" to="975" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapting pre-trained word embeddings for use in medical coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divya</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansi</forename><surname>Golakiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Birari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioNLP</title>
		<imprint>
			<biblScope unit="page" from="302" to="306" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<title level="m">An overview of multi-task learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The westbury lab wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shaoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Edmonton, AB</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Waterson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02215</idno>
		<title level="m">Swivel: Improving embeddings by noticing what&apos;s missing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1329" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word meaning in context: A simple and effective vector model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1134" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning first-order logic embeddings via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2132" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
