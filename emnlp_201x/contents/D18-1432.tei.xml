<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinnuo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical and Computer Sciences</orgName>
								<orgName type="laboratory">The Interaction Lab</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical and Computer Sciences</orgName>
								<orgName type="laboratory">The Interaction Lab</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical and Computer Sciences</orgName>
								<orgName type="laboratory">The Interaction Lab</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical and Computer Sciences</orgName>
								<orgName type="laboratory">The Interaction Lab</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3981" to="3991"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3981</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present three enhancements to existing encoder-decoder models for open-domain conversational agents, aimed at effectively modeling coherence and promoting output diversity: (1) We introduce a measure of coherence as the GloVe embedding similarity between the dialogue context and the generated response, (2) we filter our training corpora based on the measure of coherence to obtain topically coherent and lexically diverse context-response pairs, (3) we then train a response generator using a conditional varia-tional autoencoder model that incorporates the measure of coherence as a latent variable and uses a context gate to guarantee topical consistency with the context and promote lexical diversity. Experiments on the OpenSubtitles corpus show a substantial improvement over competitive neural models in terms of BLEU score as well as metrics of coherence and diversity .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end neural response generation methods are promising for developing open domain dia- logue systems as they allow to learn from very large unlabeled datasets ( <ref type="bibr" target="#b25">Shang et al., 2015;</ref><ref type="bibr" target="#b29">Sordoni et al., 2015;</ref><ref type="bibr" target="#b32">Vinyals and Le, 2015)</ref>. How- ever, these models have also been shown to gen- erate generic, uninformative, and non-coherent replies (e.g., "I don't know." in <ref type="figure">Figure 1</ref>), mainly due to the fact that neural systems tend to set- tle for the most frequent options, thus penaliz- ing length and favoring high-frequency word se- quences ( <ref type="bibr" target="#b30">Sountsov and Sarawagi, 2016;</ref>.</p><p>To address these problems, <ref type="bibr" target="#b12">Li et al. (2016a)</ref> and <ref type="bibr" target="#b16">Li et al. (2017a)</ref> attempt to promote diversity by improving the objective function, but do not model diversity explicitly. <ref type="bibr" target="#b24">Serban et al. (2017)</ref> focus on model structure without any upgrades to the ob- jective function. Other works control the style of the output by leveraging external resources ( <ref type="bibr" target="#b8">Hu et al. (2017)</ref>: sentiment classifier, time annotation; : dialogue acts) or focus on well- structured input such as paragraphs ( <ref type="bibr" target="#b14">Li and Jurafsky, 2017</ref>). This paper extends previous attempts to model diversity and coherence by enhancing all three as- pects of the learning process: the data, the model, and the objective function. While previous re- search has addressed these aspects individually, this paper is the first to address all three in a uni- fied framework. Instead of using existing linguis- tic knowledge or labeled datasets, we aim to con- trol for coherence by learning directly from data, using a fully unsupervised approach. This is also the first work encoding and evaluating coherence explicitly in the dialogue generation task, as op- posed to using diversity, style, or other properties of responses as a proxy.</p><p>In this work, given a dialogue history, we regard as a coherent response an utterance that is themat- ically correlated and naturally continuing from the previous turns, as well as lexically diverse. For example, in <ref type="figure">Figure 1</ref> the response "Specifically the stove." is a very natural and coherent response, elaborating on the topic of kitchen introduced in the previous two utterances and containing rich thematic words, whereas the response "Let's go for a walk." is unrelated and uninteresting.</p><p>In order to obtain coherent responses, we present three generic enhancements to existing encoder-decoder (E-D) models:</p><p>1. We define a measure of coherence simply as the averaged word embedding similarity between the words of the context and the response com- puted using GloVe vectors ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>).</p><p>2. We filter a corpus of conversations based on our measure of coherence, which leaves us with context-response pairs that are both topically coherent and lexically diverse.</p><p>3. We train an E-D generator recast as a con- ditional Variational Autoencoder (cVAE;  model that incorporates two latent variables, one for encoding the context and an- other for conditioning on the measure of co- herence, trained jointly as in <ref type="bibr" target="#b8">Hu et al. (2017)</ref>. We then decode using a context gate ( <ref type="bibr" target="#b31">Tu et al., 2017)</ref> to control the generation of words that directly relate to the most topical words of the context and promote coherence.</p><p>Experiments on the OpenSubtitles ( <ref type="bibr" target="#b18">Lison and Meena, 2016</ref>) corpus demonstrate the effective- ness of the overall approach. Our models achieve a substantial improvement over competitive neural models. We provide an ablation analysis, quanti- fying the contributions that come from effective modeling of coherence into our models. All our experimental code is freely available on GitHub. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Coherence-based Dialogue Generation</head><p>Our model aims to generate responses given a dialogue context, incorporating measures of co- herence estimated purely from the training data. We propose the following enhancements to the attention-based E-D architecture ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Luong et al., 2015</ref>):</p><p>• We introduce a stochastic latent variable z con- ditioned on previous dialogue context to store the global information about the conversation ( <ref type="bibr" target="#b2">Bowman et al., 2016;</ref><ref type="bibr" target="#b4">Chung et al., 2015;</ref><ref type="bibr" target="#b14">Li and Jurafsky, 2017;</ref><ref type="bibr" target="#b8">Hu et al., 2017</ref>).</p><p>1 https://github.com/XinnuoXu/CVAE_Dial</p><p>• We force the model to condition on the mea- sure of coherence explicitly by encoding a la- tent variable (code) c learned from data.</p><p>• We incorporate a context gate ( <ref type="bibr" target="#b31">Tu et al., 2017</ref>) that dynamically controls the ratio at which the generated words in the response derive di- rectly from the coherence-enhanced dialogue context or the previously generated parts of the response.</p><p>In the rest of this section, we introduce the mea- sure of coherence (Section 2.1), we present an overview of our model (Section 2.2), and finally describe the model in detail (Sections 2.3-2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Measure of Dialogue Coherence</head><p>Semantic vector space models of language repre- sent each word with a real-valued word embed- ding vector <ref type="bibr" target="#b23">(Pennington et al., 2014)</ref>. By simply taking a weighted average of all its word embed- dings, a whole sentence can be mapped into the semantic vector space. We define the coherence of a dialogue as the average distance between se- mantic vectors of preceding dialogue context and its response. . Here, w j and v i are im- portance weights for each word in the sentence. <ref type="bibr">2</ref> The measure of coherence is then defined as the cosine distance of the two semantic vectors of the dialogue context and its response:</p><formula xml:id="formula_0">C (x, y) = cos x emb , y emb (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Overview</head><p>End-to-end response generation for dialogue can be formalized as follows: Given a dialogue con- text x, a dialogue generator generates the next ut- terance y. During the training process, the aim for a dialogue generator is to maximize the probabil- ity p (y|x) over the training dataset. To encode dialogue contexts that adequately incorporate co- herence information, we build our generator based on the cVAE model of <ref type="bibr" target="#b8">Hu et al. (2017)</ref>, which has been used to control text generation with respect to linguistic properties, such as tense or sentiment.</p><p>In our model, the response y is generated condi- tioned on the previous conversation x, a diversity- promoting latent variable z, and a latent variable c indicating dialogue coherence; z and c are in- dependent. The generation probability p (y|x) is defined as:</p><formula xml:id="formula_1">p (y|x) = z,c p (y|x, z, c) p (z, c|x) dz dc = z,c p (y|x, z, c) p (z|x) p (c|x) dz dc (2)</formula><p>Unfortunately, optimizing Eq (2) during train- ing is intractable; therefore, we apply variational inference and optimize instead the variational lower bound:</p><formula xml:id="formula_2">log p (y|x) = log z,c p (y|x, z, c) p (z, c|x) dz dc ≥ E q(z|x,y)p(c|x,y) [log p (y|x, z, c)] − D KL (q (z|x, y) p (z|x))<label>(3)</label></formula><p>where p (y|x, z, c) is the probability of gener- ating utterance y given x, z and c; q (z|x, y) stands for the approximate posterior distribution of the latent variable z conditioned on dialogue context x and the gold response y; p (c|x, y) is the measure of coherence between context x and response y; p (z|x) is the true prior distribu- tion of z conditioned only on dialogue context x; D KL (·|·) denotes the KL-divergence. We assume that both q (z|x, y) and p (z|x) are Gaussian with mean vectors µ appr , µ true and covariance matrices Σ appr , Σ true .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Model Details</head><p>Optimizing Eq (3) consists of two parts: (1) min- imizing the KL-divergence between the approxi- mate posterior distribution and the true prior dis- tribution of z, (2) maximizing the probability of generating the gold response y conditioned on di- alogue context x and coherence factors z and c. <ref type="figure">Figure 2</ref> shows the pipeline of the training proce- dure.</p><p>Encoder: First, we encode a dialogue context x into a hidden state h using the context encoder, which is based on Recurrent Neural Networks (RNNs). Then the posterior network encodes both dialogue context x and gold response y into a hid- den state h appr followed by two linear transfor- mations f appr (·) and g appr (·) to map h appr into mean vector µ appr and covariance matrix Σ appr . The latent variable z can be sampled from the dis- tribution N (µ appr , Σ appr ):</p><formula xml:id="formula_3">µ appr = f appr (h appr ) Σ appr = g appr (h appr ) q (z|x, y) = N (µ appr , Σ appr )<label>(4)</label></formula><p>The prior network in <ref type="figure">Figure 2</ref> takes a form similar to the posterior network:</p><formula xml:id="formula_4">µ true = f true (h true ) Σ true = g true (h true ) p (z|x) = N (µ true , Σ true ) (5)</formula><p>where h true is the final hidden state of an RNN en- coding only the dialogue context x, and f true (·), g true (·) are linear transformations. Code c is given by the coherence measure from Eq (1).</p><p>Decoder: We build an attention-based decoder ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Luong et al., 2015</ref>) using RNNs to generate responses conditioned on en- coded dialogue context h, diversity signal z, and coherence signal c. We concatenate the latent vari- ables z and c to the context encoder hidden state h and feed them into the decoder as the initial hidden state s 0 , similar to <ref type="bibr" target="#b8">Hu et al. (2017)</ref>. During the decoding process, tokens are gener- ated sequentially under the following probability distribution:</p><formula xml:id="formula_5">p (y|x, z, c) = I i=1 p y i |y &lt;i , x, z, c = I i=1 g (y i−1 , s i , a i ) (6)</formula><p>where I is the length of the produced response; g (·) is an RNN; s i is the hidden state of the de- coder at time step i which is conditioned on the previously generated token y i−1 , the previous hid- den state s i−1 , and the weighted attention vector <ref type="figure">Figure 2</ref>: The training process of the generative model. First, the dialogue context is encoded: h is the final hidden state of the context encoder. Then we derive the diversity-promoting latent variable z. Next, we compute the latent variable c that corresponds to the measure of coherence between the dialogue context x and the generated response y. We concatenate all three vectors into s to feed the decoder. a is the attention matrix calculated for every time step of the decoding process.</p><p>a i :</p><formula xml:id="formula_6">s i = f (y i−1 , s i−1 , a i )<label>(7)</label></formula><formula xml:id="formula_7">a i = J j=1 w ij h i (8)</formula><p>where J is the number of tokens of the dialogue context; h i is the i th hidden state of the encoder; the attention weight w ij of each context hidden state h i is computed following <ref type="bibr" target="#b20">Luong et al. (2015)</ref>.</p><p>Context Gate: To increase the influence of code c, we introduce the context gate k. Unlike <ref type="bibr" target="#b31">Tu et al. (2017)</ref>, whose context gate assigns an element- wise weight to the input signal deriving from the encoder RNN, we build the context gate condi- tioned only on the coherence signal:</p><formula xml:id="formula_8">k i = λσ c − c i (9)</formula><p>where σ is the sigmoid function; λ is a bias term; 3 c is the target value of the measure of coherence, calculated by C (x, y) (see Section 2.1); c i is the measure of coherence between the dialogue con- text and the generated prefix sentence at time step i, calculated by C x, y &lt;i . Now Eq (7) with the context gate applied to s i can be rewritten as:</p><formula xml:id="formula_9">s i = f y i−1 , (1 − k i ) • s i−1 , k i • a i<label>(10)</label></formula><p>where • denotes element-wise multiplication. The coherence-informed context gate aims to dynamically control the ratio at which preceding dialogue context and previously generated tokens of the current response contribute to the generation of the next token in the response. <ref type="bibr">3</ref> We set λ empirically against the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>Our generator is trained similarly to <ref type="bibr" target="#b8">Hu et al. (2017)</ref>. The objective function is a weighted com- bination of three losses (generation, coherence, and diversity):</p><formula xml:id="formula_10">L = L G + λ c L c + λ z L z<label>(11)</label></formula><p>To teach the generator to produce responses close to the training data, we maximize the generation probability of the training response log p (y|x) given the dialogue context according to <ref type="bibr">Eq (2)</ref>. During training, we set L G = − log p (y|x) and minimize the following:</p><formula xml:id="formula_11">L G =D KL (q (z|x, y) p (z|x)) − E q(z|x,y)p(c|x,y) [log p (y|x, z, c)]<label>(12)</label></formula><p>Apart from the generation loss, the coherence measure provides an extra learning signal L c which pushes the generator to produce responses that match the coherence signal given by the latent variable c.</p><formula xml:id="formula_12">L c = −E p(z|x)p(c) log p c|x, G (x, z, c)<label>(13)</label></formula><p>In Eq (13), p (c) = N (0, 1) is the prior distribu- tion of the coherence variable c. To ensure that the loss is differentiable, we cannot sample words from the response vocabulary. Instead we define G (x, z, c) = y s = {y s 1 , . . . y s i , . . . y s I } as the se- quence of output word probability distributions.</p><formula xml:id="formula_13">p c|x, G (x, z, c)</formula><p>is predicted by the coherence measure defined in Eq (1) with y emb set as:</p><formula xml:id="formula_14">y emb = I i=1 y s j M glv<label>(14)</label></formula><p>where M glv is the word embedding matrix trained using GloVe (Section 2.1).</p><p>The last component in Eq <ref type="formula" target="#formula_10">(11)</ref> is the indepen- dent constraint L z that forces the soft distribution over the generated response G to be diverse, so that it is able to faithfully reproduce the latent vari- able z: <ref type="figure" target="#fig_1">Figure 3</ref> shows the inference process of the gen- erative model. Given a dialogue context x and an expected coherence value c, the context encoder first encodes the dialogue context into a hidden state h. The prior network then generates a sam- ple z conditioned on the dialogue context. The decoder is initialized with s, i.e., the concatena- tion of h, z and c. During decoding, the next word is generated via the context gate modulating between the attention-reweighted context and the previously generated words of the response.</p><formula xml:id="formula_15">L z = −E p(z|x)p(c) log q z|x, G (x, z, c)<label>(15)</label></formula><note type="other">where q z|x, G (x, z, c) is predicted by the pos- terior network with y s j as the soft input to the RNN encoder at each time step j.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset and Filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset for Generator</head><p>We train and evaluate our models on the OpenSub- titles corpus ( <ref type="bibr" target="#b19">Lison and Tiedemann, 2016</ref>) with automatic dialogue turn segmentation ( <ref type="bibr" target="#b18">Lison and Meena, 2016</ref>  <ref type="table">Table 1</ref>: Coherence and diversity metrics <ref type="bibr">7</ref> for the OST and fOST datasets (see Section 3 for the datasets and Section 4.2 for metrics definition).</p><p>coherence score C (x, y) ≥ 0.68. 5</p><p>Filtering of the OpenSubtitles corpus is moti- vated by the fact that by removing the video and audio modalities which the subtitles originally ac- companied, we are very often left with incomplete and incoherent dialogues. Therefore, by keep- ing dialogues with high coherence scores, we aim at building a high quality corpus with (1) more semantically coherent and topically related con- texts and responses, and (2) fewer general and dull responses. <ref type="table">Table 3</ref> shows the coherence and diversity metrics (cf. Section 4.2) between OST and fOST. Unsurprisingly, coherence for fOST is much higher than OST, with a slightly higher di- versity. We list dialogue examples for different co- herence scores in Supplemental Material B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset for Coherence Measure</head><p>In order to accurately measure coherence on our domain using the semantic distance as defined in Section 2.1, we train GloVe embeddings on the full OpenSubtitles corpus (i.e. 100K movies).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our generator model, ablative variants, and base- lines are implemented using the publicly avail- able OpenNMT-py framework ( <ref type="bibr" target="#b10">Klein et al., 2017</ref>) based on <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> and <ref type="bibr" target="#b20">Luong et al. (2015)</ref>. We used the publicly available glove- python package 8 to implement our coherence measure.</p><p>We experiment on two versions of our model: (1) cVAE with the coherence context gate as de- scribed in Section 2.3 (cVAE-XGate), (2) cVAE with the original context gate implementation of <ref type="bibr">5</ref> The coherence score is calculated as shown in Eq (1). We observed that the scores on the training set follow a nor- mal distribution with a slight tail on the negatively correlated side, so we fit a normal distribution to the data with parame- ters N (0.25, 0.22) and set the cut-off to +2σ. A histogram of coherence scores is shown in <ref type="figure">Figure 5</ref> in Supplemental Material A. <ref type="bibr">7</ref> Note that Distinct-1 and Distinct-2 are computed on a randomly selected subsets of 4k responses.</p><p>8 https://github.com/maciejkula/ glove-python ( <ref type="bibr" target="#b31">Tu et al., 2017</ref>) (cVAE-CGate). For each of these, we consider the main variant where the input co- herence measure c is preset to a fixed ideal value as estimated on development data (1.0 for OST and 0.95 for fOST), as well as an oracle variant where we use the true coherence measure between the context and the gold-standard response in the test set (indicated with "(C)" in <ref type="table" target="#tab_2">Tables 2 and 3)</ref>.</p><p>We compare against two baseline models: <ref type="formula">(1)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Settings</head><p>We set our model parameters based on preliminary experiments on the development data.</p><p>We use 2-layer RNNs with LSTM cells <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) with in- put/hidden dimension of 128 for both the context encoder and the decoder. The dropout rate is set to 0.2 and the Adam optimizer ( <ref type="bibr" target="#b9">Kingma and Ba, 2015</ref>) is used to update the parameters. A vocab- ulary of 25,000 words is shared between the en- coder and the decoder.</p><p>Both the posterior network and prior network for the latent variable learning are built with 2- layer LSTM RNNs with input/hidden dimension of 64. The dimension of the latent variable z is set to 20. Same as for the encoder and decoder, the dropout rate is 0.2 and the Adam optimizer is used to update the parameters.</p><p>The window size for GloVe computation in our coherence measure is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>We use a number of metrics to evaluate the outputs of our models:</p><p>• BLEU, B1, B2, B3 -the word-overlap score against gold-standard responses ( <ref type="bibr" target="#b22">Papineni et al., 2002</ref>) used by the vast majority of recent dialogue generation works ( <ref type="bibr" target="#b38">Yao et al., 2017;</ref><ref type="bibr" target="#b16">Li et al., 2017a</ref><ref type="bibr" target="#b15">Li et al., , 2016c</ref><ref type="bibr" target="#b29">Sordoni et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2016a;</ref><ref type="bibr">Ghazvininejad et al., 2017</ref>). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3). 9 • Coh -our novel GloVe-based coherence score calculated using Eq <ref type="formula">(1)</ref> showing the semantic distance of dialogue contexts and generated re- sponses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>All model variants described in Section 4 are trained on both OST and fOST datasets. <ref type="table" target="#tab_2">Tables 2  and 3</ref> present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respec- tive test sections of their training datasets, we also test them on the other dataset (OST-trained mod- els on fOST and vice-versa). This way, we can ob- serve the performance of the fOST-trained models in more noisy contexts and see how good the OST- trained models are when evaluated against coher- ent responses only.</p><p>Given all the evaluated model variants, we can observe the effects and contributions of the indi- vidual components of our setup:</p><p>• Data filtering: The models trained on fOST consistently outperform the same models trained on OST -for all evaluation metrics and on both test sets. This shows that coherence- based training data filtering is generally benefi- cial.    <ref type="table">Table 3</ref>: Evaluation results on the fOST test set (see <ref type="table" target="#tab_2">Section 4 and Table 2</ref> for model description; see Section 4.2 for metrics definition). BLEU score reported here is BLEU-4; B1, B2 and B3 denote lower n-gram BLEU scores.</p><formula xml:id="formula_16">Training data Model BLEU% B1% B2% B3% Coh D-1% D-2% D-Sent%</formula><p>• cVAE-Context Gate models: Nearly all cVAE- based models perform markedly better than the baselines w.r.t. BLEU, coherence, and diver- sity. <ref type="bibr">10</ref> If we look at models trained on OST and tested on fOST (the top half of <ref type="table">Table 3</ref>), we can see that all cVAE-based models, espe- cially cVAE-XGate, are able to learn to pro- duce coherent and diverse response even when trained on a noisy, incoherent corpus. Ex- amples of responses generated by the baseline MMI model and by cVAE-XGate in <ref type="figure">Figure 4</ref> show that cVAE-XGate mostly produces more diverse and coherent responses than MMI.</p><p>• Preset c vs. oracle models with gold-standard c: <ref type="table" target="#tab_2">Table 2</ref> shows that on the noisy OST test set, cVAE-based models using the gold-standard i have an audition at 4:00. B-GT: you volunteered. 1 B:</p><p>apparently now i'm cooking dinner. B-MMI: i don 't know. A:</p><p>how did this all happen? B-XGT: well , i'm going to have a dinner. A:</p><p>it's not that easy. B-GT: the waiting list would take too long. 2 B:</p><p>we can't contact potential donors. B-MMI: i don't know. A:</p><p>if we want to save frank, his brother is his only hope B-XGT: we can't save him. A:</p><p>that fish ate my fish! B-GT: you'll wait till next year to win one of these. 3 B:</p><p>you cost me £30,000 B-MMI: i'm sorry. A: sorry, folks. B-XGT: you owe me a little fish. A:</p><p>was it what you wanted? B-GT: you like model planes? 4 B:</p><p>no. B-MMI: i wanted to know. A:</p><p>i wanted an airplane. B-XGT: i wanted a helicopter. A:</p><p>great. B-GT: yeah, a vanilla decaf latte. 5 B:</p><p>thanks so much. B-MMI: no. A:</p><p>vanilla decaf latte? B-XGT: yeah , that 's a good coffee A:</p><p>to make everybody happy. B-GT: and stick it in your mouth. 6 B:</p><p>which means that sometimes B-MMI: and you got to take a shower. A:</p><p>you got to take a pile of poop B-XGT: and you got to take a walk. A:</p><p>now you'll knock again and say you forgot to ask B-GT: should i knock and make her come here for that? 7 B: if she has a friend for your friend. B-MMI: you're crazy! A:</p><p>are you crazy? B-XGT: i'm crazy, i'm crazy. A:</p><p>i knew you would say that. B-GT: you are all way too pretty to be arguing. 8 B:</p><p>mr. kerem is a charming and decent guy. B-MMI: he's a very good man. A: and he's really into you. B-XGT: i didn't say that.</p><p>Figure 4: Generation output examples comparing two models trained on the OST dataset and tested on the fOST test set: the MMI baseline (B-MMI) and our cVAE-XGate (1.0) model (B-XGT). B-GT is the ground-truth response from the test set. The three sequential dialog turns on the left are the preceding dialogue context used to generate the responses. Corresponding topical phrases are underlined. We can see that cVAE-XGate (1.0) mostly produces markedly more coherent and specific outputs than MMI (1-5). In some cases, it is comparable with MMI (6-7) and occasionally, it is less coherent (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our work fits into the context of the very active area of end-to-end generative conversation mod- els, where neural E-D approaches have been first applied by <ref type="bibr" target="#b32">Vinyals and Le (2015)</ref> and extended by many others since. Many works address the lack of diversity and coherence in E-D outputs <ref type="bibr" target="#b30">(Sountsov and Sarawagi, 2016;</ref> but do not at- tempt to model coherence directly, unlike our work: <ref type="bibr" target="#b12">Li et al. (2016a)</ref> use anti-LM reranking; <ref type="bibr" target="#b15">Li et al. (2016c)</ref> modify the beam search decoding al- gorithm, similar to <ref type="bibr" target="#b26">Shao et al. (2017)</ref> in addition to using a self-attention model. <ref type="bibr" target="#b21">Mou et al. (2016)</ref> predict keywords for the output in a preprocessing step while <ref type="bibr" target="#b35">Wu et al. (2018)</ref> preselect a vocabulary subset to be used for decoding. <ref type="bibr" target="#b13">Li et al. (2016b)</ref> focus specifically on personality generation (using personality embeddings) and  promote topic-specific outputs by language-model rescoring and sampling.</p><p>A lot of recent works explore the use of addi- tional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: <ref type="bibr">Asghar</ref>  We also draw on ideas from other areas than di- alogue generation to build our models: <ref type="bibr" target="#b31">Tu et al. (2017)</ref>'s context gates originate from machine translation and <ref type="bibr" target="#b8">Hu et al. (2017)</ref>'s cVAE training stems from free-text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We showed that explicitly modeling coherence and optimizing towards coherence and diversity leads to better-quality outputs in dialogue response generation. We introduced three extensions to cur- rent encoder-decoder response generation models: (1) we defined a measure of coherence based on GloVe embeddings ( <ref type="bibr" target="#b23">Pennington et al., 2014)</ref>, <ref type="formula">(2)</ref> we filtered the OpenSubtitles training corpus <ref type="bibr" target="#b18">(Lison and Meena, 2016)</ref> based on this measure to obtain coherent and diverse training instances, <ref type="formula" target="#formula_2">(3)</ref> we trained a cVAE model based on ( <ref type="bibr" target="#b8">Hu et al., 2017)</ref> and ( <ref type="bibr" target="#b31">Tu et al., 2017</ref>) that uses our coherence measure as one of the training signals. Our ex- perimental results showed a considerable improve- ment in the output quality over competitive mod- els, which demonstrates the effectiveness of our approach.</p><p>In future work, we plan to replace the GloVe- based measure of coherence with a trained dis- criminator that distinguishes between coherent and incoherent responses ( <ref type="bibr" target="#b14">Li and Jurafsky, 2017)</ref>. This will allow us to use extend the notion of co- herence to account for phenomena such as topic shifts. We also plan to verify the results with a human evaluation study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Let x = {x 1 , . . . x j , . . . x J } represent a dia- logue context and y = {y 1 , . . . y i , . . . y I } a re- sponse. J and I are the numbers of words in the dialogue context and its response, respectively. Semantic vector space models map each word x j into embeddings x emb j , and y i into y emb i . The se- mantic representation of a dialogue context x is then x emb = J j=1 w j x emb j ; for a response y, it is y emb = I i=1 v i y emb i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The inference process of the generative model, where the latent variable c is given as an input.</figDesc><graphic url="image-2.png" coords="6,140.03,62.81,317.48,89.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a vanilla E-D with attention (Attention) (Luong et al., 2015); (2) an enhancement where output beams are rescored using the maximum mutual in- formation anti-language model (MMI-antiLM) of Li et al. (2016a) (MMI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>D-1, D-2, D-Sent -common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>OST</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine out- puts), Xu et al. (2017) use a full adversarial train- ing setup. The most recent works explore the us- age of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hier- archical E-D model. Shen et al. (2017) use a cVAE conditioned on sentiment and response genericity (based on a handwritten list of phrases). Shen et al. (2018) combine a cVAE with a plain VAE in an adversarial fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation results on the OST test set (see Section 4 for model description and Section 4.2 for metrics 
definition). Note that the cVAE-CGate(C) / cVAE-XGate(C) models use the true c value between the context and 
the gold response as input. Other cVAE-CGate / cVAE-XGate models use fixed values for c selected on dev sets 
shown in brackets. BLEU score reported here is BLEU-4; B1, B2 and B3 denote lower n-gram BLEU scores. 

Training data Model 
BLEU% B1% B2% B3% 
Coh 
D-1% D-2% D-Sent% 

OST 

Attention 
0.86 
8.34 
2.79 
1.45 
0.284 
3.6 
14.6 
29.4 
MMI 
0.89 
8.47 
2.89 
1.48 
0.278 
3.7 
15.3 
31.5 
cVAE-CGate (C) 
1.64 
10.20 
4.17 
2.40 
0.329 
5.1 
19.4 
35.8 
cVAE-XGate (C) 
1.80 
11.70 
4.90 
2.83 
0.359 
5.2 
19.2 
39.7 
cVAE-CGate (1.0) 
2.25 
16.82 
6.81 
3.70 
0.422 
5.4 
28.2 
81.0 
cVAE-XGate (1.0) 
2.41 
18.62 
7.56 
4.09 
0.434 
4.8 
23.4 
84.0 

fOST 

Attention 
3.84 
16.65 
8.72 
5.54 
0.803 
12.8 
43.4 
88.7 
MMI 
3.84 
16.81 
8.78 
5.57 
0.803 
12.6 
42.5 
88.8 
cVAE-CGate (C) 
4.58 
17.64 
9.53 
6.30 
0.796 
12.4 
41.6 
85.5 
cVAE-XGate (C) 
4.33 
18.43 
9.59 
6.11 
0.783 
10.7 
33.1 
78.8 
cVAE-CGate (0.95) 
4.98 
20.95 10.93 
7.02 
0.814 
12.1 
51.4 
98.2 
cVAE-XGate (0.95) 
4.47 
20.98 10.43 
6.50 
0.797 
10.4 
42.5 
97.6 

</table></figure>

			<note place="foot" n="2"> We set the importance weights to 0 for a list of stop words (high-frequency words such as articles and prepositions, names, punctuation marks), 1 otherwise.</note>

			<note place="foot" n="9"> We use the Multi-BLEU script from OpenNMT to measure BLEU scores.</note>

			<note place="foot" n="10"> We performed paired bootstrap re-sampling for the best cVAE model and the best baseline model in each experiments set (Table 2 and Table 3) as is done for MT (Koehn, 2004), which confirmed statistical significance at 99% confidence level for all cases except for models trained on fOST and tested on OST (bottom half of Table 2). value of c achieve higher BLEU scores than models using preset c. This is expected since many gold-standard responses in the unfiltered set have a low coherence score;-the model can generate a more generic response if the gold-standard c is low. The models with preset c always attempt to generate coherent responses, which is apparent from the other metrics: Coh and D-Sent are consistently higher than for models using gold-standard c. On the fOST test set where only high-coherence responses are expected, models using fixed c consistently reach higher scores in all metrics including BLEU (see Table 3). This shows that in general, using a preset constant value of c works well, even better than using the goldstandard c. In sum, using our coherence measure both for data filtering and inside the models leads to output performance improvements.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research received funding from the EPSRC project MaDrIgAL (EP/N017536/1). The Titan Xp used for this research was donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Active Learning for Dialogue Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabiha</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv: 1612.03929</idno>
	</analytic>
	<monogr>
		<title level="m">6th Joint Conference on Lexical and Computational Semantics (*SEM) 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv: 1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR2015)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Variable Dialogue Models and their Diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01932</idno>
		<title level="m">Wen tau Yih, and Michel Galley. 2017. A knowledgegrounded neural conversation model</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno>ArXiv: 1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on empirical methods in natural language processing</title>
		<meeting>the 2004 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Persona-Based Neural Conversation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno>ArXiv: 1603.06155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Net Models for Open-Domain Discourse Coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>ArXiv: 1606.01545</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to decode for future success</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06547</idno>
		<title level="m">Adversarial Learning for Neural Dialogue Generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic Turn Segmentation for Movie &amp; TV Subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raveesh</forename><surname>Meena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Spoken Language Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>IEEE conference proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the 10th International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>ArXiv: 1508.04025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno>ArXiv: 1607.00970</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3349" to="3358" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Joseph</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating Long and Diverse Responses with Neural Conversation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>ArXiv: 1701.03185</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2200" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Conditional Variational Framework for Dialog Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<idno>ArXiv: 1705.00316</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Variational Encoder-Decoders in Dialogue Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<idno>ArXiv: 1802.02032</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Length bias in Encoder Decoder Models and a Case for Global Conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1516" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context Gates for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="87" to="99" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Steering output style and topic in neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2140" to="2150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Why Do Neural Dialog Systems Generate Short and Meaningless Replies?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02250[cs].ArXiv:1712.02250</idno>
	</analytic>
	<monogr>
		<title level="m">A Comparison between Dialog and Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural Response Generation with Dynamic Vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv: 1711.11191</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI 2018</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural response generation via gan with an approximate embedding layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Sun Chengjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="628" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards implicit contentintroducing for generative short-text conversation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno>ArXiv: 1703.10960</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
