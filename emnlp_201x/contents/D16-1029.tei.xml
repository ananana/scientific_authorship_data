<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="297" to="306"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatically solving algebra word problems has raised considerable interest recently. Existing state-of-the-art approaches mainly rely on learning from human annotated equations. In this paper, we demonstrate that it is possible to efficiently mine algebra problems and their numerical solutions with little to no manual effort. To leverage the mined dataset, we propose a novel structured-output learning algorithm that aims to learn from both explicit (e.g., equations) and implicit (e.g., solutions) supervision signals jointly. Enabled by this new algorithm, our model gains 4.6% absolute improvement in accuracy on the ALG-514 benchmark compared to the one without using implicit supervision. The final model also outperforms the current state-of-the-art approach by 3%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Algebra word problems express mathematical rela- tionships via narratives set in a real-world scenario, such as the one below:</p><p>Maria is now four times as old as Kate. Four years ago, Maria was six times as old as Kate. Find their ages now.</p><p>The desired output is an equation system which ex- presses the mathematical relationship symbolically: m = 4 × n and m − 4 = 6 × (n − 4) where m and n represent the age of Maria and Kate, respectively.</p><p>The solution (i.e., m = 40, n = 10) can be found by a mathematical engine given the equation systems. Building efficient automatic algebra word problem solvers have clear values for online education sce- narios. The challenge itself also provides a good test bed for evaluating an intelligent agent that un- derstands natural languages, a direction advocated by artificial intelligence researchers <ref type="bibr" target="#b6">(Clark and Etzioni, 2016</ref>).</p><p>One key challenge of solving algebra word prob- lems is the lack of fully annotated data (i.e., the an- notated equation system associated with each prob- lem). In contrast to annotating problems with binary or categorical labels, manually solving algebra word problems to provide correct equations is time con- suming. As a result, existing benchmark datasets are small, limiting the performance of supervised learning approaches. However, thousands of alge- bra word problems have been posted and discussed in online forums, where the solutions can be easily mined, despite the fact that some of them could be incorrect. It is thus interesting to ask whether a bet- ter algebra problem solver can be learned by lever- aging these noisy and implicit supervision signals, namely the solutions.</p><p>In this work, we address the technical difficulty of leveraging implicit supervision in learning an alge- bra word problem solver. We argue that the effec- tive strategy is to learn from both explicit and im- plicit supervision signals jointly. In particular, we design a novel online learning algorithm based on structured-output Perceptron. By taking both kinds of training signals together as input, the algorithm iteratively improves the model, while at the same time it uses the intermediate model to find candidate equation systems for problems with only numerical solutions.</p><p>Our contributions are summarized as follows.</p><p>• We propose a novel learning algorithm (Sec- tion 3 and 4) that jointly learns from both ex- plicit and implicit supervision. Under different settings, the proposed algorithm outperforms the existing supervised and weakly supervised algorithms (Section 6) for algebra word prob- lems.</p><p>• We mine the problem-solution pairs for alge- bra word problems from an online forum and show that we can effectively obtain the implicit supervision with little to no manual effort (Sec- tion 5). 1 • By leveraging both implicit and explicit su- pervision signals, our final solver outperforms the state-of-the-art system by 3% on ALG- 514, a popular benchmark data set proposed by ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatically solving mathematical reasoning problems expressed in natural language has been a long-studied problem <ref type="bibr" target="#b3">(Bobrow, 1964;</ref><ref type="bibr" target="#b19">Newell et al., 1959;</ref><ref type="bibr" target="#b18">Mukherjee and Garain, 2008</ref>  <ref type="bibr" target="#b20">and Roth (2015)</ref> focused on how to solve arithmetic problems without using any pre-defined template. In ( <ref type="bibr" target="#b21">Shi et al., 2015)</ref>, the authors focused on number word problems and proposed a system that is created using semi-automatically generated rules. In <ref type="bibr" target="#b31">Zhou et al. (2015)</ref>, the authors simplified the inference procedure and pushed the state-of-the-art benchmark accuracy. The idea of learning from implicit supervision is discussed in ( <ref type="bibr" target="#b31">Zhou et al., 2015;</ref><ref type="bibr" target="#b13">Koncel-Kedziorski et al., 2015)</ref>, where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4.</p><p>Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (un- der the name response-driven learning <ref type="bibr" target="#b7">(Clarke et al., 2010)</ref>) to knowledge base question answering tasks such as Geoquery ( <ref type="bibr" target="#b29">Zelle and Mooney, 1996)</ref> and <ref type="bibr">WebQuestions (Berant et al., 2013)</ref> or mapping in- structions to actions <ref type="bibr" target="#b0">(Artzi and Zettlemoyer, 2013)</ref>. In these tasks, researchers have shown that it is pos- sible to train a semantic parser only from question- answer pairs, such as "What is the largest state bor- dering <ref type="bibr">Texas?" and "New Mexico" (Clarke et al., 2010;</ref><ref type="bibr" target="#b27">Yih et al., 2015)</ref>.</p><p>One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure, paths in the knowledge graph that connect the an- swers and the entities in the narrative can be inter- preted as legitimate semantic parses. However, as we will show in our experiments, learning from im- plicit supervision alone is not a viable strategy for algebra word problems. Compared to the knowl- edge base question answering problems, one key dif- ference is that a large number (potentially infinitely many) of different equation systems could end up having the same solutions. Without a database or special rules for combining variables and coeffi- cients, the number of candidate equation systems cannot be trimmed effectively, given only the solu- tions.</p><p>From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop la- tent structured prediction models ( <ref type="bibr" target="#b28">Yu and Joachims, 2009;</ref><ref type="bibr" target="#b4">Chang et al., 2013;</ref><ref type="bibr" target="#b30">Zettlemoyer and Collins, 2007</ref>) to find latent semantic structures which best explain the answer given the question. Our algo- rithm is also influenced by the discriminative re- ranking algorithms <ref type="bibr" target="#b8">(Collins, 2000;</ref><ref type="bibr" target="#b10">Ge and Mooney, 2006;</ref><ref type="bibr" target="#b5">Charniak and Johnson, 2005</ref>) and models for learning from intractable supervision <ref type="bibr" target="#b22">(Steinhardt and Liang, 2015)</ref>.</p><p>Recently, <ref type="bibr" target="#b12">Huang et al. (2016)</ref> collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset. <ref type="bibr">2</ref>  <ref type="table">Table 1</ref> lists all the symbols representing the compo- nents in the process. The input algebra word prob- lem is denoted by x, and the output y = (T, A) is called a derivation, which consists of an equation system template T and an alignment A. A template T is a family of equation systems parameterized by a set of coefficients</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><formula xml:id="formula_0">C(T ) = {c i } k i=1</formula><p>, where each co- efficient c i aligns to a textual number (e.g., four) in a word problem. Let Q(x) be all the textual numbers in the problem x, and C(T ) be the coefficients to be determined in the template T . An alignment is a set of tuples A = {(q, c) | q ∈ Q(x), c ∈ C(T ) ∪ {}}, where the tuple (q, ) indicates that the number q is not relevant to the final equation system. By spec- ifying the value of each coefficient, it identifies an equation system belonging to the family represented by template T . Together, T and A generate a com- plete equation system, and the solution z can be de- rived by the mathematical engine E.</p><p>Following ( <ref type="bibr" target="#b31">Zhou et al., 2015)</ref>, our strategy of mapping a word problem to an equation system is to first choose a template that consists of variables and coefficients, and then align each coefficient to a textual number mentioned in the problem. We formulate the mapping between an algebra word problem and an equation system as a structured learning problem. The output space is the set of all possible derivations using templates that are observed in the training data. Our model maps x to y = (T, A) by a linear scoring function w T Φ(x, y), where w is the model parameters and Φ is the feature functions. At test time, our model scores all the derivation candidates and picks the best one according to the model score. We often refer to y as a semantic parse, as it represents the semantics of the algebra word problem. to find the correct derivation, as multiple derivations may lead to the same solution. Therefore, the learning algo- rithm has to explore the output space to guide the model in order to match the annotated response.</p><p>Properties of Implicit Supervision Signals We discuss some key properties of the implicit supervi- sion signal to explain several design choices of our algorithm. <ref type="figure">Figure 1</ref> illustrates the main differences between implicit and explicit supervision signals. Algorithms that learn from implicit supervision signals face the following challenges. First, the learning system usually does not model directly the correlations between the input x and the solution z. Instead, the mapping is handled by an external procedure such as a mathematical engine. There- fore, E(y) is effectively a one-directional function. As a result, finding semantic parses (derivations) from responses (solutions) E −1 (z) can sometimes be very slow or even intractable. Second, in many cases, even if we could find a semantic parse from responses, multiple combinations of templates and alignments could end up with the same solution set (e.g., the solutions of equations 2 + x = 4 and 2 × x = 4 are the same). Therefore, the implicit supervision signals may be incomplete and noisy, and using the solutions alone to guide the training procedure might not be sufficient. Finally, since we need to have a complete derivation before we can observe the response of the mathematical engine E, we cannot design efficient inference methods such as dynamic programming algorithms based on par- tial feedback. As a result, we have to perform explo- ration during learning to search for fully constructed semantic parses that can generate the correct solu- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Example</head><p>Word Problem x Maria is now four times as old as Kate. Four years ago, Maria was six times as old as Kate. Find their ages now.</p><formula xml:id="formula_1">Derivation (Semantic Parse) y = (T, A) ({m − a × n = −1 × a × b + b, m − c × n = 0}, A) Solution z n = 10, m = 40</formula><p>Mathematical Engine E : y → z After determining the coefficients, the equation system is {m = 4 × n, m − 4 = 6 × (n − 4)}. The solution is thus n = 10, m = 40. <ref type="table">Table 1</ref>: Notation used in this paper to formally describe the problem of mapping algebra word problems to equations.</p><formula xml:id="formula_2">Variables v m, n Textual Number 3 Q(x) {four, Four, six} Equation System Template T {m − a × n = −1 × a × b + b, m − c × n = 0} Coefficients C(T ) a, b, c Alignment A six → a, Four → b, four → c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning from Mixed Supervision</head><p>We assume that we have two sets:</p><formula xml:id="formula_3">D e = {(x e , y e )} and D m = {(x m , z m )}.</formula><p>D e contains the fully an- notated equation system y e for each algebra word problem x e , whereas in D m , we have access to the numerical solution z m to each problem, but not the equation system (y m = ∅). We refer to D e as the ex- plicit set and D m as the implicit set. For the sake of simplicity, we explain our approach by modifying the training procedure of the structured Perceptron algorithm <ref type="bibr" target="#b9">(Collins, 2002</ref>). <ref type="bibr">4</ref> As discussed in Section 3, the key challenge of learning from implicit supervision is that the map- ping E(y) is one-directional. Therefore, the correct equation system cannot be easily derived from the numerical solution. Intuitively, for data with only implicit supervision, we can explore the structure space Y and find the best possible derivatioñ y ∈ Y according to the current model. If E(˜ y) matches z, then we can update the model based oñ y. Following this intuition, we propose MixedSP (Algorithm 1).</p><p>For each example, we use an approximate search algorithm to collect top scoring candidate structures. The algorithm first ranks the top-K templates ac- cording to the model score, and forms a candidate set by expanding all possible derivations that use the K templates (Line 3). The final candidate set is Ω = {y 1 , y 2 , . . . ,</p><formula xml:id="formula_4">y K } ⊂ Y .</formula><p>When the explicit supervision is available (i.e., <ref type="bibr">4</ref> Our approach can be easily extended to other structured learning algorithms such as Structured SVM ( <ref type="bibr" target="#b24">Taskar et al., 2004;</ref><ref type="bibr" target="#b25">Tsochantaridis et al., 2004)</ref>.</p><formula xml:id="formula_5">(x i , y i ) ∈ D e )</formula><p>, our algorithm follows the standard structured prediction update procedure. We find the best scoring structurê y in Ω and then update the model using the difference of the feature vectors be- tween the gold output structure y i and the best scor- ing structurê y (Line 6). When only implicit supervision is available (i.e., (x i , z i ) ∈ D m ), our algorithm uses the current model to conduct a guided exploration, which it- eratively finds structures that best explain the im- plicit supervision, and use the explanatory structure for making updates. As mentioned in Section 3, we have to explore and examine each structure in the candidate set Ω. This is due the fact that par- tial structure cannot be used for finding the right re- sponse, as getting response E(y) requires complete derivations. In Line 9, we want to find the deriva- tions y where its solution E(y) matches the implicit supervision z i . More specifically,</p><formula xml:id="formula_6">˜ y = arg min y∈Ω ∆(E(y), z i ),<label>(1)</label></formula><p>where ∆ is a loss function to estimate the dis- agreement between E(y) and z i . In our experi- ments, we simply set ∆(E(y), z i ) to be 0 if the solution partially matches, and 1 otherwise. <ref type="bibr">5</ref> If more than one derivation achieves the minimal value of ∆(E(y), z i ), we break ties by choosing the derivation with higher score w T φ(x i , y). This tie-</p><formula xml:id="formula_7">Algorithm 1 Structured Perceptron with Mixed Super- vision. (MixedSP) Input: D e , D m , L = |D e | + |D m |, T , K, γ ∈ [0, 1) 1: for t = 1 . . . N do training epochs 2: for i = 1 . . . L do 3:</formula><p>Ω ← find top-K structures {y} approxi- mately <ref type="bibr">4:</ref> if y i = ∅ then explicit supervision 5: </p><formula xml:id="formula_8">ˆ y ← arg max y∈Ω w T φ(x i , y) 6: w ← w + η (φ(x, y i ) − φ(x, ˆ<label>y)</label></formula><formula xml:id="formula_9">w ← w + η (φ(x, ˜ y) − φ(x, ˆ y)) 11:</formula><p>Return the average of w breaking strategy is important -in practice, several derivations may lead to the gold numerical solution; however, only few of them are correct. The tie- breaking strategy relies on the current model and the structured features φ(x i , y) to filter out incor- rect derivations during training. Finally, the model is updated using˜yusing˜ using˜y in Line 10.</p><p>Similar to curriculum learning ( <ref type="bibr" target="#b1">Bengio et al., 2009)</ref>, it is important to control when the algorithm starts exploring the output space using weak super- vision. Exploring too early may mislead the model, as the structured feature weights w may not be able to help filter out incorrect derivations, while explor- ing too late may lead to under-utilization of the im- plicit supervision. We use the parameter γ to control when the model starts to learn from implicit supervi- sion signals. The parameter γ denotes the fraction of the training time that the model uses purely explicit supervision.</p><p>Key Properties of Our Algorithm The idea of us- ing solutions to train algebra word problem solvers has been discussed in (  and ( <ref type="bibr" target="#b31">Zhou et al., 2015</ref>). However, their implicit su- pervision signals are created from clean, fully super- vised data, and the experiments use little to no ex- plicit supervision examples. <ref type="bibr">6</ref> While their algorithms are interesting, the experimental setting is somewhat unrealistic as the implicit signals are simulated.</p><p>On the other hand, the goal of our algorithm is to significantly improve a strong solver with a large quantity of unlabeled data. Moreover, our implicit supervision signals are noisier given that we crawled the data automatically, and the clean labeled equa- tion systems are not available to us. As a result, we have made several design choices to address issues of learning from noisy implicit supervision signals in practice.</p><p>First, the algorithm is designed to perform up- dates conservatively. Indeed, in Line 10, the algo- rithm will not perform an update if the model could not find any parses matching the implicit signals in Line 9. That is, if ∆(E(y), z i ) = 1 for all y ∈ Ω, ˜ y = ˆ y due to the tie-breaking mechanism. This ensures that the algorithm drives the learning using only those structures which lead to the correct solu- tion, avoiding undesirable effects of noise.</p><p>Second, the algorithm does not use implicit su- pervision signals in the early stage of model train- ing. Learning only on clean and explicit supervision helps derive a better intermediate model, which later allows exploring the output space more efficiently using the implicit supervision signals.</p><p>Existing semantic parsing algorithms typically use either implicit or explicit supervision signals ex- clusively ( <ref type="bibr" target="#b30">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013)</ref>. In contrast, MixedSP makes use of both explicit and implicit su- pervised examples mixed at the training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mining Implicit Supervision Signals</head><p>In this section, we describe the process of collect- ing SOL-2K, a data set containing question-solution pairs of algebra word problems from a Web forum <ref type="bibr">7</ref> , where students and tutors interact to solve math problems.</p><p>A word problem posted on the forum is often ac- companied by a detailed explanation provided by tu- tors, which includes a list of the relevant equations. However, these posted equations are not suitable for direct use as labeled data, as they are often imprecise or incomplete. For instance, tutors often omit many simplification steps when writing the equations. A commonly observed example is that (5-3)x+2y would be directly written as 2x+2y. Despite being mathematically equivalent, learning from the latter equation is not desirable as the model may learn that 5 and 3 appearing the text are irrelevant. An ex- treme case of this is when tutors directly post the so- lution (such as x=2 and y=5), without writing any equations. Another observation is that tutors often write two-variable equation systems with only one variable. For example, instead of writing x+y=10, x-y=2, many tutors pre-compute x=10-y using the first equation and substitute it in the second one, which results in 10-y-y=2. It is also possible that the tutor wrote the incorrect equation system, but while explaining the steps, made corrections to get the right answer. These practical issues make it dif- ficult to use the crawled equations for explicit super- vision directly.</p><p>On the other hand, it is relatively easy to ob- tain question-solution pairs with simple heuristics. We use a simple strategy to generate the solution from the extracted equations. We greedily select equations in a top-down manner, declaring suc- cess as soon as we find an equation system that can be solved by a mathematical engine (we used SymPy <ref type="bibr">(Sympy Development Team, 2016)</ref>). Equa- tions that cause an exception in the solver (due to improper extraction) are rejected. Note that the solu- tion thus found may be incorrect (making the mined supervision noisy), as the equation system used by the solver may contain an incorrect equation. To en- sure the quality of the mined supervision, we use several simple rules to further filter the problems. For example, we remove questions that have more than 15 numbers. We found that usually such ques- tions were not a single word problem, but instead concatenations of several problems.</p><p>Note that our approach relies only on a few rules and a mathematical engine to generate (noisy) im- plicit supervision from crawled problems, with no human involvement. Once the solutions are gener- ated, we discarded the equation systems used to ob- tain them. Using this procedure, we collected 2,039 question-solution pairs. For example, the solution to the following mined problem was "6" (The correct solutions are 6 and 12.):</p><p>Roz is twice as old as Grace. In 5 years the sum of their ages will be 28. How old are they now?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>Explicit sets Implicit sets <ref type="table" target="#tab_3">ALG-514 DRAW-1K  SOL-2K  # temp.  24  224  Unknown  # prob.  514  1,000</ref> 2,039 Vocab.</p><formula xml:id="formula_10">(D e ) (D m ) Dataset</formula><p>1.83k 2.2k 6.8k <ref type="table" target="#tab_3">Table 2</ref>: The statistics of the data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we demonstrate the effectiveness of the proposed approach and empirically verify the de- sign choices of the algorithm. We show that our joint learning approach leverages mined implicit super- vision effectively, improving system performance without using additional manual annotations (Sec- tion 6.1). We also compare our approach to existing methods under different supervision settings (Sec- tion 6.2).  <ref type="bibr" target="#b26">and Chang, 2016)</ref>, which covers more than 200 templates and contains 1,000 alge- bra word problems. The data is split into training, development, and test sets, with 600/200/200 exam- ples, respectively. The SOL-2K dataset contains the word problem- solution pairs we mined from online forum (see Sec- tion 5). Unlike ALG-514 and DRAW-1K, there are no annotated equation systems in this dataset, and only the solutions are available. Also, no prepro- cessing or cleaning is performed, so the problem descriptions might contain some irrelevant phrases such as "please help me". Since all the datasets are generated from online forums, we carefully exam- ined and removed problems from SOL-2K that are identical to problems in ALG-514 and DRAW-1K, to ensure fairness. We set the number of iterations to 15 and the learning rate η to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>For all experiments, we report solution accuracy (whether the solution was correct). Following , we ignore the ordering of answers when calculating the solution accuracy. We report the 5-fold cross validation accuracy on ALG-514 in order to have a fair comparison with previous work. For DRAW-1K, we report the results on the test set. In all the experiments, we only use the templates that appear in the corresponding explicit supervision.</p><p>Following <ref type="figure">(Zhou et al., 2015)</ref>, we do not model the alignments between noun phrases and vari- ables. We use a similar set of features introduced in ( <ref type="bibr" target="#b31">Zhou et al., 2015)</ref>, except that our solver does not use rich NLP features from dependency parsing or coreference-resolution systems. We follow ) and set the beam-size K to 10, unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Joint Learning from Mixed Supervision</head><p>Supervision Protocols We compare the following training protocols:</p><p>• Explicit (D = {(x e , y e )}): the standard set- ting, where fully annotated examples are used to train the model (we use the structured Per- ceptron algorithm as our training algorithm here).</p><p>• Implicit (D = {(x m , z m ))}): the model is trained on SOL-2K dataset only (i.e., only im- plicit supervision). This setting is similar to the one in ( <ref type="bibr" target="#b7">Clarke et al., 2010)</ref>.</p><formula xml:id="formula_11">• Pseudo (D = {(x m , ˜ Z −1 (z m , x m ))}):</formula><p>where we use˜Zuse˜ use˜Z −1 (z, x) to denote a pseudo deriva- tion whose solutions match the mined solu- tions. Similar to the approach in ( <ref type="bibr" target="#b27">Yih et al., 2015)</ref> for question answering, here we attempts to recover (possibly incorrect) explicit supervi- sion from the implicit supervision by finding parses whose solution matches the mined so- lution. For each word problem, we generated a pseudo derivatioñ Z −1 (z, x) by finding the equation systems whose solutions that match the mined solutions. We conduct a brute force search to find˜Zfind˜ find˜Z −1 (z, x) by enumerating all possible derivations. Note that this process can be very slow for datasets like DRAW-1K be- cause the brute-force search needs to examine more than 200 templates for each word prob- lem. Ties are broken by random.</p><p>•</p><formula xml:id="formula_12">E+P (D = {(x e , y e )}∪ {(x m , ˜ Z −1 (z m , x m ))}):</formula><p>a baseline approach that jointly learns by com- bining the dataset generated by Pseudo with the Explicit supervision.</p><p>• MixedSP (D = {(x e , y e )} ∪ {(x m , z m ))}): the setting used by our proposed algorithm. The al- gorithm trained the word problem solver using both explicit and implicit supervision jointly. We set the parameter γ to 0.5 unless otherwise stated. In other words, the first half of the train- ing iterations use only explicit supervision.</p><p>Note that Explicit, E+P, and MixedSP use the same amount of labeled equations, although E+P and MixedSP use additional implicit supervised re- sources.</p><p>Results <ref type="table" target="#tab_5">Table 3</ref> lists the main results. With implicit supervision from mined question-solution pairs, MixedSP outperforms Explicit by around 4.5% on both datasets. This verifies the claim that the joint learning approach can benefit from the noisy implicit supervision. Note that with the same amount of supervision signals, E+P performs poorly and even under-performs Explicit. The reason is that the derived derivations in SOL-2K can be noisy. In- deed, we found that about 70% of the problems in the implicit set have more than one template that can produce a derivation which matches the mined solutions. Therefore, the pseudo derivation selected by the system might be wrong, even if they generate the correct answers. As a result, E+P can commit to the possibly incorrect pseudo derivations before training, and suffer from error propagation. In con- trast, MixedSP does not commit to a derivation and allows the model to choose the one best explaining the implicit signals as training progresses.</p><p>As expected, using only the implicit set D m per- forms poorly. The reason is that in both Implicit and Pseudo settings, the algorithm needs to select one from many derivations that match the labeled solutions, and use the selected derivation to update the model. When there are no explicit supervision   signals, the model can use incorrect derivations to update the model. As a result, models on both Implicit and Pseudo settings perform significantly worse than the Explicit baseline in both datasets, even if the size of SOL-2K is larger than the fully supervised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparisons to Previous Work</head><p>We now compare to previous approaches for solving algebra word problems, both in fully supervised and weakly supervised settings.</p><p>Comparisons of Overall Systems We first com- pare our systems to the systems that use the same level of explicit supervision (fully labeled exam- ples). The comparison between our system and ex- isting systems are in <ref type="figure">Fig 2a and 2b</ref>. Compared to previous systems that were trained only on explicit signals, our Explicit baseline is quite competitive. On ALG-514, the accuracy of our baseline system is 78.4%, which is 1.3% lower than the best reported accuracy achieved by the system ZDC15 ( <ref type="bibr" target="#b31">Zhou et al., 2015)</ref>. We suspect that this is due to the richer feature set used by ZDC15, which includes fea- tures based on POS tags, coreference and depen- dency parses, whereas our system only uses fea- tures based on POS tags. Our system is also the best system on DRAW-1K, and performs much better than the system KAZB14 ( ). Note that we could not run the system ZDC15 on DRAW-1K because it can only handle limited types of equation systems. Although the Explicit baseline is strong, the MixedSP algorithm is still able to im- prove the solver significantly through noisy implicit supervision signals without using manual annotation of equation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Weakly Supervised Algorithms</head><p>In the above comparisons, MixedSP benefits from the mined implicit supervision as well as using Al- gorithm 1. Since there are several practical limita- tions for us to run previously proposed weakly su- pervised algorithms in our settings, in the following, we perform a direct comparison between MixedSP and existing algorithms in their corresponding set- tings. Note that the implicit supervision in weak su- pervision settings proposed in earlier work is noise- free, as it was simulated by hiding equation systems of a manually annotated dataset. <ref type="bibr" target="#b31">Zhou et al. (2015)</ref> proposed a weak supervision setting where the system was provided with the set of all templates, as well as the solutions of all prob- lems during training. Under this setting, they re- ported 72.3% accuracy on ALG-514. Note that such high accuracy can be achieved mainly because that the complete and correct templates were supplied.</p><p>In this setting, running the MixedSP algorithm is equivalent to using the Implicit setting with clean implicit supervision signals. Surprisingly, MixedSP can obtain 74.3% accuracy, surpassing the weakly supervised model in ( <ref type="bibr" target="#b31">Zhou et al., 2015</ref>) on ALG- 514. Compared to the results in <ref type="table" target="#tab_5">Table 3</ref>, note that when using noisy implicit signals, it cannot obtain the same level of results, even though we had more training problems (2,000 mined problems instead of 514 problems). This shows that working with real, noisy weak supervision is much more challenging than working on simulated, noise-free, weak super- vision.  proposed another weak su- pervision setting (5EQ+ANS in the paper), in which explicit supervision is provided for only 5 prob- lems in the training data. For the rest of problems, only their solutions are provided. The 5 problems are chosen such that their templates constitute the 5 most common templates in the dataset. This weak supervision setting is harder than that of ( <ref type="bibr" target="#b31">Zhou et al., 2015)</ref>, as the solver only has the templates for 5 problems, instead of the templates for all prob- lems. Under this setting, our MixedSP algorithm achieves 53.8%, which is better than 46.1% reported in ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>In <ref type="figure">Figure 2c</ref>, we investigate the impact of tuning γ in MixedSP on the dataset ALG-514. Recall that γ controls the fraction of the training time that the model uses solely explicit supervision. At first glance, it may appear that we should utilize the im- Figure 2: (a) Comparisons between our system to state-of-the-art systems on ALG-514. ZDC15 is the system pro- posed in ( <ref type="bibr" target="#b31">Zhou et al., 2015)</ref>, and KAZB14 is the system proposed in ( . (b) Comparisons between our system and other systems on DRAW-1K. Note that we are not able to run ZDC15 on DRAW-1K because it cannot handle some equation systems in the dataset. (c) Analysis of the impact of γ in MixedSP.</p><p>plicit supervision throughout training (set γ = 0). But setting γ to 0 hurts overall performance, sug- gesting in this setting that the algorithm uses a weak model to guide the exploration for using implicit supervision. On the other hand, by delaying ex- ploration (γ &gt; 0.5) for too long, the model could not fully utilize the implicit supervision. We ob- serve similar trend on DRAW-1K as well. We found γ = 0.5 works well across the experiments.</p><p>We also analyze the impact of the parameter K, which controls the size of the candidate set Ω in MixedSP. Specifically, for DRAW-1K, when setting K to 5 and 10, the accuracy of MixedSP is at 59.5%. On setting K to 15, the accuracy of MixedSP im- proves to 61%. We suspect that enlarging K in- creases the chance to have good structures in the candidate set that can match the correct responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose an algorithmic approach for training a word problem solver based on both explicit and implicit supervision signals. By extract- ing the question answer pairs from a Web-forum, we show that the algebra word problem solver can be improved significantly using our proposed tech- nique, surpassing the current state-of-the-art.</p><p>Recent advances in deep learning techniques demonstrate that the error rate of machine learning models can decrease dramatically when large quan- tities of labeled data are presented ( <ref type="bibr" target="#b14">Krizhevsky et al., 2012</ref>). However, labeling natural language data has been shown to be expensive, and it has become one of the major bottleneck for advancing natural language understanding techniques ( <ref type="bibr" target="#b7">Clarke et al., 2010)</ref>. We hope the proposed approach can shed light on how to leverage data on the web, and even- tually improves other semantic parsing tasks such as knowledge base question answering and mapping natural instructions to actions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>? z2 * Input Sematic Parses</head><label></label><figDesc></figDesc><table>... 

x2 

y 1 

y 2 

y 3 

y 17650 

y 4 
... 

z 1 

z 2 

z 3 

z 17650 

z 4 
... 

x1 

y 1 

y 2 

y * 

y 17650 

y 4 
... 

z 1 

z 2 

z * 

z 17650 

z 4 

Derived 
Solutions 

Input 

Sematic 
Parses 

Derived 
Solutions 

Annotated 
Response 

Figure 1: Left: Explicit supervision signals. Note that 
the solution z can be derived by the semantic parses y. 
Right: Implicit supervision signals. In this case, we only 
have the annotated response z  *  
2 . It is difficult to use z  *  

2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the statis-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : The solution accuracies of different protocols on ALG-514 and DRAW-1K.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw</note>

			<note place="foot" n="2"> The dataset has not been made public at the time of publication.</note>

			<note place="foot" n="5"> The mined solutions are often incomplete for some variables (e.g. solution y=6 but no value for x could be mined). We allow partial matches so that the model can learn from the incomplete implicit signals as well.</note>

			<note place="foot" n="6"> Prior work (Kushman et al., 2014) has used only 5 explicit supervision examples when training with solutions.</note>

			<note place="foot" n="7"> http://www.algebra.com</note>

			<note place="foot" n="8"> https://aka.ms/datadraw</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TACL</title>
		<meeting>of TACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A question-answering system for high school algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fall Joint Computer Conference</title>
		<imprint>
			<date type="published" when="1964-10-27" />
		</imprint>
	</monogr>
	<note>Proceedings of the. Part I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A constrained latent variable model for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarse-tofine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">My computer is an honor student-but how intelligent is it? Standardized tests as a measure of AI. AI Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative reranking for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javad Mohammad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? Large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena Dumas</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TACL</title>
		<meeting>of TACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A review of methods for automatic understanding of natural language mathematical problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utpal</forename><surname>Garain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="122" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Report on a general problem-solving program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Congress</title>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="page" from="256" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning with relaxed supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SymPy: Python library for symbolic mathematics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sympy Development Team</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Annotating derivations: A new evaluation strategy and dataset for algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://aka.ms/derivationpaper" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic proramming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learn to solve algebra word problems using quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
