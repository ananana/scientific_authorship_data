<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional approaches to Chinese Semantic Role Labeling (SRL) almost heavily rely on feature engineering. Even worse, the long-range dependencies in a sentence can hardly be modeled by these methods. In this paper, we introduce bidirection-al recurrent neural network (RNN) with long-short-term memory (LSTM) to capture bidirectional and long-range dependencies in a sentence with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-of-the-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages ( <ref type="bibr" target="#b5">Gildea and Jurafsky, 2002;</ref><ref type="bibr" target="#b10">Sun and Jurafsky, 2004</ref>). <ref type="figure" target="#fig_0">Figure 1</ref> shows an example in Chi- nese Proposition Bank (CPB) <ref type="bibr" target="#b15">(Xue and Palmer, 2003)</ref>, which is a Chinese corpus annotated with semantic role labels.</p><p>Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM ( <ref type="bibr" target="#b10">Sun and Jurafsky, 2004;</ref><ref type="bibr" target="#b17">Xue, 2008;</ref><ref type="bibr" target="#b3">Ding and Chang, 2008;</ref><ref type="bibr" target="#b4">Ding and Chang, 2009;</ref><ref type="bibr" target="#b12">Sun, 2010)</ref>. However, these methods suf- fer from three major problems. Firstly, their per- formances are heavily dependent on feature engi- neering, which needs domain knowledge and la- borious work of feature extraction and selection. Secondly, although sophisticated features are de- signed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific anno- tated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can alleviate this problem. However, traditional meth- ods cannot relate distinct annotation schemas and introduce heterogeneous resource with ease.</p><p>Concerning these problems, in this paper, we propose bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem of Chinese SRL. Our approach makes the following contributions:</p><p>• We formulate Chinese SRL with bidirection- al LSTM RNN model. With bidirectional RNN, the dependencies in a sentence from both directions can be captured, and with L- STM architecture, long-range dependencies can be well modeled. The test results on the bechmark dataset CPB show a significant im- provement over the state-of-the-art methods.</p><p>• Compared with previous work that relied on a huge number of handcrafted features, our model can achieve much better performance only with minimal feature engineering.</p><p>• The framework of our model makes the intro- duction of heterogeneous resource efficient and convenient, and this can further improve our experimental performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Chinese SRL with RNN</head><p>Following previous work, we regard Chinese S- RL as a task of sequence labeling, which assigns a label for each word in the sequence. To iden- tify the boundary information of semantic roles, we adopt the IOBES tagging schema for the la- bels as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For sequence labeling, it is important to capture dependencies in the se- quence, especially for the problem of SRL, where the semantic role label for a word not only relies on its local information, but also is determined by long-range dependencies from other words. The advantage of RNN is the ability to better capture the contextual information, which is beneficial to capture dependencies in SRL. Moreover, we en- rich the basic RNN model with bidirectional LST- M RNN, which can model bidirectional and long- range dependencies simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>The architecture of our approach is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Given a sentence, we first get repre- sentation for each word to be labeled. Then af- ter a nonlinear transformation, bidirectional LST- M RNN layer is designed to combine the local in- formation of a word and its contextual information from both directions. With a nonlinear layer to form more complex features, a linear output lay- er follows. For each word to be labeled, there is an output vector, whose each dimension is a score corresponding to a kind of semantic role label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Representation</head><p>Word representation captures the features locally embedded around the word. The features used in our work are: the current word, the current POS tag, the predicate, left and right words, left and right POS tags, distance to the predicate. Note that these features are all basic information about the word, hence we alleviate the heavy job of feature engineering. All these features are introduced by embeddings. After concatenation, we get the word representation feature vector.</p><p>To get more complex features, we adopt a non- linear transformation:</p><formula xml:id="formula_0">z t = f (W 1 x t ) 1 ≤ t ≤ N</formula><p>where x t is the word representation of the t-th word, W 1 ∈ R n 1 ×n 0 , n 0 is the length of word rep- resentation, f is an activation function and we use tanh in our experiments, N is the number of words to be labeled in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bidirectional LSTM RNN</head><p>Representation z t only captures the local informa- tion. Here we adopt RNN to capture contextual information. Traditional RNN has the problem of vanishing or exploding gradients, which means the long-term dependencies can hardly be modeled. LSTM is designed to mitigate this problem.</p><p>At each word position t, the LSTM RNN com- putes six internal vectors:</p><p>C, g i , g f , g o , C t and h t for the memory cell, which is a structure used in LSTM to store information.</p><p>C computes the can- didate value for the state of the memory cell:</p><formula xml:id="formula_1">C = f (W c z t + U c h t−1 + b c )</formula><p>The activations of the memory cell's input gate, forget gate and output gate are defined as:</p><formula xml:id="formula_2">g j = σ(W j z i + U j h t−1 + b j )</formula><p>where j stands for i, f or o. σ is taken sigmoid in experiments. Then we can compute C t , the mem- ory cell's new state at position t:</p><formula xml:id="formula_3">C t = g i C + g f C t−1</formula><p>where indicates elementwise vector multiplica- tion. With the new state of the memory cell, we can compute the value of output state h t :</p><formula xml:id="formula_4">h t = g o f (C t )</formula><p>h t contains the information not only from local representation z t , but also from previous output state h t−1 , hence can capture dependencies in a sentence. Because the dependencies forward and backward are both important to label seman- tic roles, we extend LSTM with bidirectional ap- proach, resulting in:</p><formula xml:id="formula_5">a t = [ − → h t T , ← − h t T ] T 1 ≤ t ≤ N</formula><p>Further, a nonlinear transformation follows:</p><formula xml:id="formula_6">v t = f (W 2 a t ) 1 ≤ t ≤ N</formula><p>where W 2 ∈ R n 3 ×n 2 , n 2 is the dimension of a t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Output Representation</head><p>For each word to be labeled, we adopt linear trans- formation to get the output vector o t :</p><formula xml:id="formula_7">o t = W 3 v t 1 ≤ t ≤ N W 3 ∈ R n 4 ×n 3 ,</formula><p>where n 4 is the number of seman- tic role labels in IOBES tagging schema. There- fore, the resulting vector o t for the t-th word is of length n 4 , and each dimension corresponds to the score of a certain semantic role label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training Criteria</head><p>Because there are dependencies among word la- bels in a sentence, isolated training approach which independently considers each word will be inappropriate. Therefore, we adopt sentence tag approach, in which we encourage the correct path of tags, while discouraging all other valid paths. Given all our training examples:</p><formula xml:id="formula_8">T = (x (i) , y (i) )</formula><p>where x (i) denotes the i-th training sentence, y (i) is the corresponding N i (the number of words to be labeled) dimension vector, which indicates the correct path of tags, and y (i) t = k means the t-th word has the k-th semantic role label. The score of x (i) along the path y (i) is defined as follows:</p><formula xml:id="formula_9">s(x (i) , y (i) , θ) = N i t=1 o ty (i) t</formula><note type="other">where θ is an ensemble of all the parameters in the network.</note><p>The log likelihood with a single sample is then:</p><formula xml:id="formula_10">logp(y (i) |x (i) , θ) = log exp s(x (i) , y (i) , θ) y exp s(x (i) , y, θ) = s(x (i) , y (i) , θ) − log y exp s(x (i) , y, θ)</formula><p>where y ranges from all the valid paths of tags.</p><p>The full log likelihood of the whole training cor- pus is as follows:</p><formula xml:id="formula_11">J(θ) = T i=1 logp(y (i) |x (i) , θ)</formula><p>To compute the network parameter θ, we maxi- mize the log likelihood J(θ) using stochastic gra- dient ascent in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Introducing Heterogeneous Resource</head><p>A single annotated corpus with semantic role la- bels is often limited in its scalability. Heteroge- neous resource in our work is defined as another dataset annotated with semantic roles, which also provides predicate-argument structure annotation, but uses very different semantic role labels and an- notation schema. However, in spite of these differ- ences, the latent semantic meaning may be highly correlated. Therefore, the introduction of hetero- geneous data can alleviate the problem of scalabil- ity with a single annotated corpus.</p><p>Traditional approaches hardly concern the ex- istence of heterogeneous resource and are diffi- cult to relate different annotation schemas, but in the framework of our model, heterogeneous data can be introduced in a relatively convenient way. Specifically, we learn a bidirectional LSTM RN- N model based on heterogeneous data, then with the fine-tuned word embeddings we initialize the model on our experimental dataset. The princi- ple behind is that the words almost convey the same semantic meaning albeit in distinct annota- tion schemas. The introduction of heterogenous resource in this way is efficient and can lead to performance improvement on our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct experiments to compare our model with previous landmark methods on the bench- mark dataset CPB for Chinese SRL. The result</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark</head><p>Choice Word embedding dimension n word = 50 POS tag dimension n pos = 20 Distance dimension n dis = 20 Nonlinear layer n 1 = 200 RNN layer n h = 100 Nonlinear layer n 3 = 100 Learning rate α = 10 −3 <ref type="table">Table 1</ref>: Hyper parameters of our model. reveals that even with our basic model, which does not resort to other resources, our approach can significantly outperform all of the competitors. Moreover, we enrich our work with introducing heterogenous resource to make a further improve- ment on performance. And the result also shows the influence of heterogeneous resource is more evident than the standard method of pre-training for word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setting</head><p>To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. 1 We follow the same data setting as previous work <ref type="bibr" target="#b17">(Xue, 2008;</ref><ref type="bibr" target="#b11">Sun et al., 2009)</ref>, which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid. The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid. We use another annotated corpus 2 with distinct se- mantic role labels and annotation schema, which is designed by ourselves for other projects, as hetero- geneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles con- cerned are like "agent" and "patient", resulting in 21 kinds of types, which are all distinct from the semantic roles defined in CPB. We use the devel- opment set of CPB for model selection, and the hyper parameter setting of our model is reported in <ref type="table">Table 1</ref>. <ref type="table">Table 2</ref> summarizes our SRL performance com- pared to previous landmark results. The work of <ref type="bibr" target="#b2">Collobert and Weston (2008)</ref> was conducted on English SRL, we implement their approach on CP- <ref type="formula">(2008)</ref> 71.90 <ref type="bibr" target="#b2">Collobert and Weston (2008)</ref> 74.05 <ref type="bibr" target="#b11">Sun et al. (2009)</ref> 74.12 <ref type="bibr" target="#b18">Yang and Zong (2014)</ref> 75.31 Ours (Random Initialization) 77.09 + Standard Pre-training 77.21 + Heterogenous Resource 77.59 <ref type="table">Table 2</ref>: Results comparison on CPB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chinese SRL Performance</head><formula xml:id="formula_12">Method F1(%) Xue</formula><p>B for comparison. As indicated by this table, our approach significantly outperforms previous state- of-the-art methods even with all parameters ran- domly initialized, that is without introducing other resources. This result can prove the ability of our model to capture useful dependencies for Chinese SRL with minimal feature engineering. Further, we conduct experiments with the intro- duction of heterogenous resource. Previous work found that the performance can be improved by pre-training the word embeddings on large unla- beled data and using the obtained embeddings to make initialization. With the result in <ref type="table">Table 2</ref>, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec 3 on Chinese Gigaword Corpus for word pre-training). However, as shown in <ref type="table">Table 2</ref>, com- pared to standard pre-training, the influence of het- erogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improve- ment. The information provided by standard pre- training with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Semantic Role Labeling (SRL) was first defined by <ref type="bibr" target="#b5">Gildea and Jurafsky (2002)</ref>, who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Ju- rafsky (2004) did the preliminary work on Chi- nese SRL without any large semantically annotat- ed corpus and produced promising results. Af- ter CPB <ref type="bibr" target="#b15">(Xue and Palmer, 2003</ref>) was built, X- ue and <ref type="bibr" target="#b16">Palmer (2005)</ref> and Xue (2008) produced more complete and systematic research on Chi- nese SRL. <ref type="bibr" target="#b4">Ding and Chang (2009)</ref> established a word based Chinese SRL system, which is quite different from the previous parsing based ones. <ref type="bibr" target="#b11">Sun et al. (2009)</ref> extended the work of <ref type="bibr" target="#b1">Chen et al. (2006)</ref>, performed Chinese SRL with shallow parsing, which took partial parses as inputs. <ref type="bibr" target="#b18">Yang and Zong (2014)</ref> proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, <ref type="bibr" target="#b2">Collobert and Weston (2008)</ref> proposed a convolutional neural network for SRL. Their ap- proach achieved competitive performance on En- glish SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weak- ly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well alle- viated.</p><p>Our model is based on recurrent neural network (RNN), which uses iterative function loops to store contextual information. To remedy the problem of vanishing and exploding gradients when train- ing the standard RNN, Hochreiter and Schmidhu- ber (1997) proposed long-short-term memory (L- STM), which has been shown capable of storing and accessing information over very long time s- pans. Bidirectional RNN <ref type="bibr" target="#b9">(Schuster and Paliwal, 1997</ref>) and bidirectional LSTM RNN ( <ref type="bibr" target="#b6">Graves et al., 2005</ref>) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-the- art results in many NLP problems such as lan- guage modeling <ref type="bibr" target="#b8">(Mikolov et al., 2010)</ref> and ma- chine translation <ref type="bibr" target="#b14">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). <ref type="bibr" target="#b13">Sundermeyer et al. (2014)</ref> also used bidirectional LSTM RNN model to im- prove strong baselines when modeling translation. More recently, <ref type="bibr" target="#b19">Zhou and Xu (2015)</ref> proposed L- STM RNN approach for English Semantic Role Labeling, which shared similar idea with our mod- el. However, the features used and the network ar- chitecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RN- N model. In our approach, the bidirectional and long-range dependencies in a sentence, which are important for Chinese SRL, can be well modeled. And with the framework of deep neural network, the heavy job of feature engineering is much alle- viated. Moreover, our model makes the introduc- tion of heterogenous data, which can alleviate the problem of scalability with a single annotated cor- pus, more convenient. Experiments show that our approach achieves much better results than previ- ous work, and the introduction of heterogenous re- source can make further improvement on perfor- mance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sentence with semantic roles labeled from CPB.</figDesc><graphic url="image-1.png" coords="1,307.56,204.54,217.69,50.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The model architecture.</figDesc><graphic url="image-2.png" coords="2,72.00,62.80,219.97,265.74" type="bitmap" /></figure>

			<note place="foot" n="1"> https://catalog.ldc.upenn.edu/LDC2005T23 2 This Chinese dataset is available on request.</note>

			<note place="foot" n="3"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arX- iv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical study of chinese chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving chinese semantic role classification with hierarchical feature selection strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word based chinese semantic role labeling with semantic chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Processing Of Languages</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">02n03</biblScope>
			<biblScope unit="page" from="133" to="154" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="245" to="288" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks: Formal Models and Their Applications-ICANN 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shallow semantic parsing of chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2004</title>
		<meeting>NAACL 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chinese semantic role labeling with shallow parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1475" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving chinese semantic role labeling with rich syntactic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annotating the propositions in the penn chinese treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second SIGHAN workshop on Chinese language processing</title>
		<meeting>the second SIGHAN workshop on Chinese language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic semantic role labeling for chinese verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1160" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labeling chinese predicates with semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="255" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multipredicate semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
