<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntactic Parsing of Web Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Yanghua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Syntactic Parsing of Web Queries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1787" to="1796"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Syntactic parsing of web queries is important for query understanding. However, web queries usually do not observe the grammar of a written language, and no labeled syntactic trees for web queries are available. In this paper , we focus on a query&apos;s clicked sentence, i.e., a well-formed sentence that i) contains all the tokens of the query, and ii) appears in the query&apos;s top clicked web pages. We argue such sentences are semantically consistent with the query. We introduce algorithms to derive a query&apos;s syntactic structure from the dependency trees of its clicked sentences. This gives us a web query treebank without manual labeling. We then train a dependency parser on the treebank. Our model achieves much better UAS (0.86) and LAS (0.80) scores than state-of-the-art parsers on web queries.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Syntactic analysis is important in understanding a sentence's grammatical constituents, parts of speech, syntactic relations, and semantics. In this paper, we are concerned with the syntactic structure of a short text. The challenge is that short texts, for example, web queries, do not observe grammars of written languages (e.g., users often overlook capital- ization, function words, and word order when creat- * Correspondence author. The syntactic structure of query cover iphone 6 plus tells us that the head token is cover, in- dicating its intent is to shop for the cover of an iphone, instead of iphones. With this knowledge, search engines show ads of iphone covers instead of iphones. For distance earth moon, the head is distance, indicating its intent is to find the dis- tance between the earth and the moon. For faucet adapter female, the intent is to find a female faucet adapter. In summary, correctly identifying the head of a query helps identify its intent, and correctly identifying the modifiers helps rewrite the query (e.g., dropping non-essential modifiers). Syntactic parsing of web queries is challenging for at least two reasons. First, grammatical signals from function words and word order are not avail- able. Query distance earth moon is missing function words between (preposition), and (coordi- nator), and the (determiner) in conveying the intent distance between the earth and the moon. Also, it is likely that queries {distance earth moon, earth moon distance, earth distance moon, · · · } have the same intent, which means they should have the same syntactic structure. Second, there is no labeled dependency trees (treebank) for web queries, nor is there a standard for construct- ing such dependency trees. It will take a tremendous amount of time and effort to come up with such a standard and a treebank for web queries.</p><p>In this paper, we propose an end-to-end solution from treebank construction to syntactic parsing for web queries. Our model achieves a UAS of 0.830 and an LAS of 0.747 on web queries, which is dramatic improvement over state-of-the-art parsers trained from standard treebanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>The biggest challenge of syntactic analysis of web queries is that they do not contain sufficient gram- matical signals required for parsing. Indeed, web queries can be very ambiguious. For example, kids toys may mean either toys for kids or kids with toys, for which the dependency relation- ships between toys and kids are totally opposite. In view of this, why is syntactic parsing of web queries a legitimate problem? We have shown some example syntactic structures for 3 queries in Section 1. How do we know they are the correct syntactic structures for the queries? We answer these ques- tions here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Derive syntax from semantics</head><p>In many cases, humans can easily determine the syn- tax of a web query because its intent is easy to under- stand. For example, for toys kids, we are pretty sure as a web query, its intent is to look for toys for kids, instead of the other way around. Thus, toys should be the head of the query, and kids should be its modifier. In other words, when the semantics of a query is understood, we can often recover its syntax.</p><p>We may then manually annotate web queries. Specifically, given a query, a human annotator forms a sentence that is consistent with the meaning he comes up for the query. Then, from the sentence's syntactic structure (which is well understood and can be derived by a parser), the annotator derives the syntactic structure of the query. For example, for query thai food houston, the annotator may formulate the following sentence: The above approach has two issues. First, food and houston are not directly connected in the de- pendency tree of the sentence. We connected them in the query, but in general, it is not trivial to in- fer synatx of the query from sentences in a consis- tent way. There is no linguistic standard for doing this. Second, annotation is very costly. A treebank project takes years to accomplish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantics of a web query</head><p>To avoid human annotation, we derive syntactic un- derstanding of the query from semantic understand- ing of the query. Our goal is to decide for any two tokens x, y ∈ q, whether there is a dependency arc between x and y, and if yes, what the dependency is.</p><p>Context-free signals. One approach to determine the dependency between x and y is to directly model P (e|x, y), where e denotes the dependency (x → y or x ← y). It is context-free because we do not condition on the query where x and y appear in.</p><p>To acquire P (e|x, y), we may consider annotated corpora such as Google's syntactic ngram <ref type="bibr" target="#b7">(Goldberg and Orwant, 2013</ref>). For any x and y, we count the number of times that x is a dependent of y in the cor- pus. One disadvantage of this approach is that web queries and normal text differ significantly in distri- bution. Another approach ( <ref type="bibr" target="#b15">Wang et al., 2014</ref>) is to use search log to estimate P (e|x, y), where x and y are nouns. Specifically, we find queries of pat- tern x PREP y, where PREP is a preposition {of, in, for, at, on, with, · · · }. We have P (x → y|x, y) = nx,y nx,y+ny,x where n x,y denotes the number of times pattern x PREP y appears in the search log. The dis- advantage is that the simple pattern only gives de- pendency between two nouns.</p><p>Context-sensitive signals. The context-free ap- proach has two major weaknesses: (1) It is risky to decide the dependency between two tokens without considering the context. (2) Context-free signals do not reveal the type of dependency, that is, it does not reveal the linguistic relationship between the head and the modifier.</p><p>To take context into consideration, which means estimating P (e|x, y, q) for any two tokens x, y ∈ q, we are looking at the problem of building a parser for web queries. This requires a training dataset (a treebank). In this work, we propose to automati- cally create such a treebank. The feasibility is cen- tered on the following assumption: The intent of q is contained in or consistent with the semantics of its clicked sentences. We call sentence s a clicked sentence of q if i) s appears in a top clicked page for q, and ii) s contains all tokens in q. For instance, as- sume sentence s = "... my favorite Thai food in Houston ..." appears in one of the most frequently clicked pages for query q = thai food houston, then s is a clicked sentence of q. It follows from the above assumption that the de- pendency between any two tokens in q are likely to be the same as the dependency between their corre- sponding tokens in s. This allows to create a tree- bank if we can project the dependency from sen- tences to queries. However, since x and y may not be directly connected by a dependency edge in s, we need a method to derive the dependency between x, y ∈ q from the (indirect) dependency between x, y ∈ s. We propose such a method in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Treebank for Web Queries</head><p>We create a web query treebank by projecting de- pendency from clicked sentences to queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inferring a dependency tree</head><p>A query q may have multiple clicked sentences. We describe here how we project dependency to q from such a sentence s. We describe how we aggregate dependencies from multiple sentences in Sec 3.2.</p><p>Under our assumption, each token x ∈ q must appear in sentence s. But x may appear multiple times in s (especially when x is a function word). As an example, for query apple watch stand, we may get the following sentence: We use the following heuristics to derive a depen- dency tree for query q from sentence s.</p><formula xml:id="formula_0">Its</formula><p>1. Let T s denote all the subtrees of the depen- dency tree of s.</p><p>2. Find the minimum subtree t ∈ T s such that each x ∈ q has one and only one match x ∈ t.</p><p>3. Derive dependency tree t q,s for q from t as fol- lows. For any two tokens x and y in q:</p><p>(a) if there is an edge from x to y in t, we create a same edge from x to y in t q,s . (b) if there is a path 1 from x to y in t, we create an edge from x to y in t q,s , and label it temporarily as dep.</p><p>We note the following. First, we argue that if the dependency tree of s has a subtree that contains each token in q once and only once, then it is very likely that the subtree expresses the same semantics as the query. On the other hand, if we cannot find such a subtree, it is an indication that we cannot derive rea- sonable dependency information from the sentence. Second, it's possible x and y are not connected directly in s but through one or more other tokens. Thus, we do not know the label of the derived edge. We will decide on the label in Sec 3.3.</p><p>Third, we want to know whether it is meaningful to connect x and y in q while x and y are not di- rectly connected in s. We evaluated a few hundreds of query-sentence pairs. Among the cases where de- pendency trees for queries can be derived success- fully, we found that x and y are connected in 5 possible ways <ref type="table">(Table 1)</ref>. We describe them in <ref type="bibr">de</ref> For these two cases, we need to introduce a de- rived edge for the query, which will be resolved later to a specific dependency label.</p><p>Connected via modifiers. Many web queries are noun compounds. Their clicked sentences may have more modifiers. Depending on the bracketing, we may or may not have direct dependencies.</p><p>For offshore work and its clicked sentence below, missing drilling in the query does not cause any problem: offshore and work are still directly connected in the dependency tree. In this case, we create a dependency between crude and oil in the query and give it a tempo- rary label dep. We will resolve it to a specific label later. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inferring a unique dependency tree</head><p>A query corresponds to multiple clicked sentences. From each sentence, we derive a dependency tree. These dependency trees may not be the same, be- cause i) dependency parsing for sentences is not per- fect; ii) queries are ambiguous; or iii) some queries do not have well-formed clicked sentences. To choose a unique dependency tree for a query q, we define a scoring function f to measure the qual- ity of a dependency tree t q derived from q's clicked sentence s:</p><formula xml:id="formula_1">f (t q , s) = (x→y)∈tq −αdist(x, y) + log count(x → y) count(x ← y)<label>(1)</label></formula><p>where (x → y) is an edge in the tree t q , count(x → y) is the occurrence count of the edge x → y in the entire query dataset, dist(x, y) is the distance of words x and y on the original sentence parsing tree, and α is a parameter to adjust the importance between the two measures (its value is empirically determined  In the first sentence, deep and learning are indirectly connected through fry so the total dis- tance measure is 2. In the second query, the distance is 1. Therefore, query aligned with the second sen- tence is better than the first sentence.</p><p>The second term of the scoring function measures the global consistency among head modifier direc- tions. For a word pair (x, y), if in the dataset, the number of edges x → y dominates the number of edges x ← y, then the latter is likely to be incorrect.</p><p>One important thing to note is word order. Word order may influence the head-modifier relations be- tween two words. For example, child of and of child should definitely have different head- modifier relations. Therefore, we treat two words of different order as two different word pairs. <ref type="table" target="#tab_7">Table 2</ref> shows some examples of conflicting de- pendency edges and their corresponding occurrence count in queries and sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Label refinement</head><p>In Section 3.1, some dependencies are derived with a placeholder label dep. Before we use the data to train a parser, we must resolve dep to a true label, otherwise they introduce inconsistency in the training data. For example, consider a sim- ple query crude price. From clicked sen- tences that contain crude oil price, we de-rive crude dep ← − −price, but from those that contain crude price, we derive crude amod ← −− −price. To resolve dep, we resort to majority vote first.</p><p>For any x dep ← − − y, we count the occurrence of x label ←−− y in the training data for each concrete label. If the frequency of a certain label is dominating by a pre- determined threshold (10 times more frequent than any other label), then we resolve dep to that label.</p><p>With our training data, the above process is able to resolve about 90% dependencies. We can simply discard queries that contain unresolvable dependen- cies. However, such queries still contain useful in- formation, for example, the direction of this edge, and the directions and labels of all the other edges. We develop a bootstrapping method to preserve such useful information. First, we train a parser on data without dep labels. This skips about 10% queries in our experiments. Second, we use the parser to pre- dict the unknown label. If the prediction is consis- tent with the annotation except for the dep label, we use the predicted label. Third, we add the resolved queries into the training data and train a final parser. Experiments show the bootstrapping approach im- proves the quality of the parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dependency Parsing</head><p>We train a parser from the web query treebank data. We also try to incorporate context-free head- modifier signals into parsing. To make it easier to incorporate such signals, we adopt a neural network approach to train our POS tagger and parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural network POS tagger and parser</head><p>We first train a neural network POS tagger for web queries. For each word in the sentence, we construct features out of a fixed context window centered at that word. The features include the word itself, case (whether the first letter, any letter, or every letter in the word, is in uppercase), prefix, and suffix (we rec- ognize a pre-defined set of prefixes and suffixes, for the rest we use a special token "UNK"). For the word feature, we use pre-trained word2vec embeddings. For word case and prefix/suffix, we use random ini- tialization for the embeddings. The accuracy of the trained POS tagger is similar to that of ( <ref type="bibr" target="#b6">Ganchev et al., 2012)</ref>, which outperforms POS taggers trained on PTB data.</p><p>Buffer features b 1 .wt, b 2 .wt, b <ref type="bibr">3</ref> .wt Stack features s 1 .wt, s 2 .wt, s 3 .wt Tree features lc 1 (s 1 ).wtl, lc 2 (s 1 ).wtl, rc 1 (s 1 ).wtl, rc 2 (s 1 ).wtl lc 1 <ref type="figure">(lc 1 (s 1 )</ref>).wtl, rc 1 (rc 1 (s 1 )).wtl lc 1 (s 2 ).wtl, lc 2 (s 2 ).wtl, rc 1 (s 2 ).wtl, rc 2 (s 2 ).wtl lc 1 <ref type="figure">(lc 1 (s 2 )</ref>).wtl, rc 1 (rc 1 (s 2 )).wtl <ref type="table">Table 3</ref>: The feature templates. s i (i = 1, 2, ...) de- note the i th top element of the stack, b i (i = 1, 2, ...) denote the i th element on the buffer, lc k (s i ) and rc k (s i ) denote the kth leftmost and rightmost chil- dren of s i , w denotes words, t denotes POS tag, l denotes label.</p><p>We use the arc standard transition based depen- dency parsing system <ref type="bibr" target="#b11">(Nivre, 2004</ref>). The architec- ture of the neural network dependency parser is sim- ilar to that of <ref type="bibr" target="#b2">(Chen and Manning, 2014</ref>) designed for parsing sentences. The features used in parsing are shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context free features</head><p>In Section 2.2, we discussed context-free signals P (e|x, y) and context-sensitive signals P (e|x, y, q). Previous work ( <ref type="bibr" target="#b15">Wang et al., 2014</ref>) uses context-free signals for syntactic analysis of a query. Our ap- proach outperforms the context-free approach.</p><p>An interesting question is, will context-free sig- nals further improve our approach? The rationale is that although context-sensitive signals P (e|x, y, q) are more accurate in predicting the dependency be- tween x and y, such signals are also very sparse. Do context-free signals P (e|x, y) provide backoff infor- mation in parsing?</p><p>It is not straightforward to include P (e|x, y) in the neural network model. The head-modifier rela- tions P (e|x, y) may exist between any pair of tokens in the input query. Essentially, it is a pairwise graph- ical model and it is difficult to directly incorporate the signals in transition based dependency parsing.</p><p>We treat context-free signals as prior knowledge. We train head-modifier embeddings for each to- ken, and use such embeddings as pre-trained embed- dings. Specifically, we use an approach similar to training word2vec embeddings but focusing on head modifier relationships instead of co-occurrence rela- tionships. More specifically, we train an one hidden layer neural network classifier to determine whether two words have head-modifier relations. The input of the neural network is the concatenation of the em- beddings of two words. The output is whether the two words form a proper head-modifier relationship. We obtain a large set of head-modifier data from text corpus by mining "h PREP m" pattern in search log where h and m are nouns. Then, for each known head modifier pair h and m, we use (h, m) as pos- itive example and (m, h) as negative example. For each word, we also choose a few random words as negative examples. During the training process, the gradients are back propagated to the word embed- dings. After training, the embeddings should con- tain sufficient information to recover head modifier relations between any word pairs. But we did not observe improvement over the ex- isting neural network that are trained on context sen- sitive treebank data alone. The head-modifier em- beddings has about 3% advantage in UAS over ran- domized embeddings. However, using pretrained word2vec embeddings, we also achieve 3% advan- tage. Thus, it seems that context-sensitive signals plus the generalizing power of embeddings contain all the context-free signals already.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we start with some case studies. Then we describe data and compare models.</p><p>In experiments, we use the standard UAS (unla- beled attachment score) and LAS (labeled attach- ment score) score for measuring the quality of de- pendency parsing. They are calculated as:</p><formula xml:id="formula_2">U AS = # correct arc directions # total arcs<label>(2)</label></formula><p>LAS = # correct arc directions and labels # total arcs (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study</head><p>We compare dependency trees produced by our QueryParser and Stanford Parser (Chen and Man- ning, 2014) for some web queries (Stanford Parser is trained from the standard PTB treebank). <ref type="table">Table 4</ref> shows that Stanford Parser heavily relies on gram- mar signals such as function words and word or- der, while QueryParser relies more on the seman- tics of the query. For instance, in the 1st exam- ple, QueryParser identifies toys as the head, re- gardless of the word order, while Stanford parser always assumes the last token as the head. In the 2nd example, the semantics of the query is a school (vanguard school) at a certain location (lake wales). QueryParser captures the semantics and correctly identifies school as the head (root) of the query, while Stanford parser treats the entire query as a single noun compound (likely inferred from the POS tags).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Clicked Sentences</head><p>For training data, we use one-month Bing query log (between <ref type="bibr">July 25, 2015 and</ref><ref type="bibr">August 24, 2015)</ref>. From the log, we obtain web query q and its top clicked URLs {url 1 , url 2 , ..., url m }. From the urls, we re- trieve the clicked HTML document, and find sen- tences {s 1 , s 2 , ..., s n } that contain all words (regard- less to their order of occurrence) in q. Then we ex- tract query-sentence tuples (q, s, count) to serve as our training data to generate a web query treebank. The size (# of distinct query-sentence pairs) of the raw clicked sentences is 390,225,806.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Web Query Treebank</head><p>We evaluate the 3 steps of treebank generation. Af- ter each step, we sample 100 queries from the result and manually compute their UAS and LAS scores. We also count the number of total query instances in each step. The results are shown in  <ref type="table">Table 4</ref>: Case study of parsers.</p><p>3.2, each group produces one or zero depen- dency trees. The number of instances in <ref type="table" target="#tab_8">Table  5</ref> corresponds to the number of different query groups. The overall success rate is high. This is expected as the filtering process uses major- ity voting, and we already have high precision parsing trees after the first step.</p><p>• Label refinement: Dependency labels are re- fined using the methodology in Section 3.3. It shows that with majority voting and bootstrap- ing, we are able to keep all the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parser Performance</head><p>We compare QueryParser against three state-of-the- art parsers: Stanford parser, which is a transition based dependency parser based on neural network, <ref type="bibr">MSTParser (McDonald et al., 2005</ref>), which is a graph based dependency parser based on minimum spanning tree algorithms, and <ref type="bibr">LSTMParser (Dyer et al., 2015)</ref>, which is a transition based dependency parser based on stack long short-term memory cells. Here, QueryParser is trained from our web query treebank, while Stanford Parser and MSTParser are trained from standard PTB treebanks. For comparison, we manually labeled 1,000 web queries to serve as a ground truth dataset 2 . We pro- duce POS tags for the queries using our neural net- work POS tagger. To specifically measure the ability of QueryParser in parsing queries with no explicit syntax structure, we split the entire dataset All into two parts: NoFunc and Func, which correspond to queries without any function word, and queries with at least one function word. The number of queries</p><p>Step <ref type="table" target="#tab_7">Total Instances Produced Instances Success Rate UAS LAS  Inferring a dependency tree 3986300  1229860  31%  0.906 0.851  Inferring a unique tree  716261  680857  95%  0.910 0.851  Label refinement  680857  680857  100%</ref> 0.917 0.855  <ref type="table">Table 6</ref>: Parsing performance on web queries of the two datasets are 900 and 100, respectively. <ref type="table">Table 6</ref> shows the results. We use 3 versions of QueryParser. The first two use random word embedding for initialization, and the first one does not use label refinement. From the results, it can be concluded that QueryParser consistently outper- formed competitors on query parsing task. Pre- trained word2vec embeddings improve performance by 3-5 percent, and the postprocess of label refine- ment also improves the performance by 1-2 percent. <ref type="table">Table 6</ref> also shows that conventional depencency parsers trained on sentence dataset relies much more on the syntactic signals in the input. While Stanford parser and MSTParser have similar performance to our parser on Func dataset, the performance drops significantly on All and NoFunc dataset, when the majority of input has no function words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Some recent work ( <ref type="bibr" target="#b6">Ganchev et al., 2012;</ref><ref type="bibr" target="#b0">Barr et al., 2008</ref>) investigated the problem of syntactic analysis for web queries. However, current study is mostly at postag rather than dependency tree level. <ref type="bibr" target="#b0">Barr et al. (2008)</ref> showed that applying taggers trained on traditional corpora on web queries leads to poor re- sults. <ref type="bibr" target="#b6">Ganchev et al. (2012)</ref> propose a simple, ef- ficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. But they do not reveal syntactic structures of web queries.</p><p>More work has focused on resolving simple re- lations or structures in queries or short texts, par- ticularly entity-concept relations <ref type="bibr" target="#b13">(Shen et al., 2006</ref>; <ref type="bibr" target="#b8">Hua et al., 2015)</ref>, entity-attribute relations <ref type="bibr" target="#b12">(Pasca and Van Durme, 2007;</ref><ref type="bibr" target="#b9">Lee et al., 2013)</ref>, head-modifier relations ( <ref type="bibr" target="#b1">Bendersky et al., 2010;</ref><ref type="bibr" target="#b15">Wang et al., 2014</ref>). Such relations are impor- tant but not enough. The general dependency rela- tions we focus on is an important addition to query understanding.</p><p>On the other hand, there is extensive work on syn- tactic analysis of well-formed sentences <ref type="bibr" target="#b4">(De Marneffe et al., 2006</ref>). Recently, a lot of work <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b14">Vinyals et al., 2015;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014;</ref><ref type="bibr" target="#b5">Dyer et al., 2015</ref>) started using neural network for this purpose. In this work, we use similar neural network architecture for web queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Syntactic analysis of web queries is extremely im- portant as it reveals actional signals to many down- stream applications, including search ranking, ads matching, etc. In this work, we first acquire well- formed sentences that contain the semantics of the query, and then infer the syntax of the query from the sentences. This essentially creates a treebank for web queries. We then train a neural network depen- dency parser from the treebank. Our experiments show that we achieve significant improvement over traditional parsers on web queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>). The first term of the scoring function measures the compactness of the query tree. Consider two clicked</figDesc><table>Correct 
Wrong 
Query 
Sentence 
side ← effects side → effects 1110:1 11257:17 
benefits → of 
benefits ← of 
144:63 5228:0 
Full ← Movie Full → Movie 128:5 
1585:27 
coconut ← oil coconut → oil 91:10 
1507:46 
credit ← card credit → card 96:2 
4394:60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of globally inconsistent head 
modifier relations 

sentences for query deep learning: 

... learning how to deep fry chicken ... 
... 
JJ 
WRB NN IN NN IN ... 

acl 

advmod 
mark 
advmod 
dobj 

... enjoy deep learning ... 
... VBP JJ 
NN 
... 

dobj 
amod 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>QueryParser 

Stanford parser 

toys kids 
NNS NNS 

nn 

kids toys 
NNS NNS 

nn 

toys kids 
NNS NNS 

nn 

kids toys 
NNS NNS 

nn 

vanguard school lake wales 
NN 
NN NN NNS 

nn 
nn 

nn 

vanguard school lake wales 
NN 
NN NN NNS 

nn 

nn 
nn 

pretty little liars season 4 episode 6 
RB JJ NNS NN CD NN CD 

advmod 
nn 

nn 

num 

nn 

num 

pretty little liars season 4 episode 6 
RB JJ NNS NN CD NN CD 

advmod 

nn 
nn 

num 
nn 
num 

interview questions contract specialist 
NN 
NNS 
NN 
NN 

nn 
nn 

nn 

contract specialist interview question 
NN 
NN 
NN 
NN 

nn 
nn 
nn 

interview questions contract specialist 
NN 
NNS 
NN 
NN 

nn 

nn 

nn 

contract specialist interview question 
NN 
NN 
NN 
NN 

nn 

nn 

nn 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 5 : Training dataset generation statistics</head><label>5</label><figDesc></figDesc><table>System 
All (n=1000) NoFunc (n=900) Func (n=100) 
UAS LAS UAS LAS 
UAS LAS 
Stanford 
0.694 0.602 0.670 0.568 
0.834 0.799 
MSTParser 
0.699 0.616 0.683 0.691 
0.799 0.766 
LSTMParser 
0.700 0.608 0.679 0.578 
0.827 0.790 
QueryParser + label refinement 
0.829 0.769 0.824 0.761 
0.858 0.818 
QueryParser + word2vec 
0.843 0.788 0.843 0.784 
0.838 0.812 
QueryParser + label refinement + word2vec 0.862 0.804 0.858 0.795 
0.883 0.854 

</table></figure>

			<note place="foot" n="1"> A path consists of edges of the same direction.</note>

			<note place="foot">• Inferring a dependency tree: For each (query, sentence) pair, we project dependency from the sentence to the query. The number of instances shown in Table 5 are the input number of (query, sentence) pairs. It shows that we obtain dependency trees for only 31% of the queries, while the rest do not satisfy our filtering criterion. This however is not a concern. By sacrificing recall in this process, we ensure high precision. Given that query log is large, precision is more important. • Inferring a unique dependency tree: In this step, we group (query, sentence) pairs by unique queries. Using the method in Section</note>

			<note place="foot" n="2"> https://github.com/wishstudio/queryparser</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The linguistic structure of english web-search queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moira</forename><surname>Regelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning concept importance using a weighted dependence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM international conference on Web search and data mining</title>
		<meeting>the third ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transitionbased dependeny parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using search-logs to improve query tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL &apos;12</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics, ACL &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="238" to="242" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dataset of syntactic-ngrams over time from a very large corpus of english books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Orwant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (* SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="241" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Short text understanding through lexical-semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attribute extraction and scoring: A probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What you seek is what you get: Extraction of class attributes from query logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2832" to="2837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building bridges for web query classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Head, modifier, and constraint detection in short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Engineering (ICDE), 2014 IEEE 30th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Query understanding through knowledge-based conceptualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
