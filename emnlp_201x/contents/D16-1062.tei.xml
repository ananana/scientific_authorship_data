<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building an Evaluation Scale using Item Response Theory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">P</forename><surname>Lalor</surname></persName>
							<email>lalor@cs.umass.edu, hao.wu.5@bc.edu, hong.yu@umassmed.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Boston College</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Bedford VAMC and CHOIR</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building an Evaluation Scale using Item Response Theory</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="648" to="657"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (ac-curacy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items-their difficulty and discriminating power-and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in artificial intelligence have made it pos- sible to compare computer performance directly with human intelligence <ref type="bibr" target="#b7">(Campbell et al., 2002;</ref><ref type="bibr" target="#b12">Ferrucci et al., 2010;</ref><ref type="bibr" target="#b23">Silver et al., 2016</ref>). In most cases, a common approach to evaluating the performance <ref type="bibr">1</ref> Data and code will be made available for download at https://people.cs.umass.edu/lalor/irt.html of a new system is to compare it against an unseen gold-standard test dataset (GS items). Accuracy, re- call, precision and F1 scores are commonly used to evaluate NLP applications. These metrics assume that GS items have equal weight for evaluating per- formance. However, individual items are different: some may be so hard that most/all NLP systems an- swer incorrectly; others may be so easy that every NLP system answers correctly. Neither item type provides meaningful information about the perfor- mance of an NLP system. Items that are answered incorrectly by some systems and correctly by oth- ers are useful for differentiating systems according to their individual characteristics.</p><p>In this paper we introduce Item Response The- ory (IRT) from psychometrics and demonstrate its application to evaluating NLP systems. IRT is a the- ory of evaluation for characterizing test items and estimating human ability from their performance on such tests. IRT assumes that individual test ques- tions (referred to as "items" in IRT) have unique characteristics such as difficulty and discriminating power. These characteristics can be identified by fit- ting a joint model of human ability and item charac- teristics to human response patterns to the test items. Items that do not fit the model are removed and the remaining items can be considered a scale to eval- uate performance. IRT assumes that the probabil- ity of a correct answer is associated with both item characteristics and individual ability, and therefore a collection of items of varying characteristics can determine an individual's overall ability.</p><p>Our aim is to build an intelligent evaluation metric to measure performance for NLP tasks. With IRT we can identify an appropriate set of items to measure ability in relation to the overall human population as scored by an IRT model. This process serves two purposes: (i) to identify individual items appropri- ate for a test set that measures ability on a particular task, and (ii) to use the resulting set of items as an evaluation set in its own right, to measure the ability of future subjects (or NLP models) for the same task. These evaluation sets can measure the ability of an NLP system with a small number of items, leaving a larger percentage of a dataset for training.</p><p>Our contributions are as follows: First, we in- troduce IRT and describe its benefits and method- ology. Second, we apply IRT to Recognizing Tex- tual Entailment (RTE) and show that evaluation sets consisting of a small number of sampled items can provide meaningful information about the RTE task. Our IRT analyses show that different items ex- hibit varying degrees of difficulty and discrimina- tion power and that high accuracy does not always translate to high scores in relation to human perfor- mance. By incorporating IRT, we can learn more about dataset items and move past treating each test case as equal. Using IRT as an evaluation metric allows us to compare NLP systems directly to the performance of humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Item Response Theory</head><p>IRT is one of the most widely used methodologies in psychometrics for scale construction and eval- uation. It is typically used to analyze human re- sponses (graded as right or wrong) to a set of ques- tions (called "items"). With IRT individual ability and item characteristics are jointly modeled to pre- dict performance ( <ref type="bibr" target="#b1">Baker and Kim, 2004</ref>). This sta- tistical model makes the following assumptions: (a) Individuals differ from each other on an unobserved latent trait dimension (called "ability" or "factor"); (b) The probability of correctly answering an item is a function of the person's ability. This function is called the item characteristic curve (ICC) and in- volves item characteristics as parameters; (c) Re- sponses to different items are independent of each other for a given ability level of the person ("lo- cal independence assumption"); (d) Responses from different individuals are independent of each other. More formally, if we let j be an individual, i be an item, and θ j be the latent ability trait of individual j, then the probability that individual j answers item i correctly can be modeled as:</p><formula xml:id="formula_0">p ij (θ j ) = c i + 1 − c i 1 + e −a i (θ j −b i )<label>(1)</label></formula><p>where a i , b i , and c i are item parameters: a i (the slope or discrimination parameter) is related to the steepness of the curve, b i (the difficulty parameter) is the level of ability that produces a chance of cor- rect response equal to the average of the upper and lower asymptotes, and c i (the guessing parameter) is the lower asymptote of the ICC and the proba- bility of guessing correctly. Equation 1 is referred to as the three-parameter logistic (3PL) IRT model. A two-parameter logistic (2PL) IRT model assumes that the guessing parameter c i is 0. <ref type="figure" target="#fig_0">Figure 1</ref> shows an ICC of a 3PL model. The ICC for a good item will look like a sigmoid plot, and should exhibit a relatively steep increasing ICC between ability levels −3 and 3, where most peo- ple are located, in order to have appropriate power to differentiate different levels of ability. We have described a one factor IRT model where ability is uni-dimensional. Multi-factor IRT models would in- volve two or more latent trait dimensions and will not be elaborated here.</p><p>To identify the number of factors in an IRT model, the polychoric correlation matrix of the items is cal- culated and its ordered eigenvalues are plotted. The number of factors is suggested by the number of large eigenvalues. It can be further established by fitting (see below) and comparing IRT models with different numbers of factors. Such comparison may use model selection indices such as Akaike Infor- mation Criterion (AIC) and Conditional Bayesian Information Criterion (CBIC) and should also take into account the interpretablility of the loading pat- tern that links items to factors.</p><p>An IRT model can be fit to data with the marginal maximum likelihood method through an EM algo- rithm <ref type="bibr" target="#b3">(Bock and Aitkin, 1981)</ref>. The marginal likeli- hood function is the probability to observe the cur- rent response patterns as a function of the item pa- rameters with the persons' ability parameters inte- grated out as random effects. This function is max- imized to produce estimates of the item parameters. For IRT models with more than one factor, the slope parameters (i.e. loadings) that relate items and fac- tors must be properly rotated <ref type="bibr" target="#b5">(Browne, 2001</ref>) be- fore they can be interpreted. Given the estimated item parameters, Bayesian estimates of the individ- ual person's ability parameters are obtained with the standard normal prior distribution.</p><p>After determining the number of factors and fit- ting the model, the local independence assumption can be checked using the residuals of marginal re- sponses of item pairs <ref type="bibr" target="#b9">(Chen and Thissen, 1997</ref>) and the fit of the ICC for each item can be checked with item fit statistics <ref type="bibr" target="#b20">(Orlando and Thissen, 2000</ref>) to de- termine whether an item should be retained or re- moved. If both tests are passed and all items have proper discrimination power, then the set of items is considered a calibrated measurement scale and the estimated item parameters can be further used to es- timate an individual person's ability level.</p><p>IRT accounts for differences among items when estimating a person's ability. In addition, ability es- timates from IRT are on the ability scale of the pop- ulation used to estimate item parameters. For exam- ple, an estimated ability of 1.2 can be interpreted as 1.2 standard deviations above the average ability in this population. The traditional total number of cor- rect responses generally does not have such quanti- tative meaning.</p><p>IRT has been widely used in educational test- ing. For example, it plays an instrumental role in the construction, evaluation, or scoring of standard- ized tests such as the Test of English as a Foreign Language (TOEFL), Graduate Record Examinations (GRE) and the SAT college admissions standardized test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">IRT Terminology</head><p>Here we outline common IRT terminology in terms of RTE. An item refers to a pair of sentences to which humans or NLP systems assign a label (entail- ment, contradiction, or neutral). A set of responses to all items (each graded as correct or incorrect) is a response pattern. An evaluation scale is a test set of items to be administered to an NLP system and as- signs an ability score (or theta score) to the system as its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recognizing Textual Entailment</head><p>RTE was introduced to standardize the challenge of accounting for semantic variation when building models for a number of NLP applications <ref type="bibr" target="#b10">(Dagan et al., 2006</ref>). RTE defines a directional relationship between a pair of sentences, the text (T) and the hy- pothesis (H). T entails H if a human that has read T would infer that H is true. If a human would in- fer that H is false, then H contradicts T. If the two sentences are unrelated, then the pair are said to be neutral. <ref type="table" target="#tab_1">Table 1</ref> shows examples of T-H pairs and their respective classifications. Recent state-of-the- art systems for RTE require a large amount of fea- ture engineering and specialization to achieve high performance ( <ref type="bibr" target="#b2">Beltagy et al., 2015;</ref><ref type="bibr" target="#b15">Jimenez et al., 2014)</ref>.</p><p>A number of gold-standard datasets are available for RTE <ref type="bibr" target="#b19">(Marelli et al., 2014;</ref><ref type="bibr" target="#b25">Young et al., 2014;</ref><ref type="bibr" target="#b17">Levy et al., 2014</ref>). We consider the Stanford Natu- ral Language Inference (SNLI) dataset <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>. SNLI examples were obtained using only human-generated sentences with Amazon Mechan- ical Turk (AMT) to mitigate the problem of poor data that was being used to build models for RTE. In addition, SNLI included a quality control assess- ment of a sampled portion of the dataset (about 10%, 56,951 sentence pairs). This data was provided to 4 additional AMT users to provide labels (entailment, contradiction, neutral) for the sentence pairs. If at least 3 of the 5 annotators (the original annotator and 4 additional annotators) agreed on a label the item was retained. Most of the items (98%) received  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related Work</head><p>To identify low-quality annotators (spammers), <ref type="bibr" target="#b14">Hovy et al. (2013)</ref> modeled annotator responses, ei- ther answering correctly or guessing, as a random variable with a guessing parameter varying only across annotators. <ref type="bibr" target="#b21">Passonneau and Carpenter (2014)</ref> used the model of <ref type="bibr" target="#b11">Dawid and Skene (1979)</ref> in which an annotator's response depends on both the true la- bel and the annotator. In both models an annotator's response depends on an item only through its correct label. In contrast, IRT assumes a more sophisticated response mechanism involving both annotator qual- ities and item characteristics. To our knowledge we are the first to introduce IRT to NLP and to create a gold standard with the intention of comparing NLP applications to human intelligence.  analyze patterns of agreement between annotators in a case-study sen- tence categorization task, and use a latent-trait model to identify true labels. That work uses 4 an- notators at varying levels of expertise and does not consider the discriminating power of dataset items.</p><p>Current gold-standard dataset generation methods include web crawling ( <ref type="bibr" target="#b13">Guo et al., 2013</ref>), automatic and semi-automatic generation <ref type="bibr" target="#b0">(An et al., 2003)</ref>, and expert <ref type="bibr" target="#b22">(Roller and Stevenson, 2015)</ref> and non-expert human annotation <ref type="bibr" target="#b4">(Bowman et al., 2015;</ref>). In each case validation is required to ensure that the data collected is appropriate and us- able for the required task. Automatically generated data can be refined with visual inspection or post- collection processing. Human annotated data usu- ally involves more than one annotator, so that com- parison metrics such as Cohen's or Fleiss' κ can be used to determine how much they agree. Disagree- ments between annotators are resolved by researcher intervention or by majority vote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We collected and evaluated a random selection from the SNLI RTE dataset (GS RT E ) to build our IRT models. We first randomly selected a subset of GS RT E , and then used the sample in an AMT Hu- man Intelligence Task (HIT) to collect more labels for each text-hypothesis pair. We then applied IRT to evaluate the quality of the examples and used the final IRT models to create evaluation sets (GS IRT ) to measure ability for RTE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Item Selection</head><p>For our evaluation we looked at two sets of data: sentence-pairs selected from SNLI where 4 out of 5 annotators agreed on the gold-standard label (re- ferred to as 4GS), and sentence-pairs where 5 out of 5 annotators agreed on the gold-standard label (re- ferred to as 5GS). We make the assumption for our analysis that the 4GS items are harder than the 5GS items due to the fact that there was not a unanimous decision regarding the gold-standard label.</p><p>We selected the subset of GS RT E to use as an ex- amination set in 4GS and 5GS according to the fol- lowing steps: (1) Identify all "quality-control" items from GS RT E (i.e. items where 5 annotators pro- vided labels, see §2.2), (2) Identify items in this sec- tion of the data where 4 of the 5 annotators agreed on the eventual gold label (to be selected from for 4GS) and 5 of the 5 annotators agreed on the gold standard label (to be selected from for 5GS), (3) Randomly select 30 entailment sentence pairs, 30 neutral pairs, and 30 contradiction pairs from those items where 4 of 5 annotators agreed on the gold label (4GS) and those items where 5 of 5 annotators agreed on the gold label (5GS) to obtain two sets of 90 sentence pairs. 90 sentence pairs for 4GS and 5GS were sam- pled so that the annotation task (supplying 90 labels) could be completed in a reasonably short amount of time during which users remained engaged. We se- lected items from 4GS and 5GS because both groups are considered high quality for RTE. We evaluated the selected 180 sentence pairs using the model provided with the original dataset ( <ref type="bibr" target="#b4">Bowman et al., 2015)</ref> and found that accuracy scores were similar compared to performance on the SNLI test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AMT Annotation</head><p>For consistency we designed our AMT HIT to match the process used to validate the SNLI quality con- trol items <ref type="bibr" target="#b4">(Bowman et al., 2015</ref>) and to generate la- bels for the SICK RTE dataset ( <ref type="bibr" target="#b19">Marelli et al., 2014</ref>). Each AMT user was shown 90 premise-hypothesis pairs (either the full 5GS or 4GS set) one pair at a time, and was asked to choose the appropriate label for each. Each user was presented with the full set, as opposed to one-label subsets (e.g. just the entail- ment pairs) in order to avoid a user simply answering with the same label for each item.</p><p>For each 90 sentence-pair set (5GS and 4GS), we collected annotations from 1000 AMT users, result- ing in 1000 label annotations for each of the 180 sen- tence pairs. While there is no set standard for sam- ple sizes in IRT models, this sample size satisfies the standards based on the non-central χ 2 distribu- tion ( <ref type="bibr" target="#b18">MacCallum et al., 1996</ref>) used when comparing two multidimensional IRT models. This sample size is also appropriate for tests of item fit and local de- pendence that are based on small contingency tables.</p><p>Only AMT users with approval ratings above 97% were used to ensure that users were of a high qual- ity. The task was only available to users located in the United States, as a proxy for identifying English speakers. Attention check questions were included in the HIT, to ensure that users were paying attention and answering to the best of their ability. Responses where the attention-check questions were answered incorrectly were removed. After removing individ- uals that failed the attention-check, we retained 976 labels for each example in the 4GS set and 983 labels for each example in the 5GS set. Average time spent for each task was roughly 30 minutes, a reasonable amount for AMT users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistical Analysis</head><p>Data collected for 4GS and 5GS were analyzed sep- arately in order to evaluate the differences between "easier" items (5GS) and "harder" items (4GS), and to demonstrate the ability to show that theta score is consistent even if dataset difficulty varies. For both sets of items, the number of factors was identified by a plot of eigenvalues of the 90 x 90 tetrachoric correlation matrix and by a further comparison be- tween IRT models with different number of factors. A target rotation <ref type="bibr" target="#b5">(Browne, 2001</ref>) was used to iden- tify a meaningful loading pattern that associates fac- tors and items. Each factor could then be interpreted as the ability of a user to recognize the correct rela- tionship between the sentence pairs associated with that factor (e.g. contradiction).</p><p>Once the different factors were associated with different sets of items, we built a unidimensional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4GS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5GS</head><p>Overall Pairs with majority agreement 95.6% 96.7% 96.1%</p><p>Pairs with superma- jority agreement 61.1% 82.2% 71.7% IRT model for each set of items associated with a single factor. We fit and compared one-and two- factor 3PL models to confirm our assumption and the unidimensional structure underlying these items, assuming the possible presence of guessing in peo- ple's responses. We further tested the guessing pa- rameter of each item in the one factor 3PL model. If the guessing parameter was not significantly differ- ent from 0, a 2PL ICC was used for that particular item. Once an appropriate model structure was deter- mined, individual items were evaluated for goodness of fit within the model ( §2.1). If an item was deemed to fit the ICC poorly or to give rise to local depen- dence, it was removed for violating model assump- tions. Furthermore, if the ICC of an item was too flat, it was removed for low discriminating power between ability levels. The model was then refit with the remaining items. This iterative process contin- ued until no item could be removed (2 to 6 iterations depending on how many items were removed from each set).</p><p>The remaining items make up our final test set (GS IRT ), which is a calibrated scale of ability to correctly identify the relationship between the two sentence pairs. Parameters of these items were esti- mated as part of the IRT model and the set of items can be used as an evaluation scale to estimate ability of test-takers or RTE systems. We used the mirt R package ( <ref type="bibr" target="#b8">Chalmers et al., 2015</ref>) for our analyses. <ref type="table" target="#tab_2">Table 2</ref> lists key statistics from the AMT HITs. Most of the sampled sentence pairs resulted in a gold stan- dard label being identified via a majority vote. Due to the large number of individuals providing labels during the HIT, we also wanted to see if a gold stan- dard label could be determined via a two-thirds su- permajority vote. We found that 28.3% of the sen-   tence pairs did not have a supermajority gold label. This highlights the ambiguity associated with iden- tifying entailment. We believe that the items selected for analysis are appropriate for our task in that we chose high- quality items, where at least 4 annotators selected the same label, indicating a strong level of agree- ment (Section 3.1). We argue that our sample is a high-quality portion of the dataset, and further anal- ysis of items where the gold-standard label was only selected by 3 annotators originally would result in lower levels of agreement. <ref type="table" target="#tab_4">Table 3</ref> shows that the level of agreement as mea- sured by the Fleiss' κ score is much lower when the number of annotators is increased, particularly for the 4GS set of sentence pairs, as compared to scores noted in <ref type="bibr" target="#b4">Bowman et al. (2015)</ref>. The decrease in agreement is particularly large with regard to con- tradiction. This could occur for a number of rea- sons. Recognizing entailment is an inherently dif- ficult task, and classifying a correct label, particu- larly for contradiction and neutral, can be difficult due to an individual's interpretation of the sentences and assumptions that an individual makes about the key facts of each sentence (e.g. coreference). It may also be the case that the individuals tasked with cre- ating the sentence pairs on AMT created sentences that appeared to contradict a premise text, but can be interpreted differently given a different context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Response Statistics</head><p>Before fitting the IRT models we performed a vi- sual inspection of the 180 sentence pairs and re- moved items clearly not suitable for an evaluation scale due to syntactic or semantic discrepancies. For example item 10 in <ref type="table" target="#tab_1">Table 1</ref> was removed from the 5GS contradiction set for semantic reasons. While many people would agree that the statement is a con- tradiction due to the difference between football and soccer, individuals from outside the U.S. would pos- sibly consider the two to be synonyms and classify this as entailment. Six such pairs were identified and removed from the set of 180 items, leaving 174 items for IRT model-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IRT Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">IRT Models</head><p>We used the methods described in Section 3.3 to build IRT models to scale performance according to the RTE task. For both 4GS and 5GS items three factors were identified, each related to items for the three GS RT E labels (entailment, contradiction, neu- tral). This suggests that items with the same GS RT E label within each set defines a separate ability. In the subsequent steps, items with different labels were analyzed separately. After analysis, we were left with a subset of the 180 originally selected items. Refer to <ref type="table" target="#tab_1">Table 1</ref> for examples of the retained and removed items based on the IRT analysis. We re- tained 124 of the 180 items (68.9%). We were able to retain more items from the 5GS datasets (76 out of 90 -84%) than from the 4GS datasets (48 out of 90 -53.5%). Items that measure contradiction were retained at the lowest rate for both 4GS and 5GS datasets (66% in both cases). For the 4GS en- tailment items, our analysis found that a one-factor model did not fit the data, and a two-factor model failed to yield an interpretable loading pattern after rotation. We were unable to build an IRT model that accurately modeled ability to recognize entailment with the obtained response patterns. As a result, no items from the 4GS entailment set were retained. <ref type="figure" target="#fig_2">Figure 2</ref> plots the empirical spline-smoothed ICC of one item <ref type="table" target="#tab_1">(Table 1</ref>, item 9) with its estimated re- sponse curve. The ICC is not continuously increas- ing, and thus a logistic function is not appropriate. This item was spotted for poor item fit and removed. <ref type="figure" target="#fig_3">Figure 3</ref> shows a comparison between the ICC plot of a retained item <ref type="table" target="#tab_1">(Table 1</ref>, item 4) and the ICC of a removed item <ref type="table" target="#tab_1">(Table 1</ref>, item 8). Note that the re- moved item has an ICC that is very flat between -3 and 3. This item cannot discriminate individuals at any common level of ability and thus is not useful.</p><p>The items retained for each factor can be consid- ered as an evaluation scale that measures a single ability of an individual test-taker. As each factor is associated with a separate gold-standard label, each factor (θ) is a person's ability to correctly classify the relationship between the text and hypothesis for  one such label (e.g. entailment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Item Parameter Estimation</head><p>Parameter estimates of retained items for each la- bel are summarized in <ref type="table" target="#tab_6">Table 4</ref>, and show that all parameters fall within reasonable ranges. All re- tained items have 2PL ICCs, suggesting no signif- icant guessing. Difficulty parameters of most items are negative, suggesting that an average AMT user has at least 50% chance to answer these items cor- rectly. Although some minimum difficulties are quite low for standard ranges for a human popula- tion, the low range of item difficulty is appropriate for the evaluation of NLP systems. Items in each scale have a wide range of difficulty and discrimina- tion power.</p><p>With IRT we can use the heterogeneity of items to properly account for such differences in the estima- tion of a test-taker's ability. <ref type="figure" target="#fig_4">Figure 4</ref> plots the esti- mated ability of each AMT user from IRT against their total number of correct responses to the re- tained items in the 4GS contradiction item set. The two estimates of ability differ in many aspects. First, test-takers with the same total score may differ in their IRT score because they have different response   patterns (i.e. they made mistakes on different items), showing that IRT is able to account for differences among items. Second, despite a rough monotonic trend between the two scores, people with a higher number of correct responses may have a lower abil- ity estimate from IRT. We can extend this analysis to the case of RTE systems, and use the newly constructed scales to evaluate RTE systems. A system could be trained on an existing dataset and then evaluated using the re- tained items from the IRT models to estimate a new ability score. This score would be a measurement of how well the system performed with respect to the human population used to fit the model. With this approach, larger sections of datasets can be devoted to training, with a small portion held out to build an IRT model that can be used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Application to an RTE System</head><p>As a demonstration, we evaluate the LSTM model presented in <ref type="bibr" target="#b4">Bowman et al. (2015)</ref> with the items in our IRT evaluation scales. In addition to the theta scores, we calculate accuracy for the binary classi- fication task of identifying the correct label for all  LSTM trained on SNLI and tested on GSIRT . We also report the accuracy for the same LSTM tested on all SNLI quality con- trol items (see Section 3.1). All performance is based on binary classification for each label.</p><p>items eligible for each subset in <ref type="table" target="#tab_8">Table 5</ref> (e.g. all test items where 5 of 5 annotators labeled the item as en- tailment for 5GS). Note that these accuracy metrics are for subsets of the SNLI test set used for binary classifications and therefore do not compare with the standard SNLI test set accuracy measures. The theta scores from IRT in <ref type="table" target="#tab_8">Table 5</ref> show that, compared to AMT users, the system performed well above average for contradiction items compared to human performance, and performed around the av- erage for entailment and neutral items. For both the neutral and contradiction items, the theta scores are similar across the 4GS and 5GS sets, whereas the accuracy of the more difficult 4GS items is consis- tently lower. This shows the advantage of IRT to ac- count for item characteristics in its ability estimates. A similar theta score across sets indicates that we can measure the "ability level" regardless of whether the test set is easy or hard. Theta score is a con- sistent measurement, compared to accuracy which varies with the difficulty of the dataset.</p><p>The theta score and accuracy for 5GS entailment show that high accuracy does not necessarily mean that performance is above average when compared to human performance. However, theta score is not meant to contradict accuracy score, but to provide a better idea of system performance compared against a human population. The theta scores are a result of the IRT model fit using human annotator responses and provide more context about the system perfor- mance than an accuracy score can alone. If accuracy is high and theta is close to 0 (as is the case with 5GS entailment), we know that the performance of RTE is close to the average level of the AMT user pop- ulation and that 5GS entailment test set was "easy" to both. Theta score and percentile are intrinsically in reference to human performance and independent of item difficulty, while accuracy is intrinsically in reference to a specific set of items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>As NLP systems have become more sophisticated, sophisticated methodologies are required to com- pare their performance. One approach to create an intelligent gold standard is to use IRT to build mod- els to scale performance on a small section of items with respect to the tested population. IRT models can identify dataset items with different difficulty levels and discrimination powers based on human responses, and identify items that are not appropriate as scale items for evaluation. The resulting small set of items can be used as a scale to score an individ- ual or NLP system. This leaves a higher percentage of a dataset to be used in the training of the system, while still having a valuable metric for testing.</p><p>IRT is not without its challenges. A large popu- lation is required to provide the initial responses in order to have enough data to fit the models; however, crowdsourcing allows for the inexpensive collection of large amounts of data. An alternative methodol- ogy is Classical Test Theory, which has its own limi- tations, in particular that it is test-centric, and cannot provide information for individual items.</p><p>We have introduced Item Response Theory from psychometrics as an alternative method for generat- ing gold-standard evaluation datasets. Fitting IRT models allows us to identify a set of items that when taken together as a test set, can provide a meaningful evaluation of NLP systems with the different diffi- culty and discriminating characteristics of the items taken into account. We demonstrate the usefulness of the IRT-generated test set by showing that high accuracy does not necessarily indicate high perfor- mance when compared to a population of humans.</p><p>Future work can adapt this analysis to create eval- uation mechanisms for other NLP tasks. The ex- pectation is that systems that perform well using a standard accuracy measure can be stratified based on which types of items they perform well on. High quailty systems should also perform well when the models are used together as an overall test of abil- ity. This new evaluation for NLP systems can lead to new and innovative methods that can be tested against a novel benchmark for performance, instead of gradually incrementing on a classification accu- racy metric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example ICC for a 3PL model with the following parameters: a = 1.0, b = 0.0, c = 0.25.</figDesc><graphic url="image-1.png" coords="2,313.20,57.83,226.80,184.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fleiss</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Estimated (solid) and actual (dotted) response curves for a removed item.</figDesc><graphic url="image-2.png" coords="7,335.88,57.82,181.44,110.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ICCs for retained (solid) and removed (dotted) items.</figDesc><graphic url="image-3.png" coords="7,319.18,211.09,204.12,124.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of total correct answers vs. IRT scores.</figDesc><graphic url="image-4.png" coords="8,117.36,204.04,136.08,136.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Examples of retained &amp; removed sentence pairs. The selection is not based on right/wrong labels but based on IRT model 

fitting and item elimination process. Note that no 4GS entailment items were retained (Section 4.2) 

a gold-standard label. Specifics of SNLI generation 
are at Bowman et al. (2015). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Summary statistics from the AMT HITs.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Comparison of Fleiss' κ scores with scores from SNLI</head><label>3</label><figDesc></figDesc><table>quality control sentence pairs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parameter estimates of the retained items 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Theta scores and area under curve percentiles for 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the AMT Turkers who completed our an-notation task. We would like to also thank the anonymous reviewers for their insightful comments.</p><p>This work was supported in part by the HSR&amp;D award IIR 1I01HX001457 from the United States Department of Veterans Affairs (VA). We also ac-knowledge the support of HL125089 from the Na-tional Institutes of Health. This work was also sup-ported in part by the Center for Intelligent Informa-tion Retrieval. The contents of this paper do not rep-resent the views of CIIR, NIH, VA, or the United States Government</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic Acquisition of Named Entity Tagged Corpus from World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohui</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Item Response Theory: Parameter Estimation Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seock-Ho</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-07" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.06816[cs].arXiv:1505.06816</idno>
		<title level="m">Representing Meaning with a Combination of Logical Form and Vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marginal maximum likelihood estimation of item parameters: Application of an em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Aitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="459" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An overview of analytic rotation in exploratory factor analysis. Multivariate Behavioral Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael W Browne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="111" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing subjectivity: a case study in manual tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><forename type="middle">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="187" to="205" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep blue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Hoane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Hsiung</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Pritikin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Robitzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zoltak</surname></persName>
		</author>
		<title level="m">mirt: Multidimensional Item Response Theory</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local Dependence Indexes for Item Pairs Using Item Response Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Thissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="289" />
			<date type="published" when="1997-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
		<idno type="doi">DOI:10.1007/117367909</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="239" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning whom to trust with mace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Av Juan Dios</forename><surname>Btiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Av</forename><surname>Mendizbal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">732</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illinois-LH: A Denotational and Distributional Approach to Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SemEval</title>
		<imprint>
			<biblScope unit="page">329</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focused entailment graphs for open IE propositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Power analysis and determination of sample size for covariance structure modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sugawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fondazione Bruno</forename><surname>Kessler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LikelihoodBased Item-Fit Indices for Dichotomous Item Response Theory Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Thissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="2000-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The benefits of a model of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="311" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heldout versus Gold Standard: Comparison of Evaluation Strategies for Distantly Supervised Relation Extraction from Medline abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Workshop on Health Text Mining and Information Analysis (LOUHI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Development and use of a goldstandard data set for subjectivity classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">F</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>O&amp;apos;hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
