<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disfluency Detection using Auto-Correlational Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paria</forename><forename type="middle">Jamshid</forename><surname>Lou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
							<email>p.anderson@mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<email>mark.johnson@mq.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disfluency Detection using Auto-Correlational Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4610" to="4619"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4610</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In recent years, the natural language processing community has moved away from task-specific feature engineering, i.e., researchers discovering ad-hoc feature representations for various tasks, in favor of general-purpose methods that learn the input representation by themselves. However, state-of-the-art approaches to disfluency detection in spontaneous speech transcripts currently still depend on an array of hand-crafted features, and other representations derived from the output of pre-existing systems such as language models or dependency parsers. As an alternative , this paper proposes a simple yet effective model for automatic disfluency detection, called an auto-correlational neural network (ACNN). The model uses a convolutional neu-ral network (CNN) and augments it with a new auto-correlation operator at the lowest layer that can capture the kinds of &quot;rough copy&quot; dependencies that are characteristic of repair disfluencies in speech. In experiments, the ACNN model outperforms the baseline CNN on a disfluency detection task with a 5% increase in f-score, which is close to the previous best result on this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Disfluency informally refers to any interruptions in the normal flow of speech, including false starts, corrections, repetitions and filled pauses. <ref type="bibr" target="#b21">Shriberg (1994)</ref> defines three distinct parts of a speech dis- fluency, referred to as the reparandum, interreg- num and repair. As illustrated in Example 1, the reparandum to Boston is the part of the utterance that is replaced, the interregnum uh, I mean (which consists of a filled pause uh and a discouse marker I mean) is an optional part of a disfluent struc- ture, and the repair to Denver replaces the reparan- dum. The fluent version is obtained by removing reparandum and interregnum words although dis- fluency detection models mainly deal with identi- fying and removing reparanda. The reason is that filled pauses and discourse markers belong to a closed set of words and phrases and are trivial to detect ). </p><p>In disfluent structures, the repair (e.g., to Den- ver) frequently seems to be a "rough copy" of the reparandum (e.g., to Boston). In other words, they incorporate the same or very similar words in roughly the same word order. In the Switch- board training set ( <ref type="bibr" target="#b6">Godfrey and Holliman, 1993)</ref>, over 60% of the words in the reparandum are exact copies of words in the repair. Thus, this similarity is strong evidence of a disfluency that can help the model detect reparanda <ref type="bibr" target="#b1">(Charniak and Johnson, 2001;</ref>. As a result, models which are able to detect "rough copies" are likely to perform well on this task.</p><p>Currently, state-of-the-art approaches to disflu- ency detection depend heavily on hand-crafted pattern match features, specifically designed to find such "rough copies" ( <ref type="bibr" target="#b27">Zayats et al., 2016;</ref><ref type="bibr" target="#b9">Jamshid Lou and Johnson, 2017)</ref>. In contrast to many other sequence tagging tasks ( <ref type="bibr" target="#b16">Plank et al., 2016;</ref><ref type="bibr" target="#b25">Yu et al., 2017</ref>), "vanilla" convo- lutional neural networks (CNNs) and long short- term memory (LSTM) models operating only on words or characters are surprisingly poor at disflu- ency detection ( <ref type="bibr" target="#b27">Zayats et al., 2016)</ref>. As such, the task of disfluency detection sits in opposition to the ongoing trend in NLP away from task-specific feature engineering -i.e., researchers discov- ering ad-hoc feature representations for various tasks -in favor of general-purpose methods that learn the input representation by themselves <ref type="bibr" target="#b2">(Collobert and Weston, 2008)</ref>.</p><p>In this paper, we hypothesize that LSTMs and CNNs cannot not easily learn "rough copy" depen- dencies. We address this problem in the context of a CNN by introducing a novel auto-correlation operator. The resulting model, called an auto- correlational neural network (ACNN), is a gener- alization of a CNN with an auto-correlation oper- ator at the lowest layer. Evaluating the ACNN in the context of disfluency detection, we show that introducing the auto-correlation operator increases f-score by 5% over a baseline CNN. Furthermore, the ACNN -operating only on word inputs - achieves results which are competitive with much more complex approaches relying on hand-crafted features and outputs from pre-existing systems such as language models or dependency parsers. In summary, the main contributions of this paper are:</p><p>• We introduce the auto-correlational neural network (ACNN), a generalization of a CNN incorporating auto-correlation operations,</p><p>• In the context of disfluency detection, we show that the ACNN captures important properties of speech repairs including "rough copy" dependencies, and</p><p>• Using the ACNN, we achieve competitive re- sults for disfluency detection without rely- ing on any hand-crafted features or other rep- resentations derived from the output of pre- existing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Approaches to disfluency detection task fall into three main categories: noisy channel mod- els, parsing-based approaches and sequence tagging approaches. Noisy channel models (NCMs) ) use complex tree adjoining grammar (TAG) <ref type="bibr" target="#b20">(Shieber and Schabes, 1990</ref>) based chan- nel models to find the "rough copy" dependencies between words. The channel model uses the sim- ilarity between the reparandum and the repair to allocate higher probabilities to exact copy reparan- dum words. Using the probabilities of TAG chan- nel model and a bigram language model (LM) derived from training data, the NCM generates n-best disfluency analyses for each sentence at test time. The analyses are then reranked us- ing a language model which is sensitive to the global properties of the sentence, such as a syn- tactic parser based LM ( . Some works have shown that rescoring the n-best analyses with ex- ternal n-gram (Zwarts and Johnson, 2011) and deep learning LMs (Jamshid <ref type="bibr" target="#b9">Lou and Johnson, 2017</ref>) trained on large speech and non-speech cor- pora, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker ( ) improves the performance of the baseline NCM, although this creates complex runtime dependencies.</p><p>Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence. Typically, this is achieved by augmenting a transition-based de- pendency parser with a new action to detect and remove the disfluent parts of the sentence and their dependencies from the stack <ref type="bibr" target="#b18">(Rasooli and Tetreault, 2013;</ref><ref type="bibr" target="#b7">Honnibal and Johnson, 2014;</ref><ref type="bibr" target="#b24">Yoshikawa et al., 2016)</ref>. Joint parsing and disflu- ency detection can compare favorably to pipelined approaches, but requires large annotated tree- banks containing both disfluent and syntatic struc- tures for training.</p><p>Our proposed approach, based on an auto- correlational neural network (ACNN), belongs to the class of sequence tagging approaches. These approaches use classification techniques such as conditional random fields ( <ref type="bibr" target="#b14">Liu et al., 2006;</ref><ref type="bibr" target="#b15">Ostendorf and Hahn, 2013;</ref><ref type="bibr" target="#b26">Zayats et al., 2014;</ref><ref type="bibr" target="#b3">Ferguson et al., 2015)</ref>, hidden Markov models ( <ref type="bibr" target="#b14">Liu et al., 2006;</ref><ref type="bibr" target="#b19">Schuler et al., 2010</ref>) and deep learn- ing based models <ref type="bibr" target="#b8">(Hough and Schlangen, 2015;</ref><ref type="bibr" target="#b27">Zayats et al., 2016</ref>) to label individual words as fluent or disfluent. In much of the previous work on sequence tagging approaches, improved per- formance has been gained by proposing increas- ingly complicated labeling schemes. In this case, a model with begin-inside-outside (BIO) style states which labels words as being inside or outside of edit region 1 is usually used as the baseline se- quence tagging model. Then in order to come up with different pattern matching lexical cues for repetition and correction disfluencies, they ex- tend the baseline state space with new explicit re- pair states to consider the words at repair region, in addition to edit region ( <ref type="bibr" target="#b15">Ostendorf and Hahn, 2013;</ref><ref type="bibr" target="#b26">Zayats et al., 2014</ref><ref type="bibr" target="#b27">Zayats et al., , 2016</ref>. A model which uses such labeling scheme may generate illegal la- bel sequences at test time. As a solution, integer linear programming (ILP) constraints are applied to the output of classifier to avoid inconsisten- cies between neighboring labels <ref type="bibr" target="#b4">(Georgila, 2009;</ref><ref type="bibr" target="#b5">Georgila et al., 2010;</ref><ref type="bibr" target="#b27">Zayats et al., 2016)</ref>. This contrasts with our more straightforward approach, which directly labels words as being fluent or dis- fluent, and does not require any post-processing or annotation modifications.</p><p>The most similar work to ours is recent work by <ref type="bibr" target="#b27">Zayats et al. (2016)</ref> that investigated the per- formance of a bidirectional long-short term mem- ory network (BLSTM) for disfluency detection. <ref type="bibr" target="#b27">Zayats et al. (2016)</ref> reported that a BLSTM op- erating only on words underperformed the same model augmented with hand-crafted pattern match features and POS tags by 7% in terms of f-score. In addition to lexically grounded features, some works incorporate prosodic information extracted from speech ( <ref type="bibr" target="#b12">Kahn et al., 2005;</ref><ref type="bibr" target="#b3">Ferguson et al., 2015;</ref><ref type="bibr" target="#b23">Tran et al., 2018)</ref>. In this work, our primary motivation is to rectify the architectural limitations that prevent deep neural networks from automat- ically learning appropriate features from words alone. Therefore, our proposed model eschews manually engineered features and other represen- tations derived from dependency parsers, language models or tree adjoining grammar transducers that are used to find "rough copy" dependencies. In- stead, we aim to capture these kinds of dependen- cies automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional and Auto-Correlational Networks</head><p>In this section, we introduce our proposed auto-correlation operator and the resulting auto- correlational neural network (ACNN) which is the focus of this work. A convolutional or auto-correlational network computes a series of h feature representations X (0) , X (1) , . . . , X (h) , where X (0) is the input data, X (h) is the final (output) representation, and each non-input representation X (k) for k &gt; 0, is computed from the preceding representation X (k−1) using a convolution or auto-correlation op- eration followed by an element-wise non-linear function.</p><p>Restricting our focus to convolutions in one dimension, as used in the context of language processing, each representation X (k) is a ma- trix of size (n, m k ), where n is the number of words in the input and m k is the feature dimen- sion of representation k, or equivalently it can be viewed as a sequence of n row vectors</p><formula xml:id="formula_1">X (k) = (x (k) 1 , . . . , x (k) n ), where x (k) t</formula><p>is the row vector of length m k that represents the tth word at level k.</p><p>Consistent with the second interpretation, the input representation</p><formula xml:id="formula_2">X (0) = (x (0) 1 , . . . , x (0)</formula><p>n ) is a sequence of word embeddings, where m 0 is the length of the embedding vector and x (0) t is the word embedding for the tth word.</p><p>Each non-input representation X (k) , k &gt; 0 is formed by column-wise stacking the output of one or more convolution or auto-correlation oper- ations applied to the preceding representation, and then applying an element-wise non-linear func- tion. Formally, we define:</p><formula xml:id="formula_3">Y (k) = F (k,1) (X (k−1) ); . . . ; F (k,m k ) (X (k−1) ) X (k) =N (k) (Y (k) )<label>(2)</label></formula><p>where F (k,u) is the uth operator applied at layer k, and N (k) is the non-linear operation applied at layer k. Each operator F (k,u) (either convolution or auto-correlation) is a function from X (k−1) , which is a matrix of size (n, m k−1 ), to a vector of length n. A network that employs only con- volution operators is a convolutional neural net- work (CNN). We call a network that utilizes a mix- ture of convolution and auto-correlation operators an auto-correlational neural network (ACNN). In our networks, the non-linear operation N (k) is always element-wise ReLU , except for the last layer, which uses a sof tmax non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolution Operator</head><p>A one-dimensional convolution operation maps an input matrix X = (x 1 , . . . , x n ), where each x t is a row vector of length m, to an output vector y of length n. The convolution operation is defined by a convolutional kernel A, which is applied to a window of words to produce a new output rep- resentation, and kernel width parameters and r, which define the number of words to the left and right of the target word included in the convolu- tional window. For example, assuming appropri- ate input padding where necessary, element y t in the output vector y is computed as: b is a learned bias vector of dimension n,</p><formula xml:id="formula_4">y t = A · X i:j + b<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Auto-Correlation Operator</head><p>The auto-correlational operator is a generalisation of the convolution operator:</p><formula xml:id="formula_5">y t = A · X i:j + B · ˆ X i:j,i:j + b<label>(4)</label></formula><p>where y t , A, X, b, i and j are as in the convolution operator, andˆX andˆ andˆX is a tensor of size (n, n, m) such that each vec- torˆXtorˆ torˆX i,j,: is given by f (x i , x j ), f (u, v) is a binary operation on vectors, such as the Hadamard or element-wise product (i.e.,</p><formula xml:id="formula_6">f (u, v) = u • v), andˆX</formula><p>andˆ andˆX i:j,i:j is the sub-tensor formed by selecting in- dices i to j from the first two dimensions of tensorˆXtensorˆ tensorˆX, B is a learned convolutional kernel of dimension ( + r, + r, m).</p><p>Unlike convolution operations, which are linear, the auto-correlation operator introduces second- order interaction terms through the tensorˆXtensorˆ tensorˆX (since it multiplies the vector representations for each pair of input words). This naturally encodes the similarity between input words when applied at level k = 1 (or the co-activations of multiple CNN features, if applied at higher levels). As il- lustrated in <ref type="figure" target="#fig_1">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Switchboard Dataset</head><p>We evaluate the proposed ACNN model for disflu- ency detection on the Switchboard corpus of con- versational speech <ref type="bibr" target="#b6">(Godfrey and Holliman, 1993)</ref>. Switchboard is the largest available corpus (1.2 × 10 6 tokens) where disfluencies are annotated ac- cording to Shriberg's (1994) scheme:</p><formula xml:id="formula_7">[ reparandum + {interregnum} repair ]</formula><p>where (+) is the interruption point marking the end of reparandum and {} indicate optional interreg- num. We collapse this annotation to a binary clas- sification scheme in which reparanda are labeled as disfluent and all other words as fluent. We dis- regard interregnum words as they are trivial to de- tect as discussed in Section 1. Following Charniak and Johnson (2001), we split the Switchboard corpus into training, dev and test set as follows: training data consists of all sw <ref type="bibr">[23]</ref> * .dff files, dev training consists of all sw4[5-9] * .dff files and test data consists of all sw4[0-1] * .dff files. We lower-case all text and re- move all partial words and punctuations from the training data to make our evaluation both harder and more realistic ). Partial words are strong indicators of disfluency; however, speech recognition models never gener- ate them in their outputs. At inset bottom, the given patch of words is convolved with 2D kernels A of different sizes. At inset top, an auto-correlated tensor of size (n, n, m 0 ) is constructed by comparing each input vector u = x t with the input vector v = x t using a binary function f (u, v). The auto-correlated tensor is convolved with 3D kernels B of different sizes. Each kernel group A and B outputs a matrix of size (n, m 1 ) (here, we depict only the row vector relating to the target word "boston"). These outputs are added element-wise to produce the feature representation that is passed to further convolutional layers, followed by a softmax layer. "E" = disfluent, " " = fluent and m 0 = embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ACNN and CNN Baseline Models</head><p>We investigate two neural network models for dis- fluency detection; our proposed auto-correlational neural network (ACNN) and a convolutional neu- ral network (CNN) baseline. The CNN base- line contains three convolutional operators (lay- ers), followed by a width-1 convolution and a soft- max output layer (to label each input word as ei- ther fluent or disfluent). The ACNN has the same general architecture as the baseline, except that we have replaced the first convolutional operator with an auto-correlation operator, as illustrated in <ref type="figure" target="#fig_3">Fig- ure 2</ref>.</p><p>To ensure that equal effort was applied to the hyperparameter optimization of both models, we use randomized search <ref type="bibr" target="#b0">(Bergstra and Bengio, 2012</ref>) to tune the optimization and architec- ture parameters separately for each model on the dev set, and to find an optimal stopping point for training. This results in different dimensions for each model. As indicated by <ref type="table">Table 1</ref>, the result- ing ACNN configuration has far fewer kernels at each layer than the CNN. However, as the auto- correlation kernels contain an additional dimen- sion, both models have a similar number of param- eters overall. Therefore, both models should have similar learning capacity except for their architec-tural differences (which is what we wish to investi- gate). Finally, we note that the resulting maximum right kernel width r 1 in the auto-correlational layer is 6. As illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>, this is sufficient to capture almost all the "rough copies" in the Switchboard dataset (but could be increased for other datasets). For the ACNN, we considered a range of possi- ble binary functions f (u, v) to compare the input vector u = x t with the input vector v = x t in the auto-correlational layer. However, in initial exper- iments we found that the Hadamard or element- wise product (i.e. f (u, v) = u • v) achieved the best results. We also considered concatenat- ing the outputs of kernels A and B in Equation 4, but we found that element-wise addition produced slightly better results on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Implementation Details</head><p>In both models, we use ReLU for the non-linear operation, all stride sizes are one word and there are no pooling operations. We randomly initial- ize the word embeddings and all weights of the model from a uniform distribution. The bias terms are initialized to be 1. To reduce overfitting, we apply dropout ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) to the in- put word embeddings and L 2 regularization to the weights of the width-1 convolutional layer. For parameter optimization, we use the Adam opti- mizer ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>) with a mini-batch size of 25 and an initial learning rate of 0.001. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>As in previous work , we evaluate our model using precision, re- call and f-score, where true positives are the words in the edit region (i.e., the reparandum words). As <ref type="bibr" target="#b1">Charniak and Johnson (2001)</ref> observed, only 6% of words in the Switchboard corpus are disflu- ent, so accuracy is not a good measure of system performance. F-score, on the other hand, focuses more on detecting "edited" words, so it is more appropriate for highly skewed data. <ref type="table" target="#tab_2">Table 2</ref> compares the dev set performance of the ACNN model against our baseline CNN, as well as the LSTM and BLSTM models proposed by <ref type="bibr" target="#b27">Zayats et al. (2016)</ref> operating only on word inputs (i.e., without any disfluency pattern-match features). Our baseline CNN outperforms both the LSTM and the BLSTM, while the ACNN model clearly outperforms the baseline CNN, with a fur- ther 5% increase in f-score. In particular, the ACNN noticably improves recall without degrad- ing precision.  To further investigate the differences between the two CNN-based models, we randomly select 100 sentences containing disfluencies from the Switchboard dev set and categorize them accord- ing to <ref type="bibr" target="#b21">Shriberg's (1994)</ref> typology of speech re- pair disfluencies. Repetitions are repairs where the reparandum and repair portions of the disflu- ency are identical, while corrections are where the reparandum and repairs differ (so corrections are much harder to detect). Restarts are where the speaker abandons a sentence prefix, and starts a fresh sentence. As <ref type="table" target="#tab_3">Table 3</ref> shows, the ACNN model is better at detecting repetition and cor- rection disfluencies than the CNN, especially for the more challenging correction disfluencies. On the other hand, the ACNN is no better than the baseline at detecting restarts, probably because the restart typically does not involve a rough copy de- pendency. Luckily restarts are much rarer than repetition and correction disfluencies.  We also repeated the analysis of ( <ref type="bibr" target="#b26">Zayats et al., 2014</ref>) on the dev data, so we can compare our models to their extended BLSTM model with a 17-state CRF output and hand-crafted features, in- cluding partial-word and POS tag features that en- able it to capture some "rough copy" dependen- cies. As expected, the ACNN outperforms both the CNN and the extended BLSTM model, espe- cially in the "Other" category that involve the non- repetition dependencies.  Finally, we compare the ACNN model to state- of-the-art methods from the literature, evaluated on the Switchboard test set. <ref type="table" target="#tab_6">Table 5</ref> shows that the ACNN model is competitive with recent models from the literature. The three models that score more highly than the ACNN all rely on hand- crafted features, additional information sources such as partial-word features (which would not be available in a realistic ASR application), or ex- ternal resources such as dependency parsers and language models. The ACNN, on the other hand, only uses whole-word inputs and learns the "rough copy" dependencies between words without re- quiring any manual feature engineering.  Comparison of the ACNN model to the state- of-the-art methods on the Switchboard test set. The other models listed have used richer inputs and/or rely on the output of other systems, as well as pat- tern match features, as indicated by the following symbols: dependency parser, † hand-crafted con- straints/rules, prosodic cues, tree adjoining gram- mar transducer, 1 refined/external language models and ⊗ partial words. P = precision, R = recall and F = f-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Qualitative Analysis</head><p>We conduct an error analysis on the Switchboard dev set to characterize the disfluencies that the ACNN model can capture and those which are dif- ficult for the model to detect. In the following examples, the highlighted words indicate ground truth disfluency labels and the underlined ones are the ACNN predictions.</p><p>1. But if you let them yeah if you let them in a million at a time it wouldn't make that you know it wouldn't make that big a bulge in the population 2. They're handy uh they they come in handy at the most unusual times 3. My mechanics loved it because it was an old it was a sixty-five buick 4. Well I I I think we did I think we did learn some lessons that we weren't uh we weren't prepared for 5. Uh I have never even I have never even looked at one closely 6. But uh when I was when my kids were young I was teaching at a university 7. She said she'll never put her child in a in a in a in a in a preschool 8. Well I think they're at they're they've come a long way 9. I I like a I saw the the the the tapes that were that were run of marion berry's drug bust 10. But I know that in some I know in a lot of rural areas they're not that good According to examples 1-10, the ACNN detects repetition (e.g. 1, 5) and correction disfluencies (e.g. 3, 6, 10). It also captures complex struc- tures where there are multiple or nested disfluen- cies (e.g. 2, 8) or stutter-like repetitions (e.g. 4, 7, 9). 11. My point was that there is for people who don't want to do the military service it would be neat if there were an alternative . . . In some cases where repetitions are fluent, the model has incorrectly detected the first occurence of the word as disfluency (e.g. <ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">19)</ref>. Moreover, when there is a long distance between reparandum and repair words (e.g. 11, 12), the model usually fails to detect the reparanda. In some sentences, the model is also unable to detect the disfluent words which result in ungrammatical sentences <ref type="bibr">(e.g. 16, 17, 18, 20)</ref>. In these exam- ples, the undetected disfluencies "the", "did", "at" and "two the" cause the residual sentence to be un- grammatical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">I believe</head><p>We also discuss the types of disfluency captured by the ACNN model, but not by the baseline CNN. In the following examples, the ACNN predictions (underlined words) are the same as the ground truth disfluency labels (highlighted words). The bolded words indicate the CNN prediction of dis- fluencies. So we're we're part we're actually part of MIT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.">Uh well</head><p>The ACNN model has a generally better perfor- mance in detecting "rough copies" which are im- portant indicator of repetition (e.g. <ref type="bibr">21,</ref><ref type="bibr">29)</ref>, cor- rection (e.g. <ref type="bibr">22,</ref><ref type="bibr">23,</ref><ref type="bibr">24,</ref><ref type="bibr">25,</ref><ref type="bibr">27)</ref>, and stutter-like (e.g. 26, 28, 30) disfluencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a simple new model for disflu- ency detection in spontaneous speech transcripts. It relies on a new auto-correlational kernel that is designed to detect the "rough copy" dependencies that are characteristic of speech disfluencies, and combines it with conventional convolutional ker- nels to form an auto-correlational neural network (ACNN). We show experimentally that using the ACNN model improves over a CNN baseline on disfluency detection task, indicating that the auto- correlational kernel can in fact detect the rough copy dependencies between words in disfluencies. The addition of the auto-correlational kernel per- mits a fairly conventional architecture to achieve near state-of-the-art results without complex hand- crafted features or external information sources.</p><p>We expect that the performance of the ACNN model can be further improved in future by us- ing more complex similarity functions and by in- corporating similar kinds of external information (e.g. prosody) used in other disfluency models. In future work, we also intend to investigate other applications of the auto-correlational kernel. The auto-correlational layer is a generic neural net- work layer, so it can be used as a component of other architectures, such as RNNs. It might also be useful in very different applications such as im- age processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cosine similarity between word embedding vectors learned by the ACNN model for the sentence "I know they use that I mean they sell those" (with disfluent words highlighted). In the figure, darker shades denote higher cosine values. "Rough copies" are clearly indicated by darkly shaded diagonals, which can be detected by our proposed auto-correlation operator.</figDesc><graphic url="image-1.png" coords="4,94.96,62.81,172.35,165.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, blocks of similar words are indicative of "rough copies". We provide an il- lustration of the auto-correlation operation in Fig- ure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ACNN overview for labeling the target word "boston". A patch of words is fed into an auto-correlational layer. At inset bottom, the given patch of words is convolved with 2D kernels A of different sizes. At inset top, an auto-correlated tensor of size (n, n, m 0 ) is constructed by comparing each input vector u = x t with the input vector v = x t using a binary function f (u, v). The auto-correlated tensor is convolved with 3D kernels B of different sizes. Each kernel group A and B outputs a matrix of size (n, m 1 ) (here, we depict only the row vector relating to the target word "boston"). These outputs are added element-wise to produce the feature representation that is passed to further convolutional layers, followed by a softmax layer. "E" = disfluent, " " = fluent and m 0 = embedding size.</figDesc><graphic url="image-2.png" coords="5,124.16,62.81,349.21,382.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution over the number of words in between the reparandum and the interregnum in the Switchboard training set (indicating the distance between "rough copies").</figDesc><graphic url="image-3.png" coords="6,307.56,62.81,217.70,163.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>I actually my dad's my dad's almost ninety 22. Not a man not a repair man but just a friend 23. we're from a county we're from the county they marched in 24. Now let's now we're done 25. And they've most of them have been pretty good 26. I do as far as uh as far as uh as far as immi- gration as a whole goes 27. No need to use this to play around with this space stuff anymore 28. We couldn't survive in a in a juror in a trial system without a jury 29. You stay within your uh within your means 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Precision (P), recall (R) and f-score (F) on the 
dev set for the BLSTM and LSTM models using words 
alone from  *  Zayats et al. (2016), as well as our baseline 
CNN and ACNN model. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F-scores for different types of disfluencies on 
a subset of the Switchboard dev set containing 140 dis-
fluent structures -including 85 repetitions (Rep.), 51 
corrections (Cor.) and 4 restarts (Res.). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F-scores for different types of disfluencies 
for the CNN, ACNN and BLSTM (17 states)  *  (Zayats 
et al., 2016) using the Switchboard dev set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> For state labels, edit corresponds to reparandum.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their insightful comments and sugges-tions. This research was supported by a Google award through the Natural Language Understand-ing Focused Program, and under the Australian Research Councils Discovery Projects funding scheme (project number DP160102156).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edit detection and parsing for transcribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2 nd Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies (NAACL&apos;01)</title>
		<meeting>the 2 nd Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies (NAACL&apos;01)<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25 th International Conference on Machine Learning (ICML&apos;17)</title>
		<meeting>the 25 th International Conference on Machine Learning (ICML&apos;17)<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disfluency detection with a semi-Markov model and prosodic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;15)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;15)<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using integer linear programming for detecting speech disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;09)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;09)<address><addrLine>Boulder, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-domain speech disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL&apos;10)</title>
		<meeting>the 11 th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL&apos;10)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="237" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Switchboard-1 release 2 LDC97S62. Published by: Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Holliman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint incremental disfluency detection and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for incremental disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;15)</title>
		<meeting>the 16 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;15)<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
	<note>Dresden</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Disfluency detection using a noisy channel model and a deep neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamshid</forename><surname>Paria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55 th Annual Meeting of the Association for Computational Linguistics (ACL&apos;17)</title>
		<meeting>the 55 th Annual Meeting of the Association for Computational Linguistics (ACL&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="547" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A TAGbased noisy channel model of speech repairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42 nd Annual Meeting on Association for Computational Linguistics (ACL&apos;04)</title>
		<meeting>the 42 nd Annual Meeting on Association for Computational Linguistics (ACL&apos;04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An improved model for recognizing disfluencies in conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Rich Transcription Workshop</title>
		<meeting>Rich Transcription Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective use of prosody in parsing conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT&apos;05)</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT&apos;05)<address><addrLine>Tallinn, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enriching speech recognition with automatic detection of sentence boundaries and disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolckeand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Hillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1526" to="1540" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A sequential repetition model for improved disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyun</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;13)</title>
		<meeting>the 14 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;13)<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2624" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andersand</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54 th Annual Meeting of the Association for Computational Linguistics (ACL&apos;16)</title>
		<meeting>the 54 th Annual Meeting of the Association for Computational Linguistics (ACL&apos;16)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disfluency detection using multi-step stacked learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;13)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;13)<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="820" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint parsing and disfluency detection in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;13)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;13)<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Broad-coverage parsing using human-like memory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synchronous tree-adjoining grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13 th Conference on Computational Linguistics (COLING&apos;90)</title>
		<meeting>the 13 th Conference on Computational Linguistics (COLING&apos;90)<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Preliminaries to a theory of speech disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Berkeley, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing speech: A neural approach to integrating lexical and acoustic-prosodic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;18)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL&apos;18)<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint transition-based dependency parsing and disfluency detection for automatic speech recognition texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;16)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1036" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A general-purpose tagger with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Falenska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1 st Workshop on Subword and Character Level Models in NLP</title>
		<meeting>the 1 st Workshop on Subword and Character Level Models in NLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-domain disfluency and repair detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;15)</title>
		<meeting>the 15 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;15)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2907" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disfluency detection using a bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;16)</title>
		<meeting>the 17 th Annual Conference of the International Speech Communication Association (INTERSPEECH&apos;16)<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2523" to="2527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The impact of language models and loss functions on repair disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Zwarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49 th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT&apos;11)</title>
		<meeting>the 49 th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT&apos;11)<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
