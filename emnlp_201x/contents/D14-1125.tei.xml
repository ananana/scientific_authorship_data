<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1181" to="1191"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (+/-effect events). This paper addresses methods for creating a lexicon of such events, to support such work on opinion inference. Due to significant sense ambiguity, our goal is to develop a sense-level rather than word-level lexicon. To maximize the effectiveness of different types of information, we combine a graph-based method using WordNet 1 relations and a standard classifier using gloss information. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Opinion mining (or sentiment analysis) identifies positive or negative opinions in many kinds of texts such as reviews, blogs, and news articles. It has been exploited in many application areas such as review mining, election analysis, and infor- mation extraction. While most previous research focusses on explicit opinion expressions, recent work addresses a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on enti- ties ( <ref type="bibr" target="#b7">Deng et al., 2013;</ref>). We call such events +/-effect events. <ref type="bibr">2 Deng and Wiebe (2014)</ref> show how sentiments toward one <ref type="bibr">1</ref> WordNet 3.0, http://wordnet.princeton.edu/ 2 While the term goodFor/badFor is used in previous pa- pers <ref type="bibr" target="#b7">(Deng et al., 2013;</ref>), we have since decided that +/-effect is a better term. entity may be propagated to other entities via opinion inference rules. They give the following example:</p><p>(1) The bill would curb skyrocketing health care costs.</p><p>The writer expresses an explicit negative senti- ment (by skyrocketing) toward the object (health care costs). The event, curb, has a negative effect on costs, since they are reduced. We can reason that the writer is positive toward the event because it has a negative effect on costs, toward which the writer is negative. From there, we can reason that the writer is positive toward the bill, since it is the agent of the positive event.  show that such inferences may be exploited to significantly improve explicit sentiment analy- sis systems.</p><p>However, to achieve its results, the system de- veloped by  requires that all instances of +/-effect events in the corpus be manually provided as input. For the system to be fully automatic, it needs to be able to recog- nize +/-effect events automatically. This paper addresses methods for creating lexicons of such events, to support such work on opinion inference. We have discovered that there is significant sense ambiguity, meaning that words often have mix- tures of senses among the classes +effect, -effect, and Null. Thus, we develop a sense-level rather than word-level lexicon.</p><p>One of our goals is to investigate whether the +/-effect property tends to be shared among semantically-related senses, and another is to use a method that applies to all word senses, not just to the senses of words in a given word-level lexicon. Thus, we build a graph-based model in which each node is a WordNet sense, and edges represent semantic WordNet relations between senses. In addition, we hypothesized that glosses also contain useful information. Thus, we develop a supervised gloss classifier and define a hybrid model which gives the best overall performance. Finally, because all WordNet verb senses are incorporated into the model, we investigate the ability of the method to identify unlabeled senses that are likely to be +/-effect senses. We find that by iteratively labeling the top-weighted unlabeled senses and rerunning the model, it may be used as an effective method for guiding annotation efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>There are many varieties of +/-effect events, in- cluding creation/destruction (changes in states in- volving existence), gain/loss (changes in states involving possession), and benefit/injury <ref type="bibr" target="#b2">(Anand and Reschke, 2010;</ref><ref type="bibr" target="#b7">Deng et al., 2013</ref>). The cre- ation, gain, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created; 3 increasing the tax rate has a positive effect on the tax rate; and com- forting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill. <ref type="bibr">4</ref> While sentiment <ref type="bibr" target="#b9">(Esuli and Sebastiani, 2006;</ref><ref type="bibr" target="#b25">Wilson et al., 2005;</ref><ref type="bibr" target="#b22">Su and Markert, 2009</ref>) and connotation lexicons <ref type="bibr" target="#b11">(Feng et al., 2011;</ref><ref type="bibr" target="#b15">Kang et al., 2014</ref>) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example. Consider the following example: perpetrate: S: (v) perpetrate, commit, pull (perform an act, usually with a negative connota- tion) "perpetrate a crime"; "pull a bank robbery"</p><p>This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet. However, it has a positive effect on the object, a crime, since performing a crime brings it into existence.</p><p>As we mentioned, the +/-effect ambiguity can- not be avoided in a word-level lexicon. In the +/-effect corpus of <ref type="bibr" target="#b7">Deng et al. (2013)</ref>, 5 +/-effect events and their agents and objects are annotated at the word level. In that corpus, 1,411 +/-effect in- stances are annotated; 196 different +effect words and 286 different -effect words appear in these instances. Among them, 10 words appear in both +effect and -effect instances, accounting for 9.07% of all annotated instances. They show that +/-effect events (and the inferences that motivate this work) appear frequently in sentences with ex- plicit sentiment. Further, all instances of +/-effect words that are not identified as +/-effect events are false hits from the perspective of a recognition sys- tem.</p><p>The following is an example of a word with senses of different classes: purge: S: (v) purge (oust politically) "Deng Xiao Ping was purged several times throughout his lifetime" -effect S: (v) purge (clear of a charge) +effect S: (v) purify, purge, sanctify (make pure or free from sin or guilt) "he left the monastery purified" +effect S: (v) purge (rid of impurities) "purge the water"; "purge your mind" +effect This is part of the WordNet output for the word purge. In the first sense, the polarity is -effect since it has a negative effect on the object, Deng Xizo Ping. However, the other cases have positive effect on the object. Moreover, although a word may not have both +effect and -effect senses, it may have mixtures of ((+effect or -effect) and Null). A purely word-based approach is blind to these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Lexicons are widely used in sentiment analysis and opinion mining. Several works such as <ref type="bibr" target="#b14">Hatzivassiloglou and McKeown (1997)</ref>, <ref type="bibr" target="#b24">Turney and Littman (2003)</ref>, <ref type="bibr" target="#b16">Kim and Hovy (2004)</ref>, <ref type="bibr" target="#b21">Strapparava and Valitutti (2004)</ref>, and <ref type="bibr" target="#b18">Peng and Park (2011)</ref> have tackled automatic lexicon expansion or ac- quistion. However, in most such work, the lexi- cons are word-level rather than sense-level. <ref type="bibr">5</ref> Called the goodFor/badFor corpus in that paper. For the related (but different) tasks of de- veloping subjectivity, sentiment and connota- tion lexicons, some do take a sense-level ap- proach. <ref type="bibr" target="#b9">Esuli and Sebastiani (2006)</ref> construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses using WordNet lexical relations such as also-see and direct antonymy and train two clas- sifiers, one for positive and another for negative. As features, a vectorial representation of glosses is adopted. These classifiers were applied to all WordNet senses to measure positive, negative, and objective scores. In extending their work <ref type="bibr" target="#b10">(Esuli and Sebastiani, 2007)</ref>, the PageRank algorithm is applied to rank senses in terms of how strongly they are positive or negative. In the graph, each sense is one node, and two nodes are connected when they contain the same words in their Word- Net glosses. Moreover, a random-walk step is adopted to refine the scores in their recent work ( <ref type="bibr" target="#b4">Baccianella et al., 2010)</ref>. In contrast, our ap- proach uses WordNet relations and graph propa- gation in addition to gloss classification. <ref type="bibr" target="#b13">Gyamfi et al. (2009)</ref> construct a classifier to la- bel the subjectivity of word senses. The hierarchi- cal structure and domain information in WordNet are exploited to define features in terms of sim- ilarity (using the LCS metric in <ref type="bibr" target="#b19">Resnik (1995)</ref>) of target senses and a seed set of senses. Also, the similarity of glosses in WordNet is consid- ered. Even though they investigated the hierarchi- cal structure by LCS values, WordNet relations are not exploited directly. <ref type="bibr" target="#b22">Su and Markert (2009)</ref> adopt a semi-supervised mincut method to recognize the subjectivity of word senses. To construct a graph, each node cor- responds to one WordNet sense and is connected to two classification nodes (one for subjectivity and another for objectivity) via a weighted edge that is assigned by a classifier. For this classifier, WordNet glosses, relations, and monosemous features are considered. Also, several WordNet relations (e.g., antonymy, similiar-to, direct hypernym, etc.) are used to connect two nodes. Although they make use of both WordNet glosses and relations, and gloss information is utilized for a classifier, this classifier is generated only for weighting edges between sense nodes and classification nodes, not for classifying all senses. <ref type="bibr" target="#b15">Kang et al. (2014</ref>) present a unified model that assigns connotation polarities to both words and senses. They formulate the induction process as collective inference over pairwise-Markov Ran- dom Fields, and apply loopy belief propagation for inference. Their approach relies on selectional preferences of connotative predicates; the polarity of a connotation predicate suggests the polarity of its arguments. We have not discovered an analo- gous type of predicate for the problem we address. <ref type="bibr" target="#b12">Goyal et al. (2010)</ref> generate a lexicon of patient polarity verbs (PPVs) that impart positive or neg- ative states on their patients. They harvest PPVs from a Web corpus by co-occurance with Kind and Evil agents and by bootstrapping over conjunc- tions of verbs. <ref type="bibr" target="#b20">Riloff et al. (2013)</ref> learn positive sentiment phrases and negative situation phrases from a corpus of tweets with hashtag "sarcasm". However, both of these methods are word-level rather than sense-level.</p><p>Ours is the first NLP research into developing a sense-level lexicon for events that have negative or positive effects on entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">+/-Effect Word-Level Seed Lexicon and Sense Annotations</head><p>To create the corpus used in this work, we devel- oped a word-level seed lexicon, and then manually annotated all the senses of the words in that lexi- con. FrameNet 6 is based on a theory of meaning called Frame Semantics. In FrameNet, a Lexical Unit (LU) is a pairing of a word with a meaning, i.e., it corresponds to a sense in WordNet. Each LU of a polysemous word belongs to a different semantic frame, which is a description of a type of event, relation, or entity and, where appropri- ate, its participants. For instance, in the Creating frame, the definition is that a Cause leads to the formation of a Created entity. It has a positive effect on the object, Created entity. This frame contains about 10 LUs such as assemble, create, yield, and so on. FrameNet consists of about 1,000 semantic frames and about 10,000 LUs.</p><p>FrameNet is a useful resource to select +/-effect words since each semantic frame covers multi- ple LUs. We believe that using FrameNet to find +/-effect words is easier than finding +/-effect words without any information since words may be filtered by semantic frames. To select +/-effect words, an annotator (who is not a co-author) first identified promising frames as +/-effect and ex- tracted all LUs from them. Then, he went through them and picked out the LUs which he judged to be +effect or -effect. In total, 736 +effect LUs and 601 -effect LUs were selected from 463 semantic frames.</p><p>While <ref type="bibr" target="#b7">Deng et al. (2013)</ref> and  specifically focus on events affecting ob- jects (i.e., themes), we do not want to limit the lexicon to only that case. Sometimes, events have positive or negative effects on agents or other en- tities as well. Thus, in this paper, we consider a sense to be +effect (-effect) if it has +effect (-effect) on an entity, which may be the agent, the theme, or some other entity.</p><p>In a previous paper ), we con- ducted a study of the sense-level +/-effect prop- erty. For the evaluation, two annotators (who are co-authors of that paper) independently anno- tated senses of selected words, where some are from pure +effect (-effect) words (i.e., all senses of the words are classified into the same class) and some are from mixed words (i.e., the words have both +effect and -effect senses). In the agree- ment study, we calculated percent agreement and κ ( <ref type="bibr" target="#b3">Artstein and Poesio, 2008)</ref>, and achieved 0.84 percent agreement and 0.75 κ value.</p><p>For a seed set and an evaluation set in this pa- per, we need annotated sense-level +/-effect data. Mappings between FrameNet and WordNet are not perfect. Thus, we opted to manually anno- tate the senses of the words in the word-level lexi- con. We first extracted all words from 736 +effect LUs and 601 -effect LUs; this extracts 606 +effect words and 537 -effect words (the number of words is smaller than the number of LUs because one word can have more than one LU). Among them, 14 words (e.g., crush, order, etc.) are in both the +effect word set and the -effect word set. That is, these words have both +effect and -effect mean- ings. Recall that this annotator was focusing on frames, not on words -he did not look at all the senses of all the words. As we will see just below, when all the senses of all the words are annotated, a much higher percentage of the words have both +effect and -effect senses. We will also see that many of the senses are revealed to be Null, show- ing that +effect vs. Null and -effect vs. Null ambi- guities are quite prevalent.</p><p>A different annotator (a co-author) then went through all senses of all the words from the pre- vious step and manually annotated each sense as to whether it is +effect, -effect, or Null. Note that this annotator participated in an agreement study with positive results in .</p><p>For the experiments in this paper, we divided this annotated data into two equal-sized sets. One is a fixed test set that is used to evaluate both the graph model and the gloss classifier. The other set is used as a seed set by the graph model, and as a training set by the gloss classifer. <ref type="table">Table 1</ref> shows the distribution of the data. In total, there are 258 +effect senses, 487 -effect senses, and 880 Null senses. To avoid too large a bias toward the Null class, <ref type="bibr">7</ref> we randomly chose half (i.e., the Null set contains 440 senses). Half of each set is used as seed and training data, and the other half is used for evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+effect -effect Null</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Graph-based Semi-Supervised Learning for WordNet Relations</head><p>WordNet ( <ref type="bibr" target="#b17">Miller et al., 1990</ref>) is organized by se- mantic relations such as hypernymy, troponymy, grouping, and so on. These semantic relations can be used to build a network. Since the most fre- quently encoded relation is the super-subordinate relation, most verb senses are arranged into hi- erarchies; verb senses towards the bottom of the graph express increasingly specific manner. Thus, by following this hierarchical information, we hy- pothesized that +/-effect polarity tends to propa- gate. We use a graph-based semi-supervised learn- ing (GSSL) method to carry out the label propaga- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph Formulation</head><p>We formulate a graph for semi-supervised learning as follows. Let G = {X, E, W } be the undirected graph in which X is the set of nodes, E is the set of edges, and W represents the edge weights (i.e., the weight of edge E ij is W ij ). The weight matrix is a non-negative matrix. Each data point in X = {x 1 , ... ,x n } is one sense. The labeled data of X is represented as X L = {x 1 , ... ,x l } and the unlabeled data is rep- resented as X U = {x l+1 , ... ,x n }). The labeled data X L is associated with labels Y L = {y 1 , ... ,y l }, where y i ∈ {1, ..., c} (c is the number of classes). As is typical in such settings, l n: n is 13,767, i.e., the number of verb senses in WordNet. Seed/TrainSet in <ref type="table">Table 1</ref> is the labeled data.</p><p>To connect two nodes, WordNet relations are utilized. We first connect nodes by the hierar- chical relations. Since hypernym relations repre- sent more general senses and troponym relations represent more specific verb senses, we hypothe- sized that hypernyms or troponyms of a verb sense tends to have its same polarity. Verb groups rela- tions that represent verb senses having a similar meaning are also promising. Even though verb- group coverage is not large, its relations are reli- able since they are manually grouped. The entail- ment relation is defined as the verb Y is entailed by X if you must be doing Y by doing X. Since pairs connected by this relation are co-extensive, we can assume that both are the same type of event. The synonym relation is not used because it is already defined in senses (i.e., each node in the graph is a synset), and the antonym relation is also not applied since the weight matrix should be non-negative. The weight value of all edges is 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Label Propagation</head><p>Given a constructed graph, the label inference (or prediction) task is to propagate the seed labels to the unlabeled nodes. One of the classic GSSL la- bel propagation methods is the local and global consistency (LGC) method suggested by <ref type="bibr" target="#b26">Zhou et al. (2004)</ref>. The LGC method is a graph transduc- tion algorithm which is sufficiently smooth with respect to the intrinsic structure revealed by known labeled and unlabeled data. The cost function typ- ically involves a tradeoff between the smoothness of the predicted labels over the entire graph and the accuracy of the predicted labels in fitting the given labeled nodes X L .</p><p>LGC fits in a univariate regularization framework, where the output ma- trix is treated as the only variable in optimization, and the optimal solutions can be easily obtained by solving a linear system. Thus, we adopt the LGC method in this paper. Although there are some ro- bust GSSL methods for handling noisy labels, we do not need to handle noisy labels because our in- put is the annotated data.</p><p>Let F be a n × c matrix to save the output values of label propagation.</p><note type="other">So, we can label each instance x i as a label y i = argmax j≤c F ij after the label propagation. The initial discrete la- bel matrix Y , which is also n × c, is defined as Y ij = 1 if x i is labeled as y i = j in Y L , and Y ij = 0 otherwise. The vertex degree matrix D</note><formula xml:id="formula_0">= diag([D 11 , ..., Dnn]) is defined by D ii = n j=1 W ij .</formula><p>LGC defines the cost function Q which inte- grates two penalty components, global smooth- ness and local fitting (µ is the regularization pa- rameter):</p><formula xml:id="formula_1">Q = 1 2 n i=1 n j=1 W ij F i √ D ii − F j D jj 2 +µ n i=1 F i − Y i 2</formula><p>The first part of the cost function is the smoothness constraint: a good classifying func- tion should not change too much between nearby points. That is, if x i and x j are connected with an edge, the difference between them should be small. The second is the fitting constraint: a good classifying function should not change too much from the initial label assignment. The final label prediction matrix F can be obtained by minimiz- ing the cost function Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>Note that, in the rest of this paper, all tables except the last one give results on the same fixed test set (TestSet in <ref type="table">Table 1</ref>).</p><p>We can apply the graph model in two ways.</p><p>• UniGraph: All three classes (+effect, -effect, and Null) are represented in one graph.</p><p>• BiGraph: Two separate graphs are first con- structed and then combined. One graph is for classifying +effect and Other (i.e., -effect or Null). This graph is called +eGraph. The other graph, called -eGraph, is for classify- ing -effect and Other (i.e., +effect or Null).  <ref type="table">Table 2</ref>: Results of UniGraph, BiGraph, and Bi- Graph*.</p><p>These are combined into one model as fol- lows. Nodes that are labeled as +effect by +eGraph and Other by -eGraph are regarded as +effect, and nodes that are labeled as -effect by -eGraph and Other by +eGraph are regarded as -effect. If nodes are labeled as +effect by +eGraph and -effect by -eGraph, they are deemed to be Null. Nodes that are labeled Other by both graphs are also consid- ered as Null.</p><p>We had two motivations for experimenting with the BiGraph model: (1) SVM, the super- vised learning method used for gloss classifica- tion, tends to have better performance on binary classification tasks, and (2) the two graphs of the combined model can "negotiate" with each other via constraints.</p><p>In <ref type="table">Table 2</ref>, we calculate precision (P), recall (R), and f-measure (F) for all three classes. The base- line shown in the top row is the accuracy of a ma- jority class classifier. The first two columns of Ta- ble 2 show the results of UniGraph and BiGraph when they are built using the hypernym, troponym, and verb group relations. UniGraph outperforms BiGraph in this experiment.</p><p>To improve the results by performing some- thing possible with BiGraph (but not UniGraph), constraints are added when determining the class. As we explained, the label of instance x i is determined by F i in the graph. When the label of x i is decided to be j, we can say that its con- fidence value is F ij . There are two constraints as follows.</p><p>H+T +V +E +effect P 0.653 0.642 0.651 R 0.660 0.680 0.683 F 0.656 0.660 0.667 -effect P 0.784 0.779 0.786 R 0.547 0.612 0.604 F 0.644 0.686 0.683 Null P 0.557 0.583 0.564 R 0.735 0.695 0.691 F 0.634 0.634 0.621 <ref type="table">Table 3</ref>: Effect of each relation</p><p>• If a sense is labeled as +effect (-effect), but the confidence value is less than a threshold, we count it as Null.</p><p>• If a sense is labeled as both +effect and -effect by BiGraph, we choose the label with the higher confidence value only if the higher one is larger than a threshold and the lower one is less than a threshold.</p><p>The thresholds are determined on Seed/TrainSet by running BiGraph several times with different thresholds, and choosing the one that gives the best performance on Seed/TrainSet. (The chosen value is 0.025 for +effect and 0.03 for -effect).</p><p>As can be seen in <ref type="table">Table 2</ref>, BiGraph with con- straints (called BiGraph*) outperforms not only BiGraph without any constraints but also Uni- Graph. Especially, for BiGraph*, the recall of the Null class is considerably increased, showing that constraints not only help overall, but are particu- larly important for detecting Null cases. <ref type="table">Table 3</ref> gives ablation results, showing the con- tribution of each WordNet relation in BiGraph*. With only hierarchical information (i.e., hyper- nym (H) and troponym (T) relations), it already shows good performance for all classes. How- ever, they cannot cover some senses. Among the 13,767 verb senses in WordNet, 1,707 (12.4%) cannot be labeled because there are not sufficient hierarchical links to propagate polarity informa- tion. When adding the verb group (+V) rela- tion, it shows improvement in both +effect and -effect. Especially, the recall for +effect and -effect is significantly increased. In addition, the coverage of the 13,767 verb senses increases to 95.1%. For entailment (+E), whereas adding it shows a slight improvement in +effect (and in- creases coverage by 1.1 percentage points), the performance is decreased a little bit in the -effect and Null classes. Since the average f-measure for all classes is the highest with hypernym (H), tro- ponym (T), and verb group (V) relations (not en- tailment), we only consider these three relations when constructing the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supervised Learning applied to WordNet Glosses</head><p>In WordNet, each sense contains a gloss consist- ing of a definition and optional example sentences.</p><p>Since a gloss consists of several words and there are no direct links between glosses, we believe that a word vector representation is appropriate to uti- lize gloss information as in <ref type="bibr" target="#b9">Esuli and Sebastiani (2006)</ref>. For that, we adopt an SVM classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Features</head><p>Two different feature types are used. Word Features (WF): The bag-of-words model is applied. We do not ignore stop words for several reasons. Since most definitions and ex- amples are not long, each gloss contains a small number of words. Also, among them, the total vo- cabulary of WordNet glosses is not large. More- over, some prepositions such as against are some- times useful to determine the polarity (+effect or -effect).</p><p>Sentiment Features (SF): Some glosses of +effect (-effect) senses contain positive (negative) words. For instance, the definition of {hurt#4, injure#4} is "cause damage or affect negatively." It contains a negative word, negatively. Since a given event may positively (negatively) affect enti- ties, some definitions or examples already contain positive (negative) words to express this. Thus, as features, we check how many positive (negative) words a given gloss contains. To detect sentiment words, the subjectivity lexicon provided by <ref type="bibr" target="#b25">Wilson et al. (2005)</ref> 8 is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Gloss Classifier</head><p>We have three classes, +effect, -effect, and Null. Since SVM shows better performance on binary classification tasks, we generate two binary clas- sifiers, one (+eClassifier) to determine whether a given sense is +effect or Other, and another (-eClassifier) to classify whether a given sense is -effect or Other. Then, they are combined as in BiGraph.</p><p>8 Available at http://mpqa.cs.pitt.edu/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Results</head><p>Seed/TrainSet in <ref type="table">Table 1</ref> is used to train the two classifiers, and TestSet is utilized for the evalua- tion. So, the training set for +eClassifier consists of 129 +effect instances and 463 Other instances, and the training set for -eClassifier contains 243 -effect instances and 349 Other instances. As a baseline, we adopt a majority class classifier. <ref type="table" target="#tab_3">Table 4</ref> shows the results on TestSet. Perfor- mance is better for the -effect than for the +effect class, perhaps because the -effect class has more instances.</p><p>When sentiment features (SF) are added, all metric values increase, providing evidence that sentiment features are helpful to determine +/-effect classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WF WF+SF baseline accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Hybrid Method</head><p>To use more combined knowledge, the gloss clas- sifier and BiGraph* can be combined. That is, for WordNet gloss information, the gloss classifier is utilized, and for WordNet relations, BiGraph* is used. With the Hybrid method, we can see not only the effect of propagation by WordNet rela- tions but also the usefulness of gloss information and sentiment features. Also, while BiGraph* cannot cover all senses in WordNet, the Hybrid method can. The outputs of the gloss classifier and Bi- Graph* are combined as follows. The label of the gloss classifier is one of +effect, -effect, Null, or Both (when a given sense is classified as both +effect by +eClassifier and -effect by -eClassifier). Possible labels of BiGraph* are +effect, -effect, Null, Both, or None (when a given sense is not labeled by BiGraph*). There are five rules:</p><p>• If both labels are +effect (-effect), it is +effect (-effect).</p><p>• If one of them is Both and the other is +effect (-effect), it is +effect (-effect).</p><p>• If the label of BiGraph* is None, believe the label of the gloss classifier</p><p>• If both labels are Both, it is Null</p><formula xml:id="formula_2">• Otherwise, it is Null</formula><p>The results for Hybrid are given in the first row of the lower half of <ref type="table">Table 5</ref>; the results for BiGraph* are in the first row of the upper half, for comparison. Generally, the Hybrid method shows better performance than the gloss classifier and BiGraph*. In the Hybrid method, since more +/-effect senses are detected than by BiGraph*, while precision is decreased, recall is increased by more. However, by the same token, the over- all performance for the Null class is decreased. Actually, that is expected since the Null class is determined by the Other class in the gloss clas- sifier and BiGraph*. Through this experiment, we see that the Hybrid method is better for classifying +/-effect senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Model Comparison</head><p>To provide evidence for our assumption that dif- ferent models are needed for different information to maximize effectiveness, we compare the hy- brid method with the supervised learning and the graph-based learning (GSSL) methods, each uti- lizing both WordNet relations and gloss informa- tion.</p><p>Supervised Learning (onlySL): The gloss clas- sifier is trained with word features and sentiment features for WordNet Gloss. To exploit Word- Net relations in supervised learning, especially the hierarchical information, we use least com- mon subsumer (LCS) values as in <ref type="bibr" target="#b13">Gyamfi et al. (2009)</ref>, which, recall, performs supervised learn- ing of subjective/objective senses. The values are calculated as follows. For a target sense t and a seed set S, the maximum LCS value between a target sense and a member of the seed set is found as:</p><formula xml:id="formula_3">Score(t, S) = max s∈S LCS(t, s)</formula><p>With this LCS feature and the features described in Section 6, we run SVM on the same training and test data. For LCS values, the similarity using the information content proposed by <ref type="bibr" target="#b19">Resnik (1995)</ref> is measured. WordNet Similarity 9 package provides pre-computed pairwise similarity values for that. <ref type="table" target="#tab_5">Table 6</ref> shows results of onlySL. Compared to <ref type="table" target="#tab_3">Table 4</ref>, while +effect and Null classes show a slight improvement, the performance is degraded for -effect. This means that the added feature is rather harmful to -effect. Even though the hierar- chical feature is very helpful to expand +/-effect, it is not helpful for onlySL since SVM cannot cap- ture propagation according to the hierarchy.</p><p>Graph-based Learning (onlyGraph): In Sec- tion 5, the graph is constructed by using Word- Net relations. To apply WordNet gloss informa- tion in onlyGraph, we calculate a cosine similarity between glosses. If the similarity value is higher than a threshold, two nodes are connected with this similarity value. The threshold is determined by training and testing on Seed/TrainSet (the chosen value is 0.3).</p><p>Comparing <ref type="table" target="#tab_5">Tables 2 and 6</ref>, BiGraph* generally outperforms onlyGraph (the exception is precision of +effect). By gloss similarity, many nodes are connected to each other. However, since uncertain connections can cause incorrect propagation in the graph, this negatively affects the performance.</p><p>Through this experiment, we see that since each type of information has a different character, we need different models to maximize the effective- ness of each type. Thus, the hybrid method with different models can have better performance.  0.688 0.681 0.684 0.712 0.764 0.732 0.565 0.527 0.545 <ref type="table">Table 5</ref>: Results of an iterative approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Guided Annotation</head><p>Recall that Seed/TrainSet and TestSet, the data used so far, are all the senses of the words in a word-level +/-effect lexicon. This section presents evidence that our method can guide annotation ef- forts to find other words that have +/-effect senses. A bonus is that the method pinpoints particular +/-effect senses of those words. All unlabeled data are senses of words that are not included in the original lexicon. Since pre- sumably the majority of verbs do not have any +/-effect senses, a sense randomly selected from WordNet is very likely to be Null. We explore an iterative approach to guided annotation, using Bi- Graph* and Hybrid as the method for assigning labels.</p><p>The system is initially created as described above using Seed/TrainSet as the initial seed set. Each iteration has four steps: 1) rank all unlabeled data (i.e., the data other than TestSet and the cur- rent seed set) based on the F ij confidence values (see Section 5.3); 2) choose the top 5% and manu- ally annotate them (the same annotator as above did this); 3) add them to the seed set; 4) rerun the system using the expanded seed set. We per- formed four iterations in this paper.</p><p>The upper and lower parts of <ref type="table">Table 5</ref> show the intial results and the results after each iteration for BiGraph* and Hybrid. Recall that these are results on the fixed set, TestSet. Overall for both mod- els, f-measure increases for both the +effect and -effect classes as more seeds are added, mainly due to improvements in recall. The evaluation on the fixed set is also useful in the annotation process because it trades off +/-effect vs. Null annotations.</p><p>If the new manual annotations were biased, in that they incorrectly label Null senses as +/-effect, then the f-measure results would instead degrade on the fixed TestSet, since the system is created each time using the increased seed set.</p><p>We now consider the accuracy of the system on the newly labeled annotated data in Step 2. Note that our method is similar to Active Learn- ing <ref type="bibr" target="#b23">(Tong and Koller, 2001)</ref>, in that both auto- matically identify which unlabeled instances the human should annotate next. However, in active learning, the goal is to find instances that are diffi- cult for a supervised learning system. In our case, the goal is to find needles in the haystack of Word- Net senses. In Step 3, we add the newly labeled senses to the seed set, enabling the model to find unlabeled senses close to the new seeds when the system is rerun for the next iteration.</p><p>We assess the system's accuracy on the newly labeled data by comparing the system's labels with the human's new labels. Accuracy for +effect and -effect is calculated such as:</p><p>Accuracy +ef f ect = # annotated +effect # top 5% +effect data</p><p>Accuracy −ef f ect = # annotated -effect # top 5% -effect data That is, the accuracy means that out of the top 5% of the +effect (-effect) data as scored by the sys- tem, what percentage are correct as judged by a human annotator. <ref type="table">Table 7</ref> shows the accuracy for each iteration in the top part and the number of senses labeled in the bottom part. As can be seen, the accuracies range between 60% and 78%; these values are much higher than what would be ex- pected if labeling senses of words randomly cho- sen from WordNet. <ref type="bibr">10</ref> The annotator spent, on av- erage, approximately an hour to label 100 senses. For finding new words with +/-effect usages, it would be much more cost-effective if a significant percentage of the data chosen for annotation are senses of words that in fact have +/-effect senses.  <ref type="table">Table 7</ref>: Accuracy and frequency of the top 5% for each iteration</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>In this paper, we investigated methods for creat- ing a sense-level +/-effect lexicon. To maximize the effectiveness of each type of information, we combined a graph-based method using WordNet relations and a standard classifier using gloss in- formation. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual anno- tation to find +/-effect words that are not in the seed word-level lexicon. This is important, as the likelihood that a random WordNet sense (and thus word) is +effect or -effect is not large. So as not to limit the inferences that may be drawn, our annotations include events that are +effect or -effect either the agent or object. In fu- ture work, we plan to exploit corpus-based meth- ods using patterns as in <ref type="bibr" target="#b12">Goyal et al. (2010)</ref> com- bined with semantic role labeling to refine the lex- icon to distinguish which is the affected entity. Further, to actually exploit the acquired lexicon to process corpus data, an appropriate coarse-grained sense disambiguation process must be added, as  and <ref type="bibr" target="#b1">Akkaya et al. (2011)</ref> did for subjective/objective classification.</p><p>We hope the general methodology will be ef- fective for other semantic properties. In opin- ion mining and sentiment analysis this is partic-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Results of the gloss classifier.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison to onlySL and onlyGraph. </table></figure>

			<note place="foot" n="3"> Deng et al. (2013) point out that +/-effect objects are not equivalent to benefactive/malefactive semantic roles. An example they give is She baked a cake for me: a cake is the object of the +effect event baked as just noted, while me is the filler of its benefactive semantic role (Ziga and Kittil, 2010). 4 Their annotation manual, which gives additional cases, is available with the annotated data at http://mpqa.cs.pitt.edu/.</note>

			<note place="foot" n="6"> FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/</note>

			<note place="foot" n="7"> As mentioned in the introduction, we want our method to be able to identify unlabeled senses that are likely to be +/-effect senses (see Section 8); we resize the Null class to support this goal.</note>

			<note place="foot" n="9"> WordNet Similarity, http://wn-similarity.sourceforge.net/</note>

			<note place="foot" n="10"> For reference, in 5th iteration, the +effect accuracy is 60.18% and the-effect accuracy is 69.93%, and in 6th iteration, the +effect accuracy is 59.81% and the-effect accuracy is 69.12%. ularly needed, because different meanings of positive and negative are appropriate for different applications. This is a way to create lexicons that are customized with respect to one&apos;s own definitions. It would be promising to combine our method with other methods to enable it to find +effect and-effect senses that are outside the coverage of WordNet. However, a WordNet-based lexicon gives a substantial base to build from.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by DARPA-BAA-12-47 DEFT grant #12475008 and National Sci-ence Foundation grant #IIS-0916046. We would like to thank the reviewers for their helpful sug-gestions and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subjectivity word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2009</title>
		<meeting>EMNLP 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving the impact of subjectivity word sense disambiguation on contextual opinion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Verb classes as evaluativity functor classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranna</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Reschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interdisciplinary Workshop on Verbs. The Identification and Representation of Verb Features</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2200" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical acquisition for opinion inference: A sense-level lexicon of benefactive and malefactive events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)</title>
		<meeting>the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentiment propagation via implicature constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benefactive/malefactive event and writer attitude annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 51st ACL</title>
		<meeting>51st ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint inference and disambiguation of implicit sentiments via implicature constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">7988</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentiwordnet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th LREC</title>
		<meeting>5th LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pageranking wordnet synsets: An application to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="424" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning general connotation of words using graph-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1092" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically producing plot unit representations for narrative text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumeiii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrating knowledge for subjectivity sense labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaw</forename><surname>Gyamfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Akkaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT 2009</title>
		<meeting>NAACL HLT 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connotationwordnet: Learning connotation over the word+sense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Seok</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">15441554</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Determining the sentiment of opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Soo-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 20th COLING</title>
		<meeting>20th COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1367" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordnet: An on-line lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="312" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generate adjective sentiment dictionary for social media sentiment analysis using constrained nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dae Hoon</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using information content to evaluate semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th IJCAI</title>
		<meeting>14th IJCAI</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sarcasm as contrast between a positive sentiment and negative situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalindra De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="704" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet-affect: An affective extension of wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Valitutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 4th LREC</title>
		<meeting>4th LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1083" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subjectivity recognition on word senses via semi-supervised mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT 2009</title>
		<meeting>NAACL HLT 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector machin active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Measuring praise and criticism: Inference of semantic orientation from association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTEMNLP</title>
		<meeting>HLTEMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Navin</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Benefactives and malefactives, Typological perspectives and case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ziga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seppo</forename><surname>Kittil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>John Benjamins Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
