<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
							<email>noji@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science</orgName>
								<orgName type="department" key="dep2">Department of Computing Macquarie University</orgName>
								<orgName type="institution" key="instit1">Nara Institute of Science and Technology</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<email>yusuke@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science</orgName>
								<orgName type="department" key="dep2">Department of Computing Macquarie University</orgName>
								<orgName type="institution" key="instit1">Nara Institute of Science and Technology</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<email>mark.johnson@mq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science</orgName>
								<orgName type="department" key="dep2">Department of Computing Macquarie University</orgName>
								<orgName type="institution" key="instit1">Nara Institute of Science and Technology</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="33" to="43"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited center-embedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Manning , 2004). Cross-linguistic experiments on Universal Dependencies show that often our method boosts the performance from the base-line, and competes with the current state-of-the-art model in a number of languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human languages in the world are divergent, but they also exhibit many striking similarities <ref type="bibr" target="#b8">(Greenberg, 1963;</ref><ref type="bibr" target="#b9">Hawkins, 2014)</ref>. At the level of syn- tax, one attractive hypothesis for such regularities is that any grammars of languages have evolved un- der the pressures, or biases, to avoid structures that are difficult to process. For example it is known that many languages have a preference for shorter depen- dencies ( <ref type="bibr" target="#b6">Gildea and Temperley, 2010;</ref><ref type="bibr" target="#b3">Futrell et al., 2015)</ref>, which originates from the difficulty in pro- cessing longer dependencies <ref type="bibr" target="#b5">(Gibson, 2000)</ref>.</p><p>Such syntactic regularities can also be useful in applications, in particular in unsupervised ( <ref type="bibr" target="#b15">Klein and Manning, 2004;</ref><ref type="bibr">Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y, 2012;</ref><ref type="bibr">Bisk and Hockenmaier, 2013)</ref> or weakly- supervised ( <ref type="bibr" target="#b4">Garrette et al., 2015</ref>) grammar induc- tion tasks, where the models try to recover the syn- tactic structure of language without access to the syntactically annotated data, e.g., from raw or part- of-speech tagged text only. In these settings, find- ing better syntactic regularities universal across lan- guages is essential, as they work as a small cue to the correct linguistic structures. A preference ex- ploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM ( <ref type="bibr" target="#b15">Klein and Manning, 2004</ref>), or model parameters <ref type="bibr" target="#b25">(Smith and Eisner, 2006</ref>), and this has been the key to success of learning ( <ref type="bibr" target="#b7">Gimpel and Smith, 2012)</ref>.</p><p>In this paper, we explore the utility for another universal syntactic bias that has not yet been ex- ploited in grammar induction: a bias against center- embedding. Center-embedding is a syntactic con- struction on which a clause is embedded into another one. An example is "The reporter [who the senator [who Mary met] attacked] ignored the president.", where "who Mary met" is embedded in a larger relative clause. These constructions are known to cause memory overflow <ref type="bibr" target="#b17">(Miller and Chomsky, 1963;</ref><ref type="bibr" target="#b5">Gibson, 2000</ref>), and also are rarely observed cross- linguistically <ref type="bibr" target="#b14">(Karlsson, 2007;</ref><ref type="bibr" target="#b19">Noji and Miyao, 2014</ref>). Our learning method exploits this universal property of language. Intuitively during learning our models explore the restricted search space, which excludes linguistically implausible trees, i.e., those with deeper levels of center-embedding.</p><p>We describe how these constraints can be imposed in EM with the inside-outside algorithm. The central <ref type="figure">Figure 1</ref>: A set of transitions in left-corner parsing. The rules on the right side are the side conditions, in which P is the set of rules of a given CFG.</p><formula xml:id="formula_0">33 SHIFT σ d−1 a − → σ d−1 |A d A → a ∈ P SCAN σ d−1 |B/A d a − → σ d−1 |B d A → a ∈ P PRED σ d−1 |A d ε − → σ d−1 |B/C d B → A C ∈ P COMP σ d−1 |D/B d |A d+1 ε − → σ d−1 |D/C d B → A C ∈ P</formula><p>idea is to tabulate left-corner parsing, on which its stack depth captures the degree of center-embedding of a partial parse. Each chart item keeps the cur- rent stack depth and we discard all items where the depth exceeds some threshold. The technique is gen- eral and can be applicable to any model on PCFG; in this paper, specifically, we describe how to ap- ply the idea on the dependency model with valence (DMV) ( <ref type="bibr" target="#b15">Klein and Manning, 2004</ref>), a famous gen- erative model for dependency grammar induction. We focus our evaluation on grammar induction from part-of-speech tagged text, comparing the ef- fect of several biases including the one against longer dependencies. Our main empirical finding is that though two biases, avoiding center-embedding and favoring shorter dependencies, are conceptually similar (both favor simpler grammars), often they capture different aspects of syntax, leading to dif- ferent grammars. In particular our bias cooperates well with additional small syntactic cue such as the one that the sentence root tends to be a verb or a noun, with which our models compete with the strong baseline relying on a larger number of hand crafted rules on POS tags <ref type="bibr" target="#b18">(Naseem et al., 2010)</ref>.</p><p>Our contributions are: the idea to utilize left- corner parsing for a tool to constrain the models of syntax (Section 3), the formulation of this idea for DMV (Section 4), and cross-linguistic experiments across 25 languages to evaluate the universality of the proposed approach (Sections 5 and 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Left-corner parsing</head><p>We first describe (arc-eager) left-corner (LC) pars- ing as a push-down automaton (PDA), and then re- formulate it as a grammar transform. In previous work this algorithm has been called right-corner parsing (e.g., <ref type="bibr" target="#b22">Schuler et al. (2010)</ref>); we avoid this term and instead treat it as a variant of LC parsing following more recent studies, e.g., van Schijndel</p><formula xml:id="formula_1">D B i j A j + 1 k COMP = == ⇒ D B i j C A j + 1 k</formula><p>Figure 2: COMP combines two subtrees on the top of the stack. i, j, k are indices of spans.</p><p>and <ref type="bibr">Schuler (2013)</ref>. The central motivation for this technique is to detect center-embedding in a parse efficiently. We describe this mechanism after pro- viding the algorithm itself. We then give historical notes on LC parsing at the end of this section.</p><p>PDA Let us assume a CFG is given, and it is in CNF. We formulate LC parsing as a set of transi- tions between configurations, each of which is a pair of the stack and the input position (next input sym- bol). In <ref type="figure">Figure 1</ref> a transition σ 1 a − → σ 2 means that the stack is changed from σ 1 to σ 2 by reading the next input symbol a. We use a vertical bar to sig- nify the append operation, e.g., σ = σ |σ 1 denotes σ 1 is the topmost symbol of σ. Each stack symbol is either a nonterminal, or a pair of nonterminals, e.g., A/B, which represents a subtree rooted at A and is awaiting symbol B. We also decorate each symbol with depth; for example, σ d−1 |A d means the current stack depth is d, and the depth of the topmost sym- bol in σ is d − 1. The bottom symbol on the stack is always the empty symbol ε 0 with depth 0. Parsing begins with ε 0 . Given the start symbol of CFG S, it finishes when S 1 is found on the stack.</p><p>The key transition here is COMP <ref type="figure">(Figure 2</ref>). 1 Ba- sically the algorithm builds a tree by expanding the hypothesis from left to right. In COMP, a subtree rooted at A is combined with the second top subtree (D/B) on the stack. This can be done by first pre- dicting that A's parent symbol is B and its sibling is C; then it unifies two different Bs to combine them. PRED is simpler, and it just predicts the parent and sibling symbols of A. The input symbols are read by SHIFT and SCAN: SHIFT addes a new element on the stack while SCAN fills in the predicted sib- ling symbol. For an example, <ref type="figure">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>shows how</head><p>Step Transition Stack</p><formula xml:id="formula_2">Next input symbol 0 ε e 1 SHIFT E 1 f 2 PRED D/B 1 f 3 SHIFT D/B 1 F 2 g 4 PRED D/B 1 A/G 2 g 5 SCAN D/B 1 A 2 c 6 COMP D/C 1 c 7 SCAN D 1</formula><p>Figure 3: Sequence of transitions in LC PDA to parse the tree in <ref type="figure" target="#fig_0">Figure 4(a)</ref>. this PDA works for parsing a tree in <ref type="figure" target="#fig_0">Figure 4</ref>(a).</p><formula xml:id="formula_3">D B C c A G g F f E e (a) D 1 c D/C 1 A 2 g A/G 2 F 2 f D/B 1 E 1 e (b)</formula><p>Grammar transform The algorithm above can be reformulated as a grammar transform, which be- comes the starting point for our application to gram- mar induction. This can be done by extracting the operated top symbols on the stack in each transition:</p><formula xml:id="formula_4">SHIFT: A d → a (A → a ∈ P ); SCAN: B d → B/A d a (A → a ∈ P ); PRED: B/C d → A d (B → A C ∈ P ); COMP: D/C d → D/B d A d+1 (B → A C ∈ P ).</formula><p>where a rule on the right side is a condition given the set of rules P in the CFG. <ref type="figure" target="#fig_0">Figure 4</ref> shows an example of this transform. The essential point is that each CFG rule in the trans- formed parse (b) corresponds to a transition in the original algorithm <ref type="figure">(Figure 1</ref>). For example a rule D/C 1 → D/B 1 A 2 in the parse indicates that the stack configuration D/B 1 |A 2 occurs during parsing (just corresponding to the step 5 in <ref type="figure">Figure 3</ref>) and COMP is then applied. This can also be seen as an instantiation of <ref type="figure">Figure 2</ref>.</p><p>Stack depth and center-embedding We use the term center-embedding to distinguish just the tree structures, i.e., ignoring symbols. That is, the tree in <ref type="figure" target="#fig_0">Figure 4(a)</ref> is the minimal, one degree of center- embedded tree, where the constituent rooted at A is embedded into a larger constituent rooted at D. Multiple, or degree ≥ 2 of center-embedding oc- curs if this constituent is also embedded into another larger constituent.</p><p>Note that it is only COMP that consumes the top two symbols on the stack. This means that a larger stack depth occurs only when COMP is needed. Fur- thermore, from <ref type="figure">Figure 2</ref> COMP always induces a subtree involving new center-embedding, and this is the underlying mechanism that the stack depth of the algorithm captures the degree of center-embedding.</p><p>One thing to note is that to precisely associate the stack depth and the degree of center-embedding the depth calculation in COMP should be revised as:</p><formula xml:id="formula_5">COMP: D/C d → D/B d A d (B → A C ∈ P ) d = d (SPANLEN(A) = 1) d + 1 (otherwise),<label>(1)</label></formula><p>where SPANLEN(A) calculates the span length of the constituent rooted at A, which is 2 in <ref type="figure" target="#fig_0">Figure 4</ref>(b). This modification is necessary since COMP for a sin- gle token occurs for building purely right-branching structures. <ref type="bibr">2</ref> Formally, then, given a tree with de- gree λ of center-embedding the largest stack depth d * during parsing this tree is: d * = λ + 1. <ref type="bibr" target="#b22">Schuler et al. (2010)</ref> found that on English tree- banks larger stack depth such as 3 or 4 rarely oc- curs while <ref type="bibr" target="#b19">Noji and Miyao (2014)</ref> validated the lan- guage universality of this observation through cross- linguistic experiments. These suggest we may uti- lize LC parsing as a tool for exploiting universal syn- tactic biases as we discuss in Section 3.</p><p>Historical notes Rosenkrantz and <ref type="bibr" target="#b21">Lewis (1970)</ref> first presented the idea of LC parsing as a gram- mar transform. This is arc-standard, and has no relevance to center-embedding; <ref type="bibr">Resnik (1992)</ref> and Johnson (1998) formulated an arc-eager variant by extending this algorithm. The presented algorithm here is the same as <ref type="bibr" target="#b22">Schuler et al. (2010)</ref>, and is slightly different from Johnson (1998). The dif- ference is in the start and end conditions: while our parser begins with an empty symbol, Johnson's parser begins with the predicted start symbol, and finishes with an empty symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning with structural constraints</head><p>Now we discuss how to utilize LC parsing for gram- mar induction in general. An important observation in the above transform is that if we perform chart parsing, e.g., CKY, we can detect center-embedded trees efficiently in a chart. For example, by set- ting a threshold of stack depth δ, we can eliminate any parses involving center-embedding up to degree δ−1. Note that in a probabilistic setting, each weight of a transformed rule comes from the corresponding underlying CFG rule (i.e., the condition).</p><p>For learning, our goal is to estimate θ of a gen- erative model p(z, x|θ) for parse z and its yields (words) x. We take an EM-based simplest approach, and multiply the original model by a constraint fac- tor f (z, x) ∈ [0, 1] to obtain a new model:</p><formula xml:id="formula_6">p (z, x|θ) ∝ p(z, x|θ)f (z, x),<label>(2)</label></formula><p>and then optimize θ based on p (z, x|θ). This is essentially the same approach as <ref type="bibr" target="#b25">Smith and Eisner (2006)</ref>. As shown in <ref type="bibr" target="#b25">Smith (2006)</ref>, when training with EM we can increase the likelihood of p (z, x|θ) by just using the expected counts from an E-step on the unnormalized distribution p(z, x|θ)f (z, x). We investigate the following constraints in our ex- periments:</p><formula xml:id="formula_7">f (z, x) = 0 (d * z &gt; δ) 1 (otherwise),<label>(3)</label></formula><p>where d * z is the largest stack depth for z in LC pars- ing and δ is the threshold. This is a hard constraint, and can easily be achieved by removing all chart items (of LC transformed grammar) on which the depth of the symbol exceeds δ. For example, when δ = 1 the model only explores trees without center- embedding, i.e., right-or left-linear trees.</p><p>Length-based constraints By δ = 2, the model is allowed to explore trees with one degree of center- embedding. Besides these simple ones, we also in- vestigate relaxing δ = 1 that results in an intermedi- ate between δ = 1 and 2. Specifically, we relax the depth calculation in COMP (Eq. 1) as follows:</p><formula xml:id="formula_8">d = d (SPANLEN(A) ≤ ξ) d + 1 (otherwise),<label>(4)</label></formula><p>where ξ ≥ 1 controls the minimal length of a span regarded as embedded into another one. For exam- ple, when ξ = 2, the parse in <ref type="figure" target="#fig_0">Figure 4</ref>(a) is not re- garded as center-embedded because the span length of the constituent reduced by COMP (i.e., A) is 2. This modification is motivated with our observa- tion that in many cases center-embedded construc- tions arise due to embedding of small chunks, rather than clauses. An example is "... prepared [the cat 's] dinner", where "the cat 's" is center-embedded in our definition. For this sentence, by relaxing the condition with, e.g., ξ = 3, we can suppress the in- crease of stack depth. We treat ξ as a hyperparameter in our experiments, and in practice, we find that this relaxed constraint leads to higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dependency grammar induction</head><p>In this section we discuss how we can formulate the dependency model with valence (DMV) ( <ref type="bibr" target="#b15">Klein and Manning, 2004)</ref>, a famous generative model for dependency grammar induction, on LC parsing. Though as we will see, applying LC parsing for a de- pendency model is a little involved compared to sim- ple PCFG models, dependency models have been the central for the grammar induction tasks, and we consider it is most appropriate for assessing effec- tiveness of our approach.</p><p>DMV is a head-outward generative model of a dependency tree, controlled by two types of multi- nomial distributions. For stop ∈ {STOP, ¬STOP}, θ S (stop|h, dir, adj) is a Bernoulli random variable to decide whether or not to attach further dependents in dir ∈ {←, →} direction. The adjacency adj ∈ {TRUE, FALSE} is the key factor to distinguish the distributions of the first and the other dependents, which is TRUE if h has no dependent yet in dir di- rection. Another type of parameter is θ A (a|h, dir), a probability that h takes a as a dependent in dir di- rection.</p><p>For this particular model, we take the following approach to formulate it in LC parsing: 1) convert- ing a dependency tree into a binary CFG parse; 2) applying LC transform on it; and 3) encoding DMV parameters into each CFG rule of the transformed grammar. <ref type="bibr">3</ref> Below we discuss a problem for (1) and (2), and then consider parameterization. <ref type="bibr">4</ref> Spurious ambiguity The central issue for apply- ing LC parsing is the spurious ambiguity in depen- dency grammars. That is, there are more than one (binary) CFG parses corresponding to a given de- pendency tree. This is problematic mainly for two reasons: 1) we cannot specify the degree of center- embedding in a dependency tree uniquely; and 2) this one-to-many mapping prevents the inside- outside algorithm to work correctly <ref type="bibr" target="#b2">(Eisner, 2000)</ref>.</p><formula xml:id="formula_9">X[ran] X[fast] fast X[ran] X[ran] ran X[dogs] dogs (a) X[ran]1 fast X[ran/fast] 1 X[ran]1 ran X[ran/ran]1 X[dogs]1 dogs (b) X[ran] X[ran] X[fast] fast X[ran] ran X[dogs] dogs (c) X[ran] 1 fast X[ran/fast] 1 X[ran] 1 ran X[ran/ran] 1 X[dogs] 1 dogs (d)</formula><p>As a concrete example, Figures 5(a) and 5(c) show two CFG parses corresponding to the depen- dency tree dogs ran fast. We approach this prob- lem by first providing a grammar transform, which generates all valid LC transformed parses (e.g., <ref type="figure" target="#fig_1">Fig- ures 5</ref>(b) and 5(d)) and then restricting the grammar <ref type="bibr">3</ref> Another approach might be just applying the technique in Section 3 to some PCFG that encodes DMV, e.g., <ref type="bibr" target="#b10">Headden III et al. (2009)</ref>. The problem with this approach, in particular with split-head grammars <ref type="bibr" target="#b13">(Johnson, 2007)</ref>, is that the calculated stack depth no longer reflects the degree of center-embedding in the original parse correctly. As we discuss later, instead, we can speed up inference by applying head-splitting after obtaining the LC transformed grammar. <ref type="bibr">4</ref> Technical details including the chart algorithm for split- head grammars can be found in the Ph.D. thesis of the first au- thor <ref type="bibr" target="#b20">(Noji, 2016</ref>). <ref type="figure">Figure 6</ref>: The senses of the symbols as a chart item. X[w h /w p ] predicts the next dependent outside of the span while X[w p /w p ] predicts the head. Figure 7: Implicit binarization of the restricted grammar. For each token, if its parent is in the right side (e.g., b), it attaches all left children first. The be- havior is opposed when the parent is in its left (e.g., d). A dummy root token is placed at the end. for generating particular parses only.</p><note type="other">X[w h ] i h j X[w h /wp] i h j p X[wp/wp] i j p</note><p>Naive method Let us begin with the grammar be- low, which suffers from the spurious ambiguity:</p><formula xml:id="formula_10">SHIFT: X[w h ] d → w h SCAN: X[w h ] d → X[w h /w p ] d w p L-PRED: X[w p /w p ] d → X[w h ] d (w h w p ); R-PRED: X[w h /w p ] d → X[w h ] d (w h w p ); L-COMP: X[w h /w p ] d → X[w h /w p ] d X[w a ] d (w a w p ); R-COMP: X[w h /w a ] d → X[w h /w p ] d X[w p ] d (w p w a ). Here X[a/b] denotes X[a]/X[b]</formula><p>while w h denotes the h-th word in the sentence w. We can interpret these rules as the operations on chart items ( <ref type="figure">Figure  6</ref>). Note that only PRED and COMP create new de- pendency arcs and we divide them depending on the direction of the created arcs (L and R). d is calcu- lated by Eq. 4. Note also that for L-COMP and R- COMP h might equal p; <ref type="figure" target="#fig_1">Figure 5</ref>(d) is such a case for R-COMP.</p><formula xml:id="formula_11">X[ran/fast] 1 → X[ran/ran] 1 X[ran] 2 in</formula><p>Removing spurious ambiguity We can show that by restricting conditions for some rules, the spurious ambiguity can be eliminated (the proof is omitted).</p><p>1. Prohibit R-COMP when h = p;</p><formula xml:id="formula_12">2. Assume the span of X[w p ] d is (i, j) (i ≤ p ≤ j)</formula><p>. Then allow R-COMP only when i = p.</p><p>Intuitively, these conditions constraint the order that each word collects its left and right children. For example, by the condition 1, this grammar is pro- hibited to generate the parse of <ref type="figure" target="#fig_1">Figure 5</ref>(d).</p><p>Binarization Note that two CFG parses in <ref type="figure" target="#fig_1">Fig- ures 5</ref>(a) and 5(c) differ in how we binarize a given dependency tree. This observation indicates that our restricted grammar implicitly binarizes a depen- dency tree, and the incurred stack depth (or the de- gree of center-embedding) is determined based on the structure of the binarized tree. Specifically, we can show that the presented grammar performs op- timal binarization; i.e., it minimizes the incurred stack depth. <ref type="figure">Figure 7</ref> shows an example, which is not regarded as center-embedded in our procedure. In summary, our method detects center-embedding for a dependency tree, but the degree is determined based on the structure of the binarized CFG parse.</p><p>Parameterization We can encode DMV parame- ters into each rule. A new arc is introduced by one of {L/R}-{PRED/COMP}, and the stop probabilities can be assigned appropriately in each rule by cal- culating the valence from indices in the rule. For example, after L-PRED, w h does not take any right dependents so θ S (stop|w h , →, h = j), where j is the right span index of X[w h ], is multiplied.</p><p>Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n 6 ) to O(n 4 ) applying the technique simi- lar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We im- plemented this improved grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head><p>A sound evaluation metric in grammar induction is known as an open problem ( <ref type="bibr" target="#b23">Schwartz et al., 2011;</ref><ref type="bibr">Bisk and Hockenmaier, 2013)</ref>, which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks <ref type="bibr" target="#b12">(Johansson and Nugues, 2007</ref> The problem is that both trees are correct under some linguistic theories but the standard metric, un- labeled attachment score (UAS), only takes into ac- count the annotation of the current gold data. Our goal in this experiment is to assess the ef- fect of our structural constraints. To this end, we try to eliminate such arbitrariness in our evaluation as much as possible in the following way:</p><p>• We experiment on UD, in which every treebank follows the consistent UD style annotation.</p><p>• We restrict the model to explore only trees that follow the UD style annotation during learn- ing <ref type="bibr">5</ref> , by prohibiting every function word 6 in a sentence to have any dependents.</p><p>• We calculate UAS in a standard way.</p><p>We use UD of version 1.2. Some treebanks are very small, so we select the top 25 largest languages.</p><p>The input to the model is coarse universal POS tags. Punctuations are stripped off. All models are trained on sentences of length ≤ 15 and tested on ≤ 40.</p><p>Initialization Much previous work of dependency grammar induction relies on the technique called harmonic initialization, which also biases the model towards shorter dependencies ( <ref type="bibr" target="#b15">Klein and Manning, 2004</ref>). Since our focus is to see the effect of struc- tural constraints, we do not try this and initialize models uniformly. However, we add a baseline model with this initialization in our comparison to see the relative strength of our approach.</p><p>Models For the baseline, we employ a variant of DMV with features ( <ref type="bibr">Berg-Kirkpatrick et al., 2010)</ref>, which is simple yet known to boost the performance well. The feature templates are almost the same; the only change is that we add backoff features for STOP probabilities that ignore both direction and ad- jacency, which we found slightly improves the per- formance in a preliminary experiment. We set the regularization parameter to 10 though in practice we found the model is less sensitive to this value. We run 100 iterations of EM for each setting. The dif-ference of each model is then the type of constraints imposed during the E-step 7 , or initialization:</p><p>• Baseline (FUNC): Function word constraints;</p><p>• HARM: FUNC with harmonic initialization;</p><p>• DEP: FUNC + stack depth constraints (Eq. 3);</p><p>• LEN: FUNC + soft dependency length bias, which we describe below.</p><p>For DEP, we use δ = 1.ξ to denote the relaxed max- imum depth allowing span length up to ξ <ref type="figure" target="#fig_0">(Eq. 4)</ref>. LEN is the previously explored structural bias ( <ref type="bibr" target="#b25">Smith and Eisner, 2006</ref>), which penalizes longer dependencies by modifying each attachment score:</p><formula xml:id="formula_13">θ A (a|h, dir) = θ A (a|h, dir) · e −γ·(|h−a|−1) ,<label>(5)</label></formula><p>where γ (≥ 0) determines the strength of the bias and |h − a| is (string) distance between h and a.</p><p>Note that DEP and LEN are closely related; gen- erally center-embedded constructions are accompa- nied by longer dependencies so LEN also penalizes center-embedding implicitly. However, the opposite is not true and there exist many constructions with longer dependencies without center-embedding. By comparing these two settings, we discuss the worth of focusing on constraining center-embedding rela- tive to the simpler bias on dependency length.</p><p>Finally we also add the system of <ref type="bibr" target="#b18">Naseem et al. (2010)</ref> in our comparison. This system encodes many manually crafted rules between POS tags with the posterior regularization technique. For example, the model is encouraged to find NOUN → ADJ re- lationship. Our systems cannot access to these core grammatical rules so it is our strongest baseline. 8</p><p>Constraining root word We also see the effects of the constraints when a small amount of grammat- ical rule is provided. In particular, we restrict the candidate root words of the sentence to a noun or a verb; similar rules have been encoded in past work such as <ref type="bibr" target="#b7">Gimpel and Smith (2012)</ref> and the CCG in- duction system of <ref type="bibr">Bisk and Hockenmaier (2013)</ref>. Hyperparameters Selecting hyperparameters in multilingual grammar induction is difficult; some works tune values for each language based on the development set ( <ref type="bibr" target="#b25">Smith and Eisner, 2006;</ref><ref type="bibr">Bisk et al., 2015)</ref>, but this violates the assumption of unsu- pervised learning. We instead follow many works <ref type="bibr">(Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y, 2012;</ref><ref type="bibr" target="#b18">Naseem et al., 2010)</ref> and select the values with the English data. For this, we use the WSJ data, which we obtain in UD style from the Stanford CoreNLP (ver. 3.6.0). <ref type="bibr">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>WSJ <ref type="figure" target="#fig_3">Figure 8</ref> shows the result on WSJ. Both DEP and LEN have one parameter: the maximum depth δ, and γ (Eq. 5), and the figure shows the sensitivity on them. Note that x-axis = 0 represents FUNC.</p><p>For LEN, we can see the optimal parameter γ is 0.1, and degrades the performance when increasing the value; i.e., the small bias is the best. For DEP, we find the best setting is 1.3, i.e., allowing embedded constituents of length 3 or less (ξ = 3 in Eq. 4). We can see that allowing depth 2 degrades the perfor- mance, indicating that depth 2 allows too many trees and does not reduce the search space effectively. <ref type="bibr">10</ref> Multilingual results <ref type="table" target="#tab_2">Table 1</ref> shows the main mul- tilingual results. When we see "No root constraint" block, we notice that our DEP boosts the perfor- mance in many languages (e.g., Bulgarian, French,  We then move on to the settings with the con- straint on root tags. Interestingly, in these settings DEP performs the best. The model competes with Naseem et al.'s system in average, and outperforms it in many languages, e.g., Bulgarian, Czech, etc. LEN, on the other hand, decreases the average score.</p><p>Analysis Why does DEP perform well in particu- lar with the restriction on root candidates? To shed light on this, we inspected the output parses of En- glish with no root constraints, and found that the types of errors are very different across constraints. <ref type="figure">Figure 9</ref> shows a typical example of the differ- ence. One difference between trees is in the con- structions of phrase "On ... pictures". LEN pre- dicts that "On the next two" comprises a constituent, which modifies "pictures" while DEP predicts that "the ... pictures" comprises a constituent, which is correct, although the head of the determiner is in- correctly predicted. On the other hand, LEN works well to find more primitive dependency arcs between POS tags, such as arcs from verbs to nouns, which are often incorrectly recognized by DEP.</p><p>These observations may partially answer the  question above. The main source of improvements by DEP is detections of constituents, but this con- straint itself does not help to resolve some core dependency relationships, e.g., arcs from verbs to nouns. The constraint on root POS tags is thus or- thogonal to this approach, and it may help to find such core dependencies. On the other hand, the de- pendency length bias is the most effective to find basic dependency relationships between POS tags while the resulting tree may involve implausible constituents. Thus the effect of the length bias seems somewhat overlapped with the root POS constraints, which may be the reason why they do not well col- laborate with each other.</p><p>Bracket scores We verify the above intuition quantitatively. To this end, we convert both the pre- dicted and gold dependency trees into the unlabeled bracket structures, and then compare them on the standard PARSEVAL metrics. This bracket tree is not binarized; for example, we extract (X a b (X c d)) from the tree a b c d.  is in Enlgish the bracket and dependency scores are only loosely correlated. In <ref type="table" target="#tab_2">Table 1</ref>, UASs for FUNC, DEP, and LEN are 37.2, 39.8, and 52.1, respectively, though F1 of DEP is substantially higher. This sug- gests that DEP often finds more linguistically plausi- ble structures even when the improvement in UAS is modest. We conjecture that this performance change between constraints essentially arise due to the na- ture of DEP, which eliminates center-embedding, i.e., implausible constituent structures, rather than dependency arcs.</p><p>Combining DEP and LEN These results suggest DEP and LEN capture different aspects of syntax. To furuther understand this difference, we now evaluate the models with both constraints. <ref type="table" target="#tab_6">Table 3</ref> shows the average scores across languages (without root con- straints). Interestingly, the combination (DEP+LEN) performs the best in UAS while the worst in bracket F1. This indicates the ability of DEP to find good constituent boundaries is diminished by combining LEN. We feel the results are expected observing that center-embedded constructions are a special case of longer dependency constructions. In other words, LEN is a stronger constraint than DEP in that the structures penalized by DEP are only a subset of structures penalized by LEN. Thus when LEN and DEP are combined LEN overwhelms, and the ad- vantage of DEP is weakened. This also suggests not penalizing all longer dependencies is important for learning accurate grammars. The improvement of UAS suggests there are also collaborative effects in some aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have shown that a syntactic constraint that elim- inates center-embedding is helpful in dependency grammar induction. In particular, we found that our method facilitates to find linguistically correct constituent structures, and given an additional cue on dependency, the models compete with the sys- tem relying on a significant amount of prior lin- guistic knowledge. Future work includes applying our DEP constraint into other PCFG-based gram- mar induction tasks beyond dependency grammars. In particular, it would be fruitful to apply our idea into constituent structure induction for which, to our knowledge, there has been no successful PCFG- based learning algorithm. As discussed in de Mar- cken (1999) one reason for the failures of previous work is the lack of necessary syntactic biases, and our approach could be useful to alleviate this issue. Finally, though we have focused on unsupervised learning for simplicity, we believe our syntactic bias also leads to better learning in more practical scenar- ios, e.g., weakly supervised learning ( <ref type="bibr" target="#b4">Garrette et al., 2015</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of LC transform: (a) the original parse; and (b) the transformed parse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two CFG parses for "dogs ran fast" and the results of LC transform ((a) → (b); (c) → (d)). X[a/b] is an abbreviation for X[a]/X[b].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: UAS for various settings on (UD) WSJ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) :</head><label>:</label><figDesc></figDesc><table>Ivan 
is 
the 
best 
dancer 

nsbj cop 

det amod 

sbj 
nmod 
nmod 

prd 

UD 

CONLL 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Attachment scores on UD with or without 
root POS constraints. A-Greek = Ancient Greek. 
N10 = Naseem et al. (2010) with modified rules. 

Indonesian, and Portuguese), though LEN performs 
equally well and in average, LEN performs slightly 
better. Harmonic initialization does not work well. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Unlabeled bracket scores in various set-
tings. Avg. is the average score across languages. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 : Average scores of DEP, LEN, and the com- bination.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> van Schijndel and Schuler (2013) employ different transition names, e.g., L-and L+; we avoid them as they are less informative.</note>

			<note place="foot" n="2"> Schuler et al. (2010) skip this subtlety by only concerning stack depth after PRED or COMP. We do not take this approach since ours allows a flexible extension described in Section 3.</note>

			<note place="foot" n="5"> We remove the restriction at test time though we found it does not affect the performance. 6 A word with one of the following POS tags: ADP, AUX, CONJ, DET, PART, and SCONJ.</note>

			<note place="foot" n="7"> We again remove the restrictions at decoding as we observed that the effects are very small. 8 We encode the customized rules that follow UD scheme. The following 13 rules are used: ROOT → VERB, ROOT → NOUN, VERB → NOUN, VERB → ADV, VERB → VERB, VERB → AUX, NOUN → ADJ, NOUN → DET, NOUN → NUM, NOUN → NOUN, NOUN → CONJ, NOUN → ADP, ADJ → ADV.</note>

			<note place="foot" n="9"> Note that the English data in UD is Web Treebank (Silveira et al., 2014), not the standard WSJ Penn treebank. 10 We see the same effects when training with longer sentences (e.g., length ≤ 20). This is probably because a looser constraint does nothing for shorter sentences. In other words, the model can restrict the search space only for longer sentences, which are relatively small in the data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank John Pate for the help in preliminary work, as well as <ref type="table">Taylor</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Manzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evelyne</forename><surname>Tzoukermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing Using Very Large Corpora</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="191" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient parsing for bilexical context-free grammars and head automaton grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bilexical Grammars and Their Cubic-Time Parsing Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Probabilistic and Other Parsing Technologies</title>
		<editor>Harry Bunt and Anton Nijholt</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000-10" />
			<biblScope unit="page" from="29" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale evidence of dependency length minimization in 37 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="10336" to="10341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Weakly-supervised grammar-informed bayesian ccg parser learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The dependency locality theory: A distance-based theory of linguistic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image, language, brain: Papers from the first mind articulation project symposium</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="95" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do grammars minimize dependency length?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Temperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="310" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Concavity and initialization for unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="577" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some universals of grammar with particular reference to the order of meaningful elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">H</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Universals of Human Language</title>
		<editor>Joseph H. Greenberg</editor>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1963" />
			<biblScope unit="page" from="73" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cross-linguistic variatoin and efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hawkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-01" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finite-state approximation of constraint-based grammars using left-corner grammar transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NODALIDA 2007</title>
		<editor>Christian Boitet and Pete Whitelock</editor>
		<meeting>NODALIDA 2007<address><addrLine>Tartu, Estonia, May. Mark Johnson</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers / ACL</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="619" to="623" />
		</imprint>
	</monogr>
	<note>COLING-ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transforming projective bilexical dependency grammars into efficiently-parsable cfgs with unfold-fold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constraints on multiple centerembedding of clauses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="392" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting reducibility in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdeněkzdeněkˇzdeněkžabokrtsk´zdeněkžabokrtsk´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finitary models of language users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Psychology</title>
		<editor>D. Luce</editor>
		<imprint>
			<biblScope unit="page" from="2" to="419" />
			<date type="published" when="1963" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using universal linguistic knowledge to guide grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Left-corner transitions on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="2140" to="2150" />
		</imprint>
	</monogr>
	<note>Dublin City University and Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Left-corner Methods for Syntactic Modeling with Universal Structural Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<meeting><address><addrLine>Tokyo, Japan, March. Philip Resnik</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="191" to="197" />
		</imprint>
		<respStmt>
			<orgName>Graduate University for Advanced Studies</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Left-corner parsing and psychological plausibility</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deterministic left corner parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rosenkrantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Record of 11th Annual Symposium on</title>
		<imprint>
			<date type="published" when="1970-10" />
			<biblScope unit="page" from="139" to="152" />
		</imprint>
	</monogr>
	<note>Switching and Automata Theory</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Broad-coverage parsing using human-like memory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="663" to="672" />
		</imprint>
	</monogr>
	<note>Portland. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A gold standard dependency corpus for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Sydney; Baltimore, MD, October; Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Noah A. Smith</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="95" to="105" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
