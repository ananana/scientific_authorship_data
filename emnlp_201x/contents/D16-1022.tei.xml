<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annice</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets</title>
					</analytic>
					<monogr>
						<title level="j" type="main">RTI International</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas; USA 1</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="225" to="235"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
					<note>2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A core problem of opinion mining or sentiment anal- ysis is to identify each opinion/sentiment target and to classify the opinion/sentiment polarity on the tar- get ( <ref type="bibr" target="#b16">Liu, 2012)</ref>. For example, in a review sen- tence for a car, one wrote "Although the engine is slightly weak, this car is great." The person is posi- tive (opinion polarity) about the car (opinion target) as a whole, but slightly negative (opinion polarity) about the car's engine (opinion target).</p><p>Past research has proposed many techniques to extract opinion targets (we will just call them targets hereafter for simplicity) and also to classify senti- ment polarities on the targets. However, a target can be an entity or an aspect (part or attribute) of an en- tity. "Engine" in the above sentence is just one as- pect of the car, while "this car" refers to the whole car. Note that in <ref type="bibr" target="#b16">(Liu, 2012)</ref>, an entity is called a general aspect. For effective opinion mining, we need to classify whether a target is an entity or an as- pect because they refer to very different things. One can be positive about the whole entity (car) but neg- ative about some aspects of it (e.g., engine) and vice versa. This paper aims to perform the target classi- fication task, which, to our knowledge, has not been attempted before. Although in supervised extraction one can annotate entities and aspects with separate labels in the training data to build a model to extract them separately, in this paper our goal is to help un- supervised target extraction methods to classify tar- gets. Unsupervised target extraction methods are of- ten preferred because they save the time-consuming data labeling or annotation step for each domain.</p><p>Problem Statement: Given a set of opinion tar- gets T = {t 1 , . . . , t n } extracted from an opinion corpus d, we want to classify each target t i ∈ T into one of the three classes, entity, aspect, or NIL, which are called class labels. NIL means that the target is neither an entity nor an aspect and is used because target extraction algorithms can make mistakes. This paper does not propose a new target extrac- tion algorithm. We use an existing unsupervised method, called Double Propagation (DP) ( <ref type="bibr" target="#b25">Qiu et al., 2011</ref>), for extraction. We only focus on target clas- sification after the targets have been extracted. Note that an entity here can be a named entity, a prod-uct category, or an abstract product (e.g., "this ma- chine" and "this product"). An named entity can be the name of a brand, a model, or a manufacturer. An aspect is a part or attribute of an entity, e.g., "bat- tery" and "price" of the entity "camera".</p><p>Since our entities not just include the traditional named entities (e.g., "Microsoft" and "Google") but also other expressions that refer to such entities, tra- ditional named entity recognition algorithms are not sufficient. Pronouns such as "it," "they," etc., are not considered in this paper as co-reference resolution is out of the scope of this work.</p><p>We solve this problem in an unsupervised manner so that there is no need for labor-intensive manual labeling of the training data. One key observation of the problem is that although entities and aspects are different, they are closely related because aspects are parts or attributes of entities and they often have syn- tactic relationships in a sentence, e.g., "This phone's screen is super." Thus it is natural to solve the prob- lem using a relational learning method. We employ the graph labeling algorithm, Relaxation Labeling (RL) <ref type="bibr" target="#b8">(Hummel and Zucker, 1983)</ref>, which performs unsupervised belief propagation on a graph. In our case, each target extracted from the given corpus d forms a graph node and each relation identified in d between two targets forms an edge. With some initial probability assignments, RL can assign each target node the most probable class label. Although some other graph labeling methods can be applied as well, the key issue here is that just using a propa- gation method in isolation is far from sufficient due to lack of information from the given corpus, which we detail in Section 5. We then employ Lifelong Ma- chine Learning (LML) <ref type="bibr" target="#b29">(Thrun, 1998;</ref><ref type="bibr" target="#b3">Chen and Liu, 2014b</ref>) to make a major improvement.</p><p>LML works as follows: The learner has per- formed a number learning tasks in the past and has retained the knowledge gained so far. In the new/current task, it makes use of the past knowledge to help current learning and problem solving. Since RL is unsupervised, we can assume that the system has performed the same task on reviews of a large number of products/domains (or corpora). It has also saved all the graphs and classification results from those past domains in a Knowledge Base (KB). It then exploits this past knowledge to help clas- sification in the current task/domain. We call this combined approach of relaxation labeling and LML Lifelong-RL <ref type="table">. The approach is effective because there  is a significant amount of sharing of targets and tar- get relations across domains.</ref> LML is different from the classic learning paradigm (supervised or unsupervised) because classic learning has no memory. It basically runs a learning algorithm on a given data in isolation with- out considering any past learned knowledge <ref type="bibr" target="#b28">(Silver et al., 2013)</ref>. LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning.</p><p>Our experimental results show that the pro- posed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Although many target extraction methods exist ( <ref type="bibr" target="#b7">Hu and Liu, 2004;</ref><ref type="bibr" target="#b35">Zhuang et al., 2006;</ref><ref type="bibr" target="#b11">Ku et al., 2006</ref>; <ref type="bibr" target="#b30">Wang and Wang, 2008;</ref><ref type="bibr" target="#b32">Wu et al., 2009;</ref><ref type="bibr" target="#b13">Lin and He, 2009;</ref><ref type="bibr" target="#b33">Zhang et al., 2010;</ref><ref type="bibr" target="#b18">Mei et al., 2007;</ref><ref type="bibr" target="#b12">Li et al., 2010;</ref><ref type="bibr" target="#b0">Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b31">Wang et al., 2010;</ref><ref type="bibr" target="#b20">Mukherjee and Liu, 2012;</ref><ref type="bibr" target="#b6">Fang and Huang, 2012;</ref><ref type="bibr" target="#b34">Zhou et al., 2013;</ref><ref type="bibr" target="#b14">Liu et al., 2013;</ref><ref type="bibr" target="#b24">Poria et al., 2014</ref>), we are not aware of any attempt to solve the proposed problem. As mentioned in the in- troduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and labor- intensive to produce <ref type="bibr" target="#b9">(Jakob and Gurevych, 2010;</ref><ref type="bibr" target="#b5">Choi and Cardie, 2010;</ref><ref type="bibr" target="#b19">Mitchell et al., 2013)</ref>. Note that relaxation labeling was used for sentiment clas- sification in ( <ref type="bibr" target="#b23">Popescu and Etzioni, 2007)</ref>, but not for target classification. More details of opinion mining can be found in ( <ref type="bibr" target="#b16">Liu, 2012;</ref><ref type="bibr" target="#b22">Pang and Lee, 2008)</ref>.</p><p>Our work is related to transfer learning <ref type="bibr" target="#b21">(Pan and Yang, 2010)</ref>, which uses the source domain labeled data to help target domain learning, which has lit- tle or no labeled data. Our work is not just using a source domain to help a target domain. It is a continuous and cumulative learning process. Each new task can make use of the knowledge learned from all past tasks. Knowledge learned from the new task can also help improve learning of any past task. Transfer learning is not continuous, does not accumulate knowledge over time and cannot im- prove learning in the source domain. Our work is also related to multi-task learning <ref type="bibr" target="#b1">(Caruana, 1997)</ref>, which jointly optimizes a set of related learning tasks. Clearly, multi-task learning is different as we learn and save information which is more realistic when a large number of tasks are involved.</p><p>Our work is most related to Lifelong Machine Learning (LML). Traditional LML focuses on su- pervised learning <ref type="bibr" target="#b29">(Thrun, 1998;</ref><ref type="bibr" target="#b27">Ruvolo and Eaton, 2013;</ref><ref type="bibr" target="#b4">Chen et al., 2015)</ref>. Recent work used LML in topic modeling <ref type="bibr" target="#b2">(Chen and Liu, 2014a)</ref>, which is unsupervised. Basically, they used topics generated from past domains to help current domain model in- ference. However, they are just for aspect extrac- tion. So is the method in ( <ref type="bibr" target="#b15">Liu et al., 2016</ref>). They do not solve our problem. Their LML methods are also different from ours as we use a graph and results obtained in the past domains to augment the current task/domain graph to solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lifelong-RL: The General Framework</head><p>In this section, we present the proposed general framework of lifelong relaxation labeling (Lifelong- RL). We first give an overview of the relaxation la- beling algorithm, which forms the base. We then incorporate it with the LML capability. The next two sections detail how this general framework is applied to our proposed task of separating entities and aspects in opinion targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relaxation Labeling</head><p>Relaxation Labeling (RL) is an unsupervised graph- based label propagation algorithm that works iter- atively. The graph consists of nodes and edges. Each edge represents a binary relationship between two nodes. Each node t i in the graph is associated with a multinomial distribution P (L(t i )) (L(t i ) be- ing the label of t i ) on a label set Y . Each edge is associated with two conditional probability distri- butions P (L(t i )|L(t j )) and P (L(t j )|L(t i )), where P (L(t i )|L(t j )) represents how the label L(t j ) influ- ences the label L(t i ) and vice versa. The neighbors Ne(t i ) of a node t i are associated with a weight dis- tribution w(t j |t i ) with t j ∈Ne(t i ) w(t j |t i ) = 1. Given the initial values of these quantities as in- puts, RL iteratively updates the label distribution of each node until convergence. Initially, we have</p><formula xml:id="formula_0">P 0 (L(t i )). Let ∆P r+1 (L(t i )) be the change of P (L(t i )) at iteration r + 1. Given P r (L(t i )) at iter- ation r, ∆P r+1 (L(t i )) is computed by: ∆P r+1 (L(t i )) = t j ∈Ne(t i ) (w(t j |t i ) · y∈Y (P (L(t i )|L(t j ) = y)P r (L(t j ) = y)))</formula><p>(1) Then, the updated label distribution for iteration r + 1, P r+1 (L(t i )), is computed as follows:</p><formula xml:id="formula_1">P r+1 (L(t i )) = P r (L(t i ))(1+∆P r+1 (L(t i ))) y∈Y P r (L(t i )=y)(1+∆P r+1 (L(t i )=y)) (2)</formula><p>Once RL ends, the final label of node t i is its highest probable label:</p><formula xml:id="formula_2">L(t i ) = argmax y∈Y (P (L(t i ) = y)).</formula><p>Note that P (L(t i )|L(t j )) and w(t j |t i ) are not up- dated in each RL iteration but only P (L(t i )) is. P (L(t i )|L(t j )), w(t j |t i ) and P 0 (L(t i )) are pro- vided by the user or computed based on the appli- cation context. RL uses these values as input and iteratively updates P (L(t i )) based on Equations <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> until convergence. Next we discuss how to incorporate LML in RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lifelong Relaxation Labeling</head><p>For LML, it is assumed that at any time step, the system has worked on u past domain corpora D = {d 1 , . . . , d u }. For each past domain corpus d ∈ D, the same Lifelong-RL algorithm was applied and its results were saved in the Knowledge Base (KB). Then the algorithm can borrow some useful prior/past knowledge in the KB to help RL in the new/current domain d u+1 . Once the results of the current domain are produced, they are also added to the KB for future use.</p><p>We now detail the specific types of information or knowledge that can be obtained from the past do- mains to help RL in the future, which should thus be stored in the KB.</p><p>1. Prior edges: In many applications, the graph is not given. Instead, it has to be constructed based on the data from the new task/domain data d u+1 . However, due to the limited data in d u+1 , some edges between nodes that should be present are not extracted from the data. But such edges between the nodes may exist in some past domains. Then, those edges and their associated probabilities can be borrowed.</p><p>2. Prior labels: Some nodes in the current new domain may also exist in some past domains. Their labels in the past domains are very likely to be the same as those in the current domain. Then, those prior labels can give us a better idea about the initial label probability distributions of the nodes in the current domain d u+1 .</p><p>To leverage those edges and labels from the past domains, the system needs to ensure that they are likely to be correct and applicable to the current task domain. This is a challenge problem. In the next two sections, we detail how to ensure these to a large extent in our application context along with how to compute those initial probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Initialization of Relaxation Labeling</head><p>We now discuss how the proposed Lifelong-RL gen- eral framework is applied to solve our problem. In our case, each node in the graph is an extracted tar- get t i ∈ T , and each edge represents a binary re- lationship between two targets. T is the given set of all opinion targets extracted by an extraction al- gorithm from a review dataset/corpus d. The label set for each target is Y = {entity, aspect, NIL}. In this section, we describe how to use text clues in the corpus d to compute P (L(t i )|L(t j )), w(t j |t i ) and P 0 (L(t i )). In the next section, we present how these quantities are improved using prior knowledge from the past domains in the LML fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Clues for Initialization</head><p>We use two kinds of text clues, called type modifiers M (t) and relation modifiers M R to compute the ini- tial label distribution P (L(t i )) and conditional label distribution P (L(t i )|L(t j )) respectively.</p><p>Type Modifier: This has two kinds M T = {m E , m A }, where m E and m A represent entity modifier and aspect modifier respectively. For ex- ample, the word "this" as in "this camera is great" indicates that "camera" is probably an entity. Thus, "this" is a type modifier indicating M (camera) = m E . "These" is also a type modifier. Aspect mod- ifier is implicitly assumed when the number of ap- pearances of entity modifiers is less than or equal to a threshold (see Section 4.2).</p><p>Relation Modifier: Given two targets, t i and t j , we use M t j (t i ) to denote the relation modifier that the label of target t i is influenced by the label of tar- get t j . Relation modifiers are further divided into 3 kinds: M R = {m c , m A|E , m E|A }.</p><p>Conjunction modifier m c : Conjoined items are usually of the same type. For example, in "price and service", "and service" indicates a conjunction mod- ifier for "price" and vice versa.</p><p>Entity-aspect modifier m A|E : A possessive ex- pression indicates an entity and an aspect relation. For example, in "the camera's battery", "camera" in- dicates an entity-aspect modifier for "battery".</p><p>Aspect-entity modifier m E|A : Same as above ex- cept that "battery" indicates an aspect-entity modi- fier for "camera".</p><p>Modifier Extraction: These modifiers are iden- tified from the corpus d using three syntactic rules. "This" and "these" are used to extract type modifier</p><formula xml:id="formula_3">M (t) = m E . C m E (t)</formula><p>is the occurrence count of that modifier on target t, which is used in determin- ing the initial label distribution in Section 4.2.</p><p>Relation modifiers are identified by dependency relations conj(t i , t j ) and poss(t i , t j ) using the Stan- ford Parser ( <ref type="bibr" target="#b10">Klein and Manning, 2003)</ref>. Each oc- currence of a relation rule contributes one count of M t j (t i ) for t i and one count of M t i (t j ) for t j . We use C mc,t j (t i ), C m A|E ,t j (t i ) and C m E|A ,t j (t i ) to de- note the count of t j modifying t i with conjunction, entity-aspect and aspect-entity modifiers respec- tively. For example, "price and service" will con- tribute one count to C mc,price (service) and one count to C mc,service (price). Similarly, "camera's battery" will contribute one count to C m A|E ,camera (battery) and one count to C m E|A ,battery (camera).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computing Initial Probabilities</head><p>The initial label probability distribution of target t is computed based on C m E (t), i.e.,</p><formula xml:id="formula_4">P 0 (L(t)) = P m E (L(t)) if C m E (t) &gt; α P m A (L(t)) if C m E (t) ≤ α<label>(3)</label></formula><p>Here, we have two pre-defined distributions: P m E and P m A , which have a higher probability on entity and aspect respectively. The parameter α is a thresh- old indicating that if the entity modifier rarely oc- curs, the target is more likely to be an aspect. These values are set empirically (see Section 6).</p><p>Let term q(M t j (t i ) = m) be the normalized weight on the count for each kind of relation modi- fier m ∈ M R :</p><formula xml:id="formula_5">q(M t j (t i ) = m) = C m,t j (t i ) C t j (t i )<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">C t j (t i ) = m∈M R C m,t j (t i ). The conditional label distribution P (L(t i )|L(t j ))</formula><p>of t i given the label of t j is the weighted sum over the three kinds of relation modifiers:</p><formula xml:id="formula_7">P (L(t i )|L(t j )) = q(M t j (t i ) = m c ) · P mc (L(t i )|L(t j )) +q(M t j (t i ) = m A|E ) · P m A|E (L(t i )|L(t j )) +q(M t j (t i ) = m E|A ) · P m E|A (L(t i )|L(t j ))<label>(5)</label></formula><p>where P mc , P m A|E , and P m E|A are pre-defined con- ditional distributions. They are filled with values to model the label influence from neighbors and can be found in Section 6.</p><p>Finally, target t i 's neighbor weight for target t j , i.e., w(t j |t i ), is the ratio of the count of relation modifiers C t j (t i ) over the total of all t i 's neighbors:</p><formula xml:id="formula_8">w(t j |t i ) = C t j (t i ) t j ∈Ne(t i ) C t j (t i )<label>(6)</label></formula><p>If C t j (t i ) = 0, t i and t j has no edge between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Using Past Knowledge in Lifelong-RL</head><p>Due to the fact that the review corpus d u+1 in the current task domain may not be very large and that we use high quality syntactic rules to extract rela- tions to build the graph to ensure precision, the num- ber of relations extracted can be small and insuffi- cient to produce a graph that is information rich with accurate initial probabilities. We thus apply LML to help using knowledge learned in the past. The pro- posed LML process in Lifelong-RL for our task is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Our prior knowledge includes type modi- fiers, relation modifiers and labels of targets obtained from past domains in D.</p><p>Each record in the KB is stored as a 9-tuple:</p><formula xml:id="formula_9">(d, t i , t j , M d (t i ), M d (t j ), C d m,tj (t i ), C d m,ti (t j ), L d (t i ), L d (t j ))</formula><p>where d ∈ D is a past domain; t i and t j are two targets; M d (t i ), M d (t j ) are their type modifiers, C d m,t j (t i ) and C d m,t i (t j ) are counts for relation modifiers; L d (t i ) and L d (t j ) are labels decided by RL. For example, the sen- tence "This camera's battery is good" forms:</p><formula xml:id="formula_10">(d, camera, battery, m E , m A , C m E|A ,battery (camera) = 1,</formula><p>C m A|E ,camera (battery) = 1, entity, aspect) . It means that in the past domain d, "camera" and "battery" are extracted targets. Since "camera" is followed by "this", its type modifier is m E . Since "battery" is not identified by an entity modifier, it is m A . The pattern "camera's battery" contributes one count for both relation modifiers C m E|A ,battery (camera) and C m A|E ,camera (battery). RL has labeled "camera" as entity and "battery" as aspect in d.</p><p>The next two subsections present how to use the knowledge in the KB to improve the initial assign- ments for the label distributions, conditional label distributions and neighborhood weight distributions in order to achieve better final labeling/classification results for the current/new domain d u+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Exploiting Relation Modifiers in the KB</head><p>If two targets in the current domain corpus have no edge, we can check whether relation modifiers of the same two targets exist in some past domains. If so, we may be able to borrow them. But to ensure suit- ability, two consistency checks are performed.</p><p>Label Consistency Check: Since RL makes mis- takes, we need to ensure that relation modifiers in a record in the KB are consistent with target labels in that past domain. For example, "camera's battery" is confirmed by "camera" being labeled as entity and "battery" being labeled as aspect in a past domain d ∈ D. Without this consistency, the record may not be reliable and should be discarded from the KB.</p><p>We define an indicator variable I d m,t j (t i ) to en- sure that the record r's relation modifier is consistent with the labels of its two targets:</p><formula xml:id="formula_11">I d m A|E ,t j (t i ) =        1 if C d m A|E ,t j (t i ) &gt; 0 and L d (t i ) = aspect and L d (t j ) = entity 0 otherwise (7)</formula><p>For example, if "camera" is labeled as entity and "battery" is labeled as aspect in the past do- main d, we have I d m A|E ,camera (battery) = 1 and I d m E|A ,battery (camera) = 1.</p><p>Type Consistency Check: Here we ensure the type modifiers for two targets in the current domain d u+1 are consistent with these type modifiers in the past domain d ∈ D. This is because an item can be an aspect in one domain but an entity in another. For example, if the current domain is "Cellphone", bor- rowing the relation "camera's battery" from domain "Camera" can introduce an error because "camera" is an aspect in domain "Cellphone".</p><p>Syntactic pattern "this" is a good indicator for this checking. In the "Cellphone" domain, "its camera" or "the camera" are often mentioned but not "this camera". In the "Camera" domain, "this camera" is often mentioned. The type modifier of "camera" in "Cellphone" is m A , but in "Camera" it is m E .</p><p>Updating Probabilities in Current Domain d u+1 : Edges for RL are in the forms of conditional label distribution P (L(t i )|L(t j )) and neighborhood weight distribution w(t j |t i ). We now discuss how to use the KB to estimate them more accurately.</p><p>Updating Conditional Label Distribution: Equa- tion (5) tells that conditional label distribution P (L(t i )|L(t j )) is the weighted sum of relation mod- ifiers' label distributions P mc , P m A|E , and P m E|A . These 3 label distributions are pre-defined and given in <ref type="table" target="#tab_1">Table 2</ref>. They are not changed. Thus, we up- date conditional label distribution through updating the three relation modifiers' weights q(M t j (t i )) with the knowledge in the KB. Recall the three relation modifiers are M R = {m c , m A|E , m E|A }.</p><p>After consistency check, there can be multiple re- lation modifiers between two targets in similar past domains D s ⊂ D. The number of domains sup- porting a relation modifier m ∈ M R can tell which kind of relation modifiers is common and likely to be correct. For example, given many past domains like "Laptop", "Tablet", "Cellphone", etc., "camera and battery" appears more than "camera's battery", "camera" should be modified by "battery" more with m E|A rather than m c (likely to be an aspect).</p><p>Let C d u+1 m,t j (t i ) be the count that target t i modi- fied by target t j on relation m in the current domain d u+1 (not in KB). The count C (CL) is for updating the Conditional Label (CL) distributions consider- ing the information in both the current domain d u+1 and the KB. It is calculated as:</p><formula xml:id="formula_12">C (CL) m,tj (t i ) = C du+1 m,tj (t i ) if C du+1 m,tj (t i ) &gt; 0 d∈D s I d m,tj (t i ) if m∈M R C du+1 m,tj (t i )) = 0</formula><p>This equation says that if there is any relation mod- ifier existing between the two targets in the new domain d u+1 , we do not borrow edges from the KB; Otherwise, the number of similar past domains supporting the relation modifier m is used. Recall that I d m,t j (t i ) is the result calculated by Equation <ref type="formula">(7)</ref> after label consistency check.</p><p>We use count C (CL) m,t j (t i ) to update q d u+1 (M t j (t i )) using Equation <ref type="formula" target="#formula_5">(4)</ref> in Section 4.2. Then the con- ditional label distribution accommodating relation modifiers in the KB, P (LL1) (L(t i )|L(t j )), is calcu- lated by Equation, (5) using q d u+1 (M t j (t i )). LL1 denotes Lifelong Learning 1.</p><p>Updating Neighbor Weight Distribution: Equa- tion (6) says that w(t j |t i ) is the importance of target t i 's neighbor t j to t i among all t i 's neighbors. When updating conditional label distribution using the KB, the number of domains can decide which kind of re- lation modifiers m is more common between the two targets t i and t j . But we cannot tell that neighbor t j is more important than another neighbor t j to t i .</p><p>For example, given the past domains such as "Laptop", "Tablet", "Cellphone", etc., no matter how many domains believe "camera" is an aspect given "battery" is also an aspect, if the current do- main is "All-in-one desktop computer", we should not consider the strong influences from "battery" in the past domains. We should rely more on the weights of "camera"'s neighbors provided by "All- in-one desktop computer". That means "mouse", "keyboard", "screen" etc., should have strong influ- ences on "camera" than "battery" because most All- in-one desktops (e.g. iMac) do not have battery.</p><p>We The count C (w) t j (t i ) for updating the neighbor weight (w) distribution considers both the KB and the current domain d u+1 . It is as follows:</p><formula xml:id="formula_13">C (w) tj (t i ) = m∈M R C du+1 m,tj (t i ) if m∈M R C du+1 m,tj (t i ) &gt; 0 m∈M R I Du m,tj (t i ) if m∈M R C du+1 m,tj (t i ) = 0</formula><p>This equation tells that if there are relation modifiers existing between the two targets in the new domain d u+1 , we count the total times that t j modifies t i in the new domain; Otherwise, we count the total kinds of relation modifiers in M R if a relation modifier m ∈ M R existed in past domains. Let w (LL1) (t j |t i ) be the neighbor weight distribution considering knowledge from the KB and d u+1 . It is calculated by Equation (6) using C (w) t j (t i ). The initial label distribution P d u+1 ,0 is calculated by Equation (3) only using type modifiers found in the new domain d u+1 . We use Lifelong-RL-1 to de- note the method that employs P (LL1) (L(t i )|L(t j )), w (LL1) (t j |t i ) and P d u+1 ,0 as inputs for RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exploiting Target Labels in the KB</head><p>Since we have target labels from past domains, we may have a better idea about the initial label prob- abilities of targets in the current domain d u+1 . For example, after labeling domains like "Cellphone", "Laptop", "Tablet," and "E-reader", we may have a good sense that "camera" is likely to be an aspect. To use such knowledge, we need to check if the type modifier of target t in the current domain matches those in past domains and only keep those domains that have such a matching type modifier.</p><p>Let D s ⊂ D be the past domains consistent with target t's type modifier in the current domain d u+1 . Let C D s (L(t)) be the number of domains in D s that target t is labeled as L(t). Let λ be the ratio that controls how much we trust knowledge from the KB. Then the initial label probability distribu- tion P d u+1 ,0 calculated by Equation (3) only using type modifier found in d u+1 is replaced by :</p><formula xml:id="formula_14">P (LL2),0 (L(t)) = |D|×P d u+1 ,0 (L(t))+λC D s (L(t)) |D|+λ|D| (8)</formula><p>Similarly, let D s ⊂ D be the past domains con- sistent with both targets t i 's and t j 's type modifiers</p><formula xml:id="formula_15">in d u+1 . Let C D s (L(t i ), L(t j )</formula><p>) be the number of domains in D s that t i and t j are labeled as L(t i ) and L(t j ) respectively. The conditional label probabil- ity distribution accommodating relation modifiers in the KB, P (LL1) (L(t i )|L(t j )), is further updated to P (LL2) (L(t i )|L(t j )) by exploiting the target labels in KB (LL2 denotes Lifelong Learning 2):</p><formula xml:id="formula_16">P (LL2) (L(t i )|L(t j )) = |D|×P (LL1) (L(t i )|L(t j ))+λC D s (L(t i ),L(t j )) |D|+λ|D|<label>(9)</label></formula><p>For example, given "this camera", "battery" in the current domain, we are more likely to consider domains (e.g. "Film Camera", "DSLR", but not "Cellphone") that have entity modifiers on "camera" and aspect modifiers on "battery". Then we count the number of those domains that label "camera" as entity and "battery" as aspect: C D s (L(camera) = entity, L(battery) = aspect). Similarly, we count domains having other types of target labels on "cam- era" and "battery". These counts form an updated conditional label distribution that estimates "cam- era" as an entity and "battery" as an aspect.</p><p>Note that |D − D s |, the number of past do- mains not consistent with targets' type modifiers, <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_16">(9)</ref> respec- tively to make the sum over L(t i ) equal to 1. We use Lifelong-RL to denote this method which uses P (LL2),0 (L(t)), P (LL2) (L(t i )|L(t j )) and w (LL1) (t j |t i ) as input for RL.</p><formula xml:id="formula_17">is added to C D s (L(t i ) = NIL) and C D s (L(t i ) = NIL, L(t j )) for Equations</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We now evaluate the proposed method and compare with baselines. We use the DP method for target ex- traction ( <ref type="bibr" target="#b25">Qiu et al., 2011</ref>). This method uses depen- dency relations between opinion words and targets to extract targets using seed opinion words. Since our paper does not focus on extraction, interested readers can refer to (Qiu et al., <ref type="bibr">2011</ref>) for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Settings</head><p>Evaluation Datasets: We use two sets of datasets. The first set consists of eight <ref type="formula">(8)</ref> annotated review datasets. We use each of them as the new domain data in LML to compute precision, recall, F1 scores. Five of them are from ( <ref type="bibr" target="#b7">Hu and Liu, 2004)</ref>, and the remaining three are from ( <ref type="bibr" target="#b15">Liu et al., 2016</ref>). They have been used for target extraction, and thus have annotated targets, but no annotation on whether a Dataset Product Type # of Sentence # of entity # of aspect <ref type="table" target="#tab_1">D1  Computer  531  50  151  D2  Wireless Router  879  97  186  D3  Speaker  689  64  218  D4  DVD Player  740  50  159  D5  Digital Camera  597  70  239  D6  MP3 Player  1716  60  370  D7  Digital Camera  346  28  151  D8</ref> Cell Phone 546 36 188 <ref type="table">Table 1</ref>: Annotation details of the benchmark datasets. target is an entity or aspect. We made this annota- tion, which is straightforward. We used two annota- tors to annotate the datasets. The Cohen's kappa is 0.84. Through discussion, the annotators got com- plete agreement. Details of the datasets are listed in <ref type="table">Table 1</ref>. Each cell is the number of distinct terms. These datasets are not very large but they are realis- tic because many products do not have a large num- ber of reviews. The second set consists of unlabeled review datasets from 100 diverse products or domains <ref type="bibr">(Chen and Liu 2014)</ref>. Each domain has 1000 re- views. They are treated as past domain data in LML since they are not annotated and thus cannot be used for computing evaluation measures.</p><formula xml:id="formula_18">Distribution L(t) = entity L(t) = aspect L(t) = NIL P mE 0.45 0.25 0.3 P mA 0.3 0.4 0.3 P mc L(t j ) = entity L(t j ) = aspect L(t j ) = NIL L(t i ) = entity 0.8 0.0 0.33 L(t i ) = aspect 0.0 0.8 0.33 L(t i ) = NIL 0.2 0.2 0.33 P m E|A L(t j ) = entity L(t j ) = aspect L(t j ) = NIL L(t i ) = entity 0.33 0.8 0.33 L(t i ) = aspect 0.33 0.0 0.33 L(t i ) = NIL 0.33 0.2 0.33 P m A|E L(t j ) = entity L(t j ) = aspect L(t j ) = NIL L(t i ) = entity 0.0 0.33 0.33 L(t i ) = aspect 0.8 0.33 0.33 L(t i ) = NIL 0.2 0.33 0.33</formula><p>Evaluating Measures: We mainly use precision P, recall R, and F 1 -score F 1 as evaluation mea- sures. We take multiple occurrences of the same target as one count, and only evaluate entities and aspects. We will also give the accuracy results.</p><p>Compared Methods: We compare the following methods, including our proposed method, Lifelong- RL.</p><p>NER+TM: NER is Named Entity Recognition.</p><p>We can regard the extracted terms from a NER sys- tem as entities and the rest of the targets as as- pects. However, a NER system cannot identify enti- ties such as "this car" from "this car is great." Its re- sult is rather poor. But our type modifier (TM) does that, i.e., if an opinion target appears after "this" or "these" in at least two sentences, TM labels the tar- get as an entity; otherwise an aspect. However, TM cannot extract named entities. Its result is also rather poor. We thus combine the two methods to give NER+TM as they complement each other very well.</p><p>To make NER more powerful, we use two NER sys- tems: Stanford-NER 1 ( <ref type="bibr" target="#b17">Manning et al., 2014</ref>) and UIUC-NER 2 <ref type="bibr" target="#b26">(Ratinov and Roth, 2009)</ref>. NER+TM treats the extracted entities by the three systems as entities and the rest of the targets as aspects.</p><p>NER+TM+DICT: We run NER+TM on the 100 datasets for LML to get a list of entities, which we call the dictionary (DICT). For a new task, if any target word is in the list, it is treated as an entity; otherwise an aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL: This is the base method described in Section 3. It performs relaxation labeling (RL) without the help of LML.</head><p>Lifelong-RL-1: This performs LML with RL but the current task only uses the relations in the KB from previous tasks (Section 5.1).</p><p>Lifelong-RL: This is our proposed final method. It improves Lifelong-RL-1 by further incorporating target labels in the KB from previous tasks (Section 5.2).</p><p>Parameter Settings: RL has 2 initial label distri- butions P m E and P m A and 3 conditional label dis- tributions P mc , P m E|A and P m A|E . Like other belief propagation algorithms, these probabilities need to be set empirically, as shown in <ref type="table" target="#tab_1">Table 2</ref>. The parame- ter α is set to 1. Our LML method has one parameter λ for Lifelong-RL. We set it to 0.1. <ref type="table">Table 3</ref> shows the test results of all systems in pre- cision, recall and F 1 -score except NER+TM+DICT. NER+TM+DICT is not included due to space lim- itations and because it performed very poorly. The reason is that a target can be an entity in one domain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity</head><p>Aspect NER+TM RL <ref type="table">Lifelong-RL-1  Lifelong-RL  NER+TM  RL  Lifelong-RL-1  Lifelong-RL  P  R  F1  P  R  F1  P  R  F1  P  R  F1  P  R  F1  P  R  F1  P  R  F1  P  R  F1  D1  56.3  88  68.7</ref>   <ref type="table">Table 3</ref>: Comparative results on Entity and Aspect in precision, recall and F 1 score: NER+TM+DICT's results are very poor and not included (see Section 6.2) for the average results.</p><p>but an aspect in another. Its average F 1 -score for en- tity is only 49.2, and for aspect is only 50.2.</p><p>Entity Results Comparison: We observe from the table that although NER+TM combines NER and TM, its result for entities is still rather poor. We notice that phrases like "this price" causes low pre- cision. Since it does not use many other relations and NER does not recognize many named entities that are written in lower case letters (e.g., "apple is good"), its recall is also low.</p><p>RL has a higher precision as it considers rela- tion modifiers. However, its recall is low because it lacks information in its graph, which causes RL to make many wrong decisions. Lifelong-RL-1 intro- duces relation modifiers in KB from past domains into the current task. Both precision and recall in- crease markedly.</p><p>Lifelong-RL improves Lifelong-RL-1 further by considering target labels of past domains. Their counts improve the initial label probability distribu- tions and conditional label probability distributions. For example, "this price" may appear in some do- mains but "price"'s target label is mostly aspect. We consider their counts in initial label distributions and thus rectify the initial distribution of "price". This makes "price" easier to be classified as aspect and thus improves the precision for entity.</p><p>Aspect Results Comparison: For aspects, the trend is the same but the improvements are not as dramatic as for entity. This is because the distribu- tion of entity and aspect in the data is highly skewed. There are many more aspects than entities as we can see from the <ref type="table">Table 1</ref>. When an entity term is wrongly classified as an aspect, it has much less im- pact on the aspect result than on the entity result.</p><p>Accuracy Results Comparison:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper studied the problem of classifying opin- ion targets into entities and aspects. To the best of our knowledge, this problem has not been attempted in the unsupervised opinion target extraction setting. But this is an important problem because without separating or classifying them one will not know whether an opinion is about an entity as a whole or about a specific aspect of an entity. This paper proposed a novel method based on relaxation label- ing and the paradigm of lifelong machine learning to solve the problem. Experimental results showed the effectiveness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed LML process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Label Distribution for P E and P A and Condi-

tional Label Distribution for P mc , P m A|E and P m E|A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 gives the classification accuracy results considering all</head><label>4</label><figDesc></figDesc><table>Dataset NER+TM 
RL 
Lifelong-RL-1 Lifelong-RL 
D1 
64.93 
74.29 
75.51 
76.34 
D2 
62.94 
63.53 
69.8 
73.82 
D3 
70.04 
73.74 
74.83 
74.1 
D4 
70.81 
68.57 
73.33 
73.63 
D5 
82.07 
81.46 
85.22 
87.5 
D6 
74.83 
75.06 
78 
78.63 
D7 
88.18 
88.63 
89.68 
91.3 
D8 
74.54 
75.43 
79.57 
81.4 
Average 
73.55 
75.07 
78.24 
79.59 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results in accuracy: NER+TM+DICT's re-
sults are again very poor and thus not included. 

three classes. We can see the similar trend. 
NER+TM+DICT's average accuracy is only 45.89 
and is not included in the table. 

</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/software/CRF-NER.shtml 2 https://cogcomp.cs.illinois.edu/page/software view/NETagger</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by National Sci-ence Foundation (NSF) grants IIS-1407927 and IIS-1650900, and NCI grant R01CA192240. The con-tent of the paper is solely the responsibility of the authors and does not necessarily represent the offi-cial views of the NSF or NCI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="804" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining topics in documents: Standing on the shoulders of big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1116" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topic modeling using topics from many domains, lifelong learning and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lifelong learning for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="750" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical sequential learning for extracting opinions and their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine granular aspect analysis using latent structural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="333" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the foundations of relaxation labeling processes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting opinion targets in a single-and cross-domain setting with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinion extraction, summarization and tracking in news and blog corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI spring symposium: Computational approaches to analyzing weblogs</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">100107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sentiment analysis with global topics and local dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1371" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntactic patterns versus word alignment: Extracting opinion targets from online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving opinion aspect extraction using semantic similarity and aspect associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doosoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sentiment analysis and opinion mining. Synthesis lectures on human language technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Topic sentiment mixture: Modeling facets and opinions in weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqui</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1643" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;12</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orena</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing and text mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A rule-based approach to aspect extraction from product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SocialNLP</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="9" to="27" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Active task selection for lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lifelong machine learning systems: Beyond learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Lifelong Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis on review text data: A rating regression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Phrase dependency parsing for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1533" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extracting and ranking product features in opinion documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk</forename><forename type="middle">Hwan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eamonn O&amp;apos;brien-Strain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;10: Posters</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collective opinion target extraction in chinese microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1840" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
