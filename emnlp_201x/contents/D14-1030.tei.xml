<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clinical neurophysiology of language: The MEG approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Elsevier BV</publisher>
				<availability status="unknown"><p>Copyright Elsevier BV</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
							<email>lwehbe@cs.cmu.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for the Neural Basis of Computation</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
							<email>vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for the Neural Basis of Computation</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clinical neurophysiology of language: The MEG approach</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Clinical Neurophysiology</title>
						<title level="j" type="abbrev">Clinical Neurophysiology</title>
						<idno type="ISSN">1388-2457</idno>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Elsevier BV</publisher>
							<biblScope unit="volume">118</biblScope>
							<biblScope unit="issue">2</biblScope>
							<biblScope unit="page" from="237" to="254"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.clinph.2006.07.316</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context. On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is. This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c). We explore this parallelism to better understand the brain processes and the neu-ral networks representations. We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoen-cephalography (MEG) when subjects read a story. For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-byword brain activity. Our novel results show that: before a new word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context. Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brain&apos;s own representation of word i. Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay. The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions. Finally, we show that the output probability computed by the neural networks agrees with the brain&apos;s own assessment of the probability of word i, as it can be used to predict the brain activity after the word i&apos;s properties have been fetched from memory and the brain is in the process of integrating it into the context.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language processing has recently seen a surge in increasingly complex models that achieve impressive goals. Models like deep neural net- works and vector space models have become pop- ular to solve diverse tasks like sentiment analy- sis and machine translation. Because of the com- plexity of these models, it is not always clear how to assess and compare their performances as they might be useful for one task and not the other. It is also not easy to interpret their very high- dimensional and mostly unsupervised representa- tions. The brain is another computational system that processes language. Since we can record brain activity using neuroimaging, we propose a new di- rection that promises to improve our understand- ing of both how the brain is processing language and of what the neural networks are modeling by aligning the brain data with the neural networks representations.</p><p>In this paper we study the representations of two kinds of neural networks that are built to predict the incoming word: recurrent and finite context models. The first model is the Recurrent Neural Network Language <ref type="bibr">Model (Mikolov et al., 2011</ref>) which uses the entire history of words to model context. The second is the Neural Probabilistic Language Model (NPLM) which uses limited con- text constrained to the recent words (3 grams or 5 grams). We trained these models on a large Harry Potter fan fiction corpus and we then used them to predict the words of chapter 9 of Harry Potter and the Sorcerer's Stone <ref type="bibr" target="#b12">(Rowling, 2012)</ref>. In paral- lel, we ran an MEG experiment in which 3 subject read the words of chapter 9 one by one while their brain activity was recorded. We then looked for the alignment between the word-by-word vectors produced by the neural networks and the word-by- word neural activity recorded by MEG.</p><p>Our neural networks have 3 key constituents: a hidden layer that summarizes the history of the previous words ; an embeddings vector that sum- marizes the (constant) properties of a given word and finally the output probability of a word given Reading comprehension is reflected in the subsequent acti- vation of the left superior temporal cortex at 200-600 ms <ref type="bibr">(Halgren et al., 2002;</ref><ref type="bibr">Helenius et al., 1998;</ref><ref type="bibr">Pylkkänen et al., 2002</ref><ref type="bibr">Pylkkänen et al., , 2006</ref><ref type="bibr">Pylkkänen and Marantz, 2003;</ref><ref type="bibr">Simos et al., 1997</ref>). This sustained activation differentiates between words and nonwords ( <ref type="bibr">Salmelin et al., 1996;</ref><ref type="bibr">Wilson et al., 2005;</ref><ref type="bibr">Wydell et al., 2003)</ref>. Apart from lexical-se- mantic aspects it also seems to be sensitive to phonological manipulation ( <ref type="bibr">Wydell et al., 2003)</ref>.</p><p>As discussed above, in speech perception activation is concentrated to a rather small area in the brain and we have to rely on time information to dissociate between dif- ferent processes. Here, the different processes are separable both in timing and location. Because of that, one might think that it is easier to characterize language-related pro- cesses in the visual than auditory modality. However, here the difficulties appear at another level. In reading, activa- tion is detected bilaterally in the occipital cortex, along the temporal lobes, in the parietal cortex and, in vocalized reading, also in the frontal lobes, at various times with respect to stimulus onset. Interindividual variability further complicates the picture, resulting in practically excessive amounts of temporal and spatial information. The areas and time windows depicted in <ref type="figure" target="#fig_5">Fig. 5</ref>, with specific roles in reading, form a limited subset of all active areas observed during reading. In order to perform proper func- tional localization one needs to vary the stimuli and tasks systematically, in a parametric fashion. Let us now consid- er how one may extract activation reflecting pre-lexical let- ter-string analysis and lexical-semantic processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-lexical analysis</head><p>In order to tease apart early pre-lexical processes in reading, Tarkiainen and colleagues <ref type="bibr">(Tarkiainen et al., 1999</ref>) used words, syllables, and single letters, imbedded in a noisy background, at four different noise levels <ref type="figure" target="#fig_6">(Fig. 6</ref>). For control, the sequences also contained symbol strings. One sequence was composed of plain noise stimuli. The stimuli were thus varied along two major dimensions: the amount of features to process increased with noise and with the number of items, letters or symbols. On the other hand, word-likeness was highest for clearly visible complete words and lowest for symbols and noise.</p><p>At the level of the brain, as illustrated in <ref type="figure" target="#fig_7">Fig. 7</ref>, the data showed a clear dissociation between two processes within the first 200 ms: visual feature analysis occurred at about 100 ms after stimulus presentation, with the active areas around the occipital midline, along the ventral stream. In these areas, the signal increased with increasing noise and with the number of items in the string, similarly for letters and symbols. Only 50 ms later, at about 150 ms, the left inferior occipitotemporal cortex showed letter-string spe- cific activation. This signal increased with the visibility of the letter strings. It was strongest for words, weaker for syl- lables, and still weaker for single letters. Crucially, the acti- vation was significantly stronger for letter than symbol strings of equal length.</p><p>Bilateral occipitotemporal activation at about 200 ms post-stimulus is consistently reported in MEG studies of reading ( <ref type="bibr">Cornelissen et al., 2003b;</ref><ref type="bibr">Pammer et al., 2004;</ref><ref type="bibr">Salmelin et al., 1996</ref><ref type="bibr">Salmelin et al., , 2000b</ref>) but, interestingly, functional specificity for letter-strings is found most systematically in the left hemisphere. The MEG data on letter-string spe- cific activation are in good agreement with intracranial recordings, both with respect to timing and location and the pre-lexical nature of the activation (Nobre et al., 1994).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Lexical-semantic analysis</head><p>To identify cortical dynamics of reading comprehension, Helenius and colleagues <ref type="bibr">(Helenius et al., 1998</ref> Lexical-semantic analysis <ref type="figure" target="#fig_5">Fig. 5</ref>. Cortical dynamics of silent reading. Dots represent centres of active cortical patches collected from individual subjects. The curves display the mean time course of activation in the depicted source areas. Visual feature analysis in the occipital cortex (100 ms) is stimulus non-specific. The stimulus content starts to matter by 150 ms when activation reflecting letter-string analysis is observed in the left occipitotemporal cortex. Subsequent activation of the left superior temporal cortex at 200-600 ms reflects lexical-semantic analysis and, probably, also phonological analysis. Modified from <ref type="bibr">Salmelin et al. (2000a</ref>  the context. We set out to find the brain analogs of these model constituents using an MEG decod- ing task. We compare the different models and their representations in terms of how well they can be used to decode the word being read from MEG data. We obtain correspondences between the models and the brain data that are consistent with a model of language processing in which brain activity encodes story context, and where each new word generates additional brain activity, flowing generally from visual processing areas to more high level areas, culminating in an updated story context, and reflecting an overall magnitude of neural effort influenced by the probability of that new word given the previous context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Neural processes involved in reading</head><p>Humans read with an average speed of 3 words per second. Reading requires us to perceive in- coming words and gradually integrate them into a representation of the meaning. As words are read, it takes 100ms for the visual input to reach the visual cortex. 50ms later, the visual input is processed as letter strings in a specialized region of the left visual cortex <ref type="bibr" target="#b13">(Salmelin, 2007)</ref>. Be- tween 200-500ms, the word's semantic properties are processed (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Less is understood about the cortical dynamics of word integration, as multiple theories exist <ref type="bibr" target="#b5">(Friederici, 2002;</ref><ref type="bibr" target="#b7">Hagoort, 2003)</ref>. Magnetoencephalography (MEG) is a brain- imaging tool that is well suited for studying lan- guage. MEG records the change in the magnetic field on the surface of the head that is caused by a large set of aligned neurons that are changing their firing patterns in synchrony in response to a stimulus. Because of the nature of the signal, MEG recordings are directly related to neural ac- tivity and have no latency. They are sampled at a high frequency (typically 1kHz) that is ideal for tracking the fast dynamics of language processing.</p><p>In this work, we are interested in the mecha- nism of human text understanding as the meaning of incoming words is fetched from memory and integrated with the context. Interestingly, this is analogous to neural network models of language that are used to predict the incoming word. The mental representation of the previous context is analogous to the latent layer of the neural network which summarizes the relevant context before see- ing the word. The representation of the meaning of a word is analogous to the embedding that the neural network learns in training and then uses. Finally, one common hypotheses is that the brain integrates the word with inversely proportional ef- fort to how predictable the word is ( <ref type="bibr" target="#b4">Frank et al., 2013)</ref>. There is a well studied response known as the N400 that is an increase of the activity in the temporal cortex that has been recently shown to be graded by the amount of surprisal of the incoming word given the context ( <ref type="bibr" target="#b4">Frank et al., 2013)</ref>. This is analogous to the output probability of the incom- ing word from the neural network. <ref type="figure" target="#fig_1">Fig. 2</ref> shows a hypothetical activity in an MEG sensor as a subject reads a story in our experi- ment, in which words are presented one at a time for 500ms each. We conjecture that the activity in time window a, i.e. before word i is understood, is mostly related to the previous context before see- ing word i. We also conjecture that the activity in time window b is related to understanding word i and integrating it into the context, leading to a new representation of context in window c.</p><p>Using three types of features from neural net- works (hidden layer context representation, output probabilities and word embeddings) from three different models of language (one recurrent model and two finite context models), we therefore set to predict the activity in the brain in different time windows. We want to align the brain data with the various model constituents to understand where and when different types of processes are com- puted in the brain, and simultaneously, we want to</p><formula xml:id="formula_0">Harry' Harry' had' had' never' never' embedding(iC1)' embedding(i)' embedding(i+1)' context(iC1)' context(iC2)' context(i)' out.'prob.(iC1)' out.'prob.(i)' out.'prob.(i+1)'</formula><p>Studying'the'construc&lt;on'of'meaning' reading chapter 9 after it has been trained. Every word cor- responds to a fixed embedding vector (magenta). A context vector (blue) is computed before the word is seen given the previous words. Given the context vector, the probability of every word can be computed (symbolized by the histogram in green). We only use the output probability of the actual word (red circle).</p><formula xml:id="formula_1">3' word i+1 word i-1 word i 0.5's' 0.5's' 0.5's' a b c Leila'Wehbe' '…''Harry'''''''had''''''''never'…''</formula><p>[Bottom] Hypothetical activity in an MEG sensor when the subject reads the corresponding words. The time periods approximated as a, b and c can be tested for in- formation content relating to: the context of the story before seeing word i (modeled by the context vector at i), the repre- sentation of the properties of word i (the embedding of word i) and the integration of word i into the context (the output probability of word i). The periods drawn here are only a conjecture on the timings of such cognitive events.</p><p>use the brain data to shed light on what the neural network vectors are representing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Decoding cognitive states from brain data is a recent field that has been growing in popularity. Most decoding studies that study language use functional Magnetic Resonance Imaging (fMRI), while some studies use MEG. MEG's high tempo- ral resolution makes it invaluable for looking at the dynamics of language understanding. (Sudre et al., 2012) decode from MEG the word a subject is reading. The authors estimate from the MEG data the semantic features of the word and use these as an intermediate step to decode what the word is. This is in principle similar to the classification ap- proach we follow, as we will also use the feature vectors as an intermediate step for word classifica- tion. However the experimental paradigm in ( <ref type="bibr" target="#b14">Sudre et al., 2012</ref>) is to present to the subjects sin- gle isolated words and to find how the brain rep- resents their semantic features; whereas we have a much more complex and "naturalistic" experiment in which the subjects read a non-artificial passage of text, and we look at processes that exceed in- dividual word processing: the construction of the meanings of the successive words and the predic- tion/integration of incoming words.</p><p>In <ref type="bibr" target="#b4">(Frank et al., 2013)</ref>, the amount of surprisal that a word has given its context is used to pre- dict the intensity of the N400 response described previously. This is the closest study we could find to our approach. This study was concerned with analyzing the brain processes related only to sur- prisal while we propose a more integral account of the processes in the brain. The study also didn't address the major contribution we propose here, which is to shed light on the inner constituents of language models using brain imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Recurrent and finite context neural networks</head><p>Similar to standard language models, neural lan- guage models also learn probability distributions over words given their previous context. However, unlike standard language models, words are rep- resented as real-valued vectors in a high dimen- sional space. These word vectors, referred to as word embeddings, can be different for input and output words, and are learned from training data.</p><p>Thus, although at training and test time, the in- put and output to the neural language models are one-hot representation of words, it is their em- beddings that are used to compute word proba- bility distributions. After training the embedding vectors are fixed and it is these vectors that we will use later on to predict MEG data. To predict MEG data, we will also use the latent vector rep- resentations of context that these neural networks produce, as well as the probability of the current word given the context. In this section, we will describe how recurrent neural network language models and feedforward neural probabilistic lan- guage models compute word probabilities. In the interest of space, we keep this description brief, and for details, the reader is requested to refer to the original papers describing these models. </p><formula xml:id="formula_2">w(t) s(t − 1) y(t) c(t) hidden s(t) output P (wt+1 | s(t))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Neural Network Language Model</head><p>Unlike standard feedforward neural language models that only look at a fixed number of past words, recurrent neural network language models use all the previous history from position 1 to t − 1 to predict the next word. This is typically achieved by feedback connections, where the hidden layer activations used for predicting the word in posi- tion t − 1 are fed back into the network to com- pute the hidden layer activations for predicting the next word. The hidden layer thus stores the history of all previous words. We use the RNNLM archi- tecture as described in Mikolov (2012), shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The input to the RNNLM at position t are the one-hot representation of the current word, w(t), and the activations from the hidden layer at position t − 1, s(t − 1). The output of the hidden layer at position t − 1 is</p><formula xml:id="formula_3">s(t) = φ (Dw(t) + Ws(t − 1)) ,</formula><p>where D is the matrix of input word embeddings, W is a matrix that transforms the activations from the hidden layer in position t − 1, and φ is a sigmoid function, defined as φ(x) = 1 1+exp <ref type="bibr">(−x)</ref> , that is applied elementwise. We need to compute the probability of the next word w(t + 1) given the hidden state s(t). For fast estimation of out- put word probabilities, Mikolov (2012) divides the computation into two stages: First, the probability distribution over word classes is computed, after which the probability distribution over the subset of words belonging to the class are computed. The class probability of a particular class with index m at position t is computed as:</p><formula xml:id="formula_4">P (c m (t) | s(t)) = exp (s(t)Xv m ) C c=1 (exp (s(t)Xv c )) ,</formula><p>where X is a matrix of class embeddings and v m is a one-hot vector representing the class with in- dex m. The normalization constant is computed <ref type="figure">Figure 4</ref>: Neural probabilistic language model over all classes C. Each class specifies a subset V of words, potentially smaller than the entire vo- cabulary V . The probability of an output word l at position t + 1 given that its class is m is defined as:</p><formula xml:id="formula_5">u 1 u 2 input words input embeddings hidden h1 hidden h2 output P (w | u) D M C 1 C 2 D</formula><formula xml:id="formula_6">P (y l (t + 1) | c m (t), s(t)) = exp (s(t)D v l ) V k=1 (exp (s(t)D v k )) ,</formula><p>where D is a matrix of output word embeddings and v l is a one hot vector representing the word with index l. The probability of the word w(t + 1) given its class c i can now be computed as:</p><formula xml:id="formula_7">P (w(t + 1) | s(t)) =P (w(t + 1) | c i , s(t)) P (c i | s(t)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Probabilistic Language Model</head><p>We use the feedforward neural probabilistic lan- guage model architecture of <ref type="bibr" target="#b17">Vaswani et al. (2013)</ref>, as shown in <ref type="figure">Figure 4</ref>. Each context u comprises a sequence of words u j (1 ≤ j ≤ n − 1) repre- sented as one-hot vectors, which are fed as input to the neural network. At the output layer, the neu- ral network computes the probability P (w | u) for each word w, as follows.</p><p>The output of the first hidden layer h 1 is</p><formula xml:id="formula_8">h 1 = φ   n−1 j=1 C j Du j + b 1   ,</formula><p>where D is a matrix of input word embeddings which is shared across all positions, the C j are the context matrices for each word in u, b 1 is a vec- tor of biases with the same dimension as h 1 , and φ is applied elementwise. <ref type="bibr" target="#b17">Vaswani et al. (2013)</ref> use rectified linear units <ref type="bibr" target="#b11">(Nair and Hinton, 2010</ref>) for the hidden layers h 1 and h 2 , which use the activa- tion function φ(x) = max(0, x). The output of the second layer h 2 is</p><formula xml:id="formula_9">h 2 = φ (Mh 1 + b 2 ) ,</formula><p>where M is a weight matrix between h 1 and h 2 and b 2 is a vector of biases for h 2 . The probabil- ity of the output word is computed at the output softmax layer as:</p><formula xml:id="formula_10">P (w | u) = exp v w D h 2 + b T v w V w =1 exp (v w D h 2 + b T v w ) ,</formula><p>where D is the matrix of output word embed- dings, b is a vector of biases for every output word and v w its the one hot representation of the word w in the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We describe in this section our approach. In sum- mary, we trained the neural network models on a Harry Potter fan fiction database. We then ran these models on chapter 9 of Harry Potter and the Sorcerer's Stone (Rowling, 2012) and computed the context and embedding vectors and the output probability for each word. In parallel, 3 subjects read the same chapter in an MEG scanner. We build models that predict the MEG data for each word as a function of the different neural network constituents. We then test these models with a classification task that we explain below. We de- tect correspondences between the neural network components and the brain processes that under- lie reading in the following fashion. If using a neural network vector (e.g. the RNNLM embed- ding vector) allows us to classify significantly bet- ter than chance in a given region of the brain at a given time (e.g. the visual cortex at time 100- 200ms), then we can hypothesize a relationship between that neural network constituent and the time/location of the analogous brain process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training the Neural Networks</head><p>We used the freely available training tools pro- vided by Mikolov (2012) 1 and Vaswani et al. (2013) 2 to train our RNNLM and NPLM models used in our brain data classification experiments. Our training data comprised around 67.5 million words for training and 100 thousand words for val- idation from the Harry Potter fan fiction database (http://harrypotterfanfiction.com). We restricted the vocabulary to the top 100 thousand words which covered all but 4 words from Chapter 9 of Harry Potter and the Sorcerer's Stone.</p><p>For the RNNLM, we trained models with differ- ent hidden layers and learning rates and found the RNNLM with 250 hidden units to perform best on the validation set. We extracted our word embed- dings from the input matrix D <ref type="figure" target="#fig_2">(Figure 3)</ref>. We used the default settings for all other hyper parameters.</p><p>We trained 3-gram and 5-gram NPLMs with 150 dimensional word embeddings and experi- mented with different number of units for the first hidden layer (h 1 in <ref type="figure">Figure 4)</ref>, and different learn- ing rates. For both the 3-gram and 5-gram mod- els, we found 750 hidden units to perform the best on the validation set and chose those models for our final experiments. We used the output word embeddings D in our experiments. We visually inspected the nearest neighbors in the 150 dimen- sional word embedding space for some words and didn't find the neighbors from D or D to be dis- tinctly better than each other. We leave the com- parison of input and output embeddings on brain activity prediction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MEG paradigm</head><p>We recorded MEG data for three subjects (2 fe- males and one male) while they read chapter 9 of Harry Potter and the Sorcerer's Stone (Rowl- ing, 2012). The participants were native English speakers and right handed. They were chosen to be familiar with the material: we made sure they had read the Harry Potter books or seen the movies series and were familiar with the characters and the story. All the participants signed the consent form, which was approved by the University of Pittsburgh Institutional Review Board, and were compensated for their participation.</p><p>The words of the story were presented in rapid serial visual format ( <ref type="bibr" target="#b1">Buchweitz et al., 2009)</ref>: words were presented one by one at the center of the screen for 0.5 seconds each. The text was shown in 4 experimental blocks of ∼11 minutes. In total, 5176 words were presented. Chapter 9 was presented in its entirety without modifications and each subject read the chapter only once.</p><p>One can think of an MEG machine as a large helmet, with sensors located on the helmet that record the magnetic activity. Our MEG recordings were acquired on an Elekta Neuromag device at the University of Pittsburgh Medical Center Pres- byterian Hospital. This machine has 306 sensors distributed into 102 locations on the surface of the subject's head. Each location groups 3 sensors or two types: one magnometer that records the in- tensity of the magnetic field and two planar gra- diometers that record the change in the magnetic field along two orthogonal planes 3 .</p><p>Our sampling frequency was 1kHz. For prepro- cessing, we used Signal Space Separation method (SSS, (Taulu et al., 2004)), followed by its tempo- ral extension (tSSS, ( <ref type="bibr" target="#b15">Taulu and Simola, 2006)</ref>).</p><p>For each subject, the experiment data consists therefore of a 306 dimensional time series of length ∼45 minutes. We averaged the signal in ev- ery sensor into 100ms non-overlapping time bins. Since words were presented for 500ms each, we therefore obtain for every word p = 306 × 5 val- ues corresponding to 306 vectors of 5 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoding experiment</head><p>To find which parts of brain activity are related to the neural network constituents (e.g. the RNNLM context vector), we run a prediction and classifica- tion experiment in a 10-fold cross validated fash- ion. At every fold, we train a linear model to pre- dict MEG data as a function of one of the feature sets, using 90% of the data. On the remaining 10% of the data, we run a classification experiment.</p><p>MEG data is very noisy. Therefore, classify- ing single word waveforms yields a low accuracy, peaking at 60%, which might lead to false nega- tives when looking for correspondences between neural network features and brain data. To reveal informative features, one can boost signal by ei- ther having several repetitions of the stimuli in the experiment and then averaging ( <ref type="bibr" target="#b14">Sudre et al., 2012)</ref> or by combining the words into larger chunks <ref type="bibr" target="#b18">(Wehbe et al., 2014</ref>). We chose the latter because the former sacrifices word and feature diversity.</p><p>At testing, we therefore repeat the following 300 times. Two sets of words are chosen ran- domly from the test fold. To form the first set, 20 words are sampled without replacement from the test sample (unseen by the classifier). To form the second set, the k th word is chosen randomly from all words in the test fold having the same length as the k th word of the first set. Since every fold of the data was used 9 times in the training phase and once in the testing phase, and since we use a high number of randomized comparisons, this averages out biases in the accuracy estimation. Classifying sets of 20 words improves the classification accu- racy greatly while lowering its variance and makes it dissociable from chance performance. We com- pare only between words of equal length, to mini- mize the effect of the low level visual features on the classification accuracy.</p><p>After averaging out the results of multiple folds, we end up with average accuracies that reveal how related one of the models' constituents (e.g. the RNNLM context vector) is to brain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Annotation of the stimulus text</head><p>We have 9 sets of annotations for the words of the experiment. Each set j can be described as a ma- trix F j in which each row i corresponds to the vec- tor of annotations of word i. Our annotations cor- respond to the 3 model constituents for each of the 3 models: the hidden layer representation before word i, the output probability of word i and the learned embeddings for word i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Classification</head><p>In order to align the brain processes and the differ- ent constituents of the different models, we use a classification task. The task is to classify the word a subject is reading out of two possible choices from its MEG recording. The classifier uses one type of feature in an intermediate classification step. For example, the classifier learns to predict the MEG activity for any setting of the RNNLM hidden layer. Given an unseen MEG recording for an unknown word i and two possible story words i and i (one of which being the true word i), the classifier predicts the MEG activity when reading i and i from their hidden layer vectors. It then assigns the label i or i to the word recording i depending on which prediction is the closest to the recording. The following are the detailed steps of this complex classification task. However, for the rest of the paper the most useful point to keep in mind is that the main purpose of the classification is to find a correspondence between the brain data and a given feature set j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Divide the data into 10 folds, for each fold b:</head><p>(a) Isolate M b and F b j as test data. The re- mainder M −b and F −b j will be used for training <ref type="bibr">4</ref> . (b) Subtract the mean of the columns of M −b from M b and M −b and the mean of the columns of F −b j from F b j and F −b j (c) Use ridge regression to solve</p><formula xml:id="formula_11">M −b = F −b j × β t j</formula><p>by tuning the λ parameter to every one of the p output dimensions indepen- dently. λ is chosen via generalized cross validation ( <ref type="bibr" target="#b6">Golub et al., 1979</ref>). </p><formula xml:id="formula_12">P c = F c j × Γ b j and P d = F d j × Γ b j</formula><p>ii. assign to M c the label c or d depend- ing on which of P c or P d is closest (Euclidean distance). iii. assign to M d the label c or d de- pending on which of P c or P d is closest (Euclidean distance).</p><p>3. Compute the average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Restricting the analysis spatially: a searchlight equivalent</head><p>We adapt the searchlight method ( <ref type="bibr" target="#b8">Kriegeskorte et al., 2006</ref>) to MEG. The searchlight is a discovery procedure used in fMRI in which a cube is slid over the brain and an analysis is performed in each location separately. It allows to find regions in the brain where a specific phenomenon is occurring. In the MEG sensor space, for every one of the 102 sensor locations , we assign a group of sensors g . For every location , we identify the locations that immediately surround it in any direction (Anterior, Right Anterior, Right etc...) when looking at the 2D flat representation of the location of the sensors in the MEG helmet (see <ref type="figure" target="#fig_9">Fig. 9</ref> for an illustration of the 2D helmet). g therefore contains the 3 sensors at location and at the neighboring locations. The maximum number of sensors in a group is 3 × 9.</p><p>The locations at the edge of the helmet have fewer sensors because of the missing neighbor locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Restricting the analysis temporally</head><p>Instead of using the entire time course of the word, we can use only one of the corresponding 100ms time windows. Obtaining a high classification ac- curacy using one of the time windows and feature set j means that the analogous type of information is encoded at that time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Classification accuracy by time and region</head><p>The above steps compute whole brain accuracy us- ing all the time series. In order to perform a more precise spatio-temporal analysis, one can use only one time window m and one location for the clas- sification. This can answer the question of when and where different information is represented by brain activity. For every location, we will use only the columns corresponding to the time point m for the sensors belonging to the group g . Step (d) of the classification procedure is changed as such: i. predict the MEG data for c and d as:</p><formula xml:id="formula_13">P c {m,,} = F c j × Γ b j,{m,,} and P d {m,,} = F d j × Γ b j,{m,,}</formula><p>ii. assign to M c {m,,} the label c or d depend- ing on which of P c {m,,} or P d {m,,} is clos- est (Euclidean distance). iii. assign to M d {m,,} the label c or d depend- ing on which of P c {m,,} or P d {m,,} is clos- est (Euclidean distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Statistical significance testing</head><p>We determine the distribution for chance perfor- mance empirically. Because the successive word samples in our MEG and feature matrices are not independent and identically distributed, we break the relationship between the MEG and feature ma- trices by shifting the feature matrices by large de- lays (e.g. 2000 to 2500 words) and we repeat the classification using the delayed matrices. This simulates chance performance more fairly than a permutation test because it keeps the time struc- ture of the matrices. It was used in <ref type="bibr" target="#b18">(Wehbe et al., 2014</ref>) and inspired by <ref type="bibr" target="#b2">(Chwialkowski and Gretton, 2014</ref>). For every {m, } setting we can there- fore compute a standardized z-value by subtract- ing the mean of the shifted classifications and di- viding by the standard deviation. We then com- pute the p-value for the true classification accu- racy being due to chance. Since the three p-values for the three subjects for a given {m, } are inde- pendent, we combine them using Fisher's method for independent test statistics <ref type="bibr" target="#b3">(Fisher, 1925)</ref>. The statistics we obtain for every {m, } are depen- dent because they comprise nearby time and space windows. We control the false discovery rate us- ing <ref type="bibr" target="#b0">(Benjamini and Yekutieli, 2001</ref>) to adjust for the testing at multiple locations and time windows. This method doesn't assume any kind of indepen- dence or positive dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We present in <ref type="figure" target="#fig_5">Fig. 5</ref> the accuracy using all the time windows and sensors. In <ref type="figure" target="#fig_6">Fig. 6</ref> we present the classification accuracy when running the classifi- cation at every time window exclusively. In <ref type="figure" target="#fig_9">Fig. 9</ref> we present the accuracy when running the classifi- cation using different time windows and groups of sensors centered at every one of the 102 locations.</p><p>It is important to lay down some conventions to understand the complex results in these plots. To recap, we are trying to find parallels between model constituents and brain processes. We use:</p><p>• a subset of the data (for example the time window 0-100ms and all the sensors)</p><p>• one type of feature (for example the hidden context layer from the NPLM 3g model) and we obtain a classification accuracy A. If A is low, there is probably no relationship between the feature set and the subset of data. If A is high, it hints to an association between the subset of data and the mental process that is analogous to the fea- ture set. For example, when using all the sensors and time window 0-100ms, along with the NPLM 3g hidden layer, we obtain an accuracy of 0.70 (higher than chance with p &lt; 10 −14 , see <ref type="figure" target="#fig_6">Fig. 6</ref>). Since the NPLM 3g hidden layer summarizes the context of the story before seeing word i, this sug- gests that the brain is still processing the context of the story before word i between 0-100ms. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the accuracy for different types of features when using all of the time points and all the sensors to classify a word. We can see  when using different types of features as input to the clas- sifier, for different models. Accuracy is plotted in the center of the respective time window. Points marked with a circle are significantly higher than chance accuracy for the given feature set and time window after correction. similar classification accuracies for the three types of models, with RNNLM ahead for the hidden layer and embeddings and behind for the output probability features. The hidden layer features are the most powerful for classification. Between the three types of features, the hidden layer fea- tures are the best at capturing the information con- tained in the brain data, suggesting that most of the brain activity is encoding the previous context. The embedding features are the second best. Fi- nally the output probability have the smallest ac- curacies. This makes sense considering that they capture much less information than the other two high dimensional descriptive vectors, as they do not represent the complex properties of the words, only a numerical assessment of their likelihood. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the accuracy when using different windows of time exclusively, for the 100ms time windows starting at 0, 100 . . . 400ms after word presentation. We can see that using the embed- ding vector becomes increasingly more useful for classification until 300-400ms, and then its perfor- mance starts decreasing. This results aligns with the following hypothesis: the word is being per- ceived and understood by the brain gradually after its presentation, and therefore the brain represen- tation of the word becomes gradually similar to the neural network representation of the word (i.e. the embedding vector). The output probability feature accuracy peaks at a later time than the embeddings accuracy. Obtaining a higher than chance accu- racy at time window m using the output probabil- ity as input to the classifier suggests strongly that the brain is integrating the word at time window m, because it is responding differently for pre- dictable and unpredictable words <ref type="bibr">5</ref> . The integra- tion step happens after the perception step, which is probably why the output probability curves peak later than the embeddings curves.  To understand the time dynamics of the hidden layer accuracy we need to see a larger time scale than the word itself. The hidden layer captures the context before word i is seen. Therefore it seems reasonable that the hidden layer is not only related to the activity when the word is on the screen, but also related to the activity before the word is pre- sented, which is the time when the brain is inte- grating the previous words to build that context. On the other hand, as the word i and subsequent words are integrated, the context starts diverging from the context of word i (computed before see- ing word i). We therefore ran the same analysis as before, but this time we also included the time windows before and after word i in the analysis, while maintaining the hidden layer vector to be the context before word i is seen. We see the behav- ior we predicted in the results: the context before seeing word i becomes gradually more useful for classification until word i is seen, and then it grad- ually decreases until it is no longer useful since the context has changed. We observe the RNNLM hidden layer has a higher classification accuracy than the finite context NPLMs. This might be due to the fact that the RNNLM has a more complete representation of context that captures more of the properties of the previous words.</p><p>To show the consistency of the results, we plot as illustration the three curves we obtain for each subject for the RNNLM <ref type="figure" target="#fig_8">(Fig. 8)</ref>. The patterns seem very consistent indicating the phenomena we described can be detected at the subject level.</p><p>We now move on to the spatial decomposition of the analysis. When the visual input enters the brain, it first reaches the visual cortex at the back of the head, and then moves anteriorly towards the left and right temporal cortices and eventually the frontal cortex. As it flows through these areas, it is processed to higher levels of interpretations. In <ref type="figure" target="#fig_9">Fig. 9</ref>, we plot the accuracy for different regions of the brain and different time windows for the RNNLM features. To make the plots simpler we multiplied by zero the accuracies which were not significantly higher than chance. We expand a few characteristic plots. We see that in the back of the head the embedding features have an accuracy that seems to peak very early on. As we move forward in the brain towards the left and right temporal cor- tices, we see the embeddings accuracy peaking at a later time, reflecting the delay it takes for the in- formation to reach this part of the brain. The out- put probability start being useful for classification after the embeddings, and specifically in the left temporal cortex which is the cite where the N400 is reported in the literature. Finally, as we reach the frontal cortex, we see that the embeddings fea- tures have an even later accuracy peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and contributions</head><p>Novel brain data exploration We present here a novel and revealing approach to shed light on the brain processes involved in reading. This is a departure from the classical approach of control- ling for a few variables in the text (e.g. showing a sentence with an expected target word versus an unexpected one). While we cannot make clear cut causal claims because we did not control for our variables, we are able to explore the data much more and offer a much richer interpretation than is possible with artificially constrained stimuli.</p><p>Comparing two models of language Adding brain data into the equation allowed us to com- pare the performance of the models and to identify a slight advantage for the RNNLM in capturing the text contents. Numerical comparison is how- ever a secondary contribution of our approach. We showed that it might be possible to use brain data to understand, interpret and illustrate what exactly is being encoded by the obscure vectors that neural networks compute, by drawing parallels between the models constituents and brain processes.</p><p>Anecdotally, in the process of running the ex- periments, we noticed that the accuracy for the hidden layer of the RNNLM was peaking in the time window corresponding to word i−2, and that it was decreasing during word i − 1. Since this was against our expectations, we went back and looked at the code and found that it was indeed returning a delayed value and corrected the fea- tures. We therefore used the brain data in order to correct a mis-specification in our neural network model. This hints if not proves the potential of our approach for assessing language models. Future Work The work described here is our first attempt along the promising endeavor of matching complex computational models of lan- guage with brain processes using brain recordings. We plan to extend our efforts by (1) collecting data from more subjects and using various types of text and (2) make the brain data help us with training better statistical language models by using it to de- termine whether the models are expressive enough or have reached a sufficient degree of convergence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cortical dynamics of silent reading. This figure is adapted from (Salmelin, 2007). Dots represent projected sources of activity in the visual cortex (left brain sketch) and the temporal cortex (right brain sketch). The curves display the mean time course of activation in the depicted source areas for different conditions. The initial visual feature analysis in the visual cortex at ∼100 ms is non-specific to language. Comparing responses to letter strings and other visual stimuli reveals that letter string analysis occurs around 150 ms. Finally comparing the responses to words and nonwords (made-up words) reveals lexical-semantic analysis in the temporal cortex at ∼200-500ms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: [Top] Sketch of the updates of a neural network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recurrent neural network language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(d) Perform a binary classification. Sample from the set of words in b a set c of 20 words. Then sample from b another set of 20 words such that the k th word in c and d have the same number of letters. For every sample (c,d): i. predict the MEG data for c and d as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>d) Perform a binary classification. Sample from the set of words in b a set c of 20 words. Then sample from b another set of 20 words such that the k th word in c and d have the same number of letters. For every sample (c,d), and for every setting of {m, }:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average accuracy using all time windows and sensors, grouped by model (top) and type of feature (bottom). All accuracies are significantly higher than chance (p &lt; 10 −8 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average accuracy in different time windows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average accuracy in time for the different hidden layers. The analysis is extended to the time windows before and after the word is presented, the input feature is restricted to be the hidden layer before the central word is seen. The first vertical bar indicates the onset of the word, the second one indicates the end of its presentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Accuracy in time when using the RNNLM features for each of the three subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features. For each of the 102 locations the average accuracy for the group of sensors centered at that location is plotted versus time. The axes are defined in the rightmost, empty plot. Three plots have been magnified to show the increasing delay in high accuracy when using the embeddings feature, reflecting the delay in processing the incoming word as information travels through the brain. A sensor map is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.</figDesc><graphic url="image-4.png" coords="10,399.89,236.70,83.79,63.75" type="bitmap" /></figure>

			<note place="foot" n="1"> http://rnnlm.org/ 2 http://nlg.isi.edu/software/nplm</note>

			<note place="foot" n="3"> In this paper, we treat these three different sensors as three different dimensions without further exploiting their physical properties.</note>

			<note place="foot" n="1">. Normalize the columns of M (zero mean, standard deviation = 1). Pick feature set F j and normalize its columns to a minimum of 0 and a maximum of 1.</note>

			<note place="foot" n="4"> The rows from M −b and F −b j that correspond to the five words before or after the test set are ignored in order to make the test set independent.</note>

			<note place="foot" n="5"> the fact that we can classify accurately during windows 300-400ms indicates that the classifier is taking advantage of the N400 response discussed in the introduction</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by NICHD grant 5R01HD07328-02. We thank Nicole Rafidi for help with data acquisition.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The control of the false discovery rate in multiple testing under dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yekutieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1165" to="1188" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Brain activation for reading and listening comprehension: An fMRI study of modality effects and individual differences in language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Buchweitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lêda</forename><surname>Robert A Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Adam</forename><surname>Tomitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology &amp; neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="123" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kacper</forename><surname>Chwialkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.4501</idno>
		<title level="m">A kernel independence test for random processes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald Aylmer</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1925" />
			<publisher>Genesis Publishing Pvt Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word surprisal predicts N400 amplitude during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefan L Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="878" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards a neural basis of auditory sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angela D Friederici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized cross-validation as a method for choosing a good ridge parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="223" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How the brain solves the binding problem for language: a neurocomputational model of syntactic processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hagoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information-based functional brain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="3863" to="3868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RNNLMrecurrent neural network language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukar</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2011 ASRU Workshop</title>
		<meeting>of the 2011 ASRU Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical Language Models Based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Harry Potter and the Sorcerer&apos;s Stone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">K</forename><surname>Rowling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Harry Potter US. Pottermore Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clinical neurophysiology of language: the MEG approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riitta</forename><surname>Salmelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking neural coding of perceptual and semantic features of concrete nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riitta</forename><surname>Salmelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="451" to="463" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samu</forename><surname>Taulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Simola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in medicine and biology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1759</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Suppression of interference and artifacts by the signal space separation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samu</forename><surname>Taulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Kajola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Simola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain topography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="275" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
