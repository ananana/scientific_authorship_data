<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Semi-Automatic Generation of Proposition Banks for Low-Resource Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research Almaden Research Center San Jose</orgName>
								<orgName type="institution" key="instit2">IBM Research Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120, 95120</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
							<email>vishwajeetkumar86@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research Almaden Research Center San Jose</orgName>
								<orgName type="institution" key="instit2">IBM Research Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120, 95120</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CS</roleName><forename type="first">Iit</forename><surname>Bombay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research Almaden Research Center San Jose</orgName>
								<orgName type="institution" key="instit2">IBM Research Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120, 95120</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engineeering</forename><surname>Mumbai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research Almaden Research Center San Jose</orgName>
								<orgName type="institution" key="instit2">IBM Research Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120, 95120</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">India</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research Almaden Research Center San Jose</orgName>
								<orgName type="institution" key="instit2">IBM Research Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120, 95120</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research Almaden Research Center San Jose</orgName>
								<orgName type="institution" key="instit2">IBM Research Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120, 95120</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Semi-Automatic Generation of Proposition Banks for Low-Resource Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="993" to="998"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Annotation projection based on parallel corpora has shown great promise in inexpensively creating Proposition Banks for languages for which high-quality parallel corpora and syntactic parsers are available. In this paper, we present an experimental study where we apply this approach to three languages that lack such resources: Tamil, Bengali and Malay-alam. We find an average quality difference of 6 to 20 absolute F-measure points visa -vis high-resource languages, which indicates that annotation projection alone is insufficient in low-resource scenarios. Based on these results , we explore the possibility of using annotation projection as a starting point for inexpensive data curation involving both experts and non-experts. We give an outline of what such a process may look like and present an initial study to discuss its potential and challenges .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Creating syntactically and semantically annotated NLP resources for low-resource languages is known to be immensely costly. For instance, the Proposi- tion <ref type="bibr">Bank (Palmer et al., 2005</ref>) was created by an- notating predicate-argument structures in the Penn Treebank ( <ref type="bibr">Marcus et al., 1993</ref>) with shallow seman- tic labels: frame labels for verbal predicates and role labels for arguments. Similarly, the SALSA ( <ref type="bibr" target="#b3">Burchardt et al., 2006</ref>) resource added FrameNet-style annotations to the TIGER Treebank ( <ref type="bibr" target="#b3">Brants et al., 2002</ref>), the Chinese Propbank <ref type="bibr" target="#b11">(Xue, 2008</ref>) is built on the Chinese Treebank ( <ref type="bibr" target="#b10">Xue et al., 2005</ref>), and so forth. Since each such layer of annotation typ- ically requires years of manual work, the accumu- lated costs can be prohibitive for low-resource lan- guages.</p><p>Recent work on annotation projection offers a way to inexpensively label a target language corpus with linguistic annotation <ref type="bibr">(Padó and Lapata, 2009)</ref>. This only requires a word-aligned parallel corpus of labeled English sentences and their translations in the target language. English labels are then auto- matically projected onto the aligned target language words. Refer to <ref type="figure" target="#fig_0">Figure 1</ref> for an example. Low-resource languages. However, previous work that investigated Propbank annotation projection has focused only on languages for which treebanks -and therefore syntactic parsers -already exist. Since syn- tactic information is typically used to increase pro- jection accuracy <ref type="bibr">(Padó and Lapata, 2009;</ref><ref type="bibr" target="#b1">Akbik et al., 2015)</ref>, we must expect this approach to work less well for low-resource languages. In addition, low-resource languages have fewer sources of high-  quality parallel data available, further complicating annotation projection. Contributions. In this paper, we present a study in which we apply annotation projection to three low- resource languages in order to quantify the differ- ence in precision and recall vis-a-vis high-resource languages. Our study finds overall F1-measure of generated Proposition Banks to be significantly be- low state-of-the-art results, leading us to conclude that annotation projection may at best be a starting point for the generation of semantic resources for low-resource languages. To explore this idea, we outline a potential semi-automatic process in which we use crowdsourced data curation and limited ex- pert involvement to confirm and correct automati- cally projected labels. Based on this initial study, we discuss the potential and challenges of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotation Projection</head><p>Annotation projection takes as input a word-aligned parallel corpus of sentences in a source language (usually English) and their target language trans- lations. A syntactic parser and a semantic role labeler produce labels for the English sentences, which are then projected onto aligned target lan- guage words. The underlying theory is that paral- lel sentences share a degree of syntactic and, in par- ticular, semantic similarity, making such projection possible <ref type="bibr">(Padó and Lapata, 2009)</ref>. State-of-the-art. Previous work analyzed errors in annotation projection and found that they are of- ten caused by non-literal translations <ref type="bibr" target="#b1">(Akbik et al., 2015)</ref>. For this reason, previous work defined lexical and syntactic constraints to increase projection qual- ity. These include verb filters to allow only verbs to be labeled as frames (Van der Plas et al., 2011), heuristics to ensure that only heads of syntactic con- stituents are labeled as arguments <ref type="bibr">(Padó and Lapata, 2009</ref>) and the use of verb translation dictionar- ies ( <ref type="bibr" target="#b1">Akbik et al., 2015</ref>) to constrain frame mappings. Adaptation to low-resource languages. Low- resource languages, however, lack syntactic parsers to identify target language predicate-argument struc- tures. This requires us to make the following modi- fications to the approach:</p><p>Target language predicates We define lexical con- straints using verb translation dictionaries. This ensures that only target language verbs that are aligned to literal source language translations are labeled as frames.</p><p>Target language arguments To identify argu- ments, we project not only the role label of source language arguments heads, but the entire argument dependency structure. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>: Two dependency arcs are projected from English onto Tamil, giving evidence that arguments A0 and A1 in the Tamil sentence each consist of two words.</p><p>This step produces a target language corpus with se- mantically annotated predicate-argument structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Outline of a Data Curation Process</head><p>As confirmed in the experiments section of this pa- per, the quality of the Proposition Banks generated using annotation projection is significantly lower for low-resource languages. We therefore propose to use this approach only as a starting point for an in- expensive curation process as illustrated in <ref type="figure">Figure 2</ref>:</p><formula xml:id="formula_0">எ� த�ைத �ைற�� ேவைல AM-LOC A0</formula><p>work.01</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>எ� த�ைத �ைற�� ேவைல</head><p>Q1: Is ேவைல meant as in "work"?</p><p>Q2: Is எ� த�ைத the "worker"?</p><p>Q3: Is a "co-worker" mentioned somewhere in this sentence?</p><p>Tamil sentence with projected labels:</p><p>Question form:</p><p>Figure 3: Example of how data curation questions may be for- mulated for the labels projected onto Tamil in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Step 1: Crowdsourced data curation. Previous work has experimented with different approaches in crowdsourcing to generate frame-semantic annota- tions over text (Hong and Baker, 2011), including selection tasks (selecting one answer from a list of options) ( <ref type="bibr" target="#b3">Fossati et al., 2013</ref>) and marking tasks (marking text passages that evoke a certain semantic role) <ref type="bibr">(Feizabadi and Padó, 2014)</ref>. While these stud- ies only report moderate results on annotator cor- rectness and agreement, our goal is different from these works in that we only wish to curate projected labels, not generate SRL annotations from scratch. A related project in extending FrameNet with para- phrases ( <ref type="bibr" target="#b7">Pavlick et al., 2015)</ref> has shown that the crowd can effectively curate wrong paraphrases by answering a series of confirm-or-reject questions.</p><p>For our initial study, we generate human readable question-answer pairs ( <ref type="bibr">He et al., 2015</ref>) using the la- bel descriptions of the English Propbank (see <ref type="figure">Fig- ure 3)</ref>. We generate two types of questions:</p><p>Label confirmation questions are confirm-or- reject questions on whether projected labels are correct (e.g. Q1 and Q2 in <ref type="figure">Figure 3)</ref>. Workers further qualify their answers to indicate whether a sequence of words marked as an argument is incomplete.</p><p>Missing label questions are marking tasks which ask whether any core role labels of a frame are missing. For example, the BUY.01 frame has 5 core roles (labeled A0 to A4), one of which is the "price" (A3). Since no "price" is labeled in the Tamil sentence in <ref type="figure">Figure 3</ref>, question Q3 <ref type="table" target="#tab_1">Tamil   OPENSUBTITLES2016 75K  224K  21K  SPOKENTUTORIALS  31K  17K  32K</ref> Total # sentences 106K 241K 53K asks users to add this label if a "price" is men- tioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA SET Bengali Malayalam</head><p>Our goal is to effectively distribute a large part of the curation workload. In cases where the crowd unanimously agrees, we remove labels judged to be incorrect and add labels judged to be missing.</p><p>Step 2: Expert data curation. We also expect a percentage of questions for which non-experts will give conflicting answers 1 . As <ref type="figure">Figure 2</ref> shows, such cases will be passed to experts for further curation. However, for the purpose of scalability, we aim to keep expert involvement to a minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Study</head><p>We report our initial investigations over the follow- ing questions: (1) What are the differences in an- notation projection quality between low-and high- resource languages?; and (2) Can non-experts be leveraged to at least partially curate projected labels?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Languages. We evaluate three low-resource lan- guages, namely Bengali, an Indo-Aryan language, as well as Tamil and Malayalam, two South Dravid- ian languages. Between them, they are estimated to have more than 300 million first language speakers, yet there are few NLP resources available. Data sets. We use two parallel corpora (see <ref type="table" target="#tab_1">Table 1</ref>): OPENSUBTITLES2016 (Tiedemann, 2012), a cor- pus automatically generated from movie subtitles, and SPOKENTUTORIALS, a corpus of technical- domain tutorial translations. Evaluation. For the purpose of comparison to pre- vious work on high-resource languages, we replicate 1 Common problems for non-experts that we observe in our initial experiments involve ambiguities caused by implicit or causal role-predicate relationships, as well as figurative usage and hypotheticals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>995</head><p>PRED. ARGUMENT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LANG.</head><p>Match P P R F1 %Agree  earlier evaluation practice and English preprocess- ing steps <ref type="bibr" target="#b1">(Akbik et al., 2015)</ref>. After projection, we randomly select 100 sentences for each target lan- guage and pass them to a curation step by 2 non- experts. We then measure the inter-annotator agree- ment and the quality of the generated Proposition Banks in terms of predicate precision 2 and argument F1-score before and after crowdsourced curation 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The evaluation results are listed in <ref type="table" target="#tab_2">Table 2</ref>. For comparison, we include evaluation results reported for three high-resource languages: German and Chi- nese, representing average high-resource results, as well as Hindi, a below-average outlier. We make the following observations: Lower annotation projection quality. We find that the F1-scores of Bengali, Malayalam and Tamil are আর একটু িহং� , �যমনটা আজ আিব�ার করলাম A0 A0 discover.01 A1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1</head><p>discover.01 and a bit wild as today discover did a wild one , as I discovered today .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A0 A0</head><p>Figure 4: Example of a projection error. The verb discover in Bengali is a light verb construction. In addition, the pronoun I is not explicitly mentioned in the Bengali target sentence. This causes the pronoun I to be mistakenly aligned to the auxiliary of the light verb, causing it to be falsely labeled as A0.</p><p>6, 11 and 31 pp below that of an average high- resource language (as exemplified by German in <ref type="table" target="#tab_2">Ta- ble 2</ref>). Bengali and Malayalam, however, do surpass Hindi, for which only a relatively poor dependency parser was used. This suggests that syntactic annota- tion projection may be a better method for identify- ing predicate-argument structures in languages that lack fully developed dependency parsers. Impact of parallel data. We note a significant im- pact of the size and quality of available parallel data on overall quality. For instance, the lowest-scoring language in our experiments, Tamil, use the smallest amount parallel data (see <ref type="table" target="#tab_1">Table 1</ref>), most of which was from the SPOKENTUTORIALS corpus. This data is specific to the technical domain and seems less suited for annotation projection than the more general OPENSUBTITLES2016 corpus. A qualitative inspection of projection errors points to a large portion of errors stemming from translation shifts. For instance, refer to <ref type="figure">Figure 4</ref> for an English-Bengali example of the impact of even slight differences in translation: The English verb discover is expressed in Bengali as a light verb, while the pronoun I is dropped in the Bengali sen- tence (it is still implicitly evoked through the verb being in first person form). This causes the word alignment to align the English I to the Bengali auxil- iary, onto which the role label A0 is then incorrectly projected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In all three languages, we note improvements through curation. Argument F1-score improves to 77% (↑2 pp) for Bengali, to 74% (↑4 pp) for Malay-alam, and to 61% (↑11 pp) for Tamil on exact matches. Especially Tamil improves drastically, al- beit from a much lower initial score than the other languages. This supports our general observation that crowd workers are good at spotting obvious er- rors, while they often disagree about more subtle differences in semantics. These results indicate that a curation process can at least be partially crowd- sourced. An interesting question for further investi- gation is to what degree this is possible. As <ref type="table" target="#tab_2">Table 2</ref> shows, non-expert agreement in our initial study was far below reported expert agreement, with 25% to 35% of all questions problematic for non-experts.</p><p>A particular focus of our future work is therefore to quantify to which extent crowd-feedback can be valuable and how far the involvement of experts can be minimized for cost-effective resource generation. However, a Proposition Bank generated through this process would be peculiar in several ways: Crowd semantics. First, generated Proposition Banks would be created in a drastically different way than current approaches that rely on experts to create and annotate frames. Effectively, the non- expert crowd would, to a large degree, shape the selection and annotation of English frame and role annotation for new target languages. An impor- tant question therefore is to what degree an auto- generated Propbank would differ from an expertly created one. In a related line of work <ref type="bibr" target="#b2">(Akbik et al., 2016)</ref>, we have conducted a preliminary com- parison of an auto-generated Proposition Bank for Chinese and the manually created Chinese Proposi- tion <ref type="bibr">Bank (Xue and Palmer, 2005</ref>). Encouragingly, we find a significant overlap between both versions. Future work will further explore the usefulness of auto-generated Propbanks to train a semantic role la- beler  and their usefulness for downstream applications in low-resource languages. Partial syntactic annotation. Second, while cu- ration of semantically labeled predicate-argument structure can be formulated as human intelligence tasks, this will not in all likelihood be possible for full parse trees. These Propbanks would therefore lack a treebank-style syntactic layer of annotation. Would an existing Propbank facilitate the future task of creating treebanks for low-resource languages? In other words, could the traditional order of first cre- ating treebanks and then Propbanks be reversed? PROPBANK #SENTENCES #LABELS #FRAMES <ref type="table" target="#tab_1">Bengali  5,757  17,899  88  Malayalam  10,579  26,831  95  Tamil  3,486  11,765  68   Table 3</ref>: Number of labeled sentences, semantic labels and distinct frames of each auto-generated Propbank (before non- expert curation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Outlook</head><p>We applied annotation projection to low-resource languages and found a significant drop in quality vis-a-vis high-resource languages. We then pro- posed and outlined a curation process for semi- automatically generating Proposition Banks and noted encouraging results in an initial study. To encourage discussion within the research commu- nity, we make our generated Proposition Banks for Bengali, Malayalam and Tamil (see <ref type="table">Table 3</ref> for an overview) publicly available 4 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotation projection on a pair of very simple sentences. English Propbank frame (buy.01) and role (A0, A1) labels are projected onto aligned Tamil words. Furthermore, the typed dependencies between the words "my father" and "a house" (dotted lines) are projected onto their Tamil equivalents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>labels (predicates + roles) EN unlabeled corpus TL Parallel corpus semantic labels (projected, noisy)</head><label></label><figDesc></figDesc><table>Annotation 
projection 

TL 

Annotation projection 
Crowdsourced and expert data curation 

Crowd 
agrees? 

Input 

Crowdsourced 
data curation 

semantic labels 

(crowd cannot curate) 

semantic labels 

(curated, final) 

TL 

TL 

Expert data 
curation 

yes 

no 

Figure 2: Proposed process of using annotation projection in a parallel corpus from English (EN) to a target language (TL) as basis 

for crowdsourced data curation. Experts are only involved in cases where the crowd cannot agree on a label. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Parallel data sets and number of parallel sentences</head><label>1</label><figDesc></figDesc><table>used for each language. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Estimated precision and recall for Tamil, Bengali and Malayalam before and after non-expert curation. We list state- of-the-art results for German and Hindi for comparison.</figDesc><table></table></figure>

			<note place="foot" n="2"> Since we do not ask missing label questions for predicates, we cannot estimate predicate recall. 3 Following (Akbik et al., 2015), in the exact evaluation scheme, labels marked as correct and complete count as true positives. In partial, incomplete correct labels also count as true positives.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Multilingual semantic role labeling with unified labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2016, 54th Annual Meeting of the Association for Computational Linguistics: Demostration Session</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating high quality proposition banks for multilingual semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akbik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2015, 53rd Annual Meeting of the Association for Computational Linguistics Beijing</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilingual aliasing for auto-generating proposition banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akbik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, the 26th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The salsa corpus: a german 4 Datasets will be made available at this page: http: //researcher.watson.ibm.com/researcher/ view_group_subpage.php?id=7454 997 corpus resource for lexical semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2006, Fifth International Conference on Language Resources and Evaluation</title>
		<editor>Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Padó, and Manfred Pinkal</editor>
		<meeting>LREC 2006, Fifth International Conference on Language Resources and Evaluation<address><addrLine>Sofia, Bulgaria, August; Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] Jisup</forename><surname>Baker2011</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th linguistic annotation workshop</title>
		<editor>Marcus et al.1993] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini</editor>
		<meeting>the 5th linguistic annotation workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
		</imprint>
	</monogr>
	<note>How good is the crowd at real wsd?</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-lingual annotation projection for semantic roles</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="340" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Sebastian Padó and Mirella Lapata</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Framenet+: Fast paraphrastic tripling of framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="408" to="413" />
		</imprint>
	</monogr>
	<note>ACL (2)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2012, Eighth International Conference on Language Resources and Evaluation</title>
		<meeting>LREC 2012, Eighth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling up automatic cross-lingual semantic role annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Plas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer2005] Nianwen Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<editor>Citeseer. [Xue et al.2005] Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="207" to="238" />
		</imprint>
	</monogr>
	<note>Automatic semantic role labeling for chinese verbs</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Labeling chinese predicates with semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="255" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
