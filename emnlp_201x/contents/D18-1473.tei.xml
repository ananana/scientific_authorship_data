<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard Non-Monotonic Attention for Character-Level Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Shapiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">The Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hard Non-Monotonic Attention for Character-Level Transduction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4425" to="4438"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4425</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence model-ing tasks such as image captioning (Xu et al., 2015) and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language tasks are expressible as string-to-string transductions operating at the char- acter level. Probability models with recurrent neu- ral parameterizations currently hold the state of the art on many such tasks. On those string-to-string transduction tasks that involve a mapping between two strings of different lengths, it is often neces- sary to resolve which input symbols are related to which output symbols. As an example, consider the task of transliterating a Russian word into the Latin alphabet. In many cases, there exists a one-to- two mapping between Cyrillic and Latin letters: in Хурщёв (Khrushchev), the Russian Х can be con- sidered to generate the Latin letters Kh. Supervision is rarely, if ever, provided at the level of character- to-character alignments-this is the problem that attention seeks to solve in neural models.</p><p>With the rise of recurrent neural networks, this problem has been handled with "soft" attention rather than traditional hard alignment. Attention ( <ref type="bibr">Bahdanau et al., 2015</ref>) is often described as "soft," as it does not clearly associate a single input sym- bol with each output symbol, but rather offers a fuzzy notion of what input symbols may be re- sponsible for which symbols in the output. In con- trast, an alignment directly associates a given in- put symbol with a given output symbol. To ex- press uncertainty, practitioners often place a distri- bution over the exponential number of hard non- monotonic alignments, just as a probabilistic parser places a distribution over an exponential number of trees. The goal, then, is to learn the parameters of this distribution over all non-monotonic alignments through backpropagation. Incorporating hard align- ment into probabilistic transduction models dates back much farther in the NLP literature; arguably, originating with the seminal paper by <ref type="bibr">Brown et al. (1993)</ref>. Some neural approaches have moved back towards this approach of a more rigid alignment, referring to it as "hard attention." We will refer to this as "hard attention" and to more classical approaches as "alignment."</p><p>This paper offers two insights into the usage of hard alignment. First, we derive a dynamic pro- gram for the exact computation of the likelihood in a neural model with latent hard alignment: Previous work has used a stochastic algorithm to approxi- mately sum over the exponential number of align- ments between strings. In so doing, we go on to relate neural hard alignment models to the classical IBM Model 1 for alignment in machine translation. Second, we provide an experimental comparison that indicates hard attention models outperform soft attention models on three character-level string-to- string transduction tasks: grapheme-to-phoneme conversion, named-entity transliteration and mor- phological inflection. m e j r m e j r m e j <ref type="figure">Figure 1</ref>: Example of a non-monotonic character-level trans- duction from the Micronesian language of Pingelapese. The infinitive mejr is mapped through a reduplicative process to its gerund mejmejr ( <ref type="bibr" target="#b5">Rehg and Sohl, 1981)</ref>. Each input character is drawn in green and each output character is drawn in purple, connected with a line to the corresponding input character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Non-Monotonic Transduction</head><p>This paper presents a novel, neural, probabilistic latent-variable model for non-monotonic transduc- tion. As a concrete example of a non-monotonic transduction, consider the mapping of a Pinge- lapese infinitive to its gerund, as shown in <ref type="figure">Fig. 1</ref>. The mapping requires us to generate the output string left-to-right, bouncing around the input string out-of-order to determine the characters to trans- duce from. As the non-monotonic alignment is the latent variable, we will face a combinatorial prob- lem: summing over all non-monotonic alignments. The algorithmic contribution of this paper is the derivation of a simple dynamic program for com- puting this sum in polynomial time that still allows for very rich recurrent neural featurization of the model. With respect to the literature, our paper rep- resents the first instance of exact marginalization for a neural transducer with hard non-monotonic alignment; previous methods, such as <ref type="bibr" target="#b4">Rastogi et al. (2016)</ref> and <ref type="bibr">Aharoni and Goldberg (2017)</ref>, are ex- clusively monotonic.</p><p>Non-monotonic methods dominate character- level transduction. Indeed, the state of art in clas- sic character-level NLP tasks such as grapheme- to-phoneme conversion ( <ref type="bibr">Yao and Zweig, 2015)</ref>, transliteration <ref type="bibr" target="#b6">(Rosca and Breuel, 2016)</ref> and morphological inflection generation ( <ref type="bibr">Kann and Schütze, 2016</ref>) is held by the soft non-monotonic method of <ref type="bibr">Bahdanau et al. (2015)</ref>. Even though non-monotonicity is more common in word-level tasks, it also exists in character-level transduction tasks, as evidenced by our example in <ref type="figure">Fig. 1</ref> and the superior performance of non-monotonic meth- ods. Our error analysis in §8.4 sheds some light on why non-monotonic methods are the state of the art in a seemingly monotonic task.</p><p>A Note on the Character-level Focus. A natu- ral question at this point is why we are not experi- menting with word-level transduction tasks, such as machine translation. As we show in the §3.2 our method is often an order of magnitude slower, since it will involve a mixture of softmaxes. Thus, the exact marginalization scheme is practically un- workable for machine translation; we discuss future extensions for machine translation in §6. However, the slow-down is no problem for character-level tasks and we show empirical gains in §8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hard Non-Monotonic Alignment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Latent-Variable Model</head><p>An alphabet is a finite, non-empty set. Given two alphabets Σ x = {x 1 , . . . , x |Σx| } and Σ y = {y 1 , . . . , y |Σy| }, probabilistic approaches to the problem attempt to estimate a probability distribu- tion p(y | x) where y ∈ Σ * y and x ∈ Σ * x . Foreshad- owing, we will define the parameters of p to be, in part, the parameters of a recurrent neural network, in line with the state-of-the-art models. We define the set A = {1, . . . , |x|} |y| , which has an interpre- tation as the set of all (potentially non-monotonic) alignments from x to y with the restriction that each output symbol y i aligns to exactly one symbol in x ∈ Σ * x . In other words, A is the set of all many- to-one alignments between x and y where many may be as few as zero. We remark that |A| = |x| |y| , which is exponentially large in the length of the tar- get string y. For an a ∈ A, a i = j refers to the event that y i , the i th component of y, is aligned to x j , the j th component of x.</p><p>We define a probability distribution over output strings y conditioned on an input string x where we marginalize out unobserved alignments a:</p><formula xml:id="formula_0">p(y | x) = a∈A(x,y) p(y, a | x) (1) = a∈A |y| i=1 p(y i | a i , y &lt;i , x) p(a i | y &lt;i , x)</formula><p>exponential number of terms</p><formula xml:id="formula_1">(2) = |y| i=1 |x| a i =1 p(y i | a i , y &lt;i , x) p(a i | y &lt;i , x)</formula><p>polynomial number of terms</p><formula xml:id="formula_2">(3) = |y| i=1 |x| j=1 1 a i,j α j (i) p(y i | a i , y &lt;i , x)<label>(4)</label></formula><p>where we define α j (i) = p(a i | y &lt;i , x) in order to better notationally compare our model to that of <ref type="bibr">Bahdanau et al. (2015)</ref> in §5. Each distribution p(y i | a i , y &lt;i , x) in the definition of the model has a clean interpretation as a distribution over the output vocabulary Σ y , given an input string x ∈ Σ * x , where y i is aligned to x j . Thus, one way of thinking about this hard alignment model is as a product of mixture models, one mixture at each step, with mixing coefficients α j (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why Does Dynamic Programming Work?</head><p>Our dynamic program to compute the likelihood, fully specified in eq. <ref type="formula">(3)</ref>, is quite simple: The non- monotonic alignments are independent of each other, i.e., α j (i) is independent of α j (i − 1), condi- tioned on the observed sequence y. This means that we can cleverly rearrange the terms in eq. (2) using the distributive property. Were this not the case, we could not do better than having an exponential number of summands. This is immediately clear when we view our model as a graphical model, as in <ref type="figure">Fig. 2</ref>: There is no active trail from a i to a k where k &gt; i, ignoring the dashed lines. Note that this is no different than the tricks used to achieve exact inference in n th -order Markov models-one makes an independence assumption between the current bit of structure and the previous bits of structure to allow an efficient algorithm. For a proof of eq. (2)- eq. (3), one may look in <ref type="bibr">Brown et al. (1993)</ref>. Fore- shadowing, we note that certain parameterizations make use of input feeding, which breaks this inde- pendence; see §5.1.</p><p>Relation to IBM Model 1. The derivation above is similar to that of the IBM alignment model 1. We remark, however, two key generalizations that will serve our recurrent neural parameterization well in §4. First, traditionally, derivations of IBM Model 1 omit a prior over alignments p(a i | x), taking it to be uniform. Due to this omission, an additional mul- tiplicative constant ε /|x| |y| is introduced to ensure the distribution remains normalized <ref type="bibr">(Koehn, 2009)</ref>. Second, IBM Model 1 does not condition on previ- ously generated words on the output side. In other words, in their original model, <ref type="bibr">Brown et al. (1993)</ref> assume that p(y i | a i , y &lt;i , x) = p(y i | a i , x), forsaking dependence on y &lt;i . We note that there is no reason why we need to make this indepen- dence assumption-we will likely want a target- side language model in transduction. Indeed, subse- quent statistical machine translation systems, e.g., MOSES ( <ref type="bibr" target="#b0">Koehn et al., 2007)</ref>, integrate a language model into the decoder. It is of note that many models in NLP have made similar independence assumptions, e.g., the emission distribution hid- den Markov models (HMMs) are typically taken to be independent of all previous emissions <ref type="bibr" target="#b3">(Rabiner, 1989</ref>). These assumptions are generally not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithmic Analysis: Time Complexity</head><p>Let us assume that the requisite probability distribu- tions are computable in O(1) time and the softmax takes O(Σ y ). Then, by inspection, the computation of the distribution in eq. <ref type="formula" target="#formula_2">(4)</ref> is O (|x| · |y| · |Σ y |), as the sum in eq. <ref type="formula">(3)</ref> contains this many terms thanks to the dynamic program that allowed us to rearrange the sum and the product. While this "trick" is well known in the NLP literature-it dates from the seminal work in statistical machine trans- lation by <ref type="bibr">Brown et al. (1993)</ref>-it has been forgotten in recent formulations of hard alignment ( <ref type="bibr">Xu et al., 2015)</ref>, which use stochastic approximation to han- dle the exponential summands. As we will see in §5, we can compute the soft-attention model of <ref type="bibr">Bahdanau et al. (2015)</ref> in</p><formula xml:id="formula_3">O (|x| · |y| + |y| · |Σ y |) time.</formula><p>When Σ y is large, for example in case of machine translation with tens of thousands of Σ y at least, we can ignore |x| · |y| in soft-attention model, and the exact marginalization has an ex- tra |x|-factor compared to soft-attention model. In practice, <ref type="bibr">Shi and Knight (2017)</ref> show the bottle- neck of a NMT system is the softmax layer, making the extra |x|-factor practically cumbersome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Recurrent Neural Parameterization</head><p>How do we parameterize p(y i | a i , y &lt;i , x) and α j (i) in our hard, non-monotonic transduction model? We will use a neural network identical to the one proposed in the attention-based sequence- to-sequence model of <ref type="bibr" target="#b1">Luong et al. (2015)</ref> without input feeding (a variant of Bahdanau et al. <ref type="formula">(2015)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding the Input</head><p>All models discussed in this exposition will make use of the same mechanism for mapping a source string x ∈ Σ * x into a fixed-length representation in R d h . This mapping will take the form of a bidi- rectional recurrent neural network encoder, which works as follows: each element of Σ x is mapped to an embedding vector of length d e through a map- ping: e : Σ x → R de . Now, the RNN folds the</p><formula xml:id="formula_4">x a 1 a 2 a 3 a 4 h (dec) 1 h (dec) 2 h (dec) 3 h (dec) 4 y 1 y 2 y 3 y 4</formula><p>Figure 2: Our hard-attention model without input feeding viewed as a graphical model. Note that the circular nodes are random variables and the diamond nodes deterministic variables (h (dec) i is first discussed in §4.3). The independence assumption between the alignments ai when the yi are observed becomes clear. Note that we have omitted arcs from x to y1, y2, y3 and y4 for clarity (to avoid crossing arcs). We alert the reader that the dashed edges show the additional dependencies added in the input feeding version, as discussed in §5.1. Once we add these in, the ai are no longer independent and break exact marginalization. Note the hard-attention model does not enforce an exactly one-to-one constraint. Each source-side word is free to align to many of the target-side words, independent of context. In the latent variable model, the x variable is a vector of source words, and the alignment may be over more than one element of x.</p><p>following recursion over the string x left-to-right:</p><formula xml:id="formula_5">− → h (enc) j = tanh − → U (enc) e (enc) (x j )+ (5) − → V (enc) − → h (enc) j−1 + − → b (enc)</formula><p>where we fix the 0 th hidden state h (enc) 0 to the zero vector and the matrices </p><formula xml:id="formula_6">− → U (enc) ∈ R d h ×de , − → V (enc) ∈ R d</formula><formula xml:id="formula_7">j = − → h (enc) j ⊕ ← − h (enc) j</formula><p>, where ⊕ is vector concatenation.</p><p>As has become standard, we will use an exten- sion to this recursion: we apply the long short-term memory (LSTM; <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>) recursions, rather than those of a vanilla RNN (Elman network; Elman (1990)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameterization.</head><p>Now we define the alignment distribution α j (i).</p><formula xml:id="formula_8">α j (i) = exp(e ij ) |x| j =1 exp(e ij )<label>(6)</label></formula><formula xml:id="formula_9">e ij = h (dec) i T h (enc) j<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">T ∈ R d h ×2d h and h (dec) i</formula><p>, the decoder RNN's hidden state, is defined in §4.3. Importantly, the alignment distribution α j (i) at time step i will only depend on the prefix of the output string y &lt;i gen- erated so far. This is clear since the output-side decoder is a unidirectional RNN.</p><p>We also define p(y i | a i , y &lt;i , x).</p><formula xml:id="formula_11">p(y i | a i , y &lt;i , x) = softmax W f (h (dec) i , h (enc) a i )<label>(8)</label></formula><p>The function f is a non-linear and vector-valued; one popular choice of f is a multilayer perceptron with parameters to be learned. We define:</p><formula xml:id="formula_12">f (h (dec) i , h (enc) a i ) = tanh S (h (dec) i ⊕ h (enc) a i )<label>(9)</label></formula><p>where S ∈ R ds×3d h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Updating the hidden state</head><formula xml:id="formula_13">h (dec) i</formula><p>The hidden state h (dec) i is also updated through the LSTM recurrences <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>). The RNN version of the recurrence mirrors that of the encoder,</p><formula xml:id="formula_14">h (dec) i = tanh U (dec) e (dec) (y i−1 ) +<label>(10)</label></formula><formula xml:id="formula_15">V (dec) h (dec) i−1 + b (dec)</formula><p>where e (dec) : Σ y → R de produces an embedding of each of the symbols in the output alphabet. What is crucial about this RNN, like the α j (i), is that it only summarizes the characters decoded so far independently of the previous attention weights. In other words, the attention weights at time step i will have no influence from the attention weights at previous time steps, shown in <ref type="figure">Fig. 2</ref>. This is what allows for dynamic programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Transduction with Soft Attention</head><p>In order to contrast it with the hard alignment mech- anism we develop, we here introduce Luong atten- tion ( <ref type="bibr" target="#b1">Luong et al., 2015</ref>) for recurrent neural se- quence to sequence models ( <ref type="bibr">Sutskever et al., 2014)</ref>. Note that this model will also serve as an experi- mental baseline in §8.</p><p>The soft-attention transduction model defines a distribution over the output Σ * y , much like the hard- attention model, with the following expression:</p><formula xml:id="formula_16">p(y | x) = |y| i=1 p(y i | y &lt;i , x)<label>(11)</label></formula><p>where we define each conditional distribution as</p><formula xml:id="formula_17">p(y i | y &lt;i , x)<label>(12)</label></formula><formula xml:id="formula_18">= softmax W f (h (dec) i , c i )</formula><p>We reuse the function f in eq. (9). The hidden state h (dec) i , as before, is the i th state of a target-side language model that summarizes the prefix of the string decoded so far; this is explained in §4.3. And, finally, we define the context vector</p><formula xml:id="formula_19">c i = |x| j=1 α j (i) h (enc) j (13)</formula><p>using the same alignment distribution as in §4.2. In the context of the soft-attention model, this distri- bution is referred to as the attention weights.</p><p>Inspection shows that there is only a small dif- ference between the soft-attention model presented here and and our hard non-monotonic attention model. The difference is where we place the proba- bilities α j (i). In the soft-attention version, we place them inside the softmax (and the function f ), as in eq. <ref type="formula" target="#formula_17">(12)</ref>, and we have a mixture of the encoder's hidden states, the context vector, that we feed into the model. On the other hand, if we place them out- side the softmax, we have a mixture of softmaxes, as shown in eq. (3). Both models have identical set of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Input Feeding: What's That?</head><p>The equations in eq. (10), however, are not the only approach. Input-feeding is another popular approach that is, perhaps, standard at this point ( <ref type="bibr" target="#b1">Luong et al., 2015)</ref>. Input feeding refers to the setting where the architecture designer additionally feeds the attention weights into the update for the decoder's hidden state. This yields the recursion</p><formula xml:id="formula_20">h (dec) i = tanh U (dec) (e (dec) (y i−1 ) ⊕ ¯ c i−1 ) + V (dec) h (dec) i−1 + b (dec)<label>(14)</label></formula><p>where</p><formula xml:id="formula_21">¯ c i−1 = f (h (dec) i−1 , c i−1 ). Note that this re- quires that U (dec) ∈ R d h ×(de+ds)</formula><p>. This is the archi- tecture discussed in <ref type="bibr">Bahdanau et al. (2015, §3.1)</ref>. In contrast to the architecture above, this architecture has attention weights that do depend on previous attention weights due to the feeding in of the con- text vector c i . See <ref type="bibr">Cohn et al. (2016)</ref> for an attempt to incorporate structural biases into the manner in which the attention distribution is influenced by previous attention distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Combining Hard Non-Monotonic Attention with Input Feeding</head><p>To combine hard attention with input feeding, Xu et al. (2015) derive a variational lower bound on the log-likelihood though Jensen's inequality:</p><formula xml:id="formula_22">log p(y | x) = log a∈A(x,y) p(y, a | x) = log a∈A(x,y) p(a | x) · p(y | x, ) ≥ a∈A(x,y) p(a | x) log p(y | x, a)</formula><p>Note that we have omitted the dependence of p(a | x) on the appropriate prefix of y; this was done for notational simplicity. Using this bound, <ref type="bibr">Xu et al. (2015)</ref> derive an efficient approximation to the gra- dient using the REINFORCE trick of <ref type="bibr">Williams (1992)</ref>. This sampling-based gradient estimator is then used for learning, but suffers from high vari- ance. We compare to this model in §8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>Just as <ref type="bibr">Brown et al. (1993)</ref> started with IBM Model 1 and build up to richer models, we can do the same. Extensions, resembling those of IBM Model 2 and the HMM aligner ( <ref type="bibr">Vogel et al., 1996)</ref> source target</p><p>Grapheme-to-phoneme conversion a c t i o n AE K SH AH N Named-entity transliteration A A C H E N Morphological inflection N AT+ALL SG l i p u k e l i p u k k e e l l e <ref type="table">Table 1</ref>: Example of source and target string for each task as processed by the model that generalize IBM Model 1, are easily bolted on to our proposed model as well. If we are willing to perform approximate inference, we may also consider fertility as found in IBM Model 4. In order to extend our method to machine trans- lation (MT) in any practical manner, we require an approximation to the softmax. Given that the soft- max is already the bottleneck of neural MT models <ref type="bibr">(Shi and Knight, 2017</ref>), we can not afford ourselves a O(|x|) slowdown during training. Many methods have been proposed for approximating the softmax <ref type="bibr">(Goodman, 2001;</ref><ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr">Gutmann and Hyvärinen, 2010)</ref>. More recently, <ref type="bibr">Chen et al. (2016)</ref> compared methods on neural language modeling and <ref type="bibr">Grave et al. (2017)</ref> proposed a GPU-friendly method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">The Tasks</head><p>The empirical portion of the paper focuses on character-level string-to-string transduction prob- lems. We consider three tasks: G : grapheme-to- phoneme conversion, T : named-entity translitera- tion, and I : morphological inflection. We describe each briefly in turn and we give an example of a source and target string for each task in Tab. 1.</p><p>Grapheme-to-Phoneme Conversion. We use the standard grapheme-to-phoneme conversion (G2P) dataset: the Sphinx-compatible version of CMUDict <ref type="bibr">(Weide, 1998) and</ref><ref type="bibr">NetTalk (Sejnowski and</ref><ref type="bibr" target="#b7">Rosenberg, 1987)</ref>. G2P transduces a word, a string of graphemes, to its pronunciation, a string of phonemes. We evaluate with word error rate (WER) and phoneme error rate (PER) ( <ref type="bibr">Yao and Zweig, 2015)</ref>. PER is equal to the edit distance divided by the length of the string of phonemes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>The goal of the empirical portion of our paper is to perform a controlled study of the different archi- tectures and approximations discussed up to this point in the paper. §8.1 exhibits the neural archi- tectures we compare and the main experimental results 1 are in Tab. 3. In §8.2, we present the exper- imental minutiae, e.g. hyperparameters. In §8.3, we analyze our experimental findings. Finally, in §8.4, we perform error analysis and visualize the soft attention weight and hard alignment distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">The Architectures</head><p>The four architectures we consider in controlled comparison are: 1 : soft attention with input feed- ing, 2 : hard attention with input feeding, 3 : soft attention without input feeding and <ref type="bibr">4</ref> : hard atten- tion without input feeding (our system). They are also shown in Tab. 2. As a fifth system, we com- pare to the monotonic system M : Aharoni and Goldberg <ref type="bibr">(2017</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Experimental Details</head><p>We implement the experiments with PyTorch (Paszke et al., 2017) and we port the code of Aha- roni and Goldberg (2017) to admit batched training. Because we did not observe any improvements in preliminary experiments when decoding with beam search 2 , all models are decoded greedily.</p><p>Data Preparation. For G , we sample 5% and 10% of the data as development set and test set, respectively. For T , we only run experiments with 11 out of 14 language pairs 3 because we do not have access to all the data. M has quite a different architecture: The input of the decoder RNN is the concatenation of the pre- viously predicted word embedding, the encoder's hidden state at a specific step, and in the case of I , the encoding of the morphological tag. Differing from <ref type="bibr">Aharoni and Goldberg (2017)</ref>, we concate- nate all attributes' embeddings (0 for attributes that are not applicable) and merge them with a linear mapping. The dimension of the merged vector and attributes vector are d e . To ensure that it has the same number of parameters as the rest of the model, we increase the hidden size of the decoder RNN.</p><p>Optimization. We train the model with Adam ( <ref type="bibr">Kingma and Ba, 2014</ref>) with an initial learning rate of 0.001. We halve the learning rate whenever the development log-likelihood doesn't improve. We stop after the learning rate dips to 1 × 10 −5 . We save all models after each epoch and select the model with best development performance. We train the model for at most 50 epochs, though all the experiments stop early. We train on G , T , and I with batch sizes of 20, 50 and 20, respectively. We notice in the experiments that the training of 1 and U is quite unstable with the large model, probably because of the longer chain of gradient information flow. We apply gradient clipping to the large model with maximum gradient norm 5.</p><p>REINFORCE. In the REINFORCE training of R and 2 , we sample 2 and 4 positions at each time step for the small and large model, respectively. The latter is tuned on selected languages in I with search range {2,3,4,5}. To stabilize the training, we apply a baseline with a moving average reward and</p><formula xml:id="formula_23">&lt; s &gt; V A C T P S T P R F N E G I N D 2 S G l ä n t ä t ä &lt; \ s &gt;</formula><p>e t o l l u t l ä n t ä n n y t &lt; \ s &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; s &gt; V A C T P S T P R F N E G I N D</head><p>2 S G l ä n t ä t ä &lt; \ s &gt; e t o l l u t l ä n t ä n n y t &lt; \ s &gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Experimental Findings</head><p>Finding #1: Effect of Input Feeding. By com- paring 3 and 4 against 1 and 2 in Tab. 3, we find input feeding hurts performance in all settings and all tasks. This runs in contrast to the reported results of <ref type="bibr" target="#b1">Luong et al. (2015)</ref>, but they experiment on machine translation, rather than character-level transduction. This validates our independence as- sumption about the alignment distribution. Finding #3: Non-monotonicity vs. Monotonic- ity. The monotonic model M underperforms compared to non-monotonic models 3 in Tab. 3 except for one setting. It performs slightly worse on T and G due to the many-to-one alignments in the data and the fact that Aharoni and Goldberg (2017) can only use the hidden vector of the final element of the span in a many-to-one alignment to directly predict the one target element. The current state-of-the-art systems for character-level string transduction are non-monotonic models, despite the tasks' seeming monotonicity; see §8.4.</p><p>Finding #4: Approximate Hard Attention. Given our development of an exact marginalization method for neural models with hard attention, a natural question to ask is how much exact marginal- ization helps during learning. By comparing <ref type="bibr">4</ref> and R in Tab. 3, we observe that training with exact  marginalization clearly outperforms training under stochastic approximation in every setting and on every dataset. We also observe that exact marginal- ization allows faster convergence, since training with REINFORCE is quite unstable where some runs seemingly to get stuck.</p><p>Finding #5: Controlling for Parameters. Input feeding yields a more expressive model, but also leads to an increase in the number of parameters. Here, we explore what effect this has on the perfor- mance of the models. In their ablation, <ref type="bibr" target="#b1">Luong et al. (2015)</ref> did not control the number of parameters when adding input feeding. The total number of pa- rameters of U is 1.679M for the small setting and 10.541M for the large setting, which has 40% and 22.3% more parameters than the controlled setting. By comparing 1 and U in Tab. 3, we find that the increase in parameters, rather than the increase in expressivity explains the success of input feeding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Visualization and Error Analysis</head><p>We hypothesize that even though the model is non-monotonic, it can learn monotonic alignment with flexibility if necessary, giving state-of-the-art results on many seemingly monotonic character- level string transduction task. To show more in- sights, we compare the best soft attention model ( 3 ) against the best hard alignment model <ref type="formula" target="#formula_2">( 4 )</ref> on G by showing the confusion matrix of each model in Tab. 5. An alignment is non-monotonic when alignment edges predicted by the model cross.</p><p>There is an edge connecting x j and y i if the atten- tion weight or hard alignment distribution α j (i) is larger than 0.1. We find that the better-performing transducers are more monotonic, and most learned alignments are monotonic. The results indicate that there are a few transductions that are indeed non- monotonic in the dataset. However, the number is so few that this does not entirely explain why non-monotonic models outperform the monotonic models. We speculate this lies in the architecture of <ref type="bibr">Aharoni and Goldberg (2017)</ref>, which does not permit many-to-one alignments, while monotonic alignment learned by the non-monotonic model is more flexible. Future work will investigate this.</p><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, we visualize the soft attention weights ( 3 ) and the hard alignment distribution ( 4 ) side by side. We observe that the hard alignment distri- bution is more interpretable, with a clear boundary when predicting the prefixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We exhibit an efficient dynamic program for the exact marginalization of all non-monotonic align- ments in a neural sequence-to-sequence model. We show empirically that exact marginalization helps over approximate inference by REINFORCE and that models with hard, non-monotonic alignment outperform those with soft attention.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>h ×d h and the bias term − → b (enc) ∈ R d h are parameters to be learned. Performing the same procedure on the reversed string and using an RNN with different parameters, we arrive at hid- den state vectors ← − h (enc) j . The final hidden states from the encoder are the concatenation of the two, i.e., h (enc)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model Hyperparameters.</head><label></label><figDesc>The hyperparameters of all models are in Tab. 4. The hyperparameters of the large model are tuned using the baseline 3 on selected languages in I , and the search range is shown in Tab. 4. All three tasks have the same two sets of hyperparameters. To ensure that 1 has the same number of parameters as the other models, we decrease d s in eq. (9) while for the rest of the 2 Compared to greedy decoding with an average error rate of 20.1% and an average edit distance of 0.385, beam search with beam size 5 gets a slightly better edit distance of 0.381 while hurting the error rate with 20.2%. 3 Ar-En, En-Ba, En-Hi, En-Ja, En-Ka, En-Ko, En-Pe, En-Ta, En-Th, Jn-Jk and Th-En. models d s = 3d h . Additionally, we use a linear mapping to merge e (dec) (y i−1 ) and ¯ c i−1 in eq. (14) instead of concatenation. The output of the linear mapping has the same dimension as e (dec) (y i−1 ), ensuring that the RNN has the same size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention-weight ( 3 ; left) and alignment distribution ( 4 ; right) of Finnish in I. Both models predict correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Finding #2:</head><label></label><figDesc>Soft Attention vs. Hard Attention. Training with REINFORCE hurts the performance of the hard attention model; compare 1 and 2 (trained with REINFORCE), in Tab. 3. On the other hand, training with exact marginalization causes the hard attention model to outperform soft atten- tion model in nearly all settings; compare 3 and 4 in Tab. 3. This comparison shows that hard attention outperforms soft attention in character- level string transduction when trained with exact marginalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>-Entity Transliteration. We use the NEWS 2015 shared task on machine transliteration (Zhang et al., 2015) as our named-entity translitera- tion dataset. It contains 14 language pairs. Translit- eration transduces a named entity from its source language to a target language-in other words, from a string in the source orthography to a string in the target orthography. We evaluate with word accuracy in percentage (ACC) and mean F-score</figDesc><table>soft attention 
hard alignment 

input-fed 
yes 1 Bahdanau et al. (2015); Luong et al. (2015) 2 Xu et al. (2015) 

no 3 Luong et al. (2015) without input feeding 
4 This work 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The 4 architectures considered in the paper. 

(MFS) (Zhang et al., 2015). For completeness, we 
include the definition of MFS in App. A. 

Morphological Inflection. We consider the 
high-resource setting of task 1 in the CoNLL-
SIGMORPHON 2017 shared task (Cotterell et al., 
2017) as our morphological inflection dataset. It 
contains 51 languages in the high resource setting. 
Morphological inflection transduces a lemma (a 
string of characters) and a morphological tag (a se-
quence of subtags) to an inflected form of the word 
(a string of characters). We evaluate with word ac-
curacy (ACC) and average edit distance (MLD) 
(Cotterell et al., 2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Average test performance on G , T and I averaged across datasets and languages. See App. B fCor full breakdown.</head><label>3</label><figDesc></figDesc><table>Small 
Large 
Search range 

Emb. dim. 100 
200 
{50,100,200,300} 
Enc. dim. 200 
400 
{100,200,400,600} 
Enc. layer 1 
2 
{1,2,3} 
Dec. dim. 
200 
400 
{100,200,400,600} 
Dec. layer 1 
1 
{1,2,3} 
Dropout 
0.2 
0.4 
{0,0.1,0.2,0.4} 
# param. 
1.199M 8.621M N/A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model hyperparameters and search range 

controlled for, and R , a variant of 4 trained using 
REINFORCE instead of exact marginalization. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Breakdown of correct and incorrect predictions of 

monotonic and non-monotonic alignments of 3 and 4 in G , 
derived from the soft attention weights and the hard alignment 
distribution 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>WER PER WER PER WER PER WER PER WER PER WER PER WER PER</head><label></label><figDesc></figDesc><table>Roee Aharoni and Yoav Goldberg. 2017. Morphologi-
cal inflection generation with hard monotonic atten-
tion. In Proceedings of the 55th Annual Meeting of 
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2004-2015, Vancouver, 
Canada. Association for Computational Linguistics. 

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly 
learning to align and translate. In International Con-
ference on Learning Representations (ICLR), vol-
ume abs/1409.0473. 

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and 
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research, 
3(Feb):1137-1155. 

Peter F. Brown, Vincent J. Della Pietra, Stephen 
A. Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: Pa-
rameter estimation. 
Computational Linguistics, 
19(2):263-311. 

Wenlin Chen, David Grangier, and Michael Auli. 2016. 
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of the 54th Annual 
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages 
1975-1985. 

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza 
Haffari. 2016. Incorporating structural alignment bi-
ases into an attentional neural translation model. In 
Proceedings of the 2016 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 876-885, San Diego, California. Association 
for Computational Linguistics. 

Ryan Cotterell, Christo Kirov, John Sylak-Glassman, 
Géraldine Walther, Ekaterina Vylomova, Patrick 
Xia, Manaal Faruqui, Sandra Kübler, David 
Yarowsky, Jason Eisner, and Mans Hulden. 2017. 
The CoNLL-SIGMORPHON 2017 shared task: Uni-
versal morphological reinflection in 52 languages. 
In Proceedings of the CoNLL-SIGMORPHON 2017 
Shared Task: Universal Morphological Reinflection, 
Vancouver, Canada. Association for Computational 
Linguistics. 

Jeffrey L. Elman. 1990. Finding structure in time. Cog-
nitive Science, 14(2):179-211. 

Joshua Goodman. 2001. Classes for fast maximum 
entropy training. In Acoustics, Speech, and Signal 
Processing, 2001. Proceedings.(ICASSP'01). 2001 
IEEE International Conference on, volume 1, pages 
561-564. IEEE. 

Edouard Grave, Armand Joulin, Moustapha Cissé, 
David Grangier, and Hervé Jégou. 2017. Efficient 
softmax approximation for gpus. In ICML. 

Michael Gutmann and Aapo Hyvärinen. 2010. Noise-
contrastive estimation: A new estimation principle 
for unnormalized statistical models. In Proceedings 
of the Thirteenth International Conference on Artifi-
cial Intelligence and Statistics, pages 297-304. 

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Neu-
ral Computation, 9(8):1735-1780. 

Katharina Kann and Hinrich Schütze. 2016. Single-
model encoder-decoder with explicit morphological 
representation for reinflection. In Proceedings of the 
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages 
555-560, Berlin, Germany. Association for Compu-
tational Linguistics. 

Diederik P Kingma and Jimmy Ba. 2014. Adam: A 
method for stochastic optimization. arXiv preprint 
arXiv:1412.6980. 

Philipp Koehn. 2009. Statistical Machine Translation. 
Cambridge University Press. Xing Shi and Kevin Knight. 2017. Speeding up neural 
machine translation decoding by shrinking run-time 
vocabulary. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics 
(Volume 2: Short Papers), volume 2, pages 574-579. 

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. 
Sequence to sequence learning with neural networks. 
In Advances in Neural Information Processing Sys-
tems 27 (NIPS), pages 3104-3112. 

Stephan Vogel, Hermann Ney, and Christoph Tillmann. 
1996. HMM-based word alignment in statistical 
translation. In Proceedings of the 16th Conference 
on Computational Linguistics -Volume 2, COLING 
'96, pages 836-841, Stroudsburg, PA, USA. 

R.L. Weide. 1998. The carnegie mellon pronouncing 
dictionary. 

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Reinforcement Learning, pages 
5-32. Springer. 

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun 
Cho, Aaron C. Courville, Ruslan Salakhutdinov, 
Richard S. Zemel, and Yoshua Bengio. 2015. Show, 
attend and tell: Neural image caption generation 
with visual attention. 
In Proceedings of the 
32nd International Conference on Machine Learn-
ing, ICML, pages 2048-2057. 

Kaisheng Yao and Geoffrey Zweig. 2015. Sequence-
to-sequence neural net models for grapheme-to-
phoneme conversion. In INTERSPEECH 2015, 
pages 3330-3334, Dresden, Germany. 

Min Zhang, Haizhou Li, Rafael E. Banchs, and A. Ku-
maran. 2015. Whitepaper of news 2015 shared task 
on machine transliteration. In NEWS@ACL. A MFS 

LCS(c, r) = 
1 
2 
(|c| + |r| − ED(c, r)) 

R i = 
LCS(c i , r i ) 
|r i | 

P i = 
LCS(c i , r i ) 
|c i | 

F S i = 2 
R i × P i 
R i + P i 

where r i and c i is the i-th reference and prediction, 

B Full breakdown of experiments 

A full breakdown of G and T can be found in 
Tab. 6 and Tab. 7, respectively. A full breakdown 
of I can be found in Tab. 8 and Tab. 9. Small 

1 
U 
2 
3 
4 
R 
M 

CMUDict 36.2 
0.086 31.0 
0.074 35.1 
0.083 30.8 
0.073 30.5 
0.072 31.2 
0.074 32.0 
0.075 

NETtalk 
31.2 
0.075 30.2 
0.075 29.6 
0.074 29.8 
0.074 28.8 
0.073 30.3 
0.078 35.7 
0.088 

Large 

1 
U 
2 
3 
4 
R 
M 

WER PER WER PER WER PER WER PER WER PER WER PER WER PER 

CMUDict 32.3 
0.076 31.4 
0.073 36.7 
0.087 30.5 
0.073 29.8 
0.071 31.8 
0.077 30.5 
0.072 

NETtalk 
29.3 
0.071 29.4 
0.072 29.5 
0.075 26.8 
0.068 26.6 
0.066 27.7 
0.071 29.3 
0.072 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Full breakdown of G2P 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 : Full breakdown of NEWS2015</head><label>7</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Because we do not have access to the test set of T , we only report development performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="623" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ponapean Reference Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><forename type="middle">G</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>University of Hawaii Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sequenceto-sequence neural network models for transliteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09565</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parallel networks that learn to pronounce english text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
