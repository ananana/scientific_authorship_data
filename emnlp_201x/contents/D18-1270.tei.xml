<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Multilingual Supervision for Cross-lingual Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2486</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
							<email>shyamupa@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA, PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
							<email>nitishg@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA, PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<email>danroth@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA, PA, PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Multilingual Supervision for Cross-lingual Entity Linking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2486" to="2495"/>
							<date type="published">October 31-November 4, 2018. 2018. 2486</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual Entity Linking (XEL) aims to ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across 8 languages shows that our approach significantly improves over the current state-of-the-art. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic scenarios, and shows the value of joint supervision in low-resource settings. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity Linking (EL) systems ground entity men- tions in text to entries in Knowledge Bases (KB), such as Wikipedia ( <ref type="bibr" target="#b18">Mihalcea and Csomai, 2007)</ref>. Recently, the task of Cross-lingual Entity Linking (XEL) has gained attention <ref type="bibr" target="#b17">(McNamee et al., 2011;</ref><ref type="bibr" target="#b12">Ji et al., 2015;</ref><ref type="bibr" target="#b26">Tsai and Roth, 2016</ref>) with the goal of grounding entity mentions written in any language to the English Wikipedia. For instance, <ref type="figure" target="#fig_1">Figure 1</ref> shows a Tamil (a language with &gt;70 million speak- ers) and an English mention (shown <ref type="bibr">[enclosed]</ref>) <ref type="bibr">1</ref> Code at www.github.com/shyamupa/xelms  [mentions] of the entity Liverpool_F.C. from the respective Wikipedias. Tamil Wikipedia only has 9 mentions referring to Liverpool_F.C., whereas English Wikipedia has 5303 such mentions. Clearly, there is a need to augment the limited contextual evidence in low-resource languages with evidence from high-resource languages like English. Tamil sentence translates to "Suarez plays for <ref type="bibr">[Liverpool]</ref> and Uruguay."</p><p>and their mention contexts. XEL involves ground- ing the Tamil mention (which translates to 'Liv- erpool') to the football club Liverpool_F.C., and not the city or the university. XEL enables knowl- edge acquisition directly from documents in any language, without resorting to machine translation. Training an EL model requires grounded men- tions, i.e. mentions of entities that are grounded to a Knowledge Base (KB), as supervision ( <ref type="figure" target="#fig_1">Figure 1</ref>). While millions of such mentions are available in English, by virtue of hyperlinks in the English Wikipedia, this is not the case for most languages. This makes learning XEL models challenging, es- pecially for languages with limited resources (e.g., the Tamil Wikipedia is only 1% of the English Wikipedia in size). To overcome this challenge, it is desirable to augment the limited contextual evidence available in the target language with evi- dence from high-resource languages like English.</p><p>We propose XELMS (XEL with Multilingual Supervision) ( ยง2), the first approach that fulfills the above desiderata by using multilingual super- vision to train an XEL model. XELMS represents the mention contexts of the same entity from differ- ent languages in the same semantic space using a single context encoder ( ยง2.1). Language-agnostic entity representations are jointly learned with the relevant mention context representations, so that an entity and its context share similar representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Context Encoder</head><p>Tamil Mention Contexts (low-resource)</p><p>English Mention Contexts (high-resource) Everton won against <ref type="bibr">[Liverpool]</ref> in a FA Cup match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TE-Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 E V n z P T N A B Z 8 p A f S F 5 N D / T z H L b A = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L I n j w U K F f 0 I S y 2 W 7 b p Z t N 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V 1 t Y 3 N r e K 2 6 W d 3 b 3 9 A / u w 3 N J R o i h r 0 k h E q h M Q z Q S X r A k c B O v E i p E w E K w d j G 9 n f v u J K c 0 j 2 Y B p z P y Q D C U f c E r A S D 2 7 7 A G b Q O p p i h t 3 5 w + R 1 l n P r j h V Z w 6 8 S t y c V F C O e s / + 8 v o R T U I m g Q q i d d d 1 Y v B T o o B T w b K S l 2 g W E z o m Q 9 Y 1 V J K Q a T + d 3 5 7 h U 6 P 0 8 S B S p i T g u f p 7 I i W h 1 t M w M J 0 h g Z F e 9 m b i f 1 4 3 g c G 1 n 3 I Z J 8 A k X S w a J A J D h G d B 4 D 5 X j I K Y G k K o 4 u Z W T E d E E Q o m r p I J w V 1 + e Z W 0 L q q u U 3 U f L y u 1 m z y O I j p G J + g M u e g K 1 d A 9 q q M m o m i C n t E r e r M y 6 8 V 6 t z 4 W r Q U r n z l C f 2 B 9 / g C u t 5 Q x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 E V n z P T N A B Z 8 p A f S F 5 N D / T z H L b A = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L I n j w U K F f 0 I S y 2 W 7 b p Z t N 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V 1 t Y 3 N r e K 2 6 W d 3 b 3 9 A / u w 3 N J R o i h r 0 k h E q h M Q z Q S X r A k c B O v E i p E w E K w d j G 9 n f v u J K c 0 j 2 Y B p z P y Q D C U f c E r A S D 2 7 7 A G b Q O p p i h t 3 5 w + R 1 l n P r j h V Z w 6 8 S t y c V F C O e s / + 8 v o R T U I m g Q q i d d d 1 Y v B T o o B T w b K S l 2 g W E z o m Q 9 Y 1 V J K Q a T + d 3 5 7 h U 6 P 0 8 S B S p i T g u f p 7 I i W h 1 t M w M J 0 h g Z F e 9 m b i f 1 4 3 g c G 1 n 3 I Z J 8 A k X S w a J A J D h G d B 4 D 5 X j I K Y G k K o 4 u Z W T E d E E Q o m r p I J w V 1 + e Z W 0 L q q u U 3 U f L y u 1 m z y O I j p G J + g M u e g K 1 d A 9 q q M m o m i C n t E r e r M y 6 8 V 6 t z 4 W r Q U r n z l C f 2 B 9 / g C u t 5 Q x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 E V n z P T N A B Z 8 p A f S F 5 N D / T z H L b A = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L I n j w U K F f 0 I S y 2 W 7 b p Z t N 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V 1 t Y 3 N r e K 2 6 W d 3 b 3 9 A / u w 3 N J R o i h r 0 k h E q h M Q z Q S X r A k c B O v E i p E w E K w d j G 9 n f v u J K c 0 j 2 Y B p z P y Q D C U f c E r A S D 2 7 7 A G b Q O p p i h t 3 5 w + R 1 l n P r j h V Z w 6 8 S t y c V F C O e s / + 8 v o R T U I m g Q q i d d d 1 Y v B T o o B T w b K S l 2 g W E z o m Q 9 Y 1 V J K Q a T + d 3 5 7 h U 6 P 0 8 S B S p i T g u f p 7 I i W h 1 t M w M J 0 h g Z F e 9 m b i f 1 4 3 g c G 1 n 3 I Z J 8 A k X S w a J A J D h G d B 4 D 5 X j I K Y G k K o 4 u Z W T E d E E Q o m r p I J w V 1 + e Z W 0 L q q u U 3 U f L y u 1 m z y O I j p G J + g M u e g K 1 d A 9 q q M m o m i C n t E r e r M y 6 8 V 6 t z 4 W r Q U r n z l C f 2 B 9 / g C u t 5 Q x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 E V n z P T N A B Z 8 p A f S F 5 N D / T z H L b A = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L I n j w U K F f 0 I S y 2 W 7 b p Z t N 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V 1 t Y 3 N r e K 2 6 W d 3 b 3 9 A / u w 3 N J R o i h r 0 k h E q h M Q z Q S X r A k c B O v E i p E w E K w d j G 9 n f v u J K c 0 j 2 Y B p z P y Q D C U f c E r A S D 2 7 7 A G b Q O p p i h t 3 5 w + R 1 l n P r j h V Z w 6 8 S t y c V F C O e s / + 8 v o R T U I m g Q q i d d d 1 Y v B T o o B T w b K S l 2 g W E z o m Q 9 Y 1 V J K Q a T + d 3 5 7 h U 6 P 0 8 S B S p i T g u f p 7 I i W h 1 t M w M J 0 h g Z F e 9 m b i f 1 4 3 g c G 1 n 3 I Z J 8 A k X S w a J A J D h G d B 4 D 5 X j I K Y G k K o 4 u Z W T E d E E Q o m r p I J w V 1 + e Z W 0 L q q u U 3 U f L y u 1 m z y O I j p G J + g M u e g K 1 d A 9 q q M m o m i C n t E r e r M y 6 8 V 6 t z 4 W r Q U r n z l C f 2 B 9 / g C u t 5 Q x &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TC-Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L d X p 4 I X v X Z p z w / t l J 3 G m K x X a 3 s = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L v X j w U K F f 0 I S y 2 W 7 b p Z t s 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V N j a 3 t n e K u 6 W 9 / Y P D I / u 4 3 N Y y U Z S 1 q B R S d Q O i m e A R a w E H w b q x Y i Q M B O s E k / r c 7 z w x p b m M m j C L m R + S U c S H n B I w U t 8 u e 8 C m k H q a 4 m b 9 8 k F q n f X t i l N 1 F s D r x M 1 J B e V o 9 O 0 v b y B p E r I I q C B a 9 1 w n B j 8 l C j g V L C t 5 i W Y x o R M y Y j 1 D I x I y 7 a e L 2 z N 8 b p Q B H k p l K g K 8 U H 9 P p C T U e h Y G p j M k M N a r 3 l z 8 z + s l M L z 1 U x 7 F C b C I L h c N E 4 F B 4 n k Q e M A V o y B m h h C q u L k V 0 z F R h I K J q 2 R C c F d f X i f t q 6 r r V N 3 H 6 0 r t L o + j i E 7 R G b p A L r p B N X S P G q i F K J q i Z / S K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 Q H 1 i f P 6 u j l C 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L d X p 4 I X v X Z p z w / t l J 3 G m K x X a 3 s = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L v X j w U K F f 0 I S y 2 W 7 b p Z t s 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V N j a 3 t n e K u 6 W 9 / Y P D I / u 4 3 N Y y U Z S 1 q B R S d Q O i m e A R a w E H w b q x Y i Q M B O s E k / r c 7 z w x p b m M m j C L m R + S U c S H n B I w U t 8 u e 8 C m k H q a 4 m b 9 8 k F q n f X t i l N 1 F s D r x M 1 J B e V o 9 O 0 v b y B p E r I I q C B a 9 1 w n B j 8 l C j g V L C t 5 i W Y x o R M y Y j 1 D I x I y 7 a e L 2 z N 8 b p Q B H k p l K g K 8 U H 9 P p C T U e h Y G p j M k M N a r 3 l z 8 z + s l M L z 1 U x 7 F C b C I L h c N E 4 F B 4 n k Q e M A V o y B m h h C q u L k V 0 z F R h I K J q 2 R C c F d f X i f t q 6 r r V N 3 H 6 0 r t L o + j i E 7 R G b p A L r p B N X S P G q i F K J q i Z / S K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 Q H 1 i f P 6 u j l C 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L d X p 4 I X v X Z p z w / t l J 3 G m K x X a 3 s = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L v X j w U K F f 0 I S y 2 W 7 b p Z t s 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V N j a 3 t n e K u 6 W 9 / Y P D I / u 4 3 N Y y U Z S 1 q B R S d Q O i m e A R a w E H w b q x Y i Q M B O s E k / r c 7 z w x p b m M m j C L m R + S U c S H n B I w U t 8 u e 8 C m k H q a 4 m b 9 8 k F q n f X t i l N 1 F s D r x M 1 J B e V o 9 O 0 v b y B p E r I I q C B a 9 1 w n B j 8 l C j g V L C t 5 i W Y x o R M y Y j 1 D I x I y 7 a e L 2 z N 8 b p Q B H k p l K g K 8 U H 9 P p C T U e h Y G p j M k M N a r 3 l z 8 z + s l M L z 1 U x 7 F C b C I L h c N E 4 F B 4 n k Q e M A V o y B m h h C q u L k V 0 z F R h I K J q 2 R C c F d f X i f t q 6 r r V N 3 H 6 0 r t L o + j i E 7 R G b p A L r p B N X S P G q i F K J q i Z / S K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 Q H 1 i f P 6 u j l C 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L d X p 4 I X v X Z p z w / t l J 3 G m K x X a 3 s = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L v X j w U K F f 0 I S y 2 W 7 b p Z t s 2 J 1 I S 8 h f 8 e J B E a / + E W / + G 7 d t D t r 6 Y O D x 3 g w z 8 4 J Y c A 2 O 8 2 0 V N j a 3 t n e K u 6 W 9 / Y P D I / u 4 3 N Y y U Z S 1 q B R S d Q O i m e A R a w E H w b q x Y i Q M B O s E k / r c 7 z w x p b m M m j C L m R + S U c S H n B I w U t 8 u e 8 C m k H q a 4 m b 9 8 k F q n f X t i l N 1 F s D r x M 1 J B e V o 9 O 0 v b y B p E r I I q C B a 9 1 w n B j 8 l C j g V L C t 5 i W Y x o R M y Y j 1 D I x I y 7 a e L 2 z N 8 b p Q B H k p l K g K 8 U H 9 P p C T U e h Y G p j M k M N a r 3 l z 8 z + s l M L z 1 U x 7 F C b C I L h c N E 4 F B 4 n k Q e M A V o y B m h h C q u L k V 0 z F R h I K J q 2 R C c F d f X i f t q 6 r r V N 3 H 6 0 r t L o + j i E 7 R G b p A L r p B N X S P G q i F K J q i Z / S K 3 q z M e r H e r Y 9 l a 8 H K Z 0 7 Q H 1 i f P 6 u j l C 8 = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EC-Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y W g f I Q 3 Q y 5 E O 0 y v N b 3 q f k w P c 3 N 8 = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L R f D g o Y L 9 g C a U z X b b L t 1 s w u 5 E W k L + i h c P i n j 1 j 3 j z 3 7 h t c 9 D W B w O P 9 2 a Y m R f E g m t w n G + r s L a + s b l V 3 C 7 t 7 O 7 t H 9 i H 5 Z a O E k V Z k 0 Y i U p 2 A a C a 4 Z E 3 g I F g n V o y E g W D t Y F y f + e 0 n p j S P 5 C N M Y + a H Z C j 5 g F M C R u r Z Z Q / Y B F J P U 3 x b P 7 + P t M 5 6 d s W p O n P g V e L m p I J y N H r 2 l 9 e P a B I y C V Q Q r b u u E 4 O f E g W c C p a V v E S z m N A x G b K u o Z K E T P v p / P Y M n x q l j w e R M i U B z 9 X f E y k J t Z 6 G g e k M C Y z 0 s j c T / / O 6 C Q y u / Z T L O A E m 6 W L R I B E Y I j w L A v e 5 Y h T E 1 B B C F T e 3 Y j o i i l A w c Z V M C O 7 y y 6 u k d V F 1 n a r 7 c F m p 3 e R x F N E x O k F n y E V X q I b u U A M 1 E U U T 9 I x e 0 Z u V W S / W u / W x a C 1 Y + c w R + g P r 8 w e U f p Q g &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y W g f I Q 3 Q y 5 E O 0 y v N b 3 q f k w P c 3 N 8 = " &gt; A A A B + 3 i c b V B N S 8 N A E N 3 U r 1 q / Y j 1 6 W S y C F 0 s i g h 6 L R f D g o Y L 9 g C a U z X b b L t 1 s w u 5 E W k L + i h c P i n j 1 j 3 j z 3 7 h t c 9 D W B w O P 9 2 a Y m R f E g m t w n G + r s L a + s b l V 3 C 7 t 7 O 7 t H 9 i H 5 Z a O E k V Z k 0 Y i U p 2 A a C a 4 Z E 3 g I F g n V o y E g W D t Y F y f + e 0 n p j S P 5 C N M Y + a H Z C j 5 g F M C R u r Z Z Q / Y B F J P U 3 x b P 7 + P t M 5 6 d s W p O n P g V e L m p I J y N H r 2 l 9 e P a B I y C V Q Q r b u u E 4 O f E g W c C p a V v E S z m N A x G b K u o Z K E T P v p / P Y M n x q l j w e R M i U B z 9 X f E y k J t Z 6 G g e k M C Y z 0 s j c T / / O 6 C Q y u / Z T L O A E m 6 W L R I B E Y I j w L A v e 5 Y h T E 1 B B C F T e 3 Y j o i i l A w c Z V M C O 7 y y 6 u k d V F 1 n a r 7 c F m p 3 e R x F N E</head><formula xml:id="formula_0">! เฎฐ เฏ s [!เฎตr$l] เฎฎ*+m เฎ&amp;' เฎต เฏ -เฎณ เฏ เฎฏเฎพ23เฎฑเฎพr.</formula><p>Liverpool F.C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T w S I E j i x f w 4 7 R y e J L A j p q X Q A 4 P c = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U h E 0 G W x I C 5 c V L A P a E K Z T K f t 0 E k m z N y I J R T c + C t u X C j i 1 p 9 w 5 9 8 4 b b P Q 1 g M D h 3 P O 5 c 4 9 Y S K 4 B t f 9 t g p L y y u r a 8 X 1 0 s b m 1 v a O v b v X 0 D J V l N W p F F K 1 Q q K Z 4 D G r A w f B W o l i J A o F a 4 b D 6 s R v 3 j O l u Y z v Y J S w I C L 9 m P c 4 J W C k j n 3 g A 3 u A z A f A N 9 w E E y m F 3 7 l y q s 6 4 Y 5 d d x 5 0 C L x I v J 2 W U o 9 a x v / y u p G n E Y q C C a N 3 2 3 A S C j C j g V L B x y U 8 1 S w g d k j 5 r G x q T i O k g m 9 4 w x s d G 6 e K e V O b F g K f q 7 4 m M R F q P o t A k I w I D P e 9 N x P + 8 d g q 9 i y D j c Z I C i + l s U S 8 V G C S e F I K 7 X D E K Y m Q I o Y q b v 2 I 6 I I p Q M L W V T A n e / M m L p H H q e K 7 j 3 Z 6 V K 5 d 5 H U V 0 i I 7 Q C f L Q O a q g a 1 R D d U T R I 3 p G r + j N e r J e r H f r Y x Y t W P n M P v o D 6 / M H t e 2 X i A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T w S I E j i x f w 4 7 R y e J L A j p q X Q A 4 P c = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U h E 0 G W x I C 5 c V L A P a E K Z T K f t 0 E k m z N y I J R T c + C t u X C j i 1 p 9 w 5 9 8 4 b b P Q 1 g M D h 3 P O 5 c 4 9 Y S K 4 B t f 9 t g p L y y u r a 8 X 1 0 s b m 1 v a O v b v X 0 D J V l N W p F F K 1 Q q K Z 4 D G r A w f B W o l i J A o F a 4 b D 6 s R v 3 j O l u Y z v Y J S w I C L 9 m P c 4 J W C k j n 3 g A 3 u A z A f A N 9 w E E y m F 3 7 l y q s 6 4 Y 5 d d x 5 0 C L x I v J 2 W U o 9 a x v / y u p G n E Y q C C a N 3 2 3 A S C j C j g V L B x y U 8 1 S w g d k j 5 r G x q T i O k g m 9 4 w x s d G 6 e K e V O b F g K f q 7 4 m M R F q P o t A k I w I D P e 9 N x P + 8 d g q 9 i y D j c Z I C i + l s U S 8 V G C S e F I K 7 X D E K Y m Q I o Y q b v 2 I 6 I I p Q M L W V T A n e / M m L p H H q e K 7 j 3 Z 6 V K 5 d 5 H U V 0 i I 7 Q C f L Q O a q g a 1 R D d U T R I 3 p G r + j N e r J e r H f r Y x Y t W P n M P v o D 6 / M H t e 2 X i A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T w S I E j i x f w 4 7 R y e J L A j p q X Q A 4 P c = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U h E 0 G W x I C 5 c V L A P a E K Z T K f t 0 E k m z N y I J R T c + C t u X C j i 1 p 9 w 5 9 8 4 b b P Q 1 g M D h 3 P O 5 c 4 9 Y S K 4 B t f 9 t g p L y y u r a 8 X 1 0 s b m 1 v a O v b v X 0 D J V l N W p F F K 1 Q q K Z 4 D G r A w f B W o l i J A o F a 4 b D 6 s R v 3 j O l u Y z v Y J S w I C L 9 m P c 4 J W C k j n 3 g A 3 u A z A f A N 9 w E E y m F 3 7 l y q s 6 4 Y 5 d d x 5 0 C L x I v J 2 W U o 9 a x v / y u p G n E Y q C C a N 3 2 3 A S C j C j g V L B x y U 8 1 S w g d k j 5 r G x q T i O k g m 9 4 w x s d G 6 e K e V O b F g K f q 7 4 m M R F q P o t A k I w I D P e 9 N x P + 8 d g q 9 i y D j c Z I C i + l s U S 8 V G C S e F I K 7 X D E K Y m Q I o Y q b v 2 I 6 I I p Q M L W V T A n e / M m L p H H q e K 7 j 3 Z 6 V K 5 d 5 H U V 0 i I 7 Q C f L Q O a q g a 1 R D d U T R I 3 p G r + j N e r J e r H f r Y x Y t W P n M P v o D 6 / M H t e 2 X i A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T w S I E j i x f w 4 7 R y e J L A j p q X Q A 4 P c = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U h E 0 G W x I C 5 c V L A P a E K Z T K f t 0 E k m z N y I J R T c + C t u X C j i 1 p 9 w 5 9 8 4 b b P Q 1 g M D h 3 P O 5 c 4 9 Y S K 4 B t f 9 t g p L y y u r a 8 X 1 0 s b m 1 v a O v b v X 0 D J V l N W p F F K 1 Q q K Z 4 D G r A w f B W o l i J A o F a 4 b D 6 s R v 3 j O l u Y z v Y J S w I C L 9 m P c 4 J W C k j n 3 g A 3 u A z A f A N 9 w E E y m F 3 7 l y q s 6 4 Y 5 d d x 5 0 C L x I v J 2 W U o 9 a x v / y u p G n E Y q C C a N 3 2 3 A S C j C j g V L B x y U 8 1 S w g d k j 5 r G x q T i O k g m 9 4 w x s d G 6 e K e V O b F g K f q 7 4 m M R F q P o t A k I w I D P e 9 N x P + 8 d g q 9 i y D j c Z I C i + l s U S 8 V G C S e F I K 7 X D E K Y m Q I o Y q b v 2 I 6 I I p Q M L W V T A n e / M m L p H H q e K 7 j 3 Z 6 V K 5 d 5 H U V 0 i I 7 Q C f L Q O a q g a 1 R D d U T R I 3 p G r + j N e r J e r H f r Y x Y t W P n M P v o D 6 / M H t e 2 X i A = = &lt; / l a t e x i t &gt; sports team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T E + 2 D b E y Q J l H B L U S v D d I j y d + Q j M = " &gt; A A A C A H i c b V D L S s N A F J 3 U V 6 2 v q A s X b g a L 4 K o k I u i y 6 M Z l B f u A J o T J d N I O n U z C z I 1 Y Q j b + i h s X i r j 1 M 9 z 5 N 0 7 b L L T 1 w M D h n H u 4 c 0 + Y C q 7 B c b 6 t y s r q 2 v p G d b O 2 t b 2 z u 2 f v H 3 R 0 k i n K 2 j Q R i e q F R D P B J W s D B 8 F 6 q W I k D g X r h u O b q d 9 9 Y E r z R N 7 D J G V + T I a S R 5 w S M F J g H 3 n A H i H 3 A L B O E w X a C 8 D E i 8 C u O w 1 n B r x M 3 J L U U Y l W Y H 9 5 g 4 R m M Z N A B d G 6 7 z o p + D l R w K l g R c 3 L N E s J H Z M h 6 x s q S c y 0 n 8 8 O K P C p U Q Y 4 S p R 5 E v B M / Z 3 I S a z 1 J A 7 N Z E x g p B e 9 q f i f 1 8 8 g u v J z L t M M m K T z R V E m M C R 4 2 g Y e c M U o i I k h h C p u / o r p i C h C w X R W M y W 4 i y c v k 8 5 5 w 3 U a 7 t 1 F v X l d 1 l F F x + g E n S E X X a I m u k U t 1 E Y U F e g Z v a I 3 6 8 l 6 s d 6 t j / l o x S o z h + g P r M 8 f t T O X G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T E + 2 D b E y Q J l H B L U S v D d I j y d + Q j M = " &gt; A A A C A H i c b V D L S s N A F J 3 U V 6 2 v q A s X b g a L 4 K o k I u i y 6 M Z l B f u A J o T J d N I O n U z C z I 1 Y Q j b + i h s X i r j 1 M 9 z 5 N 0 7 b L L T 1 w M D h n H u 4 c 0 + Y C q 7 B c b 6 t y s r q 2 v p G d b O 2 t b 2 z u 2 f v H 3 R 0 k i n K 2 j Q R i e q F R D P B J W s D B 8 F 6 q W I k D g X r h u O b q d 9 9 Y E r z R N 7 D J G V + T I a S R 5 w S M F J g H 3 n A H i H 3 A L B O E w X a C 8 D E i 8 C u O w 1 n B r x M 3 J L U U Y l W Y H 9 5 g 4 R m M Z N A B d G 6 7 z o p + D l R w K l g R c 3 L N E s J H Z M h 6 x s q S c y 0 n 8 8 O K P C p U Q Y 4 S p R 5 E v B M / Z 3 I S a z 1 J A 7 N Z E x g p B e 9 q f i f 1 8 8 g u v J z L t M M m K T z R V E m M C R 4 2 g Y e c M U o i I k h h C p u / o r p i C h C w X R W M y W 4 i y c v k 8 5 5 w 3 U a 7 t 1 F v X l d 1 l F F x + g E n S E X X a I m u k U t 1 E Y U F e g Z v a I 3 6 8 l 6 s d 6 t j / l o x S o z h + g P r M 8 f t T O X G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T E + 2 D b E y Q J l H B L U S v D d I j y d + Q j M = " &gt; A A A C A H i c b V D L S s N A F J 3 U V 6 2 v q A s X b g a L 4 K o k I u i y 6 M Z l B f u A J o T J d N I O n U z C z I 1 Y Q j b + i h s X i r j 1 M 9 z 5 N 0 7 b L L T 1 w M D h n H u 4 c 0 + Y C q 7 B c b 6 t y s r q 2 v p G d b O 2 t b 2 z u 2 f v H 3 R 0 k i n K 2 j Q R i e q F R D P B J W s D B 8 F 6 q W I k D g X r h u O b q d 9 9 Y E r z R N 7 D J G V + T I a S R 5 w S M F J g H 3 n A H i H 3 A L B O E w X a C 8 D E i 8 C u O w 1 n B r x M 3 J L U U Y l W Y H 9 5 g 4 R m M Z N A B d G 6 7 z o p + D l R w K l g R c 3 L N E s J H Z M h 6 x s q S c y 0 n 8 8 O K P C p U Q Y 4 S p R 5 E v B M / Z 3 I S a z 1 J A 7 N Z E x g p B e 9 q f i f 1 8 8 g u v J z L t M M m K T z R V E m M C R 4 2 g Y e c M U o i I k h h C p u / o r p i C h C w X R W M y W 4 i y c v k 8 5 5 w 3 U a 7 t 1 F v X l d 1 l F F x + g E n S E X X a I m u k U t 1 E Y U F e g Z v a I 3 6 8 l 6 s d 6 t j / l o x S o z h + g P r M 8 f t T O X G w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T E + 2 D b E y Q J l H B L U S v D d I j y d + Q j M = " &gt; A A A C A H i c b V D L S s N A F J 3 U V 6 2 v q A s X b g a L 4 K o k I u i y 6 M Z l B f u A J o T J d N I O n U z C z I 1 Y Q j b + i h s X i r j 1 M 9 z 5 N 0 7 b L L T 1 w M D h n H u 4 c 0 + Y C q 7 B c b 6 t y s r q 2 v p G d b O 2 t b 2 z u 2 f v H 3 R 0 k i n K 2 j Q R i e q F R D P B J W s D B 8 F 6 q W I k D g X r h u O b q d 9 9 Y E r z R N 7 D J G V + T I a S R 5 w S M F J g H 3 n A H i H 3 A L B O E w X a C 8 D E i 8 C u O w 1 n B r x M 3 J L U U Y l W Y H 9 5 g 4 R m M Z N A B d G 6 7 z o p + D l R w K l g R c 3 L N E s J H Z M h 6 x s q S c y 0 n 8 8 O K P C p U Q Y 4 S p R 5 E v B M / Z 3 I S a z 1 J A 7 N Z E x g p B e 9 q f i f 1 8 8 g u v J z L t M M m K T z R V E m M C R 4 2 g Y e c M U o i I k h h C p u / o r p i C h C w X R W M y W 4 i y c v k 8 5 5 w 3 U a 7 t 1 F v X l d 1 l F F x + g E n S E X X a I m u k U t 1 E Y U F e g Z v a I 3 6 8 l 6 s d 6 t j / l o x S o z h + g P r M 8 f t T O X G w = = &lt; / l a t e x i t &gt;</head><p>โฆ, Everton, FA_Cup, โฆ Everton won against <ref type="bibr">[Liverpool]</ref> in an FA Cup match.</p><formula xml:id="formula_1">local context document context โฆ, Everton, FA_Cup, โฆ โฆ, Everton, FA_Cup, โฆ โฆ, Everton, FA_Cup, โฆ โฆ, Everton, FA_Cup, โฆ โฆ, ! เฎฐ เฏ s, เฎ&amp;' เฎต เฏ , โฆ</formula><p>(a) Overview of XELMS. Mentions are shown <ref type="bibr">[enclosed]</ref>.   The context g, entity e and type t vectors interact through Entity-Context loss (EC-LOSS), Type-Context loss (TC-LOSS) and Type-Entity loss (TE-LOSS). The Tamil sentence is the same as in <ref type="figure" target="#fig_1">Figure 1</ref>, and other mentions in it translate to <ref type="bibr">[Suarez]</ref> and <ref type="bibr">[Uruguay]</ref>. (b) The Mention Context Encoder ( ยง2.1) encodes the local context (neighboring words) and the document context (surfaces of other mentions in the document) of the mention into g. Internal view of local context encoder is in <ref type="figure" target="#fig_6">Figure 3</ref>.</p><formula xml:id="formula_2">Everton won against [Liverpool] in an FA Cup match. โฆ, Everton, FA_Cup, โฆ Left Local Context Encoder Right Local Context Encoder โฆ โฆ โฆ โฆ l &lt; l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " x 6 5 k F a B F / H Z p H / N g 0 a + y n M I 2 q 6 M = " &gt; A A A C A n i c b Z D L S s N A F I Z P 6 q 3 G W 9 S V u B k s B V c l E U G X R T c u K 9 g L N K F M p p N 2 6 O T C z E Q o I b j x V d y 4 U M S t T + H O t 3 H S R t D W H w Y + / n M O c 8 7 v J 5 x J Z d t f R m V l d W 1 9 o 7 p p b m 3 v 7 O 5 Z + w c d G a e C 0 D a J e S x 6 P p a U s 4 i 2 F V O c 9 h J B c e h z 2 v U n 1 0 W 9 e 0 + F Z H F 0 p 6 Y J 9 U I 8 i l j A C F b a G l h H b o j V 2 A 8 y n p v 1 H 3 Y p 5 / n A q t k N e y a 0 D E 4 J N S j V G l i f 7 j A m a U g j R T i W s u / Y i f I y L B Q j n O a m m 0 q a Y D L B I 9 r X G O G Q S i + b n Z C j u n a G K I i F f p F C M / f 3 R I Z D K a e h r z u L J e V i r T D / q / V T F V x 6 G Y u S V N G I z D 8 K U o 5 U j I o 8 0 J A J S h S f a s B E M L 0 r I m M s M F E 6 N V O H 4 C y e v A y d s 4 Z j N 5 z b 8 1 r z q o y j C s d w A q f g w A U 0 4 Q Z a 0 A Y C D / A E L / B q P B r P x p v x P m + t G O X M I f y R 8 f E N W + S X Z Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 6 5 k F a B F / H Z p H / N g 0 a + y n M I 2 q 6 M = " &gt; A A A C A n i c b Z D L S s N A F I Z P 6 q 3 G W 9 S V u B k s B V c l E U G X R T c u K 9 g L N K F M p p N 2 6 O T C z E Q o I b j x V d y 4 U M S t T + H O t 3 H S R t D W H w Y + / n M O c 8 7 v J 5 x J Z d t f R m V l d W 1 9 o 7 p p b m 3 v 7 O 5 Z + w c d G a e C 0 D a J e S x 6 P p a U s 4 i 2 F V O c 9 h J B c e h z 2 v U n 1 0 W 9 e 0 + F Z H F 0 p 6 Y J 9 U I 8 i l j A C F b a G l h H b o j V 2 A 8 y n p v 1 H 3 Y p 5 / n A q t k N e y a 0 D E 4 J N S j V G l i f 7 j A m a U g j R T i W s u / Y i f I y L B Q j n O a m m 0 q a Y D L B I 9 r X G O G Q S i + b n Z C j u n a G K I i F f p F C M / f 3 R I Z D K a e h r z u L J e V i r T D / q / V T F V x 6 G Y u S V N G I z D 8 K U o 5 U j I o 8 0 J A J S h S f a s B E M L 0 r I m M s M F E 6 N V O H 4 C y e v A y d s 4 Z j N 5 z b 8 1 r z q o y j C s d w A q f g w A U 0 4 Q Z a 0 A Y C D / A E L / B q P B r P x p v x P m + t G O X M I f y R 8 f E N W + S X Z Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 6 5 k F a B F / H Z p H / N g 0 a + y n M I 2 q 6 M = " &gt; A A A C A n i c b Z D L S s N A F I Z P 6 q 3 G W 9 S V u B k s B V c l E U G X R T c u K 9 g L N K F M p p N 2 6 O T C z E Q o I b j x V d y 4 U M S t T + H O t 3 H S R t D W H w Y + / n M O c 8 7 v J 5 x J Z d t f R m V l d W 1 9 o 7 p p b m 3 v 7 O 5 Z + w c d G a e C 0 D a J e S x 6 P p a U s 4 i 2 F V O c 9 h J B c e h z 2 v U n 1 0 W 9 e 0 + F Z H F 0 p 6 Y J 9 U I 8 i l j A C F b a G l h H b o j V 2 A 8 y n p v 1 H 3 Y p 5 / n A q t k N e y a 0 D E 4 J N S j V G l i f 7 j A m a U g j R T i W s u / Y i f I y L B Q j n O a m m 0 q a Y D L B I 9 r X G O G Q S i + b n Z C j u n a G K I i F f p F C M / f 3 R I Z D K a e h r z u L J e V i r T D / q / V T F V x 6 G Y u S V N G I z D 8 K U o 5 U j I o 8 0 J A J S h S f a s B E M L 0 r I m M s M F E 6 N V O H 4 C y e v A y d s 4 Z j N 5 z b 8 1 r z q o y j C s d w A q f g w A U 0 4 Q Z a 0 A Y C D / A E L / B q P B r P x p v x P m + t G O X M I f y R 8 f E N W + S X Z Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 6 5 k F a B F / H Z p H / N g 0 a + y n M I 2 q 6 M = " &gt; A A A C A n i c b Z D L S s N A F I Z P 6 q 3 G W 9 S V u B k s B V c l E U G X R T c u K 9 g L N K F M p p N 2 6 O T C z E Q o I b j x V d y 4 U M S t T + H O t 3 H S R t D W H w Y + / n M O c 8 7 v J 5 x J Z d t f R m V l d W 1 9 o 7 p p b m 3 v 7 O 5 Z + w c d G a e C 0 D a J e S x 6 P p a U s 4 i 2 F V O c 9 h J B c e h z 2 v U n 1 0 W 9 e 0 + F Z H F 0 p 6 Y J 9 U I 8 i l j A C F b a G l h H b o j V 2 A 8 y n p v 1 H 3 Y p 5 / n A q t k N e y a 0 D E 4 J N S j V G l i f 7 j A m a U g j R T i W s u / Y i f I y L B Q j n O a m m 0 q a Y D L B I 9 r X G O G Q S i + b n Z C j u n a G K I i F f p F C M / f 3 R I Z D K a e h r z u L J e V i r T D / q / V T F V x 6 G Y u S V N G I z D 8 K U o 5 U j I o 8 0 J A J S h S f a s B E M L 0 r I m M s M F E 6 N V O H 4 C y e v A y d s 4 Z j N 5 z b 8 1 r z q o y j C s d w A q f g w A U 0 4 Q Z a 0 A Y C D / A E L / B q P B r P x p v x P m + t G O X M I f y R 8 f E N W + S X Z Q = = &lt; / l a t e x i t &gt; r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G G t k 1 q N 9 4 p 3 l S 2 + M x R D s 9 u I E c 4 U = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M n 0 p h 0 6 m Y S Z i V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i s 9 9 i N q x k G Y D W e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z R C a Z i g W v c 8 N z F + R p X h T O C s 0 k 8 1 J p R N 6 A h 7 l k o a o f a z e e I Z O b P K k I S x s k 8 a M l d / b 2 Q 0 0 n o a B X Y y T 6 i X v V z 8 z + u l J r z 2 M y 6 T 1 K B k i 4 / C V B A T k / x 8 M u Q K m R F T S y h T 3 G Y l b E w V Z c a W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D C c / w C m + O d l 6 c d + d j M V p y i p 1 j + A P n 8 w f e 1 Z E I &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G G t k 1 q N 9 4 p 3 l S 2 + M x R D s 9 u I E c 4 U = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M n 0 p h 0 6 m Y S Z i V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i s 9 9 i N q x k G Y D W e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z R C a Z i g W v c 8 N z F + R p X h T O C s 0 k 8 1 J p R N 6 A h 7 l k o a o f a z e e I Z O b P K k I S x s k 8 a M l d / b 2 Q 0 0 n o a B X Y y T 6 i X v V z 8 z + u l J r z 2 M y 6 T 1 K B k i 4 / C V B A T k / x 8 M u Q K m R F T S y h T 3 G Y l b E w V Z c a W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D C c / w C m + O d l 6 c d + d j M V p y i p 1 j + A P n 8 w f e 1 Z E I &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G G t k 1 q N 9 4 p 3 l S 2 + M x R D s 9 u I E c 4 U = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M n 0 p h 0 6 m Y S Z i V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i s 9 9 i N q x k G Y D W e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z R C a Z i g W v c 8 N z F + R p X h T O C s 0 k 8 1 J p R N 6 A h 7 l k o a o f a z e e I Z O b P K k I S x s k 8 a M l d / b 2 Q 0 0 n o a B X Y y T 6 i X v V z 8 z + u l J r z 2 M y 6 T 1 K B k i 4 / C V B A T k / x 8 M u Q K m R F T S y h T 3 G Y l b E w V Z c a W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D C c / w C m + O d l 6 c d + d j M V p y i p 1 j + A P n 8 w f e 1 Z E I &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G G t k 1 q N 9 4 p 3 l S 2 + M x R D s 9 u I E c 4 U = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M n 0 p h 0 6 m Y S Z i V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i s 9 9 i N q x k G Y D W e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z R C a Z i g W v c 8 N z F + R p X h T O C s 0 k 8 1 J p R N 6 A h 7 l k o a o f a z e e I Z O b P K k I S x s k 8 a M l d / b 2 Q 0 0 n o a B X Y y T 6 i X v V z 8 z + u l J r z 2 M y 6 T 1 K B k i 4 / C V B A T k / x 8 M u Q K m R F T S y h T 3 G Y l b E w V Z c a W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D C c / w C m + O d l 6 c d + d j M V p y i p 1 j + A P n 8 w f e 1 Z E I &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h x m a D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H d U J E H &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + A 6 V E t o s 3 H L / 1 r 2 w K Y F o n h C 2 0 M o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h x m a D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H d U J E H &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + A 6 V E t o s 3 H L / 1 r 2 w K Y F o n h C 2 0 M o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h x m a D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H d U J E H &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + A 6 V E t o s 3 H L / 1 r 2 w K Y F o n h C 2 0 M o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h x m a D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H d U J E H &lt; / l a t e x i t &gt;</head><note type="other">local context vector document context vector g</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v m P s / L 4 D T Q + 0 3 1 t 4 y l A O l X 3 U 3 5 Q = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h N p o N q j W 3 7 s 5 B V o l X k B o U a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z p Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k T O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L E l e M s n r 5 L 2 R d 1 z 6 9 7 9 Z a 1 x U 9 R R h h M 4 h X P w 4 A o a c A d N a A E D B c / w C m + O c V 6 c d + d j M V p y i p 1 j + A P n 8 w f j Z J E L &lt; / l a t e x i t &gt;</head><p>Additionally, by encoding freely available struc- tured knowledge, like fine-grained entity types, the entity and context representations can be further improved ( ยง2.2). The ability to use multilingual supervision en- ables XELMS to learn XEL models for target lan- guages with limited resources by exploiting freely available supervision from high resource languages (like English). We show that XELMS outperforms existing state-of-the-art approaches that only use target language supervision, across 3 benchmark datasets in 8 languages ( ยง5.1). Moreover, while previous XEL models <ref type="bibr" target="#b17">(McNamee et al., 2011;</ref><ref type="bibr" target="#b26">Tsai and Roth, 2016</ref>) train separate models for differ- ent languages, XELMS can train a single model for performing XEL in multiple languages ( ยง5.2).</p><p>One of the goals of XEL is to enable understand- ing of languages with limited resources. We pro- vide experimental analyses in two such settings. In the zero-shot setting ( ยง6.1), where no supervi- sion is available in the target language, we show that the good performance of zero-shot XEL ap- proaches ( <ref type="bibr" target="#b23">Sil et al., 2018)</ref> can be attributed to the use of prior probabilities. These probabilities are computed from large amount of grounded men- tions, which are not available in realistic zero-shot settings. In the low-resource setting ( ยง6.2), where some supervision is available in the target language, we show that even when only a fraction of the avail- able supervision in the target language is provided, XELMS can achieve competitive performance by exploiting supervision from English.</p><p>The contributions of our work are, โข A new XEL approach, XELMS, that learns a XEL model for a language with limited re- sources by exploiting additional supervision from a high-resource language like English.</p><p>โข XELMS can also train a single XEL model for multiple languages jointly, which we show improves on separately trained models.</p><p>โข Analysis of XEL approaches in the zero-shot and low-resource settings. Our analysis re- veals that in realistic scenarios, zero-shot XEL is not as effective as previously shown. We also show that in low-resource settings jointly training with English leads to better utilization of target language supervision.</p><p>2 Cross-lingual EL with XELMS Given a mention m in a document D written in any language, XEL involves linking m to its gold entity e * in a KB, K = {e 1 , ยท ยท ยท , e n }. An overview of XELMS is shown in <ref type="figure" target="#fig_5">Figure 2a</ref>. XELMS computes the probability, P context (e | m), of a mention m referring to entity e โ K using a mention context vector g โ R h representing m's context, and an entity vector e โ R h , rep- resenting the entity e โ K (one vector per entity). XELMS can also incorporate structured knowledge like fine-grained entity types ( ยง2.2) using a multi- task learning approach <ref type="bibr" target="#b4">(Caruana, 1998)</ref>, by learn- ing a type vector t โ R h for each possible type t (e.g., sports_team) associated with the entity e. The entity vector e, context vector g and the type vector t are jointly trained, and interact through ap- propriately defined pairwise loss terms -an Entity- Context loss (EC-LOSS), Type-Entity loss (TE- LOSS) and a Type-Context loss (TC-LOSS).</p><p>The mention context vector g is generated by a mention context encoder ( ยง2.1), shown in <ref type="figure" target="#fig_5">Fig- ure 2b</ref>. The mention context of m in a document D consists of: (a) neighboring words around the men- tion, which we refer to as its local context and, (b) surfaces of other mentions appearing in D, which we refer as its document context. XELMS is trained using grounded mentions in mul- tiple languages (English and Tamil in <ref type="figure" target="#fig_5">Figure 2a</ref>), which can be derived from Wikipedia ( ยง4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mention Context Representation</head><p>To learn from mention contexts in multiple lan- guages, we generate mention context representa- tions using a language-agnostic mention context encoder. An overview of the mention context en- coder is shown in <ref type="figure" target="#fig_5">Figure 2b</ref>. Below we describe the components of the mention context encoder, namely multilingual word embeddings and local and document context encoders.</p><p>Multilingual Word Embeddings ( <ref type="bibr" target="#b1">Ammar et al., 2016b;</ref><ref type="bibr" target="#b24">Smith et al., 2017;</ref><ref type="bibr" target="#b6">Duong et al., 2017)</ref> jointly encode words in multiple (โฅ2) languages in the same vector space such that semantically similar words in the same language, and transla- tionally equivalent words in different languages are close (per cosine similarity). Multilingual embed- dings generalize bilingual embeddings, which do the same for two languages only.</p><p>We use FASTTEXT ( <ref type="bibr" target="#b2">Bojanowski et al., 2017;</ref><ref type="bibr" target="#b24">Smith et al., 2017)</ref>, which aligns monolingual em- beddings of multiple languages in the same space using a small dictionary (โผ2500 pairs) from each language to English. Both monolingual embed- dings and the dictionary can be easily obtained for languages with limited resources. We denote the multilingual word embeddings for a set of tokens {w 1 , w 2 , ยท ยท ยท , w n } by w 1:n = {w 1 , w 2 , ยท ยท ยท , w n }, where each w i โ R d . Local Context Representation The local con- text of a mention m, spanning tokens i to j, con- sists of left context (tokens i โ W to j) and right context (tokens i to j + W ). For example, for the mention <ref type="bibr">[Liverpool]</ref> in <ref type="figure" target="#fig_5">Figure 2b</ref>, the left and right contexts are "Everton won against Liverpool" and "Liverpool in a FA Cup match" respectively. The lo- cal context encoder <ref type="figure" target="#fig_6">(Figure 3</ref>) encodes the left and the right contexts into vectors l โ R h and r โ R h using a convolutional neural network (CNN). These two vectors are then combined to generate the local context vector c โ R h <ref type="figure" target="#fig_5">(Figure 2b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 X C J f Z L X z 6 f 3 N Z o Z n n j J 2 S k t / D w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x p 0 V 7 A O a U C b T S T t 0 M g k z N 0 I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t l l o 6 4 G B w z n 3 c s + c M J X C o O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U N k m m G W + x R C a 6 G 1 L D p V C 8 h Q I l 7 6 a a 0 z i U v B O O b 2 d + 5 4 l r I x L 1 i J O U B z E d K h E J R t F K v h 9 T H I V R f j / t i 3 6 1 5 t b d O c g q 8 Q p S g w L N f v X L H y Q s i 7 l C J q k x P c 9 N M c i p R s E k n 1 b 8 z P C U s j E d 8 p 6 l i s b c B P k 8 8 5 S c W W V A o k T b p 5 D M 1 d 8 b O Y 2 N m c S h n Z x l N M v e T P z P 6 2 U Y X Q e 5 U G m G X L H F o S i T B B M y K 4 A M h O Y M 5 c Q S y r S w W Q k b U U 0 Z 2 p o q t g R v + c u r p H 1 R 9 9 y 6 9 3 B Z a 9 w U d Z T h B E 7 h H D y 4 g g b c Q R N a w C C F Z 3 i F N y d z X p x 3 5 2 M x W n K K n W P 4 A + f z B 0 C h k c 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 X C J f Z L X z 6 f 3 N Z o Z n n j J 2 S k t / D w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x p 0 V 7 A O a U C b T S T t 0 M g k z N 0 I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t l l o 6 4 G B w z n 3 c s + c M J X C o O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U N k m m G W + x R C a 6 G 1 L D p V C 8 h Q I l 7 6 a a 0 z i U v B O O b 2 d + 5 4 l r I x L 1 i J O U B z E d K h E J R t F K v h 9 T H I V R f j / t i 3 6 1 5 t b d O c g q 8 Q p S g w L N f v X L H y Q s i 7 l C J q k x P c 9 N M c i p R s E k n 1 b 8 z P C U s j E d 8 p 6 l i s b c B P k 8 8 5 S c W W V A o k T b p 5 D M 1 d 8 b O Y 2 N m c S h n Z x l N M v e T P z P 6 2 U Y X Q e 5 U G m G X L H F o S i T B B M y K 4 A M h O Y M 5 c Q S y r S w W Q k b U U 0 Z 2 p o q t g R v + c u r p H 1 R 9 9 y 6 9 3 B Z a 9 w U d Z T h B E 7 h H D y 4 g g b c Q R N a w C C F Z 3 i F N y d z X p x 3 5 2 M x W n K K n W P 4 A + f z B 0 C h k c 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 X C J f Z L X z 6 f 3 N Z o Z n n j J 2 S k t / D w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x p 0 V 7 A O a U C b T S T t 0 M g k z N 0 I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t l l o 6 4 G B w z n 3 c s + c M J X C o O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U N k m m G W + x R C a 6 G 1 L D p V C 8 h Q I l 7 6 a a 0 z i U v B O O b 2 d + 5 4 l r I x L 1 i J O U B z E d K h E J R t F K v h 9 T H I V R f j / t i 3 6 1 5 t b d O c g q 8 Q p S g w L N f v X L H y Q s i 7 l C J q k x P c 9 N M c i p R s E k n 1 b 8 z P C U s j E d 8 p 6 l i s b c B P k 8 8 5 S c W W V A o k T b p 5 D M 1 d 8 b O Y 2 N m c S h n Z x l N M v e T P z P 6 2 U Y X Q e 5 U G m G X L H F o S i T B B M y K 4 A M h O Y M 5 c Q S y r S w W Q k b U U 0 Z 2 p o q t g R v + c u r p H 1 R 9 9 y 6 9 3 B Z a 9 w U d Z T h B E 7 h H D y 4 g g b c Q R N a w C C F Z 3 i F N y d z X p x 3 5 2 M x W n K K n W P 4 A + f z B 0 C h k c 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 X C J f Z L X z 6 f 3 N Z o Z n n j J 2 S k t / D w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x p 0 V 7 A O a U C b T S T t 0 M g k z N 0 I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t l l o 6 4 G B w z n 3 c s + c M J X C o O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U N k m m G W + x R C a 6 G 1 L D p V C 8 h Q I l 7 6 a a 0 z i U v B O O b 2 d + 5 4 l r I x L 1 i J O U B z E d K h E J R t F K v h 9 T H I V R f j / t i 3 6 1 5 t b d O c g q 8 Q p S g w L N f v X L H y Q s i 7 l C J q k x P c 9 N M c i p R s E k n 1 b 8 z P C U s j E d 8 p 6 l i s b c B P k 8 8 5 S c W W V A o k T b p 5 D M 1 d 8 b O Y 2 N m c S h n Z x l N M v e T P z P 6 2 U Y X Q e 5 U G m G X L H F o S i T B B M y K 4 A M h O Y M 5 c Q S y r S w W Q k b U U 0 Z 2 p o q t g R v + c u r p H 1 R 9 9 y 6 9 3 B Z a 9 w U d Z T h B E 7 h H D y 4 g g b c Q R N a w C C F Z 3 i F N y d z X p x 3 5 2 M x W n K K n W P 4 A + f z B 0 C h k c 8 = &lt; / l a t e x i t &gt; r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average</head><p>The CNN convolves continuous spans of k to- kens using a filter matrix F โ R kdรh to project the concatenation (โ operator) of the token embed- dings in the span. The resulting vector is passed through a ReLU unit to generate convolutional out- put O i . The outputs {O i } are pooled by averaging,</p><formula xml:id="formula_3">O i = RELU(F T (w i โ ยท ยท ยท โ w i+kโ1 ))<label>(1)</label></formula><formula xml:id="formula_4">ENC(w 1:n ) = AVG(O 1 , ยท ยท ยท , O nโk+1 )<label>(2)</label></formula><p>Left and right context vectors l and r are computed using respective ENC(.) layers,</p><formula xml:id="formula_5">l = ENC left (w iโW ยท ยท ยท w j ) (3) r = ENC right (w i ยท ยท ยท w j+W )<label>(4)</label></formula><p>These vectors together generate the local context vector c = F 2h,h (l โ r). Here F d i ,do : v i โ v o denotes a feed-forward layer that takes v i โ R d i as input, and outputs v o โ R do .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Context Representation</head><p>Presence of certain mentions in a document can help dis- ambiguate other mentions. For example, "Suarez", "Everton" in a document can help disambiguate "Liverpool". To incorporate this, we define the document context d m of a mention m appearing in document D to be the bag of all other men- tions in D. We encode d m into a dense document context vector d โ R h by a feed-forward layer</p><formula xml:id="formula_6">d = F |V |,h (d m ).</formula><p>Here V is the set containing all mention surfaces seen during training. When train- ing jointly over multiple languages, V consists of mention surfaces seen in all languages (e.g. all English and Tamil mention surfaces) during train- ing. This enables parameter sharing by embedding mention surfaces in different languages in the same low-dimensional space. The local and document context vectors c and d are combined to get the mention context vector</p><formula xml:id="formula_7">g = F 2h,h (c โ d).</formula><p>Context Conditional Probability We compute the probability of a mention m linking to entity e using its context vector g and the entity vector e,</p><formula xml:id="formula_8">P context (e | m) = exp(g T e) e โC(m) exp(g T e ) (5)</formula><p>where C(m) denotes all candidate entities of the mention m ( ยง3.1 explains how C(m) is gener- ated). We minimize the negative log-likelihood of P context (e | m) with respect to the gold entity e * against the candidate entities C(m), and call it the Entity-Context loss (EC-LOSS),</p><formula xml:id="formula_9">EC-LOSS = โ log P context (e * | m) e โC(m) P context (e | m) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Including Type Information</head><p>Incorporating the fine-grained types of a mention m can help rank entities of the appropriate type higher than others ( <ref type="bibr" target="#b15">Ling et al., 2015;</ref><ref type="bibr" target="#b10">Gupta et al., 2017;</ref><ref type="bibr" target="#b21">Raiman and Raiman, 2018)</ref>. For instance, knowing the correct type of mention <ref type="bibr">[Liverpool]</ref> as sports_team and constraining linking to entities with the relevant type, encourages disambiguation to the correct entity.</p><p>To make the mention context representation g type-aware, we predict the set of fine-grained types of m, T(m) = {t 1 , ..., t |T(m)| } using g. Each t i belongs to a pre-defined type vocabulary ฮ. <ref type="bibr">2</ref> The probability of a type t belonging to T(m) given the mention context is defined as P(t | m) = ฯ(t T g), where ฯ is the sigmoid function and t is the learn- able embedding for type t.</p><p>We define a Type-Context loss (TC-LOSS) as,</p><formula xml:id="formula_10">TC-LOSS = BCE(T(m), P(t | m)) (7)</formula><p>where BCE is the Binary Cross-Entropy Loss,</p><formula xml:id="formula_11">โ tโT(m) log P(t | m) โ t โT(m) log(1 โ P(t | m))</formula><p>We also incorporate the entity-type information in the entity representations, and define a similar Type-Entity loss (TE-LOSS).</p><p>To identify the gold types T(m) of a mention m, we make the distant supervision assumption (same as <ref type="bibr" target="#b15">Ling et al. (2015)</ref>) and assign the types of the gold entity e * to be the types of the men- tion. Gold fine-grained types of the entities can be acquired from resources like Freebase <ref type="table">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training and Inference</head><p>We explain how XELMS generates candidate enti- ties, performs inference, and combines the different training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Generation</head><p>Candidate generation identifies a small number of plausible entities for a mention m to avoid brute force comparison with all KB entities. Given m, candidate generation outputs a list of candidate entities C(m) = {e 1 , e 2 , ยท ยท ยท , e K } of size at most K (we use K=20), each associated with a prior probability P prior (e i | m) indicating the probability of m referring to e i , given only m's surface. P prior is estimated from counts over the training mentions.</p><p>We adopt <ref type="bibr" target="#b26">Tsai and Roth (2016)</ref>'s candidate gen- eration strategy with some minor modifications (Appendix A). Using other approaches like Cross- Wikis ( <ref type="bibr" target="#b25">Spitkovsky and Chang, 2012)</ref>, lead to con- sistently worse recall. We note that transliteration based candidate generation <ref type="bibr" target="#b17">(McNamee et al., 2011;</ref><ref type="bibr" target="#b20">Pan et al., 2017;</ref><ref type="bibr" target="#b27">Tsai and Roth, 2018;</ref><ref type="bibr" target="#b28">Upadhyay et al., 2018)</ref> can further improve recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>We combine the context conditional entity proba- bility P context (e | m) (eq. 5) and prior probability P prior (e | m) by taking their union:</p><formula xml:id="formula_12">P model (e | m) = P prior (e | m) + P context (e | m) โ P prior (e | m) ร P context (e | m)</formula><p>Inference for the mention m picks the entity,</p><formula xml:id="formula_13">ห e = arg max eโC(m) P model (e | m) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objective</head><p>When only training the mention context encoder and entity vectors, we minimize the EC-LOSS av- eraged over all training mentions. When using the two type-aware losses, we minimize a weighted sum of EC-LOSS, TE-LOSS, and TC-LOSS, using the weighing scheme of <ref type="bibr" target="#b13">Kendall et al. (2018)</ref>,</p><formula xml:id="formula_14">EC-LOSS 2ฮป 2 EC + TE-LOSS 2ฮป 2 TE + TC-LOSS 2ฮป 2 TC + log ฮป 2 EC + log ฮป 2 TE + log ฮป 2 TC (9)</formula><p>Here ฮป i are learnable scalar weighing parameters, and the respective 1 2ฮป 2 i and log ฮป 2 i term ensure that ฮป 2 i does not grow unboundedly. This way, the model learns the relative weight for each loss term. During training, mentions from different lan- guages are mixed using inverse-ratio mini-batch mixing strategy. That is, if two languages have training data sizes proportional to ฮฑ : ฮฒ, at any time during training, mini-batches seen from them are in the ratio 1 ฮฑ : 1 ฮฒ . This strategy prevents lan- guages with more training data from overwhelming languages with less training data. Though simple, we found this strategy yielded good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We briefly describe the training and evaluation datasets, and the previous XEL approaches from the literature used in our comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Mentions</head><p>Following previous work, we use hyperlinks from Wikipedia (dumps dated 05/20/2017) as our source of grounded mentions for supervision. Wikipedias in different languages have different pages for the same entity, which are resolved by using inter- language links (e.g., page ๅฉ ็ฉ ๆตฆ in Chinese Wikipedia resolves to Liverpool in English). Train- ing mentions statistics are shown in <ref type="table">Table 1</ref>.</p><p>We evaluate on 8 languages -German (de), Spanish (es), Italian (it), French (fr), Chinese (zh), Arabic (ar), Turkish (tr) and Tamil (ta), each of which has varying amount of grounded mentions from the respective Wikipedia <ref type="table">(Table 1)</ref>. We note that our method is applicable to any of the 293 Wikipedia languages as a target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Datasets</head><p>We evaluate XELMS on the following benchmark datasets, spanning 8 different languages, thus pro- viding an extensive evaluation.  TH-Test A subset of the dataset used in <ref type="bibr" target="#b26">(Tsai and Roth, 2016)</ref>, derived from Wikipedia. <ref type="bibr">3</ref> The mentions in the dataset fall in two categories -easy and hard, where hard mentions are those for which the most likely candidate according to the prior probability (i.e., arg max P prior (e | m)) is not the correct title. Indeed, most Wikipedia mentions can be correctly linked by selecting the most likely candidate ( <ref type="bibr" target="#b22">Ratinov et al., 2011</ref> We evaluate all models using linking accuracy on gold mentions, and assume gold mentions are pro- vided at test time. <ref type="table" target="#tab_3">Table 2</ref> summarizes the different domains of the evaluation datasets.</p><p>Tuning We avoid any dataset-specific tuning, in- stead tuning on a development set and applying the same parameters across all datasets. All tun- able parameters were tuned on a development set containing the hard mentions from the train split released by <ref type="bibr" target="#b26">Tsai and Roth (2016</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative Approaches</head><p>We compare against the following state-of-the-art (SoTA) approaches, described with the language from which they use mention contexts in (.), </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We show that: (a) XELMS can train a better en- tity linking model for a target language on various benchmark datasets by exploiting additional data from a high resource language like English ( ยง5.1).</p><p>(b) XELMS can train a single XEL model for multi- ple related languages and improve upon separately trained models ( ยง5.2). (c) Adding additional type information as multi-task loss to XELMS further improves performance ( ยง5.3).</p><p>In all tables, we report the linking accuracy of XELMS, averaged over 5 different runs, and mark with * the statistical significance (p &lt; 0.01) of the best result (shown bold) against the state-of-the-art (SoTA) using Student's one-sample t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Monolingual and Joint Models</head><p>In <ref type="table" target="#tab_6">Table 3</ref> and 4 we compare XELMS(mono), which uses monolingual supervision in the target lan- guage only, and XELMS(joint), which uses supervi-   sion from English in addition to the monolingual supervision, with the state-of-the-art approaches. We see that XELMS(mono) achieves similar or slightly better scores than respective SoTA on all datasets. The SoTA for MCN-TEST in Turkish and Chinese enhances the model by using translit- eration for candidate generation, explaining their superior performance. XELMS(joint) performs sub- stantially better than XELMS(mono) on all datasets <ref type="table" target="#tab_6">(Table 3</ref> and 4), proving that using additional super- vision from a high resource language like English leads to better linking performance. In particular, XELMS(joint) outperforms the SoTA on all lan- guages in TH-TEST, on Spanish in TAC15-Test, and on 4 of the 7 languages in MCN-TEST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multilingual Training</head><p>XELMS is the first approach that can train a single XEL model for multiple languages. To demon- strate this capability, we train a model, henceforth referred as XELMS(multi), jointly on 5 related lan- guages -Spanish, German, French, Italian and En-  glish. We compare XELMS(multi) to the respective XELMS(joint) model for each language. <ref type="table" target="#tab_7">Table 4</ref> and 5, show that XELMS(multi) is bet- ter (or at par) than XELMS(joint) on all datasets. This shows that XELMS(multi) can making more efficient use of available supervision in related lan- guages than previous approaches which trained sep- arate models per language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adding Fine-grained Type Information</head><p>To study the effect of adding fine-grained type in- formation, in <ref type="table" target="#tab_7">Table 4</ref> we compare XELMS(mono) and XELMS(joint) to XELMS(mono +type ) and XELMS(joint +type ) respectively, which are versions of XELMS(mono) and XELMS(joint) trained using the two type-aware losses.</p><p>XELMS(mono +type ) and XELMS(joint +type ) both improve compared to XELMS(mono) and XELMS(joint) on MCN-TEST and TH-TEST <ref type="table" target="#tab_6">(Table 6 vs Table 3)</ref>, showing the benefit of using structured knowledge in the form of fine-grained types. Similar trends are also seen on TAC15- TEST <ref type="table" target="#tab_7">(Table 4)</ref>, where XELMS(joint +type ) improves on the SoTA for Spanish and Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments with Limited Resources</head><p>The key motivation of XELMS is to exploit supervi- sion from high-resource languages like English to aid XEL for languages with limited resources. In this section, we examine two such scenarios, (a) Zero-shot setting i.e., no supervision available in the target language. Our analysis reveals the limitations of zero-shot XEL approaches and finds that the prior probabilities play an important role in achieving good performance ( ยง6.1), which are unavailable in realistic zero-shot scenarios. (b) Low-resource setting i.e., some supervision available in the target language. We show that  by combining supervision from a high-resource language, like English, XELMS can achieve com- petitive performance with a fraction of available supervision in the target language ( ยง6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Zero-shot Setting</head><p>We first explain how XELMS can perform zero-shot XEL, the implications of our zero-shot setting, and how it is more realistic than previous work.</p><p>Zero-shot XEL with XELMS XELMS performs zero-shot XEL by training a model using English supervision and multilingual embeddings for En- glish, and directly applying it to the test data in another language using the respective multilingual word embedding instead of English embeddings.</p><p>No Prior Probabilities Prior probabilities (or prior), i.e., P prior have been shown to be a reliable indicator of the correct disambiguation in entity linking <ref type="bibr" target="#b22">(Ratinov et al., 2011;</ref><ref type="bibr" target="#b26">Tsai and Roth, 2016)</ref>. These probabilities are estimated from counts over the training mentions in the target language. In the absence of training data for the target language, as in the zero-shot setting, these prior probabilities are not available to an XEL model.  <ref type="formula" target="#formula_3">2018)</ref>, we evaluate the performance of zero-shot XEL in more real- istic setting, and show it is adversely affected by absence of prior probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAC15-Test TH-Test McN-Test</head><p>Approach โ (es) (zh) (avg) (avg) XELMS (Z-S w/ prior) 80.3 83.9 43.5 88.1 XELMS (Z-S w/o prior) 53.5 55. <ref type="bibr">9</ref> 41.1 86.0 SoTA 83.9 85.9 54.7 89.4 <ref type="table">Table 7</ref>: Linking accuracy of the zero-shot (Z-S) approach on different datasets. Zero-shot (w/ prior) is close to SoTA for datasets like TAC15-Test, but performance drops in the more realistic setting of zero-shot (w/o prior) ( ยง6.1) on all datasets, indicating most of the performance can be attributed to the presence of prior probabilities. The slight drop in MCN-TEST is due to trivial mentions, which only have a single candidate.</p><p>Is zero-shot XEL really effective? To evaluate the effectiveness of the zero-shot XEL approach, we perform zero-shot XEL using XELMS on all datasets. <ref type="table">Table 7</ref> shows zero-shot XEL results on all datasets, both with and without using the prior during inference. Note that zero-shot XEL (with prior) is close to SoTA (Sil et al. <ref type="formula" target="#formula_3">(2018)</ref>) on TAC15-TEST, which also uses the prior for zero- shot XEL. However, for zero-shot XEL (without prior) performance drops by more than 20% for TAC15-Test, 2.4% for TH-Test and by 2.1% for McN-Test. This indicates that zero-shot XEL is not effective in a realistic zero-shot setting (i.e., when the prior is unavailable for inference). We found that the prior is indeed a strong indi- cator of the correct disambiguation. For instance, simply selecting the the most likely candidate us- ing the prior for TAC15-TEST achieved 77.2% and 78.8% for Spanish and Chinese respectively. It is interesting to note that both zero-shot XEL (with or without prior) perform worse than the best pos- sible model on TH-TEST, because TH-TEST was constructed to ensure prior probabilities are not strong indicators <ref type="bibr" target="#b26">(Tsai and Roth, 2016)</ref>. On MCN- TEST, we found that an average of 75.9% mentions have only one (the correct) candidate, making them trivial to link, regardless of the absence of priors.</p><p>The results show that most of the XEL perfor- mance in zero-shot settings can be attributed to availability of prior probabilities for the candidates. It is evident that zero-shot XEL in a realistic setting (i.e., when prior probabilities are not available) is still a challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Low-resource Setting</head><p>We analyze the behavior of XELMS in a low- resource setting, i.e. when some supervision is available in the target language. The aim of this setting is to estimate how much supervision from Linking Accuracy tr-mono tr-joint tr-best zh-mono zh-joint zh-best es-mono es-joint es-best <ref type="figure">Figure 4</ref>: Linking accuracy vs. the number of train mentions in the target language L (= Turkish (tr), Chinese (zh) and Span- ish (es)). We compare both XELMS(mono) and XELMS(joint) to the best results using all available supervision, denoted by L-best. To discount the effect of the prior, all results above are without it. For number of train mentions = 0, XELMS(joint) is equivalent to zero-shot without prior. Best viewed in color.</p><p>the target language is needed to get reasonable per- formance when using it jointly with supervision from English. To discount the effect of prior proba- bilities, we report all results without the prior. <ref type="figure">Figure 4</ref> plots results on the TH-Test dataset when training a XELMS(joint) model by gradu- ally increasing the number of mention contexts for target language L (= Spanish, Chinese and Turk- ish) that are available for supervision. <ref type="figure">Figure 4</ref> also shows the best results achieved using all avail- able target language supervision (denoted by L- best). For comparison with the mono-lingually supervised model, we also plot the performance of XELMS(mono), which only uses the target lan- guage supervision. <ref type="figure">Figure 4</ref> shows that after training on 0.75M men- tions from Turkish and Chinese (and 1.0M men- tions from Spanish), the XELMS(joint) model is within 2-3% of the respective L-best model which uses all training mentions in the target language, indicating that XELMS(joint) can reach competi- tive performance even with a fraction of the full target language supervision. For comparison, a XELMS(mono) model trained on the same number of training mentions is 5-10% behind the respective XELMS(joint) model, showing better utilization of target language supervision by XELMS(joint).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Existing approaches have taken two main direc- tions to obtain supervision for learning XEL mod- els -(a) using mention contexts appearing in the target language <ref type="bibr" target="#b17">(McNamee et al., 2011;</ref><ref type="bibr" target="#b26">Tsai and Roth, 2016)</ref>, or (b) using mention contexts appear- ing only in English ( <ref type="bibr" target="#b20">Pan et al., 2017;</ref><ref type="bibr" target="#b23">Sil et al., 2018)</ref>. We describe these directions and their limi- tations below, and explain how XELMS overcomes these limitations. <ref type="bibr" target="#b17">McNamee et al. (2011)</ref> use annotation projec- tion via parallel corpora to generate mention con- texts in the target language, while <ref type="bibr" target="#b26">Tsai and Roth (2016)</ref> learns separate XEL models for each lan- guage and only use mention contexts in the target language. Both these approach have scalability is- sues for languages with limited resources. Another limitation of these approaches is that they train sep- arate models for each language, which is inefficient when working with multiple languages. XELMS overcomes these limitations as it can use mention context from multiple languages simultaneously, and train a single model.</p><p>Other approaches only use mention contexts from English. While <ref type="bibr" target="#b20">Pan et al. (2017)</ref> compute entity coherence statistics from English Wikipedia, <ref type="bibr" target="#b23">Sil et al. (2018)</ref> perform zero-shot XEL for Chinese and Spanish by using multilingual embeddings to transfer a pre-trained English EL model. How- ever, our work suggests that mention contexts in the target language should also be used, if avail- able. Indeed, a recent study ( <ref type="bibr" target="#b14">Lewoniewski et al., 2017)</ref> found that for language sensitive topics, the quality of information can be better in the relevant language version of Wikipedia than the English ver- sion. Our analysis also shows that zero-shot XEL approaches like that of <ref type="bibr" target="#b23">Sil et al. (2018)</ref> are not ef- fective in realistic zero-shot scenarios where good prior probabilities are unlikely to be available. In such cases, we showed that combining supervision available in the target language with supervision from a high-resource language like English can yield significant performance improvements.</p><p>The architecture of XELMS is inspired by sev- eral monolingual entity linking systems <ref type="bibr">(FrancisLandau et al., 2016;</ref><ref type="bibr" target="#b19">Nguyen et al., 2016;</ref><ref type="bibr" target="#b10">Gupta et al., 2017)</ref>, approaches that use type informa- tion to aid entity linking ( <ref type="bibr" target="#b15">Ling et al., 2015;</ref><ref type="bibr" target="#b10">Gupta et al., 2017;</ref><ref type="bibr" target="#b21">Raiman and Raiman, 2018)</ref>, and the re- cent success of multilingual embeddings for several tasks <ref type="bibr" target="#b0">(Ammar et al., 2016a;</ref><ref type="bibr" target="#b6">Duong et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced XELMS, an approach that can combine supervision from multiple languages to train an XEL model. We illustrate its benefits through extensive evaluation on different bench- marks. XELMS is also the first approach that can train a single model for multiple languages, making more efficient use of available supervision than pre- vious approaches which trained separate models.</p><p>Our analysis sheds light on the poor performance of zero-shot XEL in realistic scenarios where the prior probabilities for candidates are unlikely to exist, in contrast to findings in previous work that focused on high-resource languages. We also show how in low-resource settings, XELMS makes it possible to achieve competitive performance even when only a fraction of the available supervision in the target language is provided.</p><p>Several future research directions remain open. For all XEL approaches, the task of candidate gen- eration is currently limited by existence of a target language Wikipedia and remains a key challenge. A joint inference framework which enforces co- herent predictions <ref type="bibr" target="#b5">(Cheng and Roth, 2013;</ref><ref type="bibr" target="#b9">Globerson et al., 2016;</ref><ref type="bibr" target="#b8">Ganea and Hofmann, 2017)</ref> could also lead to further improvements for XEL. Simi- lar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Everton won against [Liverpool] in an FA Cup match.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Tamil and English mention contexts containing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Training mention contexts originate from two (or more) languages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Grounded mentions from two or more languages (English and Tamil shown) can be used to supervise XELMS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Local Context Encoder, for the right context. Figure 2b shows how it fits inside Mention Context Encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Bollacker et al., 2008) or YAGO (Hoffart et al., 2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lang</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Comparison to Previous Work The only other model capable of zero-shot XEL is that of Sil et al. (2018). However, Sil et al. (2018) use prior prob- abilities and coreference chains for the target lan- guage in their zero-shot experiments, both of which will not be available in a realistic zero-shot sce- nario. Compared to Sil et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>#</head><label></label><figDesc>train mentions in target language (in millions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation datasets used in our experiments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>XELMS(joint) improves upon XELMS(mono) and 

the current State-of-The-Art (SoTA) on TH-TEST and MCN-
TEST, showing the benefit of using additional supervision 
from English. The best score is shown bold and  *  marks 
statistical significance of best against SoTA. Refer  ยง4.3 for 
details on SoTA. 

Model โ Lang. โ 

es 
zh 

(Tsai and Roth, 2016) 
82.4 
85.1 
(Sil et al., 2018) (SoTA) 
83.9 
85.9 

XELMS 

mono 
83.3 
84.4 
mono +type 
83.5 
84.8 

joint 
84.1 
85.5 
joint +type 
84.4  *  86.0 

multi 
83.9 
n/a 
multi +type 
84.4  *  
n/a 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Linking accuracy on TAC15-Test. Numbers for Sil 

et al. (2018) from personal communication. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Linking accuracy of a single XELMS(multi) model 

for four languages -German, Spanish, French and Italian. 
Individually trained XELMS(joint) scores are also shown. The 
best score is shown bold and  *  marks statistical significance 
of best against SoTA. Refer  ยง4.3 for details on SoTA. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Adding fine-grained type information further im-

proves linking accuracy (compare to Table 3). The best score 
is shown bold and  *  marks statistical significance of best 
against SoTA. Refer  ยง4.3 for details on SoTA. 

</table></figure>

			<note place="foot" n="2"> We use the type vocabulary ฮ from Ling and Weld (2012), which contains 112 fine-grained types (|ฮ| = 112)</note>

			<note place="foot" n="3"> Pan et al. (2017) also created a dataset using Wikipedia, but did not categorize mentions like Tsai and Roth (2016). Preliminary experiments on their dataset showed XELMS consistently beat Pan et al. (2017)&apos;s model. We chose TH-TEST for more controlled experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Snigdha Chaturvedi, Anne Cocos, Stephen Mayhew, Chen-Tse Tsai, Qiang Ning, Jordan Kodner, Dan Deutsch, John Hewitt and the anonymous EMNLP reviewers for their useful comments and suggestions. This work was supported by Contract HR0011-15-2-0025 and Agreement HR0011-15-2-0023 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Many Languages, One Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively Multilingual Word Embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD</title>
		<meeting>of ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to Learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational Inference for Wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual Training of Crosslingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Francis-Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Joint Entity Disambiguation with Local Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collective Entity Resolution with Multi-Focal Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ringaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity Linking via Joint Encoding of Types, Descriptions, and Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of TAC-KBP2015 Tri-lingual Entity Discovery and Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Analysis Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relative Quality and Popularity Evaluation of Multilingual Wikipedia Articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wลodzimierz</forename><surname>Lewoniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Wecel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witold</forename><surname>Abramowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Informatics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design Challenges for Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CrossLanguage Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David S</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wikify!: Linking Documents to Encyclopedic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint Learning of Local and Global Features for Entity Linking via Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><forename type="middle">Rodriguez</forename><surname>Fauceglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oktie</forename><surname>Muro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Massimiliano Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadoghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crosslingual Name Tagging and Linking for 282 Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepType: Multilingual Entity Linking by Neural Type System Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local and Global Algorithms for Disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-HLT</title>
		<meeting>of ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Doug Downey, and Mike Anderson</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural Cross-lingual Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Offline Bilingual Word Vectors, Orthogonal Transformations, and the Inverted Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Cross-Lingual Dictionary for English Wikipedia Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-lingual Wikification Using Multilingual Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Better Name Translation for Cross-Lingual Wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bootstrapping Transliteration with Constrained Discovery for Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Kodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
