<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CREST</orgName>
								<orgName type="department" key="dep2">Science and Technology Agency</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CREST</orgName>
								<orgName type="department" key="dep2">Science and Technology Agency</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new morphological analysis model that considers semantic plausi-bility of word sequences by using a recurrent neural network language model (RNNLM). In unsegmented languages, since language models are learned from automatically segmented texts and inevitably contain errors, it is not apparent that conventional language models contribute to morphological analysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outper-formed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In contrast to space-delimited languages like En- glish, word segmentation is the first and most cru- cial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai ( <ref type="bibr" target="#b5">Kudo et al., 2004;</ref><ref type="bibr" target="#b1">Kaji and Kitsuregawa, 2014;</ref><ref type="bibr" target="#b13">Shen et al., 2014;</ref><ref type="bibr" target="#b4">Kruengkrai et al., 2006</ref>). Word segmentation is usually performed jointly with related analysis: POS tagging for Chi- nese, and POS tagging and lemmatization (anal- ysis of inflected words) for Japanese. Morpho- logical analysis including word segmentation has been widely and actively studied, and for exam- ple, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations.</p><p>For example, the state-of-the-art and popu- lar Japanese morphological analyzers, JUMAN <ref type="bibr" target="#b6">(Kurohashi and Kawahara, 2009</ref>) and MeCab ( <ref type="bibr" target="#b5">Kudo et al., 2004</ref>) both analyze " (foreigner's right to vote)" not into the correct seg- mentation of (1a), but into the incorrect and awk- ward segmentation of (1b). JUMAN is a rule-based morphological analyzer, defining word-to-word (including inflection) con- nectivities and their scores. MeCab is a supervised morphological analyzer, learning the probabilities of word/POS/inflection sequence from an anno- tated corpus of tens of thousands of sentences. Both systems, however, cannot realize semanti- cally appropriate analysis, and often produce to- tally strange outputs like the above. This paper proposes a semantically appropriate morphological analysis method for unsegmented languages using a language model. For unseg- mented languages, morphological analysis and language modeling form a chicken-and-egg prob- lem. That is, if high-quality morphological analy- sis is available, we can learn a high-quality lan- guage model from a morphologically analyzed large corpus. On the other hand, if a high-quality language model is available, we can achieve high- quality morphological analysis by looking for a segmented word sequence with a large language model score. However, even if we learn a language model from a corpus analyzed by a certain level of morphological analyzer, the language model is affected by the analysis errors of the morphologi- cal analyzer and it is no practical use for the im- provement of the morphological analyzer. A lan- guage model trained by incorrectly segmented "</p><p>(foreign)/ (carrot)/ (regime)" just sup- ports that incorrect segmentation.</p><p>The point of the paper is that we have tackled the chicken-and-egg problem, not by using a lan-   guage model of raw word sequences, but by using a semantically generalized language model based on word embeddings, RNNLM (Recurrent Neural Network Language Model) ( <ref type="bibr" target="#b9">Mikolov et al., 2010;</ref><ref type="bibr" target="#b10">Mikolov et al., 2011</ref>). The RNNLM is trained on an automatically analyzed corpus of ten million sentences, which possibly includes incorrect seg- mentations such as " (foreign)/ (carrot)/ (regime)." However, on semantically gener- alized level, it is an unnatural semantic sequence like nation vegetable politics. Since the state-of- the-art morphological analyzer achieves the high accuracy, it does not often produce incorrect anal- yses which support such a semantically strange se- quence. This would prefer analysis toward seman- tically appropriate word sequences. When a mor- phological analyzer utilizes such a generalized and reasonable language model, it can penalize strange segmentations like " (foreign)/ (carrot)/ (regime)," leading to better accuracy. We furthermore retrain RNNLM using an an- notated corpus of manually segmented 45k sen- tences, which further improves morphological analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been several studies that have inte- grated language models into morphological anal- ysis. <ref type="bibr" target="#b16">Wang et al. (2011)</ref> improved Chinese word segmentation and POS tagging by using N-gram features learned from an automatically segmented corpus. However, since the auto-segmented cor- pus inevitably contains segmentation errors, fre- quent N-grams are not always correct and thus this problem might affect the performance of morphological analysis. They also divided N- gram frequencies into three binned features: high- frequency, middle-frequency and low-frequency. Such coarse features cannot express slight differ- ences in the likelihood of language models. <ref type="bibr" target="#b1">Kaji and Kitsuregawa (2014)</ref> used a bigram lan- guage model feature for Japanese word segmenta- tion and POS tagging. Their objective of using a language model is to normalize informally spelled words in microblogs. Therefore, their objective is different from ours.</p><p>Some studies have used character-based lan- guage models for Chinese word segmentation and POS tagging ( <ref type="bibr" target="#b21">Zheng et al., 2013;</ref>. Although their approaches have no drawbacks of learning incorrect segmentations, they only cap- ture more local information than word-based lan- guage models.</p><p>Word embeddings have been also used for mor- phological analysis. Neural network based models have been proposed for Chinese word segmenta- tion and POS tagging ( <ref type="bibr" target="#b12">Pei et al., 2014</ref>) or word segmentation ( <ref type="bibr" target="#b8">Mansur et al., 2013</ref>). These meth- ods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scor- ing word transitions in morphological analysis. Our usage of word embeddings is different from the previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We propose a new morphological analysis model that considers semantic plausibility of word se- quences by using RNNLM. We integrate RNNLM into morphological analysis <ref type="figure" target="#fig_3">(Figure 2)</ref>. We train the RNNLM using both an automatically analyzed corpus and a manually labeled corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Neural Network Language Model</head><p>RNNLM is a recurrent neural network language model ( <ref type="bibr" target="#b9">Mikolov et al., 2010)</ref>, which outputs a probability distribution of the next word, given the embedding of the last word and its context. We  employ the RNNME language model 1 proposed by <ref type="bibr" target="#b10">(Mikolov et al., 2011;</ref><ref type="bibr" target="#b11">Mikolov, 2012)</ref> as the implementation of RNNLM. The RNNME lan- guage model has direct connections from the input layer of the recurrent neural network to the output layer, which act as a maximum entropy model and avoid to waste a lot of parameters to describe sim- ple patterns. Hereafter, we refer to the RNNME language model simply as RNNLM.</p><p>To train RNNLM, we use a raw corpus of 10 million sentences from the web corpus <ref type="bibr" target="#b2">(Kawahara and Kurohashi, 2006</ref>). These sentences are automatically segmented by JUMAN ( <ref type="bibr" target="#b6">Kurohashi and Kawahara, 2009)</ref>. The training of RNNLM is based on lemmatized word sequences without POS tags.</p><p>The trained model contains errors caused by an automatically analyzed corpus. We retrain RNNLM using a manually labeled corpus after training RNNLM using the automatically ana- lyzed corpus as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The retraining aims to cope with errors related to function word sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Base Model</head><p>For our base model, we adopt a model for su- pervised morphological analysis, which performs segmentation, lemmatization and POS tagging jointly. We train this model using a tagged cor- pus of tens of thousands of sentences that contain gold segmentations, lemmas, inflection forms and POS tags. To predict the most probable sequence of words with lemmas and POS tags given an input sentence, we execute the following procedure:</p><p>1. Look up the string of the input sentence using a dictionary.</p><p>2. Make a word lattice.</p><p>3. Search for the path with the highest score from the lattice. <ref type="figure" target="#fig_2">Figure 1</ref> illustrates the constructed lattice during the procedure. At the dictionary lookup step, we use the basic dictionary of JUMAN and an ad- ditional dictionary comprising 0.8 million words, both of which have lemma, POS and inflection in- formation. The additional dictionary mainly con- sists of itemizations in articles and article titles in Japanese Wikipedia.</p><p>We define the scoring function as follows:</p><formula xml:id="formula_0">score B (y) = Φ(y) · ⃗ w,<label>(1)</label></formula><p>where y is a tagged word sequence, Φ(y) is a feature vector for y, and ⃗ w is a weight vector. Each element in ⃗ w gives a weight to its corre- sponding feature in Φ(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in <ref type="bibr" target="#b5">Kudo et al. (2004)</ref>. We also use additional lexical features such as character type, and trigram features used in <ref type="bibr" target="#b18">Zhang and Clark (2008)</ref>. To learn the weight vector, we adopt exact soft confidence-weighted learning ( <ref type="bibr" target="#b17">Wang et al., 2012</ref>).</p><p>To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automat- ically generate words at the lookup step by seg- menting the input string by character types 2 . For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RNNLM Integrated Model</head><p>Based on retrained RNNLM, we calculate an RNNLM score (score R (y)) to be integrated into the base model. The RNNLM score is defined as the log probability of the next word given its con- text (path). Here, the score for an OOV word is given by the following formula:</p><formula xml:id="formula_1">−C p − L p · length(n),<label>(2)</label></formula><p>where C p is a constant penalty for OOV words, L p is a factor for the character length penalty, and length(n) returns the character length of the next word n. This formula is defined to penalize longer words, which are likely to produce segmentation errors.</p><p>We then integrate the RNNLM score into the base model using the following equation:</p><formula xml:id="formula_2">score I (y) = (1 − α)score B (y) + α score R (y),<label>(3)</label></formula><p>where α is an interpolation parameter that is tuned on development data. For decoding, we employ beam search as used in <ref type="bibr" target="#b18">Zhang and Clark (2008)</ref>. Since the possi- ble context (paths in the word lattice) consid- ered in RNNLM falls into combinatorial explosion in morphological analysis, we keep only prob- able context candidates inside the beam. That is, each node keeps candidates inside the beam width. Each candidate has a vector represent- ing context, and two words of history. The re- current model makes decoding harder than non- recurrent neural network language models. How- ever, we use RNNLM because the model outper- forms other NNLMs (Mikolov, 2012) and the re- sult suggests that the model is more likely to cap- ture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of non- recurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm <ref type="bibr" target="#b15">(Thede and Harper, 1999</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>In our experiments, we used the Kyoto University Text Corpus ( <ref type="bibr" target="#b3">Kawahara et al., 2002</ref>) and Kyoto University Web Document Leads Corpus ( <ref type="bibr" target="#b0">Hangyo et al., 2012</ref>) as manually tagged corpora. We ran- domly chose 2,000 sentences from each corpus for test data, and 500 sentences for development data. We used the remaining part of the corpora as training data to train our base model and retrain RNNLM. In total, we used 45,000 sentences for training.</p><p>For comparative purposes, we used the follow- ing four baselines: the Japanese morphological an- alyzer JUMAN, the supervised morphological an- alyzer MeCab, the base model, and a model using a conventional language model. For this language model, we built a trigram language model with Kneser-Ney smoothing using SRILM <ref type="bibr" target="#b14">(Stolcke, 2002</ref>) from the same automatically segmented cor- pus. The language model is modified to have an interpolation parameter α and length penalty for OOV, L p .</p><p>We set the beam width to 5 by preliminary ex- periments. We also set a constant penalty for OOV words (C p ) as 5, which is the default value in the implementation of <ref type="bibr" target="#b10">Mikolov et al. (2011)</ref>. We tuned the parameters of our proposed model and the baseline model (α and L p ) and the parameters of language models using grid search on the de- velopment data. We set α = 0.3, L p =1.5 for the proposed model (" Base + RNNLM retrain "). <ref type="bibr">3</ref> We measured the performance of the baseline models and the proposed model by F-value of word segmentation and F-value of joint evaluation of word segmentation and POS tagging. We calcu- lated F-value for the two corpora (news and web) and the merged corpus (all).</p><p>We used the bootstrapping method ( <ref type="bibr" target="#b19">Zhang et al., 2004</ref>) to test statistical significance between proposed models and other models. Suppose we have a test set T that includes N sentences. The method repeatedly creates M new test sets by re- sampling N sentences with replacement from T . We calculate the F-value of each model on M + 1 test sets including T , and then we have M + 1 score differences. From the scores, we calculate the 95% confidence interval. If the interval does not overlap with zero, the two models are consid- ered as statistically significantly different. In our evaluation, M is set to 2,000. <ref type="table">Table 1</ref> lists the results of our proposed model and the baseline models. Our proposed model ("Base + RNNLM retrain ") significantly outperforms all the baseline models and "Base + RNNLM," which does not use retraining. In particular, we achieved a large improvement for segmentation. This can be attributed to the use of RNNLM that was learned based on lemmatized word sequence without POS tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussions</head><p>"Base + SRILM" segmented the example de- scribed in Section 1 ("") into the incorrect segmentation "//" in the same way as JUMAN. This segmentation error was caused by errors in the automatically seg- mented corpus that was used to train the language model. Our proposed model can correctly seg- ment this example if a proper context is available by semantically capturing word transitions using RNNLM.</p><p>The base model, JUMAN and "Base + SRILM" incorrectly segmented " (healthy)/ (etc.)/  <ref type="table">Table 1</ref>: Results for test datasets. * means the score of "Base + RNNLM retrain " is significantly improved from that of all other models.</p><p>(of)/ (point)/ (in)/" (in terms of health and so on) into " (healthy)/(any)/ (point)/ (in)/." Although this segmentation can be grammatically accepted, it is difficult to semantically interpret this word sequence. Our proposed model can correctly segment this exam- ple because RNNLM learns semantically plausible word sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a new model for morphological analysis that is integrated with RNNLM. We trained RNNLM on an automati- cally segmented corpus and tuned on a manually tagged corpus. The proposed model was able to significantly reduce errors in the base model by capturing semantic plausibility of word sequences using RNNLM. In the future, we will design fea- tures derived from RNNLM models, and integrate them into a unified learning framework. We also intend to apply our method to unsegmented lan- guages other than Japanese, such as Chinese and Thai.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a word lattice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Workflow for training RNNLM and base model.</figDesc></figure>

			<note place="foot" n="1"> RNNME is the abbreviation of Recurrent Neural Network trained jointly with Maximum Entropy model.</note>

			<note place="foot" n="2"> Japanese has three types of characters: Kanji, Hiragana and Katakana.</note>

			<note place="foot" n="3"> We set α = 0.1, Lp = 2.0 for &quot;Base + RNNLM&quot;, and α = 0.3, Lp = 0.5 for &quot;Base + SRILM.&quot;</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a diverse document leads corpus annotated with semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation</title>
		<meeting>the 26th Pacific Asia Conference on Language, Information, and Computation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate word segmentation and POS tagging for japanese microblogs: Corpus annotation and joint modeling with lexical normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Case frame compilation from the web using highperformance computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1344" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Construction of a Japanese relevance-tagged corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kôiti</forename><surname>Hasida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002)</title>
		<meeting>the Third International Conference on Language Resources and Evaluation (LREC-2002)<address><addrLine>Las Palmas, Canary Islands-Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05" />
			<biblScope unit="page" from="2" to="1302" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA). ACL Anthology Identifier</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A conditional random field framework for Thai morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virach</forename><surname>Sornlertlamvanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2419" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Applying conditional random fields to Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Emprical Methods in Natural Language Processing</title>
		<meeting>the Conference on Emprical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Japanese Morphological Analysis System JUMAN 6.0 Users Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<ptr target="http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning character representations for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Iwakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Modern Machine Learning and Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature-based neural language model and Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1271" to="1277" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukar</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Honza</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
	<note>Proceedings of ASRU 2011</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno university of technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chinese Morphological Analysis with Character-level POS Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<idno>ICSLP2002 -INTERSPEECH 2002</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Spoken Language Processing</title>
		<editor>John H L Hansen and Bryan L Pellom</editor>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09-16" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A secondorder Hidden Markov Model for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">P</forename><surname>Thede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011)</title>
		<meeting>the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011)<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exact soft confidence-weighted learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Conference on Machine Learning (ICML 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint word segmentation and pos tagging using a single perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpreting bleu/nist scores: How much improvement do we need to have a better system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">European Language Resources Association (ELRA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Portugal</forename><surname>Lisbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">May</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Anthology Identifier</title>
		<imprint>
			<biblScope unit="page" from="4" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
		<idno>18-21 Octo- ber 2013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
