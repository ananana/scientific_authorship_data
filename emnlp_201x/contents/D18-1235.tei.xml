<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2109</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahua</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Naturali Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Naturali Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Naturali Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Naturali Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Naturali Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2109" to="2118"/>
							<date type="published">October 31-November 4, 2018. 2018. 2109</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of machine reading comprehension (MRC) has evolved from answering simple questions from well-edited text to answering real questions from users out of web data. In the real-world setting, full-body text from multiple relevant documents in the top search results are provided as context for questions from user queries, including not only questions with a single, short, and factual answer , but also questions about reasons, procedures , and opinions. In this case, multiple answers could be equally valid for a single question and each answer may occur multiple times in the context, which should be taken into consideration when we build MRC system. We propose a multi-answer multi-task framework, in which different loss functions are used for multiple reference answers. Minimum Risk Training is applied to solve the multi-occurrence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine reading comprehension (MRC) or ques- tion answering (QA) has been a long-standing goal in Natural Language Processing. There is a surge of interest in this area due to new end-to-end mod- eling techniques and the release of several large- scale, open-domain datasets.</p><p>In earlier datasets ( <ref type="bibr" target="#b6">Hermann et al., 2015;</ref><ref type="bibr" target="#b8">Hill et al., 2016;</ref><ref type="bibr" target="#b25">Yang et al., 2015;</ref><ref type="bibr" target="#b14">Rajpurkar et al., 2016)</ref>, the questions did not arise from actual end users. Instead, they were constructed in cloze style or created by crowdworkers given a short passage from well-edited sources such as Wikipedia and CNN/Daily Mail. As a consequence, the questions * * Corresponding author: D. Lin (lindek@naturali.io).</p><p>are usually well-formed and about simple facts, and the answers are guaranteed to exist as short spans in the given candidate passages.</p><p>In MS-MARCO ( <ref type="bibr" target="#b13">Nguyen et al., 2016)</ref>, the questions were sampled from actual search queries, which may have typos and may not be phrased as questions. 1 Multiple short passages, which might have the answer to the query, were extracted from webpages by a separate informa- tion retrieval system. <ref type="bibr" target="#b5">He et al. (2017)</ref> made the DuReader dataset a more realistic reflection of the real-world prob- lem by including not only questions with relatively short and factual answers, but also questions about complex descriptions, procedures, opinions, etc. which may have multiple, much longer answers, or no answer at all. Furthermore, full-body text from webpages listed in top search results are di- rectly provided as context. These documents tend to be much noiser than Wikipedia and CNN. They are much longer (5 times longer than those in MS-MARCO on average) and contain many para- graphs that are irrelevant to the query.</p><p>New problems arise as we now consider the task of machine reading comprehension in a much more challenging real-world setting. First, multi- ple valid answers to a single question are not only possible but quite common. <ref type="figure" target="#fig_0">Figure 1</ref> shows some examples of questions with multiple answers from the DuReader dataset. There could be multiple ways to perform the same task (Question 1), mul- tiple opinions about the same subject (Question 2), or multiple explanations for the same observation (Question 3). However, few works have been done with multiple answers in machine reading com- prehension. To address this problem, we propose a multi-answer multi-task scheme which incorpo- rates multiple reference answers in the objective Another problem is the multiple occurrences of the same answer. As rich context is provided for a single question, the same answer could occur more than one time in different passages, or even at dif- ferent places of the same passage. In this case, using only one gold span for the answer could be problematic, as the model is forced to choose one span over others that contain the same con- tent. To solve this problem, we propose to apply Minimum Risk Training (MRT), which uses the expected metric as the loss and gives reward to all spans that are similar with the gold answer.</p><p>In this paper, we present a multi-answer, multi- task objective function to train an end-to-end MRC/QA system. We experiment with various al- ternatives on the DuReader dataset and show that our model out-performs other competing systems and increases the state-of-the-art ROUGE-L score by about 7 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various datasets have been released in recent years, which fuel the research for reading compre- hension and question answering. The CNN/Daily- Mail dataset ( <ref type="bibr" target="#b6">Hermann et al., 2015</ref>) and the Chil- dren's Book Test <ref type="bibr" target="#b8">(Hill et al., 2016</ref>) evaluate com- prehension by filling in missing words from well- edited texts. <ref type="bibr">SQuAD (Rajpurkar et al., 2016</ref>) is one of the most popular datasets for reading com- prehension, where a span in a Wikipedia passage is to be extracted to answer questions generated by annotators. The WikiQA ( <ref type="bibr" target="#b25">Yang et al., 2015</ref>) is another dataset from Wikipedia, where one sin- gle sentence is to be selected to answer ques- tions from search engine logs. Different from the above datasets, the MS-MARCO dataset <ref type="bibr" target="#b13">(Nguyen et al., 2016</ref>) was built in a real-world setting. The questions were real anonymized Bing queries and multiple passages are extracted from related web pages by a separate system. The DuReader ( <ref type="bibr" target="#b5">He et al., 2017</ref>) is a Chinese dataset, similarly con- structed from user queries as MS MACRCO, but in a more realistic setting using Baidu Web Search and Baidu Answers (Zhidao) data. While a small proportion of questions were labeled with multiple answers in MS MARCO (9.93%), more than half of the DuReader queries were annotated with mul- tiple answers, which provides the perfect setup for our work.</p><p>Great effort has been put into the development of sophisticated neural models for machine read- ing comprehension. The attention mechanism was first introduced by <ref type="bibr" target="#b6">Hermann et al. (2015)</ref> into reading comprehension and soon became the dom-inating model. <ref type="bibr" target="#b19">Wang and Jiang (2017)</ref> proposed to solve machine comprehension using Match- LSTM and answer pointer. <ref type="bibr" target="#b15">Seo et al. (2017)</ref> and <ref type="bibr" target="#b24">Xiong et al. (2017)</ref> applied different ways to match the question and the context with bi- directional attention.  used iter- ative aligner to match the question and the pas- sage with feature-rich encoder. <ref type="bibr" target="#b4">Cui et al. (2017)</ref> employed one more layer of attention over the bi-directional attention mechanism.  applied a self-matching mechanism to ag- gregate evidence from the context. <ref type="bibr" target="#b18">Tan et al. (2018)</ref> proposed to generate answer from ex- tracted answer span. <ref type="bibr" target="#b26">Yu et al. (2018)</ref> proposed to use convolution with self-attention instead of re- current models in reading comprehension.</p><p>Recently there are some emerging works start- ing to touch the reading comprehension task from the answer side. <ref type="bibr" target="#b20">Wang et al. (2018a)</ref> proposed to use evidence aggregation to re-rank answer candi- dates extracted from different passages, and <ref type="bibr" target="#b22">Wang et al. (2018b)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>proposed Cross-Passage Answer Verification model for the same purpose. Neither of them involved multiple answers as in this work. Minimum Risk Training (MRT) has been widely used in various tasks in NLP. Shen et al. (2016) introduced MRT into Neural Machine</head><p>Translation, and <ref type="bibr" target="#b0">Ayana et al. (2016)</ref> applied it in Text Summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this section we describe in details the architec- ture of our model which is depicted in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Passage Extraction</head><p>Unlike most other datasets where the source of an- swers is a short passage with a few hundred words, the DuReader dataset provides up to 5 full docu- ments, which may contain up to 100K words. This incurs exorbitant demand on memory and train- ing time. To deal with this issue, previous ap- proaches select a single representative paragraph for each document, on which the answer extrac- tion is performed. The original paper of DuReader ( <ref type="bibr" target="#b5">He et al., 2017</ref>) employed a simple heuristic strat- egy, and <ref type="bibr" target="#b22">Wang et al. (2018b)</ref> trained a paragraph ranking model, while <ref type="bibr" target="#b2">Clark and Gardner (2017)</ref> applied TF-IDF based method for the TriviaQA dataset ( <ref type="bibr" target="#b10">Joshi et al., 2017</ref>) which was in a simi- lar situation. However, answers could come from more than one paragraph. We apply a simple yet effective method to extract contents from multi- ple paragraphs of the document, aiming to include as much information for the answer extraction as possible.</p><p>We concatenate the title and the whole docu- ment as the passage if it is shorter than a prede- fined maximum length. If not, we employ passage extraction in the following way:</p><p>• The title of the document is extracted.</p><p>Whether a document is relevant to the ques- tion could be easily seen from the title.</p><p>• We compute BLEU-4 score of each para- graph relative to the question, and select the one that appears first in the document among paragraphs with top-k scores.</p><p>• We extract the full body of this selected para- graph and the next paragraph.</p><p>• For each of the following paragraphs, the first sentence is extracted as it probably contains the main point.</p><p>• We concatenate all the extracted contents to form the extracted passage, and it is truncated to the maximum length if it is longer than the predefined value.</p><p>We apply our model on the basis of the extracted passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Representation of Word</head><p>Given a word sequence of question Q = {w q t } m t=1 , and a word sequence of extracted passage P = {w p t } n t=1 , we combine different useful information to form the representation of each question word w q t and passage word w p t :</p><p>• Word-level embedding: each word w in the question and passage is mapped to its corre- sponding n-dimensional embedding we.</p><p>• POS tag embedding: we use a POS tagger to tag each word in the question and passage. Each POS tag is mapped to a m-dimensional embedding pe.</p><p>•  Each question word is represented as the con- catenation of the word embedding we, and the POS tag embedding pe, denoted as x q = [we; pe]. Each passage word is additionally concatenated with the word-in-question feature wiq: x p = [we; pe; wiq].</p><p>It should be noted that, character-level embed- ding is an important part of word representation in English MRC models ( <ref type="bibr" target="#b15">Seo et al., 2017;</ref><ref type="bibr" target="#b23">Weissenborn et al., 2017;</ref><ref type="bibr" target="#b18">Tan et al., 2018)</ref>. Character sequence would give informa- tion which helps to relieve the OOV problem, as many English words share the same stem and dif- fer only in prefix or suffix. However, this is not the case in Chinese, and we observe no significant im- provement incorporating character-level embed- ding into our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Layer</head><p>Following previous work, we use a bi-directional LSTM to obtain contextual encoding for each word in the question and passage respectively:</p><formula xml:id="formula_0">u q i = BiLST M (u q i−1 , x q i )<label>(1)</label></formula><formula xml:id="formula_1">u p j = BiLST M (u p j−1 , x p j )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Match Layer</head><p>To fuse question encoding and passage encoding, we adopt the Attention Flow Layer ( <ref type="bibr" target="#b15">Seo et al., 2017</ref>) with a simpler similarity function. The sim- ilarity score between the contextual encoding for a query word u q i and that for a passage word u p j is defined as:</p><formula xml:id="formula_2">s ij = u q i T · u p j<label>(3)</label></formula><p>The context-to-query attention vectors c p j are computed from the similarity scores:</p><formula xml:id="formula_3">a ij = exp(s ij ) m k=1 exp(s kj )<label>(4)</label></formula><formula xml:id="formula_4">c p j = m i=1 a ij u q i<label>(5)</label></formula><p>The query-to-context attention vector d p is com- puted as:</p><formula xml:id="formula_5">z j = max i s ij (6) b j = exp(z j ) n k=1 exp(z k )<label>(7)</label></formula><formula xml:id="formula_6">d p = n j=1 b j u p j<label>(8)</label></formula><p>Another BiLSTM is applied on top of them to get the question-aware passage representation:</p><formula xml:id="formula_7">h p j = BiLST M (h p j−1 , [u p j ; c p j ; u p j • c p j ; u p j • d p ])<label>(9)</label></formula><p>3.5 Multi-answer multi-task loss function In the boundary model with pointer network ( <ref type="bibr" target="#b19">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b18">Tan et al., 2018</ref>), two probability distributions y 1 j and y 2 j (j = 1 . . . n), which denote the probability that position j is the beginning or the end of the answer span respectively, are computed as follows:</p><formula xml:id="formula_8">s t j = v T tanh(W p h h p j + W P a h a j−1 )<label>(10)</label></formula><formula xml:id="formula_9">y t j = exp(s t j ) n k=1 exp(s t k )<label>(11)</label></formula><formula xml:id="formula_10">c t = n j=1 y t j h p j<label>(12)</label></formula><formula xml:id="formula_11">h a t = BiLST M (h a t−1 , c t )<label>(13)</label></formula><p>where t = 1, 2, and the initial hidden state h a 0 is generated by an attention-pooling over the ques- tion representation following :</p><formula xml:id="formula_12">s i = v T tanh(W q u u q i + b)<label>(14)</label></formula><formula xml:id="formula_13">a i = exp(s i ) m k=1 exp(s k )<label>(15)</label></formula><formula xml:id="formula_14">h a 0 = m i=1 a i u q i<label>(16)</label></formula><p>Note that all passages for the same question are concatenated in order to predict one answer span. The loss is defined as the sum of negative log prob- abilities of the ground truth start and end position based on the predicted distributions:</p><formula xml:id="formula_15">L = −(log y 1 start + log y 2 end )<label>(17)</label></formula><p>We propose three different ways to incorporate multiple answers. A simple solution is to compute the average loss for multiple answer spans:</p><formula xml:id="formula_16">L avg = − 1 A A k=1 (log y 1 start k + log y 2 end k )<label>(18)</label></formula><p>L avg treats all answer spans as equally good. However, some of them may be closer to human- generated answers than others. We therefore de- fine the weighted average loss as follow:</p><formula xml:id="formula_17">L wavg = − A k=1 w k (log y 1 start k + log y 2 end k ) (19)</formula><p>where w k is the F-score between the answer span and the corresponding human-generated answer, normalized by the sum of the scores of each an- swer.</p><p>Another solution is to use the minimal value of the loss from each span:</p><formula xml:id="formula_18">L min = min k (−(log y 1 start k + log y 2 end k )) (20)</formula><p>Instead of predicting all answer spans, this loss will encourage the model to predict only the easi- est answer span for it.</p><p>The answer span prediction loss L ap is defined as the average of any of the loss functions de- scribed above over the training set. <ref type="bibr" target="#b18">Tan et al. (2018)</ref> showed that their single-answer, multi-passage MRC model benefits from using multi-task learning by adding an auxiliary loss to predict the correct passage to extract the answer from. We adapt the idea to compute passage se- lection loss L ps in multi-answer setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Passage selection with multi-answer</head><p>We first apply attention-pooling over the pas- sage representation {h p j } n j=1 , and then calculate a matching score g for each passage:</p><formula xml:id="formula_19">s j = v T tanh(W p u h p j + b)<label>(21)</label></formula><formula xml:id="formula_20">a j = exp(s j ) n k=1 exp(s k )<label>(22)</label></formula><formula xml:id="formula_21">r p = n j=1 a j h p j (23) g = sigmoid(v T sp r p )<label>(24)</label></formula><p>Since multiple answers are provided in the DuReader dataset, multiple passages may contain correct answers. The match score g for different passages are not in competition against one an- other. We therefore used pointwise sigmoid func- tion instead of the softmax function (as in <ref type="bibr" target="#b18">Tan et al. (2018)</ref>) in the passage selection loss L ps :</p><formula xml:id="formula_22">L ps = − 1 K K k=1 (y k log g k +(1 − y k ) log(1 − g k ))<label>(25)</label></formula><p>where y k = 1 if one of the gold span comes from this passage, y k = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Joint training</head><p>We train our model by jointly optimizing answer span prediction loss and passage selection loss:</p><formula xml:id="formula_23">L = L ap + λ ps L ps (26)</formula><p>where λ ps is a hyper-parameter tuned on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Minimum Risk Training</head><p>Minimum Risk Training (MRT) has been widely used in various tasks in NLP. The basic idea is to directly optimize the evaluation metric instead of maximizing the log likelihood of training data using Maximum Likelihood Estimation (MLE) as described above. In MRT, the object is to mini- mize the expected loss with respect to the posterior distribution:</p><formula xml:id="formula_24">J M RT (θ) = 1 N N i=1 E y i |x i ;θ [∆(y i , y * i )] (27)</formula><p>where ∆(y i , y * i ) is a function which indicates the difference between the predicted result y i and the label result y * i .</p><p>In this work, we apply MRT to solve the prob- lem of multi-occurrence of answer in machine reading comprehension, directly using the metric (ROUGE-L) as ∆. As an answer occurs multi- ple times in the context, each span in which the answer occurs will have minimum difference with the answer, and is thus given a high score by a model trained with MRT.</p><p>In machine translation and many other tasks, to compute the expected metric with respect to the posterior distribution is often intractable. Thus sampling methods are commonly used in MRT training. However, in our span extraction model, we use all spans without sampling.</p><p>Formally, the MRT loss in our model is defined as: </p><formula xml:id="formula_25">J M RT (θ) = 1 N N i=1 |P | k=1 |P | l=k+1 y 1 k y 2 l ∆(P k,l , A)<label>(28)</label></formula><formula xml:id="formula_26">J(θ) = J M LE (θ) + λJ M RT (θ)<label>(29)</label></formula><p>where J M LE (θ) refers to L in equation 26 and λ is a hyper-parameter tuned on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conduct our experiment on the DuReader dataset ( <ref type="bibr" target="#b5">He et al., 2017)</ref>, where multiple passages containing full-body text are provided for each question, and over half of the questions have mul- tiple answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>The DuReader dataset consists of 201574 ques- tions in total, with 181574 in the training set, 10000 in the development set, and 10000 in the test set. The questions are sampled from fre- quently occurring queries from Baidu search en- gines, and the full-body text of top-5 search results from the web are provided as the context. BLEU-4 and ROUGE-L are used in evaluation on DuReader. However, the implementations for the two metrics are quite different in the official evaluation tool. As in MS-MARCO, the BLEU- 4 score is normalized across all questions, essen- tially giving different weights to different ques- tions, while the ROUGE-L is averaged across dif- ferent questions. We mainly focus on ROUGE- L as each question in a reading comprehension   dataset should have equal weight in evaluation ( <ref type="bibr" target="#b18">Tan et al., 2018)</ref>. For a single question with mul- tiple reference answers, the maximum score with any reference answers is used, as implemented in the official tool for ROUGE-L. This is reasonable as providing one valid answer is good enough in many cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Word and POS Tag Embedding</head><p>We train a segmentation model with one-layer BiLSTM using the DuReader dataset, and apply it to a subset of SogouT corpus 2 , which con- tains a large amount of Chinese web pages( <ref type="bibr" target="#b12">Liu et al., 2012</ref>). 256-dimension word embeddings are trained on this data with language model task us- ing one-layer BiLSTM model. As for POS tag, we use a POS tagger trained on the Chinese Treebank (CTB) data to tag each word in questions and passages in the DuReader dataset. 64-dimension POS tag embeddings are trained on this data using one-layer BiLSTM model.</p><p>We keep all word and POS tag embeddings fixed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Training and Parameters</head><p>The maximum length of each passage is set to be 500. The batch size is set to be 32. The dimen- sion of hidden vector is set to be 150 for all lay- ers. Dropout ( <ref type="bibr" target="#b17">Srivastava et al., 2014</ref>) is applied be- tween layers, with a dropout rate of 0.15. We use λ ps = 3 for passage selection loss and λ = 10 for   Our model is optimized with Adam algorithm ( <ref type="bibr" target="#b11">Kingma and Ba, 2014)</ref>, and the learning rate is fixed to 0.001 during training. <ref type="table">Table 1</ref> shows the results for passage extraction and rich-feature representation (pre-trained word, POS, and word-in-query embeddings) on the de- velopment set. Both of them dramatically in- crease ROUGE-L and BLEU-4 score over the BiDAF baseline from the original DuReader pa- per. Together they form our single-answer base- line, on which we test the effectiveness of the multi-answer multi-task loss and Minimum Risk Training. <ref type="table" target="#tab_1">Table 2</ref> shows the experimental results with three different multi-answer loss functions introduced in Section 3.5.1. All of them offer improvement over the single-answer baseline, which shows the effectiveness of utilizing multiple answers. The average loss performs better than the min loss, which suggests that forcing the model to predict all possible answers is better than asking it to just find the easiest one. Not surprisingly, by taking into account the quality of different answer spans, the weighted average loss outperforms the average loss and achieves the best result among the three. All later experiments are conducted based on the weighted average loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Single-Answer Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Different loss functions with multi-answer</head><p>Model ROUGE-L BLEU-4 BiDAF ( <ref type="bibr" target="#b5">He et al., 2017)</ref> 39.0 31.8 Match-LSTM ( <ref type="bibr" target="#b5">He et al., 2017)</ref> 39.2 31.9 PR+BiDAF ( <ref type="bibr" target="#b22">Wang et al., 2018b)</ref> 41.81 <ref type="bibr">37.55 PE+BiDAF (ours)</ref> 45.93 38.86 V-Net ( <ref type="bibr" target="#b22">Wang et al., 2018b)</ref> 44.18 40.97 Our complete model 51.09 43.76 Human 57.4 56.1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Multi-task Loss and Minimum Risk Training</head><p>As we can see in <ref type="table" target="#tab_3">Table 3</ref>, the ROUGE-L score on the DuReader development set increases to 49.77 by incorporating multi-answer into the loss func- tion. Joint learning with passage selection loss yields an increase of 0.19. And with Minimum Risk Training, our model can reach a ROUGE-L score of 50.62, with a further increment of 0.66.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.4</head><p>Comparison with State-of-the-art <ref type="table" target="#tab_4">Table 4</ref> shows the performance of our model and other state-of-the-art models on the DuReader test set. First, we compare our passage extraction method with the paragraph ranking model from <ref type="bibr" target="#b22">Wang et al. (2018b)</ref>. Based on the same BiDAF model described in Section 3.4, our method (PE+BiDAF) significantly outperforms the trained model from <ref type="bibr" target="#b22">Wang et al. (2018b)</ref> (PR+BiDAF) on the DuReader test set. As we can see, our complete model achieves the state-of-the-art per- formance in both ROUGE-L and BLEU-4, and greatly narrows the performance gap between MRC system and human in the challenging real- world setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Further Analysis</head><p>For further analysis, we construct two sets from the development set. Q s contains 2787 questions with a single reference answer, and Q m contains 6650 questions with more than one reference an- swer. 563 questions from the development set are labeled with no answer, and thus not included in Q s or Q m . <ref type="table" target="#tab_5">Table 5</ref> shows the performance of our model on Q s and Q m . It can be seen that even questions with single answer (Q s ) can ben- efit from using multiple answers in training. The improvement for Q m is higher than that for Q s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we focus on real-world machine reading comprehension. We propose a multi- answer multi-task framework to tackle the multi- answer problem which is common in everyday world. Minimum Risk Training is applied to solve the multi-occurrence problem of the answer. We also propose a simple method for passage extrac- tion which solves the length issue of the pas- sage. Experimental results indicate that our model achieves state-of-the-art performance in the chal- lenging DuReader dataset. Despite using multiple answers in training, our system only predicts a single answer in decoding time. However, in some cases (e.g. for questions about opinion), finding all possible answers may be desirable. In the future, we plan to design mod- els which could generate all possible answers for a single question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of questions with multiple answers from the DuReader dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Word-in-question feature: following Chen et al. (2017) and Weissenborn et al. (2017), we use one additional binary feature wiq for each passage word, indicating whether this word occurs in the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model Architecture</figDesc><graphic url="image-1.png" coords="4,115.21,53.93,364.69,365.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 5 .</head><label>5</label><figDesc>1 Answer prediction with multi-answer A reading comprehension model is typically trained as an extractor of an answer span from a candidate passage. In DuReader dataset, multiple reference answers are provided for a single ques- tion. For each of the reference answers, we add the span with the highest F1 score to the gold answer spans. For models considering only a single an- swer span (baseline model), the gold answer span is the one with the highest F1 score relative to any of the reference answers (He et al., 2017; Wang et al., 2018b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>As in Hu et al. (2017), we minimize the linear combination of MLE and MRT loss:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison among different choices for the 
loss function with multiple answers on the development 
set 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results with multi-answer multi-task loss and 
Minimum Risk Training on the development set 

MRT. All parameters are tuned on the DuReader 
development set. 
As MRT training is more time-consuming than 
MLE training, our MRT model is initialized with 
model trained with MLE. It usually obtains the 
best result in just one epoch, which results in fea-
sible training time. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of our model and competing models on the DuReader test set 

ROUGE-L 
Model 
Q s 
Q m 
single-answer 38.01 53.8 
multi-answer 38.66 54.65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Results on Q s and Q m</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We use these two terms, question and query, interchangeably in the following content</note>

			<note place="foot" n="2"> http://www.sogou.com/labs/resource/t. php</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural headline generation with minimum risk training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<idno type="arXiv">arXiv:1710.10723</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dureader: a chinese machine reading comprehension dataset from realworld applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<editor>C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798.Version5</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying web spam with the wisdom of the crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weize</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyun</forename><surname>Ru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">S-net: From answer extraction to answer synthesis for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-passage machine reading comprehension with cross-passage answer verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02220</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making neural qa as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
