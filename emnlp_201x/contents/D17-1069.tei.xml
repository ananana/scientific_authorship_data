<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
							<email>dheerajm@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIT Kanpur</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
								<orgName type="institution" key="instit4">IIT Kanpur</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIT Kanpur</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
								<orgName type="institution" key="instit4">IIT Kanpur</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargavi</forename><surname>Paranjape</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIT Kanpur</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
								<orgName type="institution" key="instit4">IIT Kanpur</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIT Kanpur</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
								<orgName type="institution" key="instit4">IIT Kanpur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="659" to="669"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a feature vector formation technique for documents-Sparse Composite Document Vector (SCDV)-which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV, word em-beddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outper-form the previous state-of-the-art method, NTSG (Liu et al., 2015a). We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval. Moreover, we achieve significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds-better performance with lower time and space complexity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word embeddings represent words as dense, low-dimensional and real-valued vectors that can capture their semantic and syntactic prop- erties. These embeddings are used abundantly by machine learning algorithms in tasks such as text classification and clustering. Traditional bag- of-word models that represent words as indices into a vocabulary don't account for word ordering and long-distance semantic relations. Represen- tations based on neural network language models *Represents equal contribution <ref type="bibr" target="#b22">(Mikolov et al., 2013b</ref>) can overcome these flaws and further reduce the dimensionality of the vec- tors. The success of the method is recently math- ematically explained using the random walk on discourses model ( <ref type="bibr" target="#b3">Arora et al., 2016a</ref>). However, there is a need to extend word embeddings to en- tire paragraphs and documents for tasks such as document and short-text classification.</p><p>Representing entire documents in a dense, low- dimensional space is a challenge. A simple weighted average of the word embeddings in a large chunk of text ignores word ordering, while a parse tree based combination of embeddings ( <ref type="bibr" target="#b31">Socher et al., 2013)</ref> can only extend to sentences. ( <ref type="bibr" target="#b15">Le and Mikolov, 2014</ref>) trains word and para- graph vectors to predict context but shares word- embeddings across paragraphs. However, words can have different semantic meanings in different contexts. Hence, vectors of two documents that contain the same word in two distinct senses need to account for this distinction for an accurate se- mantic representation of the documents. ( <ref type="bibr" target="#b18">Ling et al., 2015)</ref>, ( <ref type="bibr" target="#b19">Liu et al., 2015a</ref>) map word em- beddings to a latent topic space to capture differ- ent senses in which words occur. However, they represent complex documents in the same space as words, reducing their expressive power. These methods are also computationally intensive.</p><p>In this work, we propose the Sparse Compos- ite Document Vector(SCDV) representation learn- ing technique to address these challenges and cre- ate efficient, accurate and robust semantic repre- sentations of large texts for document classifica- tion tasks. SCDV combines syntax and semantics learnt by word embedding models together with a latent topic model that can handle different senses of words, thus enhancing the expressive power of document vectors. The topic space is learnt effi- ciently using a soft clustering technique over em- beddings and the final document vectors are made sparse for reduced time and space complexity in tasks that consume these vectors.</p><p>The remaining part of the paper is organized as follows. Section 2 discusses related work in docu- ment representations. Section 3 introduces and ex- plains SCDV in detail. This is followed by exten- sive and rigorous experiments together with anal- ysis in section 4 and 5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>( <ref type="bibr" target="#b15">Le and Mikolov, 2014</ref>) proposed two models for distributional representation of a document, namely, Distributed Memory Model Paragraph Vectors (PV-DM) and Distributed BoWs para- graph vectors (PV-DBoW). In PV-DM, the model is learned to predict the next context word us- ing word and paragraph vectors. In PV-DBoW, the paragraph vector is directly learned to predict randomly sampled context words. In both mod- els, word vectors are shared across paragraphs. While word vectors capture semantics across dif- ferent paragraphs of the text, documents vectors are learned over context words generated from the same paragraph and potentially capture only local semantics <ref type="bibr" target="#b30">(Singh and Mukerjee, 2015)</ref>. Moreover, a paragraph vector is embedded in the same space as word vectors though it can contain multiple top- ics and words with multiple senses. As a result, doc2vec <ref type="bibr" target="#b15">(Le and Mikolov, 2014</ref>) doesn't perform well on Information Retrieval as described in <ref type="bibr" target="#b0">(Ai et al., 2016a</ref>) and ( <ref type="bibr" target="#b29">Roy et al., 2016)</ref>. Consequently, we expect a paragraph vector to be embedded in a higher dimensional space.</p><p>A paragraph vector also assumes all words con- tribute equally, both quantitatively (weight) and qualitatively (meaning). They ignore the impor- tance and distinctiveness of a word across all doc- uments ( <ref type="bibr" target="#b30">Singh and Mukerjee, 2015)</ref>. <ref type="bibr" target="#b30">Mukerjee et al. (Singh and Mukerjee, 2015)</ref> proposed idf- weighted averaging of word vectors to form doc- ument vectors. This method tries to address the above problem. However, it assumes that all words within a document belong to the same se- mantic topic. Intuitively, a paragraph often has words originating from several semantically dif- ferent topics. In fact, Latent Dirichlet Allocation ( <ref type="bibr" target="#b6">Blei et al., 2003</ref>) models a document as a distri- bution of multiple topics.</p><p>These shortcomings are addressed in three novel composite document representations called Topical word embedding (TWE-1,TWE-2 and TWE-3) by ( <ref type="bibr" target="#b19">Liu et al., 2015a</ref>). TWE-1 learns word and topic embeddings by considering each topic as a pseudo word and builds the topical word embed- ding for each word-topic assignment. Here, the interaction between a word and the topic to which it is assigned is not considered. TWE-2 learns a topical word embedding for each word-topic as- signment directly, by considering each word-topic pair as a pseudo word. Here, the interaction be- tween a word and its assigned topic is considered but the vocabulary of pseudo-words blows up. For each word and each topic, TWE-3 builds distinct embeddings for the topic and word and concate- nates them for each word-topic assignment. Here, the word embeddings are influenced by the corre- sponding topic embeddings, making words in the same topic less discriminative. ( <ref type="bibr" target="#b11">Gupta et al., 2016)</ref> proposed a method to form a composite document vector using word embed- dings and tf-idf values, called the Bag of Words Vector (BoWV). In BoW V , each document is rep- resented by a vector of dimension D = K * d+K, where K is the number of clusters and d is the dimension of the word embeddings. The core idea behind BoW V is that semantically different words belong to different topics and their word vectors should not be averaged. Further, BoW V computes inverse cluster frequency of each clus-ter (icf) by averaging the idf values of its mem- ber terms to capture the importance of words in the corpus. However, BoW V does hard clustering using K-means algorithm, assigning each word to only one cluster or semantic topic but a word can belong to multiple topics. For example, the word apple belongs to topic food as a fruit, and belongs to topic Information Technology as an IT company. Moreover, BoW V is a non-sparse, high dimen- sional continuous vector and suffers from compu- tational problems like large training time, predic- tion time and storage requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sparse Composite Document Vectors</head><p>In this section, we present the proposed Sparse Composite Document Vector (SCDV) representa- tion as a novel document vector learning algo- rithm. The feature formation algorithm can be di- vided into three steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Vector Clustering</head><p>We begin by learning d dimensional word vec- tor representations for every word in the vocab- ulary V using the skip-gram algorithm with neg- ative sampling (SGNS) ( <ref type="bibr" target="#b21">Mikolov et al., 2013a</ref>). We then cluster these word embeddings using the Gaussian Mixture Models(GMM) (Reynolds, 2015) soft clustering technique. The number of clusters, K, to be formed is a parameter of the SCDV model. By inducing soft clusters, we en- sure that each word belongs to every cluster with some probability P (c k |w i ).</p><formula xml:id="formula_0">p(c k = 1) = π k p(c k = 1|w) = π k N (w|µ k , Σ k ) Σ K j=1 π j N (w|µ j , Σ j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Topic-vector Formation</head><p>For each word w i , we create K different word- cluster vectors of d dimensions ( wcv ik ) by weight- ing the word's embedding with its probability dis- tribution in the k th cluster, P (c k |w i ). We then concatenate all K word-cluster vectors ( wcv ik ) into a K×d dimensional embedding and weight it with inverse document frequency of w i to form a word-topics vector ( wtv i ). Finally, for all words appearing in document D n , we sum their word- topic vectors wtv i to obtain the document vector dv Dn .</p><formula xml:id="formula_1">wcv ik = wv i × P (c k |w i ) Algorithm 1: Sparse Composite Document Vector Data: Documents D n , n = 1 . . . N Result: Document vectors SCDV Dn , n = 1 . . . N 1 Obtain word vector ( wv i ), for each word w i ; 2 Calculate idf values, idf (w i ), i = 1..|V | ; / * |V | is vocabulary size * / 3</formula><p>Cluster word vectors wv using GMM clustering into K clusters;</p><p>4 Obtain soft assignment P (c k |w i ) for word w i and cluster c k ; / * Loop 5-10 can be pre-computed * / 5 for each word w i in vocabulary V do <ref type="bibr">6</ref> for each cluster c k do </p><formula xml:id="formula_2">7 wcv ik = wv i × P (c k |w i ); 8 end 9 wtv i = idf (w i ) × K k=1 wcv ik ; / * is concatenation *</formula><formula xml:id="formula_3">wtv i = idf (w i ) × K k=1 wcv ik</formula><p>where, is concatenation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sparse Document Vectors</head><p>After normalizing the vector, we observed that most values in dv Dn are very close to zero. <ref type="figure" target="#fig_3">Fig- ure 3</ref> verifies this observation. We utilize this fact to make the document vector dv Dn sparse by zero- ing attribute values whose absolute value is close to a threshold (specified as a parameter), which re- sults in the Sparse Composite Document Vector SCDV Dn .</p><p>In particular, let p be percentage sparsity thresh- old parameter, a i the value of the i th attribute of the non-Sparse Composite Document Vector and n represent the n th document in the training set:   </p><formula xml:id="formula_4">a i = a i if |a i | ≥ p 100 * t 0 otherwise t = |a min | + |a max | 2 a min = avg n (min i (a i )) a max = avg n (max i (a i ))</formula><p>Flowcharts depicting the formation of word- topics vector and Sparse Composite Document Vectors are shown in <ref type="figure" target="#fig_1">figure 1 and figure 2</ref> respec- tively. Algorithm 1 describes SCDV in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform multiple experiments to show the ef- fectiveness of SCDV representations for multi- class and multi-label text classification. For all ex- periments and baselines, we use Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz, 40 working cores, 128GB RAM machine with Linux Ubuntu 14.4. However, we utilize multiple cores only during Word2Vec training and when we run the one-vs- rest classifier for Reuters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We consider the following baselines: Bag-of- Words (BoW) model <ref type="bibr" target="#b12">(Harris, 1954</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Classification</head><p>We run multi-class experiments on 20NewsGroup dataset 2 and multi-label classification experi- ments on Reuters-21578 dataset <ref type="bibr">3</ref> . We use the script 4 for preprocessing the Reuters-21578 dataset. We use LinearSVM for multi-class classi-fication and Logistic regression with OneVsRest setting for multi-label classification in baselines and SCDV.</p><p>For SCDV, we set the dimension of word- embeddings to 200 and the number of mixture components in GMM to 60. All mixture com- ponents share the same spherical co-variance ma- trix. We learn word vector embedding using Skip- Gram with window size of 10, Negative Sampling (SGNS) of 10 and minimum word frequency as 20. We use 5-fold cross-validation on F1 score to tune parameter C of SVM and the sparsity thresh- old for SCDV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Multi-class classification</head><p>We evaluate classifier performance using standard metrics like accuracy, macro-averaging precision, recall and F-measure. <ref type="table" target="#tab_1">Table 1</ref> shows a compari- son with the current state-of-art (NTSG) document representations on the 20Newsgroup dataset. We observe that SCDV outperforms all other current models by fair margins. We also present the class- wise precision and recall for 20Newsgroup on an almost balanced dataset with SVM over Bag of Words model and the SCDV embeddings in <ref type="table" target="#tab_2">Table  2</ref> and observe that SCDV improves consistently over all classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multi-label classification</head><p>We evaluate multi-label classification perfor- mance using Precision@K, nDCG@k (Bhatia et al., 2015), Coverage error, Label ranking av- erage precision score (LRAPS) 5 and F1-score. All measures are extensively used for the multi- label classification task. However, F1-score is an appropriate metric for multi-label classifica- tion as it considers label biases when train-test splits are random. <ref type="table" target="#tab_3">Table 3</ref> show evaluation results for multi-label text classification on the Reuters- 21578 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Effect of Hyper-Parameters</head><p>SCDV has three parameters: the number of clus- ters, word vector dimension and sparsity threshold parameter. We vary one parameter by keeping the other two constant. Performance on varying all three parameters in shown in <ref type="figure" target="#fig_7">Figure 4</ref>. We ob- serve that performance improves as we increase the number of clusters and saturates at 60. The performance improves until a word vector dimen- sion of 300 after which it saturates. Similarly, we observe that the performance improves as we increase p till 4 after which it declines. At 4% thresholding, we reduce the storage space by 80% compared to the dense vectors. We observe that SCDV is robust to variations in training Word2Vec  <ref type="table" target="#tab_1">Tables 1, 3</ref> are the average values obtained across 5 separate runs of SCDV , each run training a dif- ferent Word2Vec and GMM model with identical hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topic Coherence</head><p>We evaluate the topics generated by GMM cluster- ing on 20NewsGroup for quantitative and qualita- tive analysis. We used Bayes rule to compute the P (w k |c i ) for a given topic c i and given word w j and com- pute the score of the top 10 words for each topic.</p><formula xml:id="formula_5">P (w k |c i ) = P (c i |w k )P (w k ) P (c i )</formula><p>where,</p><formula xml:id="formula_6">P (c i ) = K i=1 P (c i |w k )P (w k ) P (w k ) = #(w k ) V i=1 #(w i )</formula><p>Here, #(w k ) denotes the number of times word w k appears in the corpus and V represents vocab- ulary size.</p><p>We calculated the topic coherence score for all topics for SCDV , LDA and LT SG ( <ref type="bibr" target="#b14">Law et al., 2017)</ref>. Averaging the score of all 80 topics, GMM clustering scores -85.23 compared to -108.72 of LDA and -92.23 of LTSG. Thus, SCDV creates more coherent topics than both LDA and LTSG. <ref type="table">Table 4</ref> shows top 10 words of 3 topics from GM M clustering, LDA model and LT SG model on 20NewsGroup and SCDV shows higher topic coherence. Words are ranked based on their prob- ability distribution in each topic. Our results also support the qualitative results of ( <ref type="bibr" target="#b27">Randhawa et al., 2016)</ref>, <ref type="bibr" target="#b32">(Sridhar, 2015)</ref> paper, where k- means, GMM was used respectively over word vectors to find topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Context-Sensitive Learning</head><p>In order to demonstrate the effects of soft clus- tering (GMM) during SCDV formation, we se- lect some words (w j ) with multiple senses from 20Newsgroup and their soft cluster assignments to find the dominant clusters. We also select top scoring words (w k ) from each cluster (c i ) to rep- resent the meaning of that cluster. <ref type="table" target="#tab_5">Table 5</ref> shows polysemic words and their dominant clusters with assignment probabilities. This indicates that using soft clustering to learn word vectors helps com- bine multiple senses into a single embedding vec- tor. <ref type="figure" target="#fig_1">(Arora et al., 2016b</ref>) also reported similar re- sults for polysemous words.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Information Retrieval</head><formula xml:id="formula_7">P P V (w|d) = exp( w. d) V i=1 exp( w i . d)</formula><p>and the score for document d and query string Q is given by</p><formula xml:id="formula_8">score(q, d) = w∈Q P (w)P (w|d)</formula><p>where P (w) is obtained from the unigram query model and score(q, d) is used to rank documents. (Ai et al., 2016b) do not directly make use of paragraph vectors for the retrieval task, but im- prove the document language model. To di- rectly make use of paragraph vectors and make computations more tractable, we directly inter- polate the language model query-document score score(q, d) with the similarity score between the normalized query and document vectors to gener- ate score P V (q, d), which is then used to rank doc- uments.</p><formula xml:id="formula_9">score P V (q, d) = (1 − λ)score(q, d) + λ q. d</formula><p>Directly evaluating the document similarity score with the query paragraph vector rather than col- lecting similarity scores for individual words in the query helps avoid confusion amongst distinct query topics and makes the interpolation operation faster. In <ref type="table" target="#tab_6">Table 6</ref>, we report Mean Average Pre- cision(MAP) values for four datasets, Associated Press 88-89 (topics 51-200), Wall Street Journal (topics 51-200), San Jose Mercury (topics 51-150) and <ref type="bibr">Disks 4 &amp; 5 (topics 301-450</ref>) in the TREC collection. We learn λ on a held out set of topics.</p><p>We observe consistent improvement in MAP for all datasets. We marginally improve the MAP re- ported by (Ai et al., 2016b) on the Robust04 task. In addition, we also report the improvements in MAP score when Model based relevance feedback <ref type="bibr">(Zhai and Lafferty, 2001</ref>) is applied over the ini- tially retrieved results from both models. Again, we notice a consistent improvement in MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head><p>SCDV overcomes several challenges encountered while training document vectors, which we had mentioned above. <ref type="table">Table 4</ref>: Top words of some topics from GMM and LDA on 20NewsGroup for K = 80. Higher score represent better coherent topics.</p><p>Topic <ref type="table">Image  Topic Health  Topic Mail  GMM  LTSG  LDA  GMM  LTSG  LDA  GMM  LTSG  LDA  file  image  image  heath  stimulation  doctor  ftp  anonymous  list  bit  jpeg  file  study  diseases  disease  mail  faq  mail  image  gif  color  medical  disease  coupons  internet  send  information  files  format  gif  drug  toxin  treatment  phone  ftp  internet  color  file  jpeg  test  toxic  pain  email  mailing  send  format  files  file</ref>   1. Clustering word-embeddings to discover top- ics improves performance of classification as <ref type="figure" target="#fig_7">Figure 4</ref> (left) indicates, while also gener- ating coherent clusters of words <ref type="table">(Table 4</ref>). <ref type="figure">Figure 5</ref> shows that clustering gives more discriminative representations of documents than paragraph vectors do since it uses K × d dimensions while paragraph vectors embed documents and words in the same space. This enables SCDV to represent complex docu- ments. Fuzzy clustering allows words to belong to multiple topics, thereby recogniz- ing polysemic words, as <ref type="table" target="#tab_5">Table 5</ref> indicates. Thus it mimics the word-context interaction in NTSG and LTSG.</p><p>2. Semantically different words are assigned to different topics. Moreover, a single docu- ment can contain words from multiple differ- ent topics. Instead of a weighted averaging of word embeddings to form document vec- tors, as in most previous work, concatenat- ing word embeddings for each topic (cluster) avoids merging of semantically different top- ics.</p><p>3. It is well-known that in higher dimensions, structural regularizers such as sparsity help overcome the curse of dimensionality <ref type="bibr" target="#b33">(Wainwright, 2014</ref>). <ref type="figure" target="#fig_3">Figure 3</ref> demonstrates this, since majority of the features are close to zero. Sparsity also enables linear SVM to scale to large dimensions. On 20News- Groups, BoWV model takes up 1.1 GB while SCDV takes up only 236MB( 80% decrease).</p><p>Since GMM assigns a non-zero probability to every topic in the word embedding, noise can accumulate when document vectors are cre- ated and tip the scales in favor of an unrelated topic. Sparsity helps to reduce this by zeroing out very small values of probability.</p><p>4. SCDV uses Gaussian Mixture Model (GMM) while T W E, N T SG and LT SG use LDA for finding semantic topics respectively. GMM time complexity is O(V N T 2 ) while that of LDA is O(V 2 N T ). Here, V = Vo- cabulary size, N = number of documents and T = number of topics. Since num- ber of topics T &lt; vocabulary size V, GMM is faster. Empirically, compared to T W E, SCDV reduces document vector formation, training and prediction time significantly. Ta- ble 7 shows training and prediction times for BoWV, SCDV and TWE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a document feature for- mation technique for topic-based document rep- resentation. SCDV outperforms state-of-the-art models in multi-class and multi-label classifica- tion tasks. SCDV introduces sparsity in document vectors to handle high dimensionality. <ref type="table" target="#tab_7">Table 7</ref> in-  dicates that SCDV shows considerable improve- ments in feature formation, training and prediction times for the 20NewsGroups dataset. We show that fuzzy GMM clustering on word-vectors lead to more coherent topic than LDA and can also be used to detect Polysemic words. SCDV embed- dings also provide a robust estimation of the query and document language models, thus improving the MAP of language model based retrieval sys- tems. In conclusion, SCDV is simple, efficient and creates a more accurate semantic representation of documents.</p><p>Chengxiang Zhai and John Lafferty. 2001. Model- based feedback in the language modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowl- edge management, pages 403-410. ACM.</p><p>Chengxiang <ref type="bibr">Zhai and John Lafferty. 2004</ref>. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Infor- mation Systems (TOIS), 22(2):179-214.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Liu et al., 2015a) proposed an architecture called Neural tensor skip-gram model (NTSG-1, NTSG-2, NTSG-3, NTSG-4), that learns multi- prototype word embeddings and uses a tensor layer to model the interaction of words and top- ics to capture different senses. N T SG outper- forms other embedding methods like T W E −1 on the 20 newsgroup data-set by modeling context- sensitive embeddings in addition to topical-word embeddings. LT SG (Law et al., 2017) builds on N T SG by jointly learning the latent topic space and context-sensitive word embeddings. All three, T W E, N T SG and LT SG use LDA and suf- fer from computational issues like large training time, prediction time and storage space. They also embed document vectors in the same space as terms. Other works that harness topic modeling like W T M (Fu et al., 2016), w2v−LDA (Nguyen et al., 2015), T V + M eanW V (Li et al., 2016a), LT SG (Law et al., 2017), Gaussian − LDA (Das et al., 2015), T opic2V ec (Niu et al., 2015), (Moody, 2016) and M vT M (Li et al., 2016b) also suffer from similar issues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Word-topics vector formation.</figDesc><graphic url="image-2.png" coords="4,77.59,221.79,207.09,116.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sparse Composite Document Vector formation.</figDesc><graphic url="image-3.png" coords="4,103.96,389.05,154.35,116.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of attribute feature vector values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>), Bag of Word Vector (BoWV) (Gupta et al., 2016) model, para- graph vector models (Le and Mikolov, 2014), Topical word embeddings (TWE-1) (Liu et al., 2015b), Neural Tensor Skip-Gram Model (NTSG- 1 to NTSG-3) (Liu et al., 2015a), tf-idf weighted average word-vector model (Singh and Mukerjee, 2015) and weighted Bag of Concepts (weight- BoC) (Kim et al., 2017), where we build topic- document vectors by counting the member words in each topic. We use the best parameter settings as reported in all our baselines to generate their results. We use 200 dimensions for tf-idf weighted word-vector model, 400 for paragraph vector model, 80 top- ics and 400 dimensional vectors for TWE, NTSG, LTSG and 60 topics and 200 dimensional word vectors for BOWV. We also compare our results with reported results of other topic modeling based document embedding methods like W T M (Fu et al., 2016), w2v − LDA (Nguyen et al., 2015), LDA (Chen and Liu, 2014), T V + M eanW V (Li et al., 2016a), LT SG (Law et al., 2017), Gaussian − LDA (Das et al., 2015), T opic2V ec (Niu et al., 2015), (Moody, 2016) and M vT M (Li et al., 2016b). Implementation of SCDV and re- lated experiments is available here 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Instead of using perplexity (Chang et al., 2011), which doesn't correlate with seman- tic coherence and human judgment of individ- ual topics, we used the popular topic coherence (Mimno et al., 2011), (Arora et al., 2013), (Chen and Liu, 2014) measure. A higher topic coherence score indicates a more coherent topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>Ai et al., 2016b) used (Mikolov et al., 2013b)'s paragraph vectors to enhance the basic language model based retrieval model. The language model(LM) probabilities are estimated from the corpus and smoothed using a Dirichlet prior (Zhai and Lafferty, 2004). In (Ai et al., 2016b), this language model is then interpolated with the para- graph vector (PV) language model as follows. P (w|d) = (1 − λ)P LM (w|d) + λP P V (w|d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of varying number of clusters (left), varying word vector dimension (center) and varying sparsity parameter (right) on performance for 20NewsGroup with SCDV</figDesc><graphic url="image-5.png" coords="7,97.80,256.82,401.94,130.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance on multi-class classification 
(Values in red show best performance, the SCDV 
algorithm of this paper) 

Model 
Acc Prec Rec F-mes 
SCDV 
84.6 84.6 84.5 
84.6 
NTSG-1 
82.6 82.5 81.9 
81.2 
NTSG-2 
82.5 83.7 82.8 
82.4 
BoWV 
81.6 81.1 81.1 
80.9 
NTSG-3 
81.9 83.0 81.7 
81.1 
LTSG 
82.8 82.4 81.8 
81.8 
WTM 
80.9 80.3 80.3 
80.0 
w2v-LDA 
77.7 77.4 77.2 
76.9 
TV+MeanWV 72.2 71.8 71.5 
71.6 
MvTM 
72.2 71.8 71.5 
71.6 
TWE-1 
81.5 81.2 80.6 
80.6 
lda2Vec 
81.3 81.4 80.4 
80.5 
lda 
72.2 70.8 70.7 
70.0 
weight-AvgVec 81.9 81.7 81.9 
81.7 
BoW 
79.7 79.5 79.0 
79.0 
weight-BOC 
71.8 71.3 71.8 
71.4 
PV-DBoW 
75.4 74.9 74.3 
74.3 
PV-DM 
72.4 72.1 71.5 
71.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Class-level results on the balanced 20newsgroup dataset.</head><label>2</label><figDesc></figDesc><table>BoW 
SCDV 
Class Name 
Pre. Rec. Pre. Rec. 
alt.atheism 
67.8 72.1 80.2 79.5 
comp.graphics 
67.1 73.5 75.3 77.4 
comp.os.ms-windows.misc 77.1 66.5 78.6 77.2 
comp.sys.ibm.pc.hardware 62.8 72.4 75.6 73.5 
comp.sys.mac.hardware 
77.4 78.2 83.4 85.5 
comp.windows.x 
83.2 73.2 87.6 78.6 
misc.forsale 
81.3 88.2 81.4 85.9 
rec.autos 
80.7 82.8 91.2 90.6 
rec.motorcycles 
92.3 87.9 95.4 95.7 
rec.sport.baseball 
89.8 89.2 93.2 94.7 
rec.sport.hockey 
93.3 93.7 96.3 99.2 
sci.crypt 
92.2 86.1 92.5 94.7 
sci.electronics 
70.9 73.3 74.6 74.9 
sci.med 
79.3 81.3 91.3 88.4 
sci.space 
90.2 88.3 88.5 93.8 
soc.religion.christian 
77.3 87.9 83.3 92.3 
talk.politics.guns 
71.7 85.7 72.7 90.6 
talk.politics.mideast 
91.7 76.9 96.2 95.4 
talk.politics.misc 
71.7 56.5 80.9 59.7 
talk.religion.misc 
63.2 55.4 73.5 57.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Performance on various metrics for multi-label classification for Reuters(Values in red show best performance, the SCDV algorithm of this paper)</head><label>3</label><figDesc></figDesc><table>Model 
Prec@1 
nDCG@1 
Prec 
@5 
nDCG 
@5 
Coverage 
Error 
LRAPS F1-Score 

SCDV 
94.20 
36.98 
49.55 
6.48 
93.30 
81.75 
BoWV 
92.90 
36.14 
48.55 
8.16 
91.46 
79.16 
TWE-1 
90.91 
35.49 
47.54 
8.16 
91.46 
79.16 
PV-DM 
87.54 
33.24 
44.21 
13.15 
86.21 
70.24 
PV-DBoW 
88.78 
34.51 
46.42 
11.28 
87.43 
73.68 
AvgVec 
89.09 
34.73 
46.48 
9.67 
87.28 
71.91 
tfidf AvgVec 
89.33 
35.04 
46.83 
9.42 
87.90 
71.97 

and GMM. The performance metrics reported in 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Words with multiple senses assigned to 
multiple clusters with significant probabilities 

Word 
Cluster Words 
P(ci|wj) 
subject:1 
physics, chemistry, math, science 
0.27 
subject:2 
mail, letter, email, gmail 
0.72 
interest:1 
information, enthusiasm, question 
0.65 
interest:2 
bank, market, finance, investment 
0.32 
break:1 
vacation, holiday, trip, spring 
0.52 
break:2 
encryption, cipher, security, privacy 
0.22 
break:3 
if, elseif, endif, loop, continue 
0.23 
unit:1 
calculation, distance, mass, length 
0.25 
unit:2 
electronics, KWH, digital, signal 
0.69 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 : Mean average precision (MAP) for IR on four IR datasets</head><label>6</label><figDesc></figDesc><table>Dataset 
LM 
LM+SCDV 
MB 
MB + SCDV 
AP 
0.2742 
0.2856 
0.3283 
0.3395 
SJM 
0.2052 
0.2105 
0.2341 
0.2409 
WSJ 
0.2618 
0.2705 
0.3027 
0.3126 
Robust04 0.2516 
0.2684 
0.2819 
0.2933 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Time Comparison (20NewsGroup) (Val-
ues in red show least time, the SCDV algorithm of 
this paper) 

Time (sec) 
BoWV TWE-1 SCDV 
DocVec Formation 
1250 
700 
160 
Total Training 
1320 
740 
200 
Total Prediction 
780 
120 
25 

</table></figure>

			<note place="foot" n="1"> https://github.com/dheeraj7596/SCDV 2 http://qwone.com/∼jason/20Newsgroups/ 3 https://goo.gl/NrOfu 4 https://gist.github.com/herrfz/7967781</note>

			<note place="foot" n="5"> Section 3.3.3.2 of https://goo.gl/4GrR3M</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors want to thank Nagarajan Natarajan (Post-Doc, Microsoft Research, India), Praneeth Netrapalli (Researcher, Microsoft Research, In-dia), Raghavendra Udupa (Researcher, Microsoft Research, India), Prateek Jain (Researcher, Mi-crosoft Research, India) for encouraging and valu-able feedback .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of the paragraph vector model for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM on International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving language estimation with the paragraph vector model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A latent variable model approach to pmi-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03764</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topic modeling using topics from many domains, lifelong learning and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gaussian lda for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 53th Annual Meeting of the Association of Computational Linguistic (ACL)</title>
		<meeting>The 53th Annual Meeting of the Association of Computational Linguistic (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
	<note>Association of Computational Linguistic</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving distributed word representation and topic model by word-topic mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangwang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 8th Asian Conference on Machine Learning</title>
		<meeting>The 8th Asian Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="190" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Product classification in ecommerce using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashendra</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradhuman</forename><surname>Jhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag-of-concepts: Comprehending document representation through clustering words in distributed representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjoong</forename><surname>Han Kyul Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarvan</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Hankz Hankui Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07117</idno>
		<title level="m">Ltsg: Latent topical skip-gram for mutually learning topic model and vector representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative topic embedding: a continuous representation of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 54th Annual Meeting of the Association for Computational Linguistics (ACL). Association of Computational Linguistic</title>
		<meeting>The 54th Annual Meeting of the Association for Computational Linguistics (ACL). Association of Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrating topic modeling with word embeddings by mixtures of vmfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of wordvec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wand</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. North American Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning context-sensitive word embeddings with neural tensor skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2014, the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>IJCAI 2014, the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1284" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Topical word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2015, the 29th Twenty-Ninth Association for the Advancement of Artificial Intelligence</title>
		<meeting>AAAI 2015, the 29th Twenty-Ninth Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mixing dirichlet topic models and word embeddings to make lda2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher E Moody</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02019</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topic2vec: learning distributed representations of topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Asian Language Processing (IALP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Topic modeling using distributed word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Ramandeep S Randhawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gagan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Reynolds</surname></persName>
		</author>
		<title level="m">Gaussian mixture models. Encyclopedia of biometrics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="827" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Representing documents and queries as sets of word embedded vectors for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwaipayan</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07869</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Words are not equal: Graded weighting model for building composite document vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranjal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitabha</forename><surname>Mukerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth International Conference on Natural Language Processing</title>
		<meeting>the twelfth International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICON-2015</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised topic modeling for short texts using distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kumar Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS@ HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="192" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured regularizers for high-dimensional problems: Statistical and computational issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="253" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
