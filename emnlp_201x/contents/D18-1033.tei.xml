<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-lingual Lexical Sememe Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University State Key</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University State Key</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University State Key</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="institution">Jiangsu Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University State Key</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Search Product Center</orgName>
								<orgName type="department" key="dep2">WeChat Search Application Department</orgName>
								<address>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University State Key</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-lingual Lexical Sememe Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="358" to="368"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>358</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many NLP tasks. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction , aiming to automatically predict se-memes for words in other languages. We propose a novel framework to model correlations between sememes and multilingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at https: //github.com/thunlp/CL-SP.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Words are regarded as the smallest meaningful unit of speech or writing that can stand by themselves in human languages, but not the smallest indivisi- ble semantic unit of meaning. That is, the meaning of a word can be represented as a set of semantic components. For example, "Man = human + male + adult" and "Boy = human + male + child". In lin- guistics, the minimum semantic unit of meaning is named sememe <ref type="bibr" target="#b3">(Bloomfield, 1926)</ref>. Some people believe that semantic meanings of concepts such as words can be composed of a limited closed set of sememes. And sememes can help us comprehend human languages better.</p><p>Unfortunately, the lexical sememes of words are not explicit in most human languages. Hence, peo- ple construct sememe-based linguistic knowledge * Indicates equal contribution † Corresponding author apple apple (brand) apple (fruit) computer fruit bases (KBs) via manually annotating every words with a pre-defined closed set of sememes. HowNet ( <ref type="bibr" target="#b9">Dong and Dong, 2003</ref>) is one of the most well- known sememe-based linguistic KBs. Different from WordNet <ref type="bibr" target="#b31">(Miller, 1995)</ref> which focuses on the relations between senses, it annotates each word with one or more relevant sememes. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the word apple has two senses includ- ing apple (fruit) and apple (brand) in HowNet. The sense apple (fruit) has one sememe fruit, and the sense apple (brand) has five sememes includ- ing computer, PatternValue, able, bring and Speci- ficBrand. There exist about 2, 000 sememes and over 100 thousand labeled Chinese and English words in HowNet. HowNet has been widely used in various NLP applications such as word simi- larity computation ( <ref type="bibr" target="#b25">Liu and Li, 2002</ref>), word sense disambiguation ( <ref type="bibr" target="#b45">Zhang et al., 2005</ref>), question clas- sification ( <ref type="bibr" target="#b37">Sun et al., 2007</ref>) and sentiment classifi- cation ( <ref type="bibr" target="#b7">Dang and Zhang, 2010)</ref>. However, most languages do not have such sememe-based linguistic KBs, which prevents us understanding and utilizing human languages to a greater extent. Therefore, it is important to build sememe-based linguistic KBs for various languages. Manual construction for sememe- based linguistic KBs requires efforts of many linguistic experts, which is time-consuming and labor-intensive. For example, the construction of HowNet has cost lots of Chinese linguistic experts more than 10 years.</p><p>To address the issue of the high labor cost of manual annotation, we propose a new task, cross- lingual lexical sememe prediction (CLSP) which aims to automatically predict lexical sememes for words in other languages. CLSP aims to assist in the annotation of linguistic experts. There are two critical challenges for CLSP: (1) There is not a consistent one-to-one match between words in different languages. For example, English word "beautiful" can refer to Chinese words of either "美丽" or "漂亮". Hence, we cannot simply trans- late HowNet into another language. And how to recognize the semantic meaning of a word in other languages becomes a critical problem. (2) Since there is a gap between the semantic meanings of words and sememes, we need to build semantic representations for words and sememes to capture the semantic relatedness between them.</p><p>To tackle these challenges, in this paper, we pro- pose a novel model for CLSP, which aims to trans- fer sememe-based linguistic KBs from source lan- guage to target language. Our model contains three modules including (1) monolingual word embed- ding learning which is intended for learning se- mantic representations of words for source and tar- get languages respectively; (2) cross-lingual word embedding alignment which aims to bridge the gap between the semantic representations of words in two languages; (3) sememe-based word embed- ding learning whose objective is to incorporate se- meme information into word representations. For simplicity, we do not consider the hierarchy infor- mation in HowNet in this paper.</p><p>In experiments, we take Chinese as source lan- guage and English as target language to show the effectiveness of our model. Experimental results show that our proposed model could effectively predict lexical sememes for words with differ- ent frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Since HowNet was published <ref type="bibr" target="#b9">(Dong and Dong, 2003)</ref>, it has attracted wide attention of re- searchers. Most of related works focus on apply- ing HowNet to specific NLP tasks ( <ref type="bibr" target="#b25">Liu and Li, 2002;</ref><ref type="bibr" target="#b45">Zhang et al., 2005;</ref><ref type="bibr" target="#b37">Sun et al., 2007;</ref><ref type="bibr" target="#b7">Dang and Zhang, 2010;</ref><ref type="bibr" target="#b14">Fu et al., 2013;</ref><ref type="bibr" target="#b33">Niu et al., 2017;</ref><ref type="bibr" target="#b43">Zeng et al., 2018;</ref><ref type="bibr" target="#b16">Gu et al., 2018)</ref>. To the best of our knowledge, only  and <ref type="bibr" target="#b19">Jin et al. (2018)</ref> conduct studies of augmenting HowNet by recommending sememes for new words. How- ever, both of the two works are aimed to recom- mend sememes for monolingual words and not ap- plicable to cross-lingual circumstance. Accord- ingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs.</p><p>Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe ( <ref type="bibr" target="#b34">Pennington et al., 2014</ref>) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL meth- ods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic infor- mation of KBs into WRL ( <ref type="bibr" target="#b11">Faruqui et al., 2015;</ref><ref type="bibr" target="#b32">Mrkšic et al., 2016;</ref><ref type="bibr" target="#b4">Bollegala et al., 2016)</ref>. Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs.</p><p>There also have been plenty of studies work- ing on cross-lingual WRL ( <ref type="bibr" target="#b38">Upadhyay et al., 2016;</ref><ref type="bibr" target="#b35">Ruder, 2017)</ref>. Most of them require parallel cor- pora ( <ref type="bibr" target="#b46">Zou et al., 2013;</ref><ref type="bibr" target="#b1">AP et al., 2014;</ref><ref type="bibr" target="#b21">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b21">Kočiskỳ et al., 2014;</ref><ref type="bibr" target="#b15">Gouws et al., 2015;</ref><ref type="bibr" target="#b27">Luong et al., 2015;</ref><ref type="bibr" target="#b6">Coulmance et al., 2015</ref>). Some of them adopt unsupervised or weakly supervised methods ( <ref type="bibr" target="#b30">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b40">Vulić and Moens, 2015;</ref><ref type="bibr" target="#b5">Conneau et al., 2017;</ref><ref type="bibr" target="#b2">Artetxe et al., 2017)</ref>. There are also some works using a seed lexicon as the cross-lingual sig- nal ( <ref type="bibr" target="#b8">Dinu et al., 2014;</ref><ref type="bibr" target="#b12">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b22">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b36">Shi et al., 2015;</ref><ref type="bibr" target="#b26">Lu et al., 2015;</ref><ref type="bibr" target="#b15">Gouws et al., 2015;</ref><ref type="bibr" target="#b41">Wick et al., 2016;</ref><ref type="bibr" target="#b0">Ammar et al., 2016;</ref><ref type="bibr" target="#b10">Duong et al., 2016;</ref><ref type="bibr" target="#b39">Vulić and Korhonen, 2016</ref>).</p><p>In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised methods are not appropriate either as they are gen- erally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lex- icon method in our model, and further introduce matching mechanism that is inspired by <ref type="bibr" target="#b44">Zhang et al. (2017)</ref> to enhance its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce our novel model for CLSP. Here we define the language with sememe annotations as source language and the language without sememe annotations as target language. The main idea of our model is to learn word em- beddings of source and target languages jointly in a unified semantic space, and then predict se- memes for words in target language according to the words with similar semantic meanings in source language.</p><p>Our method consists of three parts: monolingual word representation learning, cross-lingual word embedding alignment and sememe-based word representation learning. Hence, we define the ob- jective function of our method corresponding to the three parts:</p><formula xml:id="formula_0">L = L mono + L cross + L sememe .<label>(1)</label></formula><p>Here, the monolingual term L mono is designed for learning monolingual word embeddings from non- parallel corpora for source and target languages re- spectively. The cross-lingual term L cross aims to align cross-lingual word embeddings in a unified semantic space. And L sememe can draw sememe information into word representation learning and conduce to better word embeddings for sememe prediction. In the following subsections, we intro- duce the three parts in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Word Representation</head><p>Monolingual word representation is responsible for explaining regularities in monolingual corpora of source and target languages. Since the two cor- pora are non-parallel, L mono comprises two mono- lingual sub-models that are independent of each other:</p><formula xml:id="formula_1">L mono = L S mono + L T mono ,<label>(2)</label></formula><p>where the superscripts S and T denote source and target languages respectively.</p><p>As a common practice, we choose the well es- tablished Skip-gram model to obtain monolingual word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. For- mally, taking the source side for example, given a training word sequence {w S 1 , · · · , w S n }, Skip-gram model intends to minimize:</p><formula xml:id="formula_2">L S mono = − n−K ∑ c=K+1 ∑ −K≤k≤K,k̸ =0 log P (w S c+k |w S c ),<label>(3)</label></formula><p>where K is the size of the sliding window. P (w S c+k |w S c ) stands for the predictive probability of one of the context words conditioned on the cen- tered word w S c , formalized by the following soft- max function:</p><formula xml:id="formula_3">P (w S c+k |w S c ) = exp(w S c+k · w S c ) ∑ w S s ∈V S exp(w S s · w S c )</formula><p>, <ref type="formula">(4)</ref> in which V s indicates the word vocabulary of source language. L T mono can be formulated simi- larly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-lingual Word Embedding Alignment</head><p>Cross-lingual word embedding alignment aims to build a unified semantic space for the words in source and target languages. Inspired by <ref type="bibr" target="#b44">Zhang et al. (2017)</ref>, we align the cross-lingual word em- beddings with signals of a seed lexicon and self- matching. Formally, L cross is composed of two terms in- cluding alignment by seed lexicon L seed and align- ment by matching L match :</p><formula xml:id="formula_4">L cross = λ s L seed + λ m L match ,<label>(5)</label></formula><p>where λ s and λ m are hyperparameters for control- ling relative weightings of the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment by Seed Lexicon</head><p>The seed lexicon term L seed encourages word em- beddings of translation pairs in a seed lexicon D to be close, which can be achieved via a L 2 regular- izer:</p><formula xml:id="formula_5">L seed = ∑ ⟨w S s ,w T t ⟩∈D ∥w S s − w T t ∥ 2 ,<label>(6)</label></formula><p>in which w S s and w T t indicate the words in source and target languages in the seed lexicon respec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment by Matching Mechanism</head><p>As for the matching process, it is founded on an as- sumption that each target word should be matched to a single source word or a special empty word, and vice versa. The goal of the matching process is to find the matched source (target) word for each target (source) word and maximize the matching probabilities for all the matched word pairs. The loss of this part can be formulated as:</p><formula xml:id="formula_6">L match = L T 2S match + L S2T match ,<label>(7)</label></formula><p>where L T 2S match is the term for target-to-source matching and L S2T match is the term for source-to- target matching.</p><p>Next, we give a detailed explanation of target-to-source matching, and the source-to- target matching is defined in the same way. We first introduce a latent variable m t ∈ {0, 1, · · · , |V S |} (t = 1, 2, · · · , |V T |) for each target word w T t , where |V S | and |V T | indicate the vocabulary size of source and target languages re- spectively. Here, m t specifies the index of the source word that w T t matches with, and m t = 0 signifies the empty word is matched. Then we have m = {m 1 , m 2 , · · · , m |V T | }, and can formal- ize the target-to-source matching term:</p><formula xml:id="formula_7">L T 2S match = − log P (C T |C S ) = − log ∑ m P (C T , m|C S ),<label>(8)</label></formula><p>where C T and C S denote the target and source cor- pus respectively. Here, we simply assume that the matching processes of target words are indepen- dent of each other. Therefore, we have:</p><formula xml:id="formula_8">P (C T , m|C S ) = ∏ w T ∈C T P (w T , m|C S ) = |V T | ∏ t=1 P (w T t |w S mt ) c(w T t ) ,<label>(9)</label></formula><p>where w S mt is the source word that w T t matches with, and c(w T t ) is the number of times w T t occurs in the target corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sememe-based Word Representation</head><p>Sememe-based word representation is intended for improving word embeddings for sememe predic- tion by introducing the information of sememe- based linguistic KBs of source language. In this section, we present two methods of sememe-based word representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Relation-based Approach</head><p>A simple and intuitive method is to let words with similar sememe annotations tend to have similar word embeddings, which we name word relation- based approach. To begin with, we construct a syn- onym list from sememe-based linguistic KBs of source language, where we regard words sharing a certain number of sememes as synonyms. Next, we force synonyms to have closer word embed- dings.</p><p>Formally, we let w S i be original word embed- ding of w S i andˆwandˆ andˆw S i be its adjusted word embed- ding. And let Syn(w S i ) denote the synonym set of word w S i . Then the loss function is:</p><formula xml:id="formula_9">L sememe = ∑ w S i ∈V S [ α i ∥w S i − ˆ w S i ∥ 2 + ∑ w S j ∈Syn(w S i ) β ij ∥ ˆ w S i − ˆ w S j ∥ 2 ] ,<label>(10)</label></formula><p>where α and β control the relative strengths of the two terms.</p><p>It should be noted that the idea of forcing similar words to have close word embeddings is similar to the state-of-the- art retrofitting approach <ref type="bibr" target="#b11">(Faruqui et al., 2015)</ref>. However, retrofitting approach cannot be applied here because sememe-based linguistic KBs such as HowNet cannot directly provide its needed syn- onym list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sememe Embedding-based Approach</head><p>Simple and effective as the word relation-based approach is, it cannot make full use of the infor- mation of sememe-based linguistic KBs because it disregards the complicated relations between se- memes and words as well as relations between different sememes. To address this limitation, we propose sememe embedding-based approach, which learns both sememe and word embeddings jointly.</p><p>In this approach, we represent sememes with distributed vectors as well and place them into the same semantic space as words. Similar to SPSE , which learns sememe embed- dings by decomposing word-sememe matrix and sememe-sememe matrix, our method utilizes se- meme embeddings as regularizers to learn better word embeddings. Different from SPSE, we do not use pre-trained word embeddings. Instead, we learn word embeddings and sememe embeddings simultaneously.</p><p>More specifically, from HowNet we can ex- tract a source-side word-sememe matrix M S with M S sj = 1 indicating word w S s is annotated with sememe x j , otherwise M S sj = 0. Hence by fac- torizing M S , we can define the loss function as:</p><formula xml:id="formula_10">L sememe = ∑ w S s ∈V S ,x j ∈X (w S s ·x j +b s +b ′ j −M S sj ) 2 ,<label>(11)</label></formula><p>where b s and b ′ j are the biases of w S s and x j , and X denotes sememe set.</p><p>In this approach, we obtain word and sememe embeddings in a unified semantic space. The se- meme embeddings bear all the information about the relationships between words and sememes, and they inject the information into word embeddings. Therefore, the word embeddings are expected to be more suitable for sememe prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>When training monolingual word embeddings, we use negative sampling following <ref type="bibr" target="#b29">Mikolov et al. (2013a)</ref>. In the optimization of sememe part, we adopt the iterative updating method following <ref type="bibr" target="#b11">Faruqui et al. (2015)</ref> for word relation-based ap- proach and stochastic gradient descent (SGD) for sememe embedding-based approach. As for the optimization of the seed lexicon term of cross- lingual part, we also apply SGD.</p><p>Nevertheless, due to the existence of the la- tent variable, optimization of the matching process in cross-lingual part poses a challenge. We set- tle on Viterbi EM algorithm to address the prob- lem. Next, we still take the target-to-source side as an example and give a detailed description of the training process using Viterbi EM algorithm.</p><p>Viterbi EM algorithm alternates between a Viterbi E step and a subsequent M step. The Viterbi E step aims to find the most probable matched word pairs given the current parameters. Considering the independence, we can seek the match for each word individually:</p><formula xml:id="formula_11">ˆ m t = arg max s∈{0,1,··· ,|V S |} P (w T t |w S s ).<label>(12)</label></formula><p>As for the parametrization of the matching prob- ability, there are various choices. For computa- tional simplicity, we select cosine similarity:</p><formula xml:id="formula_12">P (w T t |w S s ) = { ϵ if s = 0, cos(w T t , w S s ) otherwise,<label>(13)</label></formula><p>where ϵ is a hyperparameter indicating the proba- bility of matching the empty word. Therefore, the Viterbi E step computes matching by:</p><formula xml:id="formula_13">˜ m t = arg max s∈{1,··· ,|V S |} cos(w T t , w S s ),<label>(14)</label></formula><formula xml:id="formula_14">ˆ m t = { ˜ m t if cos(w T t , w S ˜ mt ) &gt; ϵ, 0 otherwise.<label>(15)</label></formula><p>From this, we can see that ϵ serves as a threshold to keep out unreliable matched pairs. The Viterbi M step performs maximization as if the latent variable has been observed in the Viterbi E step. Thus, we can treat the matched pairs as cor- rect translations, and use a L 2 regularizer as well. Consequently, the M step computes:</p><formula xml:id="formula_15">( ˆ w S , ˆ w T ) = arg max w S ,w T M(w S , w T ),<label>(16)</label></formula><p>where M(w S , w T ) is defined as:</p><formula xml:id="formula_16">M(w S , w T ) = − |V T | ∑ t=1 I[ ˜ m t ̸ = 0] c(w T t ) |C T | ∥w T t −w S ˜ mt ∥ 2 .<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Since we assume that words with similar sememe annotations are similar and similar words should have similar sememes, which resembles collabora- tive filtering in personalized recommendation, we can recommend sememes for target words accord- ing to their most similar source words. Formally, we define the score function P (x j |w T t ) of sememes x j given a target word w T t as:</p><formula xml:id="formula_17">P (x j |w T t ) = ∑ w S s ∈V S cos(w S s , w T t )·M S sj ·c rs ,<label>(18)</label></formula><p>where r s is the descending rank of word simi- larity cos(w S s , w T t ) for the source word w S s , and c ∈ (0, 1) is a hyperparameter. Thus, c rs is a de- clined confidence factor which can eliminate the noise from irrelevant source words and concentrate on the most similar source words when predicting sememes for target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the dataset used in the experiments and then describe the experi- mental settings of both baseline method and our model. Next, we present the experimental results of different methods on the task of cross-lingual lexical sememe prediction. And then we con- duct detailed analysis and exhaustive case stud- ies. Following this, we investigate the effect of word frequency on cross-lingual sememe predic- tion results. Finally, we perform further quantita- tive analysis through two sub-tasks including bilin- gual lexicon induction and word similarity compu- tation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use sememe annotations in HowNet for se- meme prediction. HowNet annotates sememes for 118, 346 Chinese words and 104, 025 En- glish words. The number of sememes in to- tal is 1, 983. Since some sememes only appear few times in HowNet, which are expected to be unimportant, we filter out those low-frequency se- memes. Specifically, the frequency threshold is 5, and the final number of distinct sememes used in our experiments is 1, 400.</p><p>In our experiments, Chinese is source language and English is target language. To learn Chi- nese and English monolingual word embeddings, we extract about 2.0G text from Sogou-T 1 and Wikipedia 2 respectively. And we use THULAC 3 (Li and Sun, 2009) for Chinese word segmentation.</p><p>As for seed lexicon, we build it in a similar way to <ref type="bibr" target="#b44">Zhang et al. (2017)</ref>. First, we employ Google Translation API 4 to translate the source side (Chi- nese) vocabulary. Then the translations in the tar- get language (English) are queried again in the re- verse direction to translate back to the source lan- guage (Chinese). And we only keep the translation pairs whose back translated words match with the original source words.</p><p>In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.0 5 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 ( <ref type="bibr" target="#b13">Finkelstein et al., 2002</ref>) and SimLex-999 ( <ref type="bibr" target="#b18">Hill et al., 2015</ref>) datasets for English to evaluate the performance of our model. These datasets contain word pairs as well as human-assigned similarity scores. The word vectors are evaluated by ranking the word pairs ac- cording to their cosine similarities, and measuring Spearman's rank correlation coefficient with the human ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>We empirically set the dimension of word and se- meme embeddings to 200. And the embeddings are all randomly initialized. In monolingual word embedding learning, we follow the optimal param- eter settings in <ref type="bibr" target="#b29">Mikolov et al. (2013a)</ref>. We set the window size K to 5, down-sampling rate for high- frequency words to 10 −5 , learning rate to 0.025 and the number of negative samples to 5. In cross- lingual word embedding alignment, the seed lexi- con term weight λ s is 0.01, and the matching term weight λ m is 1, 000. In sememe-based word repre- sentation, the number of shared sememes for syn- onyms in the word relation-based approach is 2. In the training of matching process, we set ϵ to 0.5 empirically. When predicting sememes for words in target language, we only consider 100 most sim- ilar source words for each target word and the at- tenuation parameter c is 0.8. The testing set for cross-lingual lexical sememe prediction contains 2, 000 randomly selected English words from the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-lingual Lexical Sememe Prediction</head><p>We evaluate our model by recommending se- memes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label clas- sification task. We use mean average precision (MAP) and F 1 score to evaluate the sememe pre- diction results.</p><p>We compare our model that incorporates se- meme information with word relation-based ap- proach (named CLSP-WR) and our model which jointly trains word and sememe embeddings (named CLSP-SE) with a baseline method BiLex ( <ref type="bibr" target="#b44">Zhang et al., 2017)</ref>, a bilingual WRL model with- out incorporation of sememe information. For BiLex, we use its trained bilingual word embed- dings to predict sememes for the words in target language with our sememe prediction approach.  <ref type="table" target="#tab_0">Table 1</ref>: Evaluation results of cross-lingual lexi- cal sememe prediction with different seed lexicon sizes.</p><p>seed lexicon sizes in {1000, 2000, 4000, 6000 6 }.</p><p>From the table, we can clearly see that:</p><p>(1) Our two models perform much better com- pared with BiLex in all the seed lexicon size set- tings. It indicates that incorporating sememe infor- mation into word embeddings can effectively im- prove the performance of predicting sememes for target words. The reason is that both of our models make words with similar sememe annotations have similar embeddings, and as a result, we can recom- mend better sememes for target words according to its related source words.</p><p>(2) CLSP-SE model achieves better results than CLSP-WR model. The reason is that by represent- ing sememes in a latent semantic space, CLSP- SE model can further capture the relatedness be- tween sememes as well as the relatedness between words and sememes, which is helpful for model- ing the representations of those words with similar sememes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>In case study, we conduct qualitative analysis to explain the effectiveness of our models with de- tailed cases. We show two examples of cross- lingual word sememe prediction, in which we pre- dict sememes for handcuffs and canoeist. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the embeddings of five closest Chinese and English words to handcuffs and canoeist, and the vector of each word is projected down to two di- mensions using t-SNE <ref type="bibr" target="#b28">(Maaten and Hinton, 2008)</ref>. <ref type="bibr">6</ref> The largest seed lexicon size is 6000 because that is the maximum number of translation word pairs that we can obtain from the bilingual corpora.  <ref type="table">Table 2</ref> lists top-5 sememes we predict for the two words and the sememes annotated for each word in HowNet are in boldface. In the table, we also exhibit the annotated sememes of the five closest Chinese words.</p><p>In the first example, our model finds the best translated word for handcuffs in Chinese ⼿ 铐 "handcuffs", whose sememe annotations are ex- actly the same as those of handcuffs. In addition, the second closest Chinese word 镣 铐 "shack- les" is a synonym for ⼿铐 "handcuffs" and also has the same sememe annotations. Therefore, our model predicts all the correct sememes success- fully. From the prediction results of this exam- ple, we notice that our model can accurately pre- dict general sememes like 用具 "tool" and ⼈ "hu- man", which are supposed to be difficult to predict.</p><p>In the second example, accurate Chinese trans- lated counterpart for canoeist does not exist, but our model still hits all the three annotated sememes in the top-5 predicted sememes. By observing the most similar Chinese words, we can find that al- though these words do not have the same meaning as canoeist, they are related to canoeist in different aspects. For example, 短跑 "sprint" and canoeist are both in the sports domain so that they share the sememes 锻炼 "exercise" and 体育 "sport". 名将 "sports star" has the meaning of sports star and it can provide the sememe ⼈ "human" in sememe prediction. Furthermore, it is noteworthy that our model predicts 船 "ship" due to the nearest Chi- nese words 独 ⽊ ⾈ "canoe" and 皮 艇 "kayak", whereas 船 "ship" is not annotated for canoeist in HowNet. It is obvious that 船 "ship" is an appro- priate sememe for canoeist. Since HowNet is man- ually annotated by experts, misannotated words al- ways exist inevitably, which in some cases under- estimates our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Words Sememes</p><p>English Word handcuffs 用具 "tool", "police", "detain", ⼈ "human", "guilty" 5 Nearest Chinese Words ⼿铐 "handcuffs" "guilty", "police", ⼈ "human", "detain", 用具 "tool" 镣铐 "shackles" "guilty", "police", ⼈ "human", "detain", 用具 "tool" 绑 "tie" 包扎 "wrap" 螺丝⼑ "screwdriver" 用具 "tool", 放松 "loosen", 勒紧 "tighten" 绳 "rope" 线 "linear", 材料 "material", 拴连 "fasten"</p><p>English Word canoeist 锻炼 "exercise", ⼈ "human", 体育 "sport", 事情 "fact", 船 "ship" 5 Nearest Chinese Words 短跑 "sprint" 事情 "fact" 锻炼 "exercise" 体育 "sport" 独⽊⾈ "canoe" 船 "ship" 皮艇 "kayak" 船 "ship" 名将 "sports star" 著名 "famous", ⼈ "human", 官 "official", 军 "military" 皮划艇 "kayak" 事情 "fact", 锻炼 "exercise", 体育 "sport" <ref type="table">Table 2</ref>: Two examples of cross-lingual lexical sememe prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Word Frequency</head><p>To explore how frequencies of target words affect cross-lingual sememe prediction results, we split the testing set into four subsets according to word frequency and then calculate the sememe predic- tion MAP and F 1 score for each subset. The results are shown in  From the table we can see that: (1) The more frequently a target word appears in the corpus, the better its predicted sememes are. It is because high-frequency words normally have better word embeddings, which are crucial to sememe predic- tion. (2) Our models evidently perform better than BiLex in different word frequencies, especially in low frequency. It indicates that by considering external information of HowNet, our models are more robust and can competently handle sparse scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Further Quantitative Analysis</head><p>In this section, we conduct two typical auxiliary experiments to further analyze the superiority of our models quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Lexicon Induction</head><p>Our models learn bilingual word embeddings in one unified semantic space. Here we use transla- tion top-1 and top-5 average precision (P@1 and P@5) to evaluate bilingual lexicon induction per- formance of our models and BiLex. The seed lex- icon size also varies in {1000, 2000, 4000, 6000}.  The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. From this ta- ble, we observe that our models, especially CLSP- SE model, enhance the performance of word trans- lation compared to BiLex no matter how large the seed lexicon is. It indicates that our models can bind bilingual word embeddings better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Similarity Computation</head><p>We also evaluate the task of monolingual word similarity computation on WordSim-240 (WS- 240) and WordSim-297 (WS-297) datasets for Chinese, and <ref type="bibr">WordSim-353 (WS-353)</ref> and <ref type="bibr">SimLex-999 (SL-999)</ref>   <ref type="table">Table 5</ref>: Performance on monolingual word simi- larity computation with seed lexicon size 6000. <ref type="table">Table 5</ref> shows the results of monolingual word similarity computation on four datasets. From the table, we find that: (1) Our models per- form better than BiLex on both Chinese word similarity datasets. It signifies incorporating se- meme information helps learn better monolingual embeddings; (2) CLSP-WR model does not en- hance English word similarity results but CLSP- SE model does. It is because CLSP-WR model only post-processes Chinese word embeddings and keeps English word embeddings unchanged while CLSP-SE model undertakes bilingual alignment and sememe information incorporation together, which makes English word embeddings improve with Chinese word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we introduce a new task of cross- lingual sememe prediction. This task is very im- portant because the construction of sememe-based linguistic knowledge bases in various languages is beneficial to better understanding these lan- guages. We propose a simple and effective model for this task, including monolingual word repre- sentation learning, cross-lingual word representa- tion alignment and sememe-based word represen- tation learning. Experimental results on real-world datasets show that our model achieves consistent and significant improvements compared to base- line method in cross-lingual sememe prediction.</p><p>In the future, we will explore the following re- search directions: (1) In this paper, for simplifi- cation, we ignore the rich hierarchy information in HowNet and also ignore the fact that a word may have multiple senses. We will extend our models to consider the structure information of se- meme and multiple senses of words; (2) In fact, our framework for cross-lingual lexical sememe prediction can be transferred to other cross-lingual tasks. We will explore the effectiveness of our model in these tasks such as cross-lingual infor- mation retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of HowNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two examples of nearest English and Chinese words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 exhibits the evaluation results of cross- lingual lexical sememe prediction with different</head><label>1</label><figDesc></figDesc><table>Method 
Seed 
Lexicon 

Sememe Prediction 

MAP 
F1 Score 

BiLex 

1000 
27.57 
16.08 
2000 
33.79 
22.33 
4000 
35.78 
25.74 
6000 
38.29 
28.71 

CLSP-WR 

1000 
28.12 
18.55 
2000 
33.78 
23.64 
4000 
38.30 
27.74 
6000 
41.23 
30.64 

CLSP-SE 

1000 
31.78 
18.22 
2000 
37.70 
24.31 
4000 
40.77 
29.33 
6000 
43.16 
32.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Method 
Word 
Frequency 

Sememe Prediction 

MAP 
F1 Score 

BiLex 

&lt;200 
30.35 
21.83 
200 -500 
34.83 
25.95 
501 -2500 
40.21 
28.62 
&gt;2500 
47.56 
35.80 

CLSP-WR 

&lt;200 
34.73 
24.41 
200 -500 
39.50 
29.49 
501 -2500 
43.92 
33.87 
&gt;2500 
47.33 
34.99 

CLSP-SE 

&lt;200 
36.54 
27.49 
200 -500 
41.46 
30.09 
501 -2500 
45.35 
35.01 
&gt;2500 
49.34 
37.16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation results of cross-lingual lexical 
sememe prediction with different word frequen-
cies. The number of words in each frequency range 
is 497, 458, 522 and 523 respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Bilingual lexicon induction performance 
with different seed lexicon sizes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>datasets for English.</head><label></label><figDesc></figDesc><table>Method 
Chinese (source) 
English (target) 

WS-240 WS-297 WS-353 SL-999 

BiLex 
60.36 
62.17 
60.46 
27.22 

CLSP-WR 
61.27 
65.25 
60.46 
27.22 

CLSP-SE 
60.84 
65.62 
62.47 
28.79 

</table></figure>

			<note place="foot" n="1"> Sogou-T is a corpus of web pages provided by a Chinese commercial search engine. https://www.sogou.com/ labs/resource/t.php 2 https://dumps.wikimedia.org/ 3 http://thulac.thunlp.org/ 4 https://cloud.google.com/translate/ 5 https://catalog.ldc.upenn.edu/ LDC2002L27</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is funded by the National 973 project <ref type="bibr">(No. 2014CB340501)</ref>. It is also partially sup-ported by the NExT++ project, the National Re-search Foundation, Prime Minister's Office, Sin-gapore under its IRC@Singapore Funding Initia-tive. Hao Zhu is supported by Tsinghua University Initiative Scientific Research Program. We also thank the anonymous reviewers for their valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A set of postulates for the science of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Bloomfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="1926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint word representation learning using a corpus and a semantic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Alsuhaibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transgram, fast cross-lingual word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Method of discriminant for chinese sentence sentiment orientation based on hownet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Application Research of Computers</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6568</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hownet-a hybrid language and knowledge resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLP-KE</title>
		<meeting>NLP-KE</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning crosslingual word embeddings without bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL</title>
		<meeting>the EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-aspect sentiment analysis for chinese online social reviews based on topic modeling and hownet lexicon. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilbowa: fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language modeling with sparse product of sememe experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual distributed representations without word alignment</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incorporating chinese characters of words for lexical sememe prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 4: Evaluating chinese word similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of *SEM</title>
		<meeting>eddings of *SEM</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning bilingual word representations by marginalizing alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Punctuation as implicit annotations for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word similarity computing based on hownet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics &amp; Chinese Language Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="76" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multilingual correlation for improved word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAANL-HLT</title>
		<meeting>NAANL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid</forename><surname>Oséaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved word representation learning with sememes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A survey of cross-lingual embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04902</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning cross-lingual word embeddings via matrix co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hownet based chinese question automatic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanju</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual models of word embeddings: An empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Minimally-constrained multilingual embeddings via artificial code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallika</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">Craig</forename><surname>Pocock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lexical sememe prediction via word embeddings and matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chinese liwc lexicon expansion via hierarchical classification of word embeddings with sememe attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bilingual lexicon induction from non-parallel data with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Bo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chinese word sense disambiguation using hownet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Natural Computation</title>
		<meeting>International Conference on Natural Computation</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
