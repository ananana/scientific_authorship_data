<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Identify Metaphors from a Corpus of Proverbs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözdë</forename><surname>Ozbal</surname></persName>
							<email>gozbalde@gmail.com, {strappa, tekiroglu}@fbk.eu, biondo@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serra</forename><surname>Sinem</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tekiro˘</forename><surname>Glu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Trento, Italy</roleName><forename type="first">†</forename><surname>Fbk-Irst</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Switzerland</forename><surname>Google -Zürich</surname></persName>
						</author>
						<title level="a" type="main">Learning to Identify Metaphors from a Corpus of Proverbs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2060" to="2065"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen a growing attention to- wards attempts to understand figurative language in text ( <ref type="bibr" target="#b18">Steen et al., 2010</ref><ref type="bibr" target="#b16">, Shutova and Teufel, 2010</ref><ref type="bibr" target="#b20">, Turney et al., 2011</ref><ref type="bibr" target="#b14">, Neuman et al., 2013</ref><ref type="bibr" target="#b12">, Klebanov et al., 2015</ref>. Recently, ¨ <ref type="bibr">Ozbal et al. (2016)</ref> published a resource consisting of 1,054 proverbs annotated with metaphors at the word and sentence level, mak- ing it possible for the first time to test existing mod- els for metaphor detection on such data. More than in other genres, such as news, fiction and essays, in proverbs metaphors can resolve a significant amount of the figurative meaning <ref type="bibr" target="#b6">(Faycel, 2012)</ref>. The rich- ness of proverbs in terms of metaphors is very fas- cinating from a linguistic and cultural point of view. Due to this richness, proverbs constitute a challeng- ing benchmark for existing computational models of metaphoricity.</p><p>In this paper, we devise novel feature sets es- pecially tailored to cope with the peculiarities of proverbs, which are generally short and figuratively rich. To the best of our knowledge, this is the first attempt to design a word-level metaphor rec- ognizer specifically tailored to such metaphorically rich data. Even though some of the resources that we use (e.g., imageability and concreteness) have been used for this task before, we propose new ways of encoding this information, especially with respect to the density of the feature space and the way that the context of each word is modeled. On the proverb data, the novel features result in compact models that significantly outperform existing features de- signed for word-level metaphor detection in other genres ( <ref type="bibr" target="#b11">Klebanov et al., 2014</ref>), such as news and es- says. By also testing the new features on these other genres, we show that their generalization power is not limited to proverbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section we provide a brief overview of the efforts of the NLP community to build metaphor datasets and utilize them to develop computational techniques for metaphor processing. <ref type="bibr" target="#b18">Steen et al. (2010)</ref> construct the Amsterdam Metaphor Cor- pus (VUAMC) by annotating a subset of BNC Baby <ref type="bibr">1</ref> . Linguistic metaphors in VUAMC are an- notated by utilizing the Metaphor Annotation Pro- cedure (MIP) proposed by <ref type="bibr" target="#b9">Group (2007)</ref>. VUAMC contains 200,000 words in sentences sampled from various genres (news, fiction, academic, and conver- sations) and 13.6% of the words are annotated as metaphoric <ref type="bibr" target="#b17">(Shutova, 2010)</ref>. Another metaphor an- notation study following the MIP procedure is con- ducted by <ref type="bibr" target="#b16">Shutova and Teufel (2010)</ref>. A subset of the British National Corpus (BNC) <ref type="bibr" target="#b4">(Burnard, 2000</ref>) is annotated to reveal word-level verb metaphors and to determine the conceptual mappings of the metaphorical verbs. <ref type="bibr" target="#b20">Turney et al. (2011)</ref> introduce an algorithm to classify word-level metaphors expressed by an ad- jective or a verb based on their concreteness levels in association with the nouns they collocate. Sim- ilarly, <ref type="bibr" target="#b14">Neuman et al. (2013)</ref> extend the concrete- ness model with a selectional preference approach to detect metaphors formed of concrete concepts. They focus on three types of metaphors: i) IS-A, ii) verb-noun, iii) adjective-noun. Rather than re- stricting the identification task to a particular POS or metaphoric structure, <ref type="bibr" target="#b10">Hovy et al. (2013)</ref> aim to recognize any word-level metaphors given an un- restricted text, and they create a corpus containing sentences where one target token for each sentence is annotated as metaphorical or literal. They use SVM and CRF models with dependency tree-kernels to capture the anomalies in semantic patterns. <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref> propose a supervised approach to predict the metaphoricity of all content words in a running text. Their model combines unigram, topic model, POS and concreteness features and it is eval- uated on VUAMC and a set of essays written for a large-scale assessment of college graduates. Follow- ing this study, <ref type="bibr" target="#b12">Klebanov et al. (2015)</ref> improve their model by re-weighting the training examples and re- designing the concreteness features.</p><p>The experiments in this paper are carried out on PROMETHEUS <ref type="bibr">( ¨ Ozbal et al., 2016</ref>), a dataset con- sisting of 1,054 English proverbs and their equiv- alents in Italian. Proverbs are annotated with word- level metaphors, overall metaphoricity, meaning and century of first appearance. For our experiments, we only use the word-level annotations on the English data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word-level metaphor detection</head><p>Similarly to <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>, we classify each content word (i.e., adjective, noun, verb or adverb) appearing in a proverb as being used metaphorically or not. Out of 1,054 proverbs in PROMETHEUS, we randomly sample 800 for training, 127 for develop- ment and 127 for testing. We carry out the develop- ment of new features on the development set; then we compare the performance of different feature sets using 10-fold cross validation on the combination of the development and training data. Finally, we test the most meaningful configurations on the held-out test data. As a baseline, we use a set of features very similar to the one proposed by <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>. To obtain results more easily comparable with <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>, we use the same clas- sifier, i.e., logistic regression, in the implementation bundled with the scikit-learn package <ref type="bibr" target="#b15">(Pedregosa et al., 2011</ref>). For all the experiments, we adjust the weight of the examples proportionally to the inverse of the class frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline features (B)</head><p>Unigrams (u B ): <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref> use all con- tent word forms as features without stemming or lemmatization. To reduce sparsity, we consider lem- mas along with their POS tag. Part-of-speech (p B ): The coarse-grained part-of- speech (i.e., noun, adjective, verb or adverb) of con- tent words 2 . Concreteness (c B ): We extract the concreteness features from the resource compiled by <ref type="bibr" target="#b3">Brysbaert et al. (2014)</ref>. Similarly to <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>, the mean concreteness ratings, ranging from 1 to 5, are binned in 0.25 increments. We also add a binary fea- ture which encodes the information about whether the lemma is found in the resource. Topic models (t B ): We use Latent Dirichlet Alloca- tion (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) using Gibbs sampling for parameter estimation and inference <ref type="bibr" target="#b8">(Griffiths, 2002</ref>). We run LDA on the full British National Cor- pus <ref type="bibr">(Consortium and others, 2001</ref>) to estimate 100 topics, using 2000 Gibbs sampling iterations, and keeping the first 1000 words for each topic. As topic model features for a lemma, we use the conditional probability of the topic given the lemma for each of the 100 topics generated by LDA. Besides, we use a binary feature that encodes whether the lemma ex- ists in the LDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Novel features (N )</head><p>We introduce five feature sets that capture other as- pects of the data which we consider to be meaningful for the peculiar characteristics of proverbs.</p><p>Imageability (i) and Concreteness (c): Imageabil- ity and concreteness of the metaphor constituents were found to be highly effective in metaphor iden- tification by several studies in the literature <ref type="bibr" target="#b20">(Turney et al., 2011</ref><ref type="bibr" target="#b2">, Broadwell et al., 2013</ref><ref type="bibr" target="#b14">, Neuman et al., 2013</ref><ref type="bibr" target="#b19">, Tsvetkov et al., 2014</ref>). We obtain the image- ability and concreteness scores of each lemma from the resource constructed by <ref type="bibr" target="#b19">Tsvetkov et al. (2014)</ref>, as it accounts for both dimensions. The imageabil- ity (concreteness) feature set contains the following four features:</p><p>• Has score: A binary feature that indicates whether the lemma exists in the relevant re- source.</p><p>• Score value: The imageability (concreteness) score of the lemma.</p><p>• Average sentence score: The average image- ability (concreteness) score of the other lem- mas in the sentence.</p><p>• Score difference: The difference between Av- erage sentence score and Score value. The last two features take the context of the target lemma into account and encode the intuition that metaphorical lemmas often have higher imageability (concreteness) than the rest of the sentence <ref type="bibr" target="#b2">(Broadwell et al., 2013)</ref>. Metaphor counts (m): This feature set consists of three features. The first two features encode the number of times a lemma-POS pair is used as a metaphor and a non-metaphor in the data. The third feature evaluates to the difference between these counts 3 . Standard domains (d s ) and normalized domains (d n ): These features reflect our intuition that there is a strong prior for some domains to be used as a source for metaphors. This notion is backed by the analysis of PROMETHEUS carried out by¨Ozbalby¨ by¨Ozbal et al. (2016). We also expect that words which are clearly out of context with respect to the rest of the sentence are more likely to be used as metaphors. The correlation between word and sentence domains described below aims to model such phenomenon. For each lemma-POS pair, we collect the domain information from WordNet Domains 4 ( <ref type="bibr" target="#b13">Magnini et al., 2002</ref><ref type="bibr" target="#b0">, Bentivogli et al., 2004</ref>) for the standard <ref type="bibr">3</ref> Counts are estimated on training folds. To reduce over- fitting, lemmas are randomly sampled with a probability of 2/3. <ref type="bibr">4</ref> We always select the first sense of the lemma-POS.</p><p>Feature sets C P R F B # 0.9 0.666 0.832 0.738 N * 0.6 0.785 0.884 0.833 B ∪ N * 0.6 0.798 0.875 0.834     imageability (concreteness) of the rest of the sen- tence. The third feature is set to 1 if the lemma was observed more frequently as a metaphor than not, as estimated on training data. <ref type="table" target="#tab_0">Table 1</ref> shows the results of the 10-fold cross valida- tion on the English proverb data. The value reported in the column labeled C is the optimal inverse of regularization strength, determined via grid-search in the interval [0.1, 1.0] with a step of 0.1. Using only baseline features (B) we measure an average F1 score of 0.738. The performance goes up to 0.833 when the novel features are used in isolation (N ) (statistically significant with p &lt; 0.001). We believe that the difference in performance is at least in part due to the sparser B features requiring more data to be able to generalize. But most importantly, un- like B, N accounts for the context and the peculiar- ity of the target word with respect to the rest of the sentence. The combination of the two feature sets (B ∪ N ) very slightly improves over N (0.834), but the difference is not significant. The second block of rows in <ref type="table" target="#tab_0">Table 1</ref> presents a summary of the ablation tests that we conducted to assess the contribution of the different feature groups. Each lowercase letter indicates one of the feature sets introduced in the previous section. All configurations reported, except</p><formula xml:id="formula_0">N \ i * 0.6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><formula xml:id="formula_1">N \ (d s ∪ d n ), significantly outperform B. In two cases, N \ m and N \ (d s ∪ d n )</formula><p>, there is a significant loss of performance with respect to N . The worst performance is observed when all the domain fea- tures are removed (i.e.,</p><formula xml:id="formula_2">N \ (d s ∪ d n ))</formula><p>. These results suggest that the prior knowledge about the domain of a word and the frequency of its metaphorical use are indeed strong predictors of a word metaphoricity in context. The fact that N \ d n and N \ d s do not re- sult in the same loss of performance as</p><formula xml:id="formula_3">N \ (d s ∪ d n )</formula><p>indicates that both d n and d s are adequately expres- sive to model the figuratively rich proverb data. In one case (i.e., N \ s), the F1 measure is slightly higher than N , even though the difference does not appear to be statistically significant. Our intuition is that each of the three binary indicators is a very good predictor of metaphoricity per se, and due to the rel- atively small size of the data the classifier may tend to over-fit on these features. As another configura- tion, the last row shows the results obtained by re- placing our domain features d s and d n with the topic features t from B. With this experiment, we aim to understand the extent to which the two features are interchangeable. The results are significantly worse than N , which is a further confirmation of the suit- ability of the domain features to model the proverbs dataset.</p><p>We then evaluated the best configuration from the cross-fold validation (N \ s) and the three feature sets B, N and B ∪ N on the held-out test data. The results of this experiment reported in <ref type="table" target="#tab_1">Table 2</ref> are similar to the cross-fold evaluation, and in this case the contribution of N features is even more ac- centuated. Indeed, the absolute F 1 of N and B ∪ N is slightly higher on test data, while the f-measure of B decreases slightly. This might be explained by the low-dimensionality of N , which makes it less prone to overfitting the training data. On test data, N \ s is not found to outperform N . Interestingly, N \ s is the only configuration having higher recall than precision. As shown by the feature ablation experi-ments, one of the main reasons for the performance difference between N and B is the ability of the for- mer to model domain information. This finding can be further confirmed by inspecting the cases where B misclassifies metaphors that are correctly detected by N . Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as "grist" (domain: "gas- tronomy") in "All is grist that comes to the mill", or "horse" (domain: "animals") in "You can take a horse to the water , but you can't make him drink".</p><p>Finally, <ref type="table" target="#tab_2">Table 3</ref> shows the effect of the different feature sets on VUAMC used by <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>. We use the same 12-fold data split as <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>, and also in this case we per- form a grid-search to optimize the meta-parameter C of the logistic regression classifier. The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p &lt; 0.01). On the other hand, for the genres "academic" and "fiction", combining N and B fea- tures improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more bal- anced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref>, and to the high dimensionality of B's lex- icalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. It should also be noted that the results reported in ( <ref type="bibr" target="#b11">Klebanov et al., 2014</ref>) are not the same, due to the mentioned differences in the im- plementation of the features and possibly other dif- ferences in the experimental setup (e.g., data filter- ing, pre-processing and meta-parameter optimiza- tion). In particular, our implementation of the B features performs better than reported by <ref type="bibr" target="#b11">Klebanov et al. (2014)</ref> on all four genres, namely: 0.52 vs. 0.51 for "news", 0.51 vs. 0.28 for "academic", 0.39 vs. 0.28 for "conversation" and 0.42 vs. 0.33 for "fiction".</p><p>Even though the evidence is not conclusive, these results suggest that the insights derived from the analysis of PROMETHEUS and captured by the fea- ture set N can also be applied to model word-level metaphor detection across very different genres. In particular, we believe that our initial attempt to en- code context and domain information for metaphor detection deserves further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We designed a novel set of features inspired by the analysis of PROMETHEUS, and used it to train and test models for word-level metaphor detection. The comparison against a strong set of baseline features demonstrates the effectiveness of the novel features at capturing the metaphoricity of words for proverbs. In addition, the novel features show a positive con- tribution for metaphor detection on "fiction" and "academic" genres. The experimental results also highlight the peculiarities of PROMETHEUS, which stands out as an especially dense, metaphorically rich resource for the investigation of the linguistic and computational aspects of figurative language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The same process is repeated for the normalized domains. For normalization, we use a reduced set of domains (43 distinct domains) by considering the middle level of the WordNet Domains hierarchy. For instance, VOLLEY or BASKETBALL domains are mapped to the SPORT domain. Normalization al- ready proved to be beneficial in tasks such as word sense disambiguation (Gliozzo et al., 2004). It al- lows for a good level of abstraction without losing relevant information and it helps to overcome data sparsity. The set of normalized domain features (d n ) consists of 46 features (45 binary, 1 real valued). Dense signals (s): This set includes three binary features which summarize the concreteness, image- ability and metaphor count feature sets. The first (second) feature is set to 1 if the imageability (con- creteness) of the lemma is higher than the average</figDesc><table>Cross-validation performance on the proverb training 

and development data. The meta-parameter C is the inverse of 

the regularization strength.  *  : significantly different from B 

with p &lt; .001; # : s.d. from N with p &lt; .001. 

domains feature set, which consists of 167 features 
(1 real valued, 166 binary). It includes a binary in-
dicator set to 1 if the lemma is found in WordNet 
Domains. A domain vector consisting of 164 binary 
indicators mark the domains to which the lemma be-
longs. Then, we compute a sentence domain vector 
by summing the vectors for all the other lemmas in 
the sentence, and we encode the Pearson correlation 
coefficient between the two vectors (lemma and sen-
tence) as a real valued feature. Finally, a binary fea-
ture accounts for the cases in which no other lemma 
in the sentence has associated domain information. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on the proverb test data.  *  : significantly 

different from B with p &lt; .001. # : significantly different from 

N with p &lt; .001. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cross-validation performance on VUAMC. B is al-

ways significantly different from N (p &lt; .001), and B ∪ N is 

always significantly different from both B and N (p &lt; .001). 

</table></figure>

			<note place="foot" n="1"> http://www.natcorp.ox.ac.uk/corpus/ babyinfo.html</note>

			<note place="foot" n="2"> Klebanov et al. (2014) consider the Penn Treebank tagset generated by Stanford POS tagger.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Revising the Wordnet Domains Hierarchy: Semantics, Coverage and Balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multilingual Linguistic Ressources</title>
		<meeting>the Workshop on Multilingual Linguistic Ressources</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using Imageability and Topic Chaining to Locate Metaphors in Linguistic Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umit</forename><surname>George Aaron Broadwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Boz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomek</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Computing, Behavioral-Cultural Modeling and Prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Concreteness Ratings for 40 Thousand Generally Known English Word Lemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="904" to="911" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Burnard</surname></persName>
		</author>
		<title level="m">Reference guide for the British National Corpus</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>World Edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The British National Corpus, version 2 (BNC World)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bnc Consortium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Distributed by Oxford University Computing Services</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Food Metaphors in Tunisian Arabic Proverbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahklaoui</forename><surname>Faycel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Rice Working Papers in Linguistics 3/1</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised and Supervised Exploitation of Semantic Domains in Lexical Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="299" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gibbs Sampling in the Generative Model of Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MIP: A Method for Identifying Metaphorically Used Words in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pragglejaz</forename><surname>Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse. Metaphor and Symbol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Identifying Metaphorical Word Use with Tree Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Different Texts, Same Metaphors: Unigrams and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Wee</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Metaphor in NLP</title>
		<meeting>the Second Workshop on Metaphor in NLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised Word-Level Metaphor Detection: Experiments with Concreteness and Reweighting of Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Wee</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL HLT</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Role of Domain Information in Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pezzulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">):e62343. Gözdë Ozbal, Carlo Strapparava, and Serra Sinem Tekiro˘ glu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newton</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France, may</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>PROMETHEUS: A Corpus of Proverbs Annotated with Metaphors. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metaphor Corpus Annotated for Source-Target Domain Mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic Metaphor Interpretation as a Paraphrasing Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1029" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerard J Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">A</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Kaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krennmayr</surname></persName>
		</author>
		<title level="m">Metaphor in Usage. Cognitive Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="765" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metaphor Detection with Cross-Lingual Model Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Literal and Metaphorical Sense Identification through Concrete and Abstract Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
