<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Reranking of Discourse Parses Using Tree Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ALT Research Group Qatar Computing Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ALT Research Group Qatar Computing Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Reranking of Discourse Parses Using Tree Kernels</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2049" to="2060"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present a discrimina-tive approach for reranking discourse trees generated by an existing probabilistic discourse parser. The reranker relies on tree kernels (TKs) to capture the global dependencies between discourse units in a tree. In particular, we design new computational structures of discourse trees, which combined with standard TKs, originate novel discourse TKs. The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77% to 82.15%, a relative error reduction of 11.8%, which in turn pushes the state-of-the-art document-level accuracy from 55.8% to 57.3%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clauses and sentences in a well-written text are interrelated and exhibit a coherence structure. Rhetorical Structure Theory (RST) <ref type="bibr" target="#b14">(Mann and Thompson, 1988)</ref> represents the coherence struc- ture of a text by a labeled tree, called discourse tree (DT) as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. The leaves cor- respond to contiguous clause-like units called ele- mentary discourse units (EDUs). Adjacent EDUs and larger discourse units are hierarchically con- nected by coherence relations (e.g., ELABORA- TION, CAUSE). Discourse units connected by a re- lation are further distinguished depending on their relative importance: nuclei are the core parts of the relation while satellites are the supportive ones.</p><p>Conventionally, discourse analysis in RST in- volves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that dis- course analysis is central to many NLP appli- cations, the state-of-the-art document-level dis- course parser ( <ref type="bibr" target="#b13">Joty et al., 2013</ref>) has an f -score of only 55.83% using manual discourse segmen- tation on the RST Discourse Treebank (RST-DT).</p><p>Although recent work has proposed rich lin- guistic features <ref type="bibr" target="#b7">(Feng and Hirst, 2012</ref>) and pow- erful parsing models ( <ref type="bibr" target="#b12">Joty et al., 2012)</ref>, discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the human- annotated DT ( <ref type="figure" target="#fig_1">Figure 1a</ref>) and the DT generated by the discourse parser of <ref type="bibr" target="#b13">Joty et al. (2013)</ref>  <ref type="figure" target="#fig_1">(Figure  1b)</ref> for the same text. The parser makes a mistake in finding the right structure: it considers only e 3 as the text to be attributed to e 2 , where all the text spans from e 3 to e 6 (linked by CAUSE and ELAB- ORATION) compose the statement to be attributed. Such errors occur because existing systems do not encode long range dependencies between DT con- stituents such as those between e 3 and e 4−6 .</p><p>Reranking models can make the global struc- tural information available to the system as fol- lows: first, a base parser produces several DT hypotheses; and then a classifier exploits the en- tire information in each hypothesis, e.g., the com- plete DT with its dependencies, for selecting the best DT. Designing features capturing such global properties is however not trivial as it requires the selection of important DT fragments. This means selecting subtree patterns from an exponential fea- ture space. An alternative approach is to implicitly generate the whole feature space using tree kernels (TKs) ( <ref type="bibr" target="#b3">Collins and Duffy, 2002;</ref><ref type="bibr" target="#b18">Moschitti, 2006</ref>).</p><p>In this paper, we present reranking models for discourse parsing based on Support Vector Ma- chines (SVMs) and TKs. The latter allows us to represent structured data using the substructure space thus capturing structural dependencies be- tween DT constituents, which is essential for ef- fective discourse parsing. Specifically, we made the following contributions. First, we extend the  existing discourse parser 1 ( <ref type="bibr" target="#b13">Joty et al., 2013</ref>) to produce a list of k most probable parses for each input text, with associated probabilities that define the initial ranking. Second, we define a set of discourse tree ker- nels (DISCTK) based on the functional composi- tion of standard TKs with structures representing the properties of DTs. DISCTK can be used for any classification task involving discourse trees.</p><p>Third, we use DISCTK to define kernels for reranking and use them in SVMs. Our rerankers can exploit the complete DT structure using TKs. They can ascertain if portions of a DT are compat- ible, incompatible or simply not likely to coexist, since each substructure is an exploitable feature. In other words, problematic DTs are expected to be ranked lower by our reranker.</p><p>Finally, we investigate the potential of our ap- proach by computing the oracle f -scores for both document-and sentence-level discourse parsing. However, as demonstrated later in Section 6, for document-level parsing, the top k parses often miss the best parse. For example, the oracle f - scores for 5-and 20-best document-level parsing are only 56.91% and 57.65%, respectively. Thus the scope of improvement for the reranker is rather narrow at the document level. On the other hand, the oracle f -score for 5-best sentence-level dis- course parsing is 88.09%, where the base parser (i.e., 1-best) has an oracle f -score of 79.77%. Therefore, in this paper we address the following two questions: (i) how far can a reranker improve the parsing accuracy at the sentence level? and (ii) how far can this improvement, if at all, push the (combined) document-level parsing accuracy?</p><p>To this end, our comparative experiments on 1 Available from http://alt.qcri.org/tools/ RST-DT show that the sentence-level reranker can improve the f -score of the state-of-the-art from 79.77% to 82.15%, corresponding to a relative error reduction of 11.8%, which in turn pushes the state-of-the-art document-level f -score from 55.8% to 57.3%, an error reduction of 3.4%.</p><p>In the rest of the paper, after introducing the TK technology in Section 2, we illustrate our novel structures, and how they lead to the design of novel DISCTKs in Section 3. We present the k- best discourse parser in Section 4. In Section 5, we describe our reranking approach using DISCTKs. We report our experiments in Section 6. We briefly overview the related work in Section 7, and finally, we summarize our contributions in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Kernels for Structural Representation</head><p>Tree kernels <ref type="bibr" target="#b3">(Collins and Duffy, 2002;</ref><ref type="bibr">ShaweTaylor and Cristianini, 2004;</ref><ref type="bibr" target="#b18">Moschitti, 2006</ref>) are a viable alternative for representing arbitrary sub- tree structures in learning algorithms. Their ba- sic idea is that kernel-based learning algorithms, e.g., SVMs or perceptron, only need the scalar product between the feature vectors representing the data instances to learn and classify; and kernel functions compute such scalar products in an effi- cient way. In the following subsections, we briefly describe the kernel machines and three types of tree kernels (TKs), which efficiently compute the scalar product in the subtree space, where the vec- tor components are all possible substructures of the corresponding trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Kernel Machines</head><p>Kernel Machines ( <ref type="bibr" target="#b5">Cortes and Vapnik, 1995)</ref>, e.g., SVMs, perform binary classification by learning a hyperplane <ref type="figure" target="#fig_3">Figure 2</ref>.4: A tree (left) and all of its proper subtrees (right). Proper Subtree A proper subtree ti comprises node vi along with all of its de- scendants (see <ref type="figure" target="#fig_3">figure 2</ref>.4 for an example of a tree along with all its proper subtrees).</p><formula xml:id="formula_0">H( x) = w · x + b = 0, where b c e g ) b c e g c e g c e</formula><p>Subset Tree A subset tree is a subtree for which the following constraint is sat- isfied: either all of the children of a node belong to the subset tree or none of them.</p><p>The reason for adding such a constraint can be understood by considering the fact that subset trees were defined for measuring the similarity of parse trees in natural language applications. In that context a node along with all of its children represent a grammar production. <ref type="figure" target="#fig_3">Figure 2</ref>.5 gives an example of a tree along with some of its subset trees. x ∈ R n is the feature vector representation of an object o ∈ O to be classified and w ∈ R n and b ∈ R are parameters learned from the training data. One can train such machines in the dual space by rewriting the model parameter w as a lin- ear combination of training examples, i.e., w = i=1..l y i α i x i , where y i is equal to 1 for positive examples and −1 for negative examples, α i ∈ R + and x i ∀i ∈ {1, .., l} are the training instances. Then, we can use the data object o i ∈ O directly in the hyperplane equation considering their map- ping function φ : O → R n , as follows:</p><formula xml:id="formula_1">H(o) = i=1..l y i α i x i · x + b = i=1..l y i α i φ(o i ) · φ(o) + b = i=1..l y i α i K(o i , o) + b, where the product K(o i , o) = φ(o i ) · φ(o)</formula><p>is the kernel function (e.g., TK) associated with the mapping φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tree Kernels</head><p>Convolution TKs compute the number of com- mon tree fragments between two trees T 1 and T 2 without explicitly considering the whole fragment space. A TK function over T 1 and T 2 is defined as:</p><formula xml:id="formula_2">T K(T 1 , T 2 ) = n 1 ∈N T 1 n 2 ∈N T 2 ∆(n 1 , n 2 ),</formula><p>where N T 1 and N T 2 are the sets of the nodes of T 1 and T 2 , respectively, and ∆(n 1 , n 2 ) is equal to the number of common fragments rooted in the n 1 and n 2 nodes. <ref type="bibr">2</ref> The computation of ∆ function depends on the shape of fragments, conversely, a different ∆ determines the richness of the kernel space and thus different tree kernels. In the following, we briefly describe two existing and well-known tree kernels. Please see several tutorials on kernels <ref type="bibr" target="#b21">(Moschitti, 2013;</ref><ref type="bibr" target="#b20">Moschitti, 2012;</ref><ref type="bibr" target="#b19">Moschitti, 2010</ref>) for more details. <ref type="bibr">3</ref> Syntactic Tree Kernels (STK) produce fragments such that each of their nodes includes all or none of its children. <ref type="figure" target="#fig_3">Figure 2</ref> shows a tree T and its three fragments (do not consider the single nodes) in the STK space on the left and right of the ar- 2 To get a similarity score between 0 and 1, it is common to apply a normalization in the kernel space, i.e.</p><formula xml:id="formula_3">T K(T 1 ,T 2 ) √ T K(T 1 ,T 1 )×T K(T 2 ,T 2 )</formula><p>. <ref type="bibr">3</ref> Tutorials notes available at http://disi.unitn. it/moschitti/ b c e g 3 1 <ref type="figure" target="#fig_3">Figure 2</ref>.2: A positional Tree. The number over an arc represents the position of the node with respect to its parent. node. The maximum out-degree of a tree is the highest index of all the nodes of the tree. The out-degree of a node for an ordered tree corresponds to the number of its children. The depth of a node vi with respect to one of its ascendants vj is defined as the number of nodes comprising the path from vj to vi. When not specified, the node with respect to the depth is computed, is the root.</p><p>A tree can be decomposed in many types of substructures.</p><p>Subtree A subtree t is a subset of nodes in the tree T , with corresponding edges, which forms a tree. A subtree rooted at node vi will be indicated with ti, while a subtree rooted at a generic node v will be indicated by t(v). When t is used in a context where a node is expected, t refers to the root node of the subtree t. The set of subtrees of a tree will be indicated by NT . When clear from the context NT may refer to specific type of subtrees.  row, respectively. STK(T ,T ) counts the number of common fragments, which in this case is the number of subtrees of T , i.e., three. In the figure, we also show three single nodes, c, e, and g, i.e., the leaves of T , which are computed by a vari- ant of the kernel, that we call</p><formula xml:id="formula_4">STK b . The com- putational complexity of STK is O(|N T 1 ||N T 2 |),</formula><p>but the average running time tends to be linear</p><formula xml:id="formula_5">(i.e. O(|N T 1 | + |N T 2 |)) for syntactic trees (Mos- chitti, 2006).</formula><p>Partial Tree Kernel (PTK) generates a richer set of tree fragments. Given a target tree T , PTK can generate any subset of connected nodes of T , whose edges are in T . For example, <ref type="figure" target="#fig_6">Figure 3</ref> shows a tree with its nine fragments including all single nodes (i.e., the leaves of T ). PTK is more general than STK as its fragments can include any subsequence of children of a target node. The time</p><formula xml:id="formula_6">complexity of PTK is O(pρ 2 |N T 1 ||N T 2 |),</formula><p>where p is the largest subsequence of children that one wants to consider and ρ is the maximal out-degree observed in the two trees. However, the average running time again tends to be linear for syntactic trees <ref type="bibr" target="#b18">(Moschitti, 2006</ref>). ing one of the TKs described in Section 2.2, i.e.,</p><formula xml:id="formula_7">T K(T 1 , T 2 ) = φ T K (T 1 ) · φ T K (T 2 ). If we apply T K to the objects o i transformed by φ M (), we obtain T K(φ M (o 1 ), φ M (o 2 )) = φ T K (φ M (o 1 )) · φ T K (φ M (o 2 ))= φ T K •φ M (o 1 )· φ T K •φ M (o 2 ) = DiscT K(o 1 , o 2 )</formula><p>, which is a new kernel 4 in- duced by the mapping</p><formula xml:id="formula_8">φ DiscT K = φ T K • φ M .</formula><p>We define two different mappings φ M to trans- form the discourse parses generated by the base parser into two different tree structures: (i) the Joint Relation-Nucleus tree (JRN), and (ii) the Split Relation Nucleus tree (SRN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint Relation-Nucleus Tree (JRN)</head><p>As shown in <ref type="figure" target="#fig_7">Figure 4a</ref>, JRN is a direct mapping of the parser output, where the nuclearity statuses (i.e., satellite or nucleus) of the connecting nodes are attached to the relation labels. <ref type="bibr">5</ref> For example, the root BACKGROUND Satellite−N ucleus in <ref type="figure" target="#fig_7">Figure  4a</ref> denotes a Background relation between a satel- lite discourse unit on the left and a nucleus unit on the right. Text spans (i.e., EDUs) are represented as sequences of Part-of-Speech (POS) tags con- nected to the associated words, and are grouped under dummy SPAN nodes. We experiment with two lexical variations of the trees: (i) All includes all the words in the EDU, and (ii) Bigram includes only the first and last two words in the EDU.</p><p>When JRN is used with the STK kernel, an ex- ponential number of fragments are generated. For example, the upper row of <ref type="figure" target="#fig_8">Figure 5</ref> shows two <ref type="bibr">4</ref> People interested in algorithms may like it more design- ing a complex algorithm to compute φT K • φM . However, the design of φM is conceptually equivalent and more effec- tive from an engineering viewpoint.</p><p>5 This is a common standard followed by the parsers.</p><p>smallest (atomic) fragments and one subtree com- posed of two atomic fragments. Note that much larger structures encoding long range dependen- cies are also part of the feature space. These frag- ments can reveal if the discourse units are orga- nized in a compatible way, and help the reranker to detect the kind of errors shown earlier in <ref type="figure" target="#fig_1">Fig- ure 1b</ref>. However, one problem with JRN repre- sentation is that since the relation nodes are com- posed of three different labels, the generated sub- trees tend to be sparse. In the following, we de- scribe SRN that attempts to solve this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Split Relation Nucleus Tree (SRN)</head><p>SRN is not very different from JRN as shown in <ref type="figure" target="#fig_7">Figure 4b</ref>. The only difference is that instead of attaching the nuclearity statuses to the relation la- bels, in this representation we assign them to their respective discourse units. When STK kernel is applied to SRN it again produces an exponential number of fragments. For example, the lower row of <ref type="figure" target="#fig_8">Figure 5</ref> shows two atomic fragments and one subtree composed of two atomic fragments. Com- paring the two examples in <ref type="figure" target="#fig_8">Figure 5</ref>, it is easy to understand that the space of subtrees extracted from SRN is less sparse than that of JRN. Note that, as described in Secion 2.2, when the PTK kernel is applied to JRN and SRN trees, it can generate a richer feature space, e.g., features that are paths containing relation labels (e.g., BACK- GROUND -CAUSE -ELABORATION or ATTRIBU- TION -CAUSE -ELABORATION).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generation of k-best Discourse Parses</head><p>In this section we describe the 1-best discourse parser of <ref type="bibr" target="#b13">Joty et al. (2013)</ref>, and how we extend it to k-best discourse parsing. <ref type="bibr" target="#b13">Joty et al. (2013)</ref> decompose the problem of document-level discourse parsing into two stages as shown in <ref type="figure" target="#fig_10">Figure 6</ref>. In the first stage, the intra- sentential discourse parser produces discourse subtrees for the individual sentences in a docu- ment. Then the multi-sentential parser combines the sentence-level subtrees and produces a DT for the document. Both parsers have the same two components: a parsing model and a parsing al- gorithm. The parsing model explores the search space of possible DTs and assigns a probability to every possible DT. Then the parsing algorithm se- lects the most probable DT(s). While two separate parsing models are employed for intra-and multi- sentential parsing, the same parsing algorithm is used in both parsing conditions. The two-stage parsing exploits the fact that sentence boundaries correlate very well with discourse boundaries. For example, more than 95% of the sentences in RST- DT have a well-formed discourse subtree in the full document-level discourse tree.</p><p>The choice of using two separate models for intra-and multi-sentential parsing is well justified for the following two reasons: (i) it has been ob- served that discourse relations have different dis- tributions in the two parsing scenarios, and (ii) the models could independently pick their own infor- mative feature sets. The parsing model used for intra-sentential parsing is a Dynamic Conditional Random Field (DCRF) ( <ref type="bibr" target="#b29">Sutton et al., 2007)</ref> shown in <ref type="figure" target="#fig_11">Figure 7</ref>. The observed nodes U j at the bottom layer represent the discourse units at a certain level of the DT; the binary nodes S j at the middle layer predict whether two adjacent units U j−1 and U j should be connected or not; and the multi-class nodes R j at the top layer predict the discourse relation between U j−1 and U j . Notice that the model represents the structure and the label of a DT constituent jointly, and captures the sequential dependencies between the DT constituents. Since the chain-structured DCRF model does not scale up to multi-sentential parsing of long documents,    the multi-sentential parsing model is a CRF which breaks the chain structure of the DCRF model. The parsing models are applied recursively at different levels of the DT in their respective pars- ing scenarios (i.e., intra-and multi-sentential), and the probabilities of all possible DT con- stituents are obtained by computing the posterior marginals over the relation-structure pairs (i.e., P (R j , S j =1|U 1 , · · · , U t , Θ), where Θ are model parameters). These probabilities are then used in a CKY-like probabilistic parsing algorithm to find the globally optimal DT for the given text.</p><p>Let</p><note type="other">U b x and U e x denote the beginning and end EDU Ids of a discourse unit U x , and R[U b i , U e m , U e j ] refers to a coherence relation R that holds between the discourse unit con- taining EDUs U b i through U e m and the unit containing EDUs U e m +1</note><p>through U e j . Given n discourse units, the parsing algorithm uses the upper-triangular portion of the n×n dynamic programming table A, where cell A[i, j] (for i &lt; j) stores: In addition to A, which stores the probability of the most probable constituents of a DT, the pars- ing algorithm also simultaneously maintains two other tables B and C for storing the best structure (i.e., U e m * ) and the relations (i.e., r * ) of the corre- sponding DT constituents, respectively. For exam- ple, given 4 EDUs e 1 · · · e 4 , the B and C tables at the left side in <ref type="figure" target="#fig_12">Figure 8</ref> together represent the DT shown at the right. More specifically, to generate the DT, we first look at the top-right entries in the two tables, and find B <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> = 2 and C[1, 4] = r 2 , which specify that the two discourse units e 1:2 and e 3:4 should be connected by the relation r 2 (the root in the DT). Then, we see how EDUs e 1 and e 2 should be connected by looking at the entries B <ref type="bibr">[1,</ref><ref type="bibr">2]</ref> and C <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>, and find B[1, 2] = 1 and C[1, 2] = r 1 , which indicates that these two units should be connected by the relation r 1 (the left pre-terminal). Finally, to see how EDUs e 3 and e 4 should be linked, we look at the entries B <ref type="bibr">[3,</ref><ref type="bibr">4]</ref> and C <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>, which tell us that they should be linked by the relation r 4 (the right pre-terminal).</p><formula xml:id="formula_9">A[i, j] = P (r * [U b i , U e m * , U e j ]), where (m * , r * ) = argmax i≤m&lt;j ; R P (R[U b i , U e m , U e j ])× A[i, m] × A[m + 1, j]<label>(1)</label></formula><note type="other">1 1 2 2 2 3 B r1 r3 r2 r2 r3 r4 C r2</note><p>It is straight-forward to generalize the above al- gorithm to produce k most probable DTs. When filling up the dynamic programming tables, rather than storing a single best parse for each subtree, we store and keep track of k-best candidates si- multaneously. More specifically, each cell in the dynamic programming tables (i.e., A, B and C) should now contain k entries (sorted by their prob- abilities), and for each such entry there should be a back-pointer that keeps track of the decoding path.</p><p>The algorithm works in polynomial time. For n discourse units and M number of relations, the 1-best parsing algorithm has a time complexity of O(n 3 M ) and a space complexity of O(n 2 ), where the k-best version has a time and space complexi- ties of O(n 3 M k 2 log k) and O(n 2 k), respectively. There are cleverer ways to reduce the complexity (e.g., see <ref type="bibr" target="#b9">(Huang and Chiang, 2005</ref>) for three such ways). However, since the efficiency of the algo- rithm did not limit us to produce k-best parses for larger k, it was not a priority in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Kernels for Reranking Discourse Trees</head><p>In Section 3, we described DISCTK, which essen- tially can be used for any classification task involv- ing discourse trees. For example, given a DT, we can use DISCTK to classify it as correct vs. in- correct. However, such classification is not com- pletely aligned to our purpose, since our goal is to select the best (i.e., the most correct) DT from k candidate DTs; i.e., a ranking task. We adopt a preference reranking technique as described in <ref type="bibr" target="#b6">Dinarelli et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Preference Reranker</head><p>Preference reranking (PR) uses a classifier C of pairs of hypotheses h i , h j , which decides if h i (i.e., a candidate DT in our case) is better than h j . We generate positive and negative examples to train the classifier using the following approach. The pairs h 1 , h i constitute positive examples, where h 1 has the highest f -score accuracy on the Relation metric (to be described in Section 6) with respect to the gold standard among the candidate hypotheses, and vice versa, h i , h 1 are considered as negative examples. At test time, C classifies all pairs h i , h j generated from the k-best hypothe- ses. A positive decision is a vote for h i , and a neg- ative decision is a vote for h j . Also, the classifier score can be used as a weighted vote. Hypotheses are then ranked according to the number (sum) of the (weighted) votes they get. <ref type="bibr">6</ref> We build our reranker using simple SVMs. 7 <ref type="bibr">6</ref> As shown by <ref type="bibr" target="#b3">Collins and Duffy (2002)</ref>, only the classifi- cation of k hypotheses (paired with the empty one) is needed in practice, thus the complexity is only O(k). <ref type="bibr">7</ref> Structural kernels, e.g., TKs, cannot be used in more ad- vanced algorithms working in structured output spaces, e.g., SVM struct . Indeed, to our knowledge, no one could suc- cessfully find a general and exact solution for the argmax equation, typically part of such advanced models, when struc- tural kernels are used. Some approximate solutions for sim- ple kernels, e.g., polynomial or gaussian kernels, are given in (Joachims and Yu, 2009), whereas (Severyn and Moschitti, 2011; Severyn and Moschitti, 2012) provide solutions for using the cutting-plane algorithm (which requires argmax computation) with structural kernels but in binary SVMs.</p><p>Since in our problem a pair of hypotheses h i , h j constitutes a data instance, we now need to define the kernel between the pairs. However, notice that DISCTK only works on a single pair.</p><p>Considering that our task is to decide whether h i is better than h j , it can be convenient to represent the pairs in terms of differences be- tween the vectors of the two hypotheses, i.e., φ K (h i ) − φ K (h j ), where K (i.e., DISCTK) is de- fined between two hypotheses (not on two pairs of hypotheses). More specifically, to compute this difference implicitly, we can use the follow- ing kernel summation:</p><formula xml:id="formula_10">P K(h 1 , h 2 , h 1 , h 2 ) = (φ K (h 1 ) − φ K (h 2 )) • (φ K (h 1 ) − φ K (h 2 )) = K(h 1 , h 1 ) + K(h 2 , h 2 ) − K(h 1 , h 2 ) − K(h 2 , h 1 ).</formula><p>In general, Preference Kernel (PK) works well because it removes many identical features by tak- ing differences between two huge implicit TK- vectors. In our reranking framework, we also in- clude traditional feature vectors in addition to the trees. Therefore, each hypothesis h is represented as a tuple T, v composed of a tree T and a fea- ture vector v. We then define a structural kernel (i.e., similarity) between two hypotheses h and h as follows: <ref type="bibr">v, v )</ref>, where DISCTK maps the DTs T and T to JRN or SRN and then applies STK, STK b or PTK defined in Sections 2.2 and 3, and F V is a standard kernel, e.g., linear, polynomial, gaussian, etc., over feature vectors (see next section).</p><formula xml:id="formula_11">K(h, h ) = DiscT K(T, T ) + F V (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Vectors</head><p>We also investigate the impact of traditional (i.e., not subtree) features for reranking discourse parses. Our feature vector comprises two types of features that capture global properties of the DTs.</p><p>Basic Features. This set includes eight global features. The first two are the probability and the (inverse) rank of the DT given by the base parser. These two features are expected to help the reranker to perform at least as good as the base parser. The other six features encode the structural properties of the DT, which include depth of the DT, number of nodes connecting two EDUs (i.e., SPANs in <ref type="figure" target="#fig_7">Figure 4</ref>), number of nodes connecting two relational nodes, number of nodes connecting a relational node and an EDU, number of nodes that connects a relational node as left child and an EDU as right child, and vice versa.</p><p>Relation Features. We encode the relations in the DT as bag-of-relations (i.e., frequency count). This will allow us to assess the impact of a flat rep- resentation of the DT. Note that more important relational features would be the subtree patterns extracted from the DT. However, they are already generated by TKs in a simpler way. See <ref type="bibr" target="#b22">(Pighin and Moschitti, 2009;</ref><ref type="bibr" target="#b23">Pighin and Moschitti, 2010)</ref> for a way to extract the most relevant features from a model learned in the kernel space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Our experiments aim to show that reranking of discourse parses is a promising research direction, which can improve the state-of-the-art. To achieve this, we (i) compute the oracle accuracy of the k- best parser, (ii) test different kernels for reranking discourse parses by applying standard kernels to our new structures, (iii) show the reranking perfor- mance using the best kernel for different number of hypotheses, and (iv) show the relative impor- tance of features coming from different sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Data. We use the standard RST-DT corpus <ref type="bibr" target="#b1">(Carlson et al., 2002</ref>), which comes with discourse an- notations for 385 articles (347 for training and 38 for testing) from the Wall Street Journal. We ex- tracted sentence-level DTs from a document-level DT by finding the subtrees that exactly span over the sentences. This gives 7321 and 951 sentences in the training and test sets, respectively. Follow- ing previous work, we use the same 18 coarser re- lations defined by <ref type="bibr" target="#b0">Carlson and Marcu (2001)</ref>.</p><p>We create the training data for the reranker in a 5-fold cross-validation fashion. 8 Specifically, we split the training set into 5 equal-sized folds, and train the parsing model on 4 folds and apply to the rest to produce k most probable DTs for each text. Then we generate and label the pairs (by compar- ing with the gold) from the k most probable trees as described in Section 5.1. Finally, we merge the 5 labeled folds to create the full training data. SVM Reranker. We use SVM-light-TK to train our reranking models, 9 which enables the use of tree kernels <ref type="bibr" target="#b18">(Moschitti, 2006</ref>) in SVM-light <ref type="bibr" target="#b11">(Joachims, 1999</ref>). We build our new kernels for reranking exploiting the standard built-in TK func- tions, such as STK, STK b and PTK. We applied a linear kernel to standard feature vectors as it showed to be the best on our development set.</p><p>Metrics. The standard procedure to evaluate dis- course parsing performance is to compute Pre- cision, Recall and f -score of the unlabeled and labeled metrics proposed by <ref type="bibr" target="#b16">Marcu (2000b)</ref>. <ref type="bibr">10</ref> Specifically, the unlabeled metric Span measures how accurate the parser is in finding the right structure (i.e., skeleton) of the DT, while the la- beled metrics Nuclearity and Relation measure the parser's ability to find the right labels (nuclearity and relation) in addition to the right structure. Op- timization of the Relation metric is considered to be the hardest and the most desirable goal in dis- course parsing since it gives aggregated evaluation on tree structure and relation labels. Therefore, we measure the oracle accuracy of the k-best dis- course parser based on the f -scores of the Relation metric, and our reranking framework aims to op- timize the Relation metric. 11 Specifically, the ora- cle accuracy for k-best parsing is measured as fol-</p><formula xml:id="formula_12">lows: ORACLE = N i=1 max k j=1 f −scorer(g i ,h j i ) N</formula><p>, where N is the total number of texts (sentences or docu- ments) evaluated, g i is the gold DT annotation for text i, h j i is the j th parse hypothesis generated by the k-best parser for text i, and f -score r (g i , h j i ) is the f -score accuracy of hypothesis h j i on the Re- lation metric. In all our experiments we report the f -scores of the Relation metric. <ref type="table">Table 1</ref> presents the oracle scores of the k- best intra-sentential parser PAR-S on the standard RST-DT test set. The 1-best result corresponds to the accuracy of the base parser (i.e., 79.77%). The 2-best shows dramatic oracle-rate improve- ment (i.e., 4.65% absolute), suggesting that the base parser often generates the best tree in its top 2 outputs. 5-best increases the oracle score to 88.09%. Afterwards, the increase in accuracy slows down, achieving, e.g., 90.37% and 92.57% at 10-best and 20-best, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Oracle Accuracy</head><p>The results are quite different at the document level as <ref type="table">Table 2</ref> shows the oracle scores of the k- best document-level parser PAR-D. <ref type="bibr">12</ref> The results k 1 2 5 10 15 20 PAR-S 79.77 84.42 88.09 90.37 91.74 92.57 <ref type="table">Table 1</ref>: Oracle scores as a function of k of k-best sentence- level parses on RST-DT test set.  <ref type="table">Table 2</ref>: Oracle scores as a function of k of k-best document-level parses on RST-DT test set.</p><p>suggest that the best tree is often missing in the top k parses, and the improvement in oracle-rate is very little as compared to the sentence-level pars- ing. The 2-best and the 5-best improve over the base accuracy by only 0.7% and 1.0%, respec- tively. The improvement becomes even lower for larger k. For example, the gain from 20-best to 30-best parsing is only 0.09%. This is not sur- prising because generally document-level DTs are big with many constituents, and only a very few of them change from k-best to k+1-best parsing. These small changes do not contribute much to the overall f -score accuracy. 13 In summary, the results in <ref type="table">Tables 1 and 2</ref> demonstrate that a k-best reranker can potentially improve the parsing accu- racy at the sentence level, but may not be a suit- able option for improving parsing at the document level. In the following, we report our results for reranking sentence-level discourse parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance of Different DISCTKs</head><p>Section 3 has pointed out that different DISCTKs can be obtained by specifying the TK type (e.g., STK, STK b , PTK) and the mapping φ M (i.e., JRN, SRN) in the overall kernel function <ref type="table" target="#tab_3">Table 3</ref> reports the per- formance of such model compositions using the 5- best hypotheses on the RST-DT test set. Addition- ally, it also reports the accuracy for the two ver- sions of JRN and SRN, i.e., Bigram and All. From these results, we can note the following. Firstly, the kernels generally perform better on Bigram than All lexicalization. This suggests that using all the words from the text spans (i.e., EDUs) produces sparse models.</p><formula xml:id="formula_13">φ T K • φ M (o 1 )· φ T K •φ M (o 2 ).</formula><p>pose two approaches to combine intra-and multi-sentential parsers, namely 1S-1S (1 Sentence-1 Subtree) and Sliding window. In this work we extend 1S-1S to k-best document- level parser PAR-D since it is not only time efficient but it also achieves better results on the Relation metric. <ref type="bibr">13</ref> Note that <ref type="bibr" target="#b12">Joty et al. (2012;</ref><ref type="bibr" target="#b21">2013)</ref> report lower f -scores both at the sentence level (i.e., 77.1% as opposed to our 79.77%) and at the document level (i.e., 55.73% as opposed to our 55.83%). We fixed a crucial bug in their (1-best) pars- ing algorithm, which accounts for the improved performance.   <ref type="bibr">[CAUSE [ELABORATION]</ref>], which may add too much uncertainty on the signature of the relations contained in the DT. We verified this hypothesis by running an experiment with PTK constraining it to only generate fragments whose nodes preserve all or none of their children. The accuracy of such fragments approached the ones of STK, suggesting that relation information should be used as a whole for engineering features.</p><p>Finally, STK b is slightly (but not significantly) better than STK suggesting that the lexical infor- mation is already captured by the base parser.</p><p>Note that the results in <ref type="table" target="#tab_3">Table 3</ref> confirms many other experiments we carried out on several devel- opment sets. For any run: (i) STK always performs as well as STK b , (ii) STK is always better than PTK, and (iii) SRN is always better than JRN. In what follows, we show the reranking performance based on STK applied to SRN with Bigram. <ref type="table" target="#tab_8">Table 4</ref> reports the performance of our reranker (RR) in comparison with the oracle (OR) accuracy for different values of k, where we also show the corresponding relative error rate reduction (ERR) with respect to the baseline. To assess the general- ity of our approach, we evaluated our reranker on both the standard test set and the entire training set using 5-fold cross validation. 15 <ref type="bibr">14</ref> Statistical significance is verified using paired t-test. <ref type="bibr">15</ref> The reranker was trained on 4 folds and tested on the rest   We note that: (i) the best result on the standard test set is 82.15% for k = 4 and 5, which gives an ERR of 11.76%, and significantly (p-value &lt; 0.01) outperforms the baseline, (ii) the improve- ment is consistent when we move from standard test set to 5-folds, (iii) the best result on the 5-folds is 80.86 for k = 6, which is significantly (p-value &lt; 0.01) better than the baseline 78.57, and gives an ERR of 11.32%. We also experimented with other values of k in both training and test sets (also increasing k only in the test set), but we could not improve over our best result. This suggests that outperforming the baseline (which in our case is the state of the art) is rather difficult. <ref type="bibr">16</ref> In this respect, we also investigated the im- pact of traditional ranking methods based on fea- ture vectors, and compared it with our TK-based model. <ref type="table" target="#tab_5">Table 5</ref> shows the 5-best reranking accu- racy for different feature subsets. The Basic fea- tures (Section 5.2) alone do not significantly im- prove over the Baseline. The only relevant fea- tures are the probability and the rank of each hy- pothesis, which condense all the information of the local model (TKs models always used them).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Insights on DISCTK-based Reranking</head><p>Similarly, adding the relations as bag-of- relations in the vector (Rel. feat.) does not pro- vide any gain, whereas the relations encoded in the tree fragments (Tree) gives improvement. This shows the importance of using structural depen- dencies for reranking discourse parses.</p><p>Finally, <ref type="table" target="#tab_6">Table 6</ref> shows that if we use our sentence-level reranker in the document-level parser of <ref type="bibr" target="#b13">Joty et al. (2013)</ref>, the accuracy of the lat- ter increases from 55.8% to 57.3%, which is a sig- nificant improvement (p &lt; 0.01), and establishes a new state-of-the-art for document-level parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Error Analysis</head><p>We looked at some examples where our reranker failed to identify the best DT. Unsurprisingly, it Standard test set 5-folds <ref type="table" target="#tab_3">(average)  k=1  k=2  k=3  k=4  k=5  k=6  k=1  k=2  k=3  k=4  k=5  k=6  RR</ref>   happens many times for small DTs containing only two or three EDUs, especially when the re- lations are semantically similar. <ref type="figure" target="#fig_13">Figure 9</ref> presents such a case, where the reranker fails to rank the DT with Summary ahead of the DT with Elabo- ration. Although we understand that the reranker lacks enough structural context to distinguish the two relations in this example, we expected that in- cluding the lexical items (e.g., (CFD)) in our DT representation could help. However, similar short parenthesized texts are also used to elaborate as in Senate Majority Leader George Mitchell (D., Maine), where the text (D., Maine) (i.e., Democrat from state Maine) elaborates its preceding text. This confuses our reranker. We also found er- ror examples where the reranker failed to distin- guish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Early work on discourse parsing applied hand- coded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shift- reduce discourse parser. This work was then con- siderably improved by <ref type="bibr" target="#b27">Soricut and Marcu (2003)</ref>. They presented probabilistic generative models for sentence-level discourse parsing based on lexico- syntactic patterns. <ref type="bibr" target="#b28">Sporleder and Lapata (2005)</ref> investigated the necessity of syntax in discourse analysis. More recently, <ref type="bibr" target="#b8">Hernault et al. (2010)</ref> presented the HILDA discourse parser that itera- tively employs two SVM classifiers in pipeline to build a DT in a greedy way. <ref type="bibr" target="#b7">Feng and Hirst (2012)</ref> improved the HILDA parser by incorporating rich linguistic features, which include lexical seman- tics and discourse production rules. <ref type="bibr" target="#b13">Joty et al. (2013)</ref> achieved the best prior results by (i) jointly modeling the structure and the la- bel of a DT constituent, (ii) performing optimal rather than greedy decoding, and (iii) discriminat- ing between intra-and multi-sentential discourse parsing. However, their model does not con- sider long range dependencies between DT con- stituents, which are encoded by our kernels. Re- garding the latter, our work is surely inspired by <ref type="bibr" target="#b3">(Collins and Duffy, 2002</ref>), which uses TK for syn- tactic parsing reranking or in general discrimina- tive reranking, e.g., <ref type="bibr" target="#b4">(Collins and Koo, 2005;</ref><ref type="bibr" target="#b2">Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b6">Dinarelli et al., 2011</ref>). However, such excellent studies do not regard discourse parsing, and in absolute they achieved lower improvements than our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we have presented a discriminative approach for reranking discourse trees generated by an existing discourse parser. Our reranker uses tree kernels in SVM preference ranking frame- work to effectively capture the long range struc- tural dependencies between the constituents of a discourse tree. We have shown the reranking per- formance for sentence-level discourse parsing us- ing the standard tree kernels (i.e., STK and PTK) on two different representations (i.e., JRN and SRN) of the discourse tree, and compare it with the traditional feature vector-based approach. Our results show that: (i) the reranker improves only when it considers subtree features computed by the tree kernels, (ii) SRN is a better representation than JRN, (iii) STK performs better than PTK for reranking discourse trees, and (iv) our best result outperforms the state-of-the-art significantly.</p><p>In the future, we would like to apply our reranker to the document-level parses. However, this will require a better hypotheses generator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>b) A discourse tree generated by Joty et al. (2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of human-annotated and system-generated discourse trees for the text [what's more,]e 1 [he believes]e 2 [seasonal swings in the auto industry this year aren't occurring at the same time in the past,]e 3 [because of production and pricing differences]e 4 [that are curbing the accuracy of seasonal adjustments]e 5 ] [built into the employment data.]e 6 Horizontal lines indicate text segments; satellites are connected to their nuclei by curved arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . 5 :</head><label>25</label><figDesc>Figure 2.5: A tree (left) and all of its subset trees (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A tree with its STK subtrees; STK b also includes leaves as features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Figure 2.3: A tree (left) and some of its subtrees (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>3 gives an example of a tree together with its subtrees. Various types of subtrees can be defined for a tree T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A tree with its PTK fragments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DISCTK trees: (a) Joint Relation-Nucleus (JRN), and (b) Split Relation Nucleus (SRN).</figDesc><graphic url="image-17.png" coords="4,41.02,156.81,513.90,86.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fragments from JRN in Figure 4a (upper row) and SRN in Figure 4b (lower row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The two-stage document-level discourse parser proposed by Joty et al. (2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The intra-sentential parsing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The B and C dynamic programming tables (left), and the corresponding discourse tree (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An error made by our reranker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>φT K • φM</head><label></label><figDesc></figDesc><table>JRN 
SRN 
Bigram 
All 
Bigram 
All 
STK 
81.28 
80.04 
82.15 
80.04 
STK b 
81.35 
80.28 
82.18 
80.25 
PTK 
81.63 
78.50 
81.42 
78.25 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Reranking performance of different discourse tree kernels on different representations.</head><label>3</label><figDesc></figDesc><table>Secondly, while the tree kernels perform sim-
ilarly on the JRN representation, STK performs 
significantly better (p-value &lt; 0.01) than PTK 
on SRN. 14 This result is interesting as it pro-
vides indications of the type of DT fragments use-
ful for improving parsing accuracy. As pointed 
out in Section 2.2, PTK includes all features 
generated by STK, and additionally, it includes 
fragments whose nodes can have any subsets of 
the children they have in the original DT. Since 
this does not improve the accuracy, we speculate 
that complete fragments, e.g., [CAUSE [ATTRI-
BUTION][ELABORATION]] are more meaningful 
than the partial ones, e.g., [CAUSE [ATTRIBU-
TION]] and </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Comparison of features from different sources for 5-best discourse reranking.</head><label>5</label><figDesc></figDesc><table>(Joty et al., 2013) With Reranker 
PAR-D 
55.8 
57.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Document-level parsing results with 5-best 

sentence-level discourse reranker. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 : Reranking performance (RR) in comparison with oracle (OR) accuracy for different values of k on the standard testset and 5-folds of RST-DT. Second row shows the relative error rate reduction (ERR).</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> Discourse Tree Kernels (DISCTK) Engineering features that can capture the dependencies between DT constituents is a difficult task. In principle, any dependency between words, relations and structures (see Figure 1) can be an important feature for discourse parsing. This may lead to an exponential number of features, which makes the feature engineering process very hard. The standard TKs described in the previous section serve as a viable option to get useful subtree features automatically. However, the definition of the input to a TK, i.e., the tree representing a training instance, is extremely important as it implicitly affects the subtree space generated by the TK, where the target learning task is carried out. This can be shown as follows. Let φ M () be a mapping from linguistic objects o i , e.g., a discourse parse, to a meaningful tree T i , and let φ T K () be a mapping into a tree kernel space us</note>

			<note place="foot" n="8"> Note that our earlier experiments with a 2-fold cross validation process yielded only 50% of our current improvement. 9 http://disi.unitn.it/moschitti/Tree-Kernel.htm</note>

			<note place="foot" n="10"> Precision, Recall and f-score are the same when the discourse parser uses manual discourse segmentation. Since all our experiments in this paper are based on manual discourse segmentation, we only report the f-scores. 11 It is important to note that optimizing Relation metric may also result in improved Nuclearity scores. 12 For document-level parsing, Joty et al. (2013) pro</note>

			<note place="foot" n="16"> The human agreement on sentence-level parsing is 83%.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is part of the Interactive sYstems for Answer Search (Iyas) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Qatar Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Discourse Tagging Reference Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<idno>ISI-TR- 545</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California Information Sciences Institute</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<title level="m">RST Discourse Treebank (RSTDT) LDC2002T07. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL&apos;05</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics, ACL&apos;05<address><addrLine>NJ, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative Reranking for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support Vector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative Reranking for Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Dinarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">526539</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text-level Discourse Parsing with Rich Linguistic Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL &apos;12</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics, ACL &apos;12<address><addrLine>Jeju Island, Korea. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HILDA: A Discourse Parser Using Support Vector Machine Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Better Kbest Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology, Parsing &apos;05</title>
		<meeting>the Ninth International Workshop on Parsing Technology, Parsing &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Sparse Kernel SVMs via Cutting-Plane Training. Machine Learning</title>
		<imprint>
			<publisher>ECML</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="179" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making large-Scale SVM Learning Practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Novel Discriminative Framework for Sentence-Level Discourse Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="904" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining Intra-and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL &apos;13</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL &apos;13<address><addrLine>Sofia, Bulgaria. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Toward a Functional Theory of Text Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="395" to="448" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Theory and Practice of Discourse Parsing and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling via Tree Kernel Joint Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X)</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning (CoNLL-X)<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel Engineering for Fast and Easy Design of Natural Language Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Tutorials)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">State-of-the-Art Kernels for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial Abstracts of ACL 2012</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernel-based Learning to Rank with Syntactic and Semantic Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reverse Engineering of Tree Kernel Feature Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On Reverse Feature Engineering of Syntactic Tree Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast Support Vector Machines for Structural Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD (3)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="175" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast Support Vector Machines for Convolution Tree Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="357" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Taylor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentence Level Discourse Parsing Using Syntactic and Lexical Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter<address><addrLine>Edmonton, Canada. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
	<note>NAACL&apos;03</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discourse Chunking and its Application to Sentence Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLTEMNLP&apos;05</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLTEMNLP&apos;05<address><addrLine>Vancouver, British Columbia, Canada. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="693" to="723" />
			<date type="published" when="2007" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
