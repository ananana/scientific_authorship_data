<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Cross-Lingual Word Embeddings by Meeting in the Middle</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yerai</forename><surname>Doval</surname></persName>
							<email>yerai.doval@uvigo.es</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution" key="instit1">Grupo COLE</orgName>
								<orgName type="institution" key="instit2">Escola Superior de Enxeñaría Informática Universidade de Vigo</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
							<email>camachocolladosj@cardiff.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
							<email>espinosa-ankel@cardiff.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
							<email>schockaerts1@cardiff.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Cross-Lingual Word Embeddings by Meeting in the Middle</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="294" to="304"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>294</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these em-beddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are one of the most widely used resources in NLP, as they have proven to be of enormous importance for modeling linguistic phe- nomena in both supervised and unsupervised set- tings. In particular, the representation of words in cross-lingual vector spaces (henceforth, cross- lingual word embeddings) is quickly gaining in popularity. One of the main reasons is that they play a crucial role in transferring knowledge from one language to another, specifically in down- stream tasks such as information retrieval <ref type="bibr" target="#b42">(Vuli´cVuli´c and Moens, 2015b</ref>), entity linking <ref type="bibr" target="#b38">(Tsai and Roth, 2016)</ref> and text classification <ref type="bibr" target="#b23">(Mogadala and Rettinger, 2016</ref>), while at the same time providing im- provements in multilingual NLP problems such as machine translation ( <ref type="bibr" target="#b48">Zou et al., 2013</ref>).</p><p>There exist different approaches for obtaining these cross-lingual embeddings. One of the most successful methodological directions, which con- stitutes the main focus of this paper, attempts to learn bilingual embeddings via a two-step process: first, word embeddings are trained on monolin- gual corpora and then the resulting monolingual spaces are aligned by taking advantage of bilin- gual dictionaries <ref type="bibr" target="#b22">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b7">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b44">Xing et al., 2015)</ref>.</p><p>These alignments are generally modeled as lin- ear transformations, which are constrained such that the structure of the initial monolingual spaces is left unchanged. This can be achieved by im- posing an orthogonality constraint on the linear transformation ( <ref type="bibr" target="#b44">Xing et al., 2015;</ref><ref type="bibr">Artetxe et al., 2016)</ref>. Our hypothesis in this paper is that such approaches can be further improved, as they rely on the assumption that the internal structure of the two monolingual spaces is identical. In reality, however, this structure is influenced by language- specific phenomena, e.g., the fact that Spanish dis- tinguishes between masculine and feminine nouns <ref type="bibr" target="#b4">(Davis, 2015)</ref> as well as the specific biases of the different corpora from which the monolingual spaces were learned. Because of this, monolingual embedding spaces are not isomorphic ( <ref type="bibr" target="#b16">Kementchedjhieva et al., 2018</ref>). On the other hand, simply dropping the orthogonality constraints leads to overfitting, and is thus not ef- fective in practice.</p><p>The solution we propose is to start with existing state-of-the-art alignment models ( <ref type="bibr">Artetxe et al., 2017;</ref><ref type="bibr" target="#b3">Conneau et al., 2018)</ref>, and to apply a fur- ther transformation to the resulting initial align- ment. For each word w with translation w , this additional transformation aims to map the vector representations of both w and w onto their aver- age, thereby creating a cross-lingual vector space which intuitively corresponds to the average of the two aligned monolingual vector spaces. Similar to the initial alignment, this mapping is learned from a small bilingual lexicon.</p><p>Our experimental results show that the proposed additional transformation does not only benefit cross-lingual evaluation tasks, but, perhaps sur- prisingly, also monolingual ones. In particular, we perform an extensive set of experiments on stan- dard benchmarks for bilingual dictionary induc- tion and monolingual and cross-lingual word simi- larity, as well as on an extrinsic task: cross-lingual hypernym discovery.</p><p>Code and pre-trained embeddings to reproduce our experiments and to apply our model to any given cross-lingual embeddings are available at https://github.com/yeraidm/meemi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Bilingual word embeddings have been extensively studied in the literature in recent years. Their na- ture varies with respect to the supervision signals used for training ( <ref type="bibr" target="#b39">Upadhyay et al., 2016;</ref>. Some common signals to learn bilingual embeddings come from parallel <ref type="bibr">(Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b20">Luong et al., 2015;</ref><ref type="bibr" target="#b19">Levy et al., 2017)</ref> or comparable corpora <ref type="bibr" target="#b41">(Vuli´cVuli´c and Moens, 2015a;</ref><ref type="bibr" target="#b35">Søgaard et al., 2015;</ref><ref type="bibr" target="#b43">Vuli´cVuli´c and Moens, 2016)</ref>, or lexical resources such as WordNet, ConceptNet or BabelNet ( <ref type="bibr" target="#b37">Speer et al., 2017;</ref><ref type="bibr" target="#b24">Mrksic et al., 2017;</ref><ref type="bibr" target="#b10">Goikoetxea et al., 2018)</ref>. However, these sources of supervision may be scarce, limited to certain domains or may not be directly available for certain language pairs.</p><p>Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embed- dings ( <ref type="bibr" target="#b22">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b7">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b0">Ammar et al., 2016;</ref><ref type="bibr">Artetxe et al., 2016)</ref>. <ref type="bibr" target="#b22">Mikolov et al. (2013b)</ref> was one of the first attempts into this line of research, applying a linear trans- formation in order to map the embeddings from one monolingual space into another. They also noted that more sophisticated approaches, such as using multilayer perceptrons, do not improve with respect to their linear counterparts. <ref type="bibr" target="#b44">Xing et al. (2015)</ref> built upon this work by normaliz- ing word embeddings during training and adding an orthogonality constraint. In a complementary direction, <ref type="bibr" target="#b7">Faruqui and Dyer (2014)</ref> put forward a technique based on canonical correlation anal- ysis to obtain linear mappings for both monolin- gual embedding spaces into a new shared space. <ref type="bibr">Artetxe et al. (2016)</ref> proposed a similar linear mapping to <ref type="bibr" target="#b22">Mikolov et al. (2013b)</ref>, generalizing it and providing theoretical justifications which also served to reinterpret the methods of <ref type="bibr" target="#b7">Faruqui and Dyer (2014)</ref> and <ref type="bibr" target="#b44">Xing et al. (2015)</ref>. <ref type="bibr" target="#b34">Smith et al. (2017)</ref> further showed how orthogonality was required to improve the consistency of bilin- gual mappings, making them more robust to noise. Finally, a more complete generalization providing further insights on the linear transformations used in all these models can be found in <ref type="bibr">Artetxe et al. (2018a)</ref>.</p><p>These approaches generally require large bilin- gual lexicons to effectively learn multilingual em- beddings ( <ref type="bibr">Artetxe et al., 2017)</ref>. Recently, how- ever, alternatives which only need very small dic- tionaries, or even none at all, have been proposed to learn high-quality embeddings via linear map- pings ( <ref type="bibr">Artetxe et al., 2017;</ref><ref type="bibr" target="#b3">Conneau et al., 2018)</ref>. More details on the specifics of these two ap- proaches can be found in Section 3.1. These mod- els have in turn paved the way for the development of machine translation systems which do not re- quire any parallel corpora ( <ref type="bibr">Artetxe et al., 2018b;</ref>. Moreover, the fact that such approaches only need monolingual embeddings, instead of parallel or comparable corpora, makes them easily adaptable to different domains (e.g., social media or web corpora).</p><p>In this paper we build upon these state-of-the- art approaches by applying an additional trans- formation, which aims to map each word and its translation onto the average of their vector repre- sentations. This strategy bears some resemblance with the idea of learning meta-embeddings <ref type="bibr" target="#b46">(Yin and Schütze, 2016)</ref>. Meta-embeddings are vector space representations which aggregate several pre- trained word embeddings from a given language (e.g., trained using different corpora and/or dif- ferent word embedding models). Empirically it was found that such meta-embeddings can often outperform the individual word embeddings from which they were obtained. In particular, it was re- cently argued that word vector averaging can be a highly effective approach for learning such meta- embeddings <ref type="bibr" target="#b2">(Coates and Bollegala, 2018)</ref>. The main difference between such approaches and our work is that because we rely on a small dictionary, we cannot simply average word vectors, since for most words we do not know the corresponding translation. Instead, we train a regression model to predict this average word vector from the vector representation of the given word only, i.e., without using the vector representation of its translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our approach for improving cross-lingual embed- dings consists of three main steps, where the first two steps are the same as in existing methods. In particular, given two monolingual corpora, a word vector space is first learned independently for each language. This can be achieved with common word embedding models, e.g., <ref type="bibr">Word2vec (Mikolov et al., 2013a</ref>), GloVe ( <ref type="bibr" target="#b25">Pennington et al., 2014</ref>) or FastText ( <ref type="bibr">Bojanowski et al., 2017)</ref>. Sec- ond, a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space (Section 3.1). Third, a final trans- formation is applied on the aligned embeddings so the word vectors from both languages are re- fined and further integrated with each other (Sec- tion 3.2). This third step is the main contribution of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Aligning monolingual spaces</head><p>Once the monolingual word embeddings have been obtained, a linear transformation is applied in order to integrate them into the same vector space. This linear transformation is generally carried out using a supervision signal, typically in the form of a bilingual dictionary. In the following we explain two state-of-the-art models performing this linear transformation.</p><p>VecMap <ref type="bibr">(Artetxe et al., 2017)</ref>. VecMap uses an orthogonal transformation over normalized word embeddings. An iterative two-step proce- dure is also implemented in order to avoid the need of starting with a large seed dictionary (e.g., in the original paper it was tested with a very small bilin- gual dictionary of just 25 pairs). In this proce- dure, first, the linear mapping is estimated using a small bilingual dictionary, and then, this dictio- nary is augmented by applying the learned trans- formation to new words from the source language. Lastly, the process is repeated until some conver- gence criterion is met. MUSE ( <ref type="bibr" target="#b3">Conneau et al., 2018)</ref>. In this case, the transformation matrix is learned through an iter- ative Procrustes alignment <ref type="bibr" target="#b31">(Schönemann, 1966)</ref>. <ref type="bibr">1</ref> The anchor points needed for this alignment can be obtained either through a supplied bilingual dictionary or through an unsupervised model. This unsupervised model is trained using adversarial learning to obtain an initial alignment of the two monolingual spaces, which is then refined by the Procrustes alignment using the most frequent words as anchor points. A new distance met- ric for the embedding space, referred to as cross- domain similarity local scaling, is also introduced. This metric, which takes into account the near- est neighbors of both source and target words, was shown to better handle high-density regions of the space, thus alleviating the hubness problem of word embedding models <ref type="bibr" target="#b27">(Radovanovi´cRadovanovi´c et al., 2010;</ref><ref type="bibr" target="#b5">Dinu et al., 2015)</ref>, which arises when a few points (known as hubs) become the nearest neigh- bors of many other points in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meeting in the middle</head><p>After the initial alignment of the monolingual word embeddings, our proposed method leverages an additional linear model to refine the result- ing bilingual word embeddings. This is because the methods presented in the previous section ap- ply constraints to ensure that the structure of the monolingual embeddings is largely preserved. As already mentioned in the introduction, conceptu- ally this may not be optimal, as embeddings for different languages and trained from different cor- pora can be expected to be structured somewhat differently. Empirically, as we will see in the eval- uation, after applying methods such as VecMap and MUSE there still tend to be significant gaps between the vector representations of words and their translations. Our method directly attempts to reduce these gaps by moving each word vec- tor towards the middle point between its current representation and the representation of its trans- lation. In this way, by bringing the two monolin- gual fragments of the space closer to each other, we can expect to see an improved performance on cross-lingual evaluation tasks such as bilin- gual dictionary induction. Importantly, the inter- nal structure of the two monolingual fragments themselves is also affected by this step. By aver-aging between the representations obtained from different languages, we hypothesize that the im- pact of language-specific phenomena and corpus specific biases will be reduced, thereby ending up with more "neutral" monolingual embeddings.</p><p>In the following, we detail our methodologi- cal approach. First, we leverage the same bilin- gual dictionary that was used to obtain the ini- tial alignment (Section 3.1). Specifically, let D = {(w, w )} be the given bilingual dictionary, where w ∈ V and w ∈ V , with V and V representing the vocabulary of the first and second language, respectively. For pairs (w, w ) ∈ D, we can simply compute the corresponding average vector</p><formula xml:id="formula_0">µ w,w = vw+ v w 2</formula><p>. Then, using the pairs in D as training data, we learn a linear mapping X such that X v w ≈ µ w,w for all (w, w ) ∈ D. This map- ping X can then be used to predict the averages for words outside the given dictionary. To find the mapping X, we solve the following least squares linear regression problem:</p><formula xml:id="formula_1">E = (w,w )∈D X w − µ w,w 2<label>(1)</label></formula><p>Similarly, for the other language, we separately learn a mapping X such that X v w ≈ µ w,w . It is worth pointing out that we experimented with several variants of this linear regression for- mulation. For example, we also tried using a mul- tilayer perceptron to learn non-linear mappings, and we experimented with several regularization terms to penalize mappings that deviate too much from the identity mapping. None of these vari- ants, however, were found to improve on the much simpler formulation in (1), which can be solved exactly and efficiently. Furthermore, one may wonder whether the initial alignment is actually needed, since e.g., <ref type="bibr" target="#b2">Coates and Bollegala (2018)</ref> obtained high-quality meta-embeddings without such an alignment set. However, when applying our approach directly to the initial monolingual non-aligned embedding spaces, we obtained re- sults which were competitive but slightly below the two considered alignment strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We test our bilingual embedding refinement ap- proach on both intrinsic and extrinsic tasks. In Section 4.1 we describe the common training setup for all experiments and language pairs. The languages we considered are English, Spanish, Italian, German and Finnish. Throughout all the experiments we use publicly available resources in order to make comparisons and reproducibility of our experiments easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-lingual embeddings training</head><p>Corpora. In our experiments we make use of web-extracted corpora. For English we use the 3B-word UMBC WebBase Corpus ( <ref type="bibr" target="#b11">Han et al., 2013</ref>), while we chose the Spanish Billion Words Corpus <ref type="bibr" target="#b1">(Cardellino, 2016)</ref> for Spanish. For Italian and German, we use the itWaC and sdeWaC cor- pora from the WaCky project ( <ref type="bibr">Baroni et al., 2009)</ref>, containing 2 and 0.8 billion words, respectively. <ref type="bibr">2</ref> Lastly, for Finnish, we use the Common Crawl monolingual corpus from the Machine Translation of News Shared Task 2016 3 , composed of 2.8B words. All corpora are tokenized and lowercased. Bilingual dictionaries. We use the bilingual dictionaries packaged together by <ref type="bibr">Artetxe et al. (2017)</ref>, each one conformed by 5000 word trans- lations. They are used both for the initial bilingual mappings and then again for our linear transfor- mation.</p><p>Initial mapping. Following previous works, for the purpose of obtaining the initial alignment, En- glish is considered as source language and the remaining languages are used as target. We make use of the open-source implementations of VecMap 4 (Artetxe et al., 2017) and MUSE 5 (Con- neau et al., 2018), which constitute strong base- lines for our experiments (cf. Section 3.1). Both of them were used with the recommended parame- ters and in their supervised setting, using the afore- mentioned bilingual dictionaries.</p><p>Meeting in the Middle. Then, once the initial cross-lingual embeddings are trained, and as ex- plained in Section 3.2, we obtain our linear trans- formation by using the exact solution to the least 2 UMBC, Spanish Billion-Words and ItWaC are the offi- cial corpora of the hypernym discovery SemEval task (Sec- tion 4.2.3) for English, Spanish and Italian, respectively.</p><p>3 http://www.statmt.org/wmt16/ translation-task.html 4 github.com/artetxem/vecmap 5 github.com/facebookresearch/MUSE</p><note type="other">Model EN-ES EN-IT EN-DE EN-FI</note><p>P @1 P @5 P @10 P @1 P @5 P @10 P @1 P @5 P @10 P @1 P @5 P @10 squares linear regression problem. To this end, we use the same bilingual dictionaries as in the pre- vious step. Henceforth, we will refer to our trans- formed models as VecMap µ and MUSE µ , depend- ing on the initial mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We test our cross-lingual word embeddings in two intrinsic tasks, i.e., bilingual dictionary induc- tion (Section 4.2.1) and word similarity (Section 4.2.2), and an extrinsic task, i.e., cross-lingual hy- pernym discovery (Section 4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Bilingual dictionary induction</head><p>The dictionary induction task consists in auto- matically generating a bilingual dictionary from a source to a target language, using as input a list of words in the source language.</p><p>Experimental setting For this task, and follow- ing previous works, we use the English-Italian test set released by <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> and those re- leased by <ref type="bibr">Artetxe et al. (2017)</ref> for the remaining language pairs. These test sets have no overlap with respect to the training and development sets, and contain around 1900 entries each. Given an input word from the source language, word trans- lations are retrieved through a nearest-neighbor search of words in the target language, using co- sine distance. Note that this gives us a ranked list of candidates for each word from the source language. Accordingly, the performance of the embeddings is evaluated with the precision at k (P @k) metric, which evaluates for what percent- age of test pairs, the correct answer is among the k highest ranked candidates.</p><p>Results As can be seen in <ref type="table">Table 1</ref>, our refine- ment method consistently improves over the base- lines (i.e., VecMap and MUSE) on all language pairs and metrics. The higher scores indicate that the two monolingual embedding spaces become more tightly integrated because of our additional transformation. It is worth highlighting here the case of English-Finnish, where the gains obtained in P @5 and P @10 are considerable. This might indicate that our approach is especially useful for morphologically richer languages such as Finnish, where the limitations of the previous bilingual mappings are most apparent.</p><p>Analysis When analyzing the source of errors in P @1, we came to similar conclusions as <ref type="bibr">Artetxe et al. (2017)</ref>. <ref type="bibr">6</ref> Several source words are translated to words that are closely related to the one in the gold reference in the target language; e.g., for the English word essentially we obtain básicamente (basically) instead of fundamentalmente (funda- mentally) in Spanish, both of them closely re- lated, or the closest neighbor for dirt being mu- gre (dirt) instead of suciedad (dirt), which in fact was among the five closest neighbors. We can also find multiple examples of the higher performance of our models compared to the baselines. For in- stance, in the English-Spanish cross-lingual mod- els, after the initial alignment, we can find that sec- onds has minutos (minutes) as nearest neighbour, but after applying our additional transformation, seconds becomes closest to segundos (seconds). Similarly, paint initially has tintado (tinted) as the closest Spanish word, and then pintura (paint).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Word similarity</head><p>We perform experiments on both monolingual and cross-lingual word similarity. In monolingual sim- ilarity, models are tested in their ability to deter- mine the similarity between two words in the same language, whereas in cross-lingual similarity the words belong to different languages. While in the monolingual setting the main objective is to test the quality of the monolingual subsets of the bilin-    <ref type="bibr" target="#b29">and Goodenough, 1965)</ref>. The cor- responding cross-lingual datasets from SemEval- 18, WordSim-353 and RG-65 were considered for the cross-lingual word similarity evaluation 9 . Co- sine similarity is again used as comparison mea- sure.</p><p>Results <ref type="table" target="#tab_2">Tables 2 and 3</ref> show the monolingual 10 and cross-lingual word similarity results 11 , respec- tively. For both the monolingual and cross-lingual settings, we can notice that our models generally outperform the corresponding baselines. More- over, in cases where no improvement is obtained, the differences tend to be minimal, with the excep- tion of RG-65, but this is a very small test set for which larger variations can thus be expected. In contrast, there are a few cases where substantial gains were obtained by using our model. This is most notable for English WordSim and SimLex in the monolingual setting. Analysis In order to further understand the movements of the space with respect to the orig- inal VecMap and MUSE spaces, <ref type="figure" target="#fig_2">Figure 1</ref> dis- plays the average similarity values on the Se- mEval cross-lingual datasets (the largest among all benchmarks) of each model. As expected, the figure clearly shows how our model consistently brings the words from both languages closer on all language pairs. Furthermore, this movement is performed smoothly across all pairs, i.e., our model does not make large changes to specific words but rather small changes overall. This can be verified by inspecting the standard deviation of the difference in similarity after applying our transformation. These standard deviation scores range from 0.031 (English-Spanish for VecMap) to 0.039 (English-Italian for MUSE), which are rel- atively small given that the cosine similarity scale ranges from -1 to 1.</p><p>As a complement of this analysis we show some qualitative results which give us further insights on the transformations of the vector space after our average approximation. In particular, we analyze the reasons behind the higher quality displayed by our bilingual embeddings in monolingual set- tings. While VecMap and MUSE do not trans- form the initial monolingual spaces, our model transforms both spaces simultaneously. In this analysis we focus on the source language of our experiments (i.e., English). We found interest- ing patterns which are learned by our model and <ref type="table">SemEval  RG-65  SemEval WordSim SemEval WordSim RG-65  r  ρ  r  ρ  r  ρ  r  ρ  r  ρ  r  ρ  r</ref> ρ VecMap 71.7 71.6 82.1 82.4 69.6 69.6 60.2 63.1 71.6 71.3 64.1 65.9 78.1 78.8 VecMap µ 71.7 71.3 82.1 82.8 70.2 69.9 61.3 63.0 72.0 71.5 64.2 65.4 78.6 79.7 MUSE 72.0 72.0 81.9 82.3 69.4 69.4 59.9 62.7 70.4 70.1 63.5 65.1 78.4 79.5 MUSE µ 72.2 71.8 82.3 82.5 70.5 70.1 61.2 62.7 71.9 71.4 64.1 65.3 78.8 80.5 <ref type="table">Table 3</ref>: Cross-lingual word similarity results. Pearson (r) and Spearman (ρ) correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English-Spanish English-Italian English-German</head><p>help understand these monolingual gains. For ex- ample, a recurring pattern is that words in En- glish which are translated to the same word, or to semantically close words, in the target lan- guage end up closer together after our transfor- mation. For example, in the case of English- Spanish the following pairs were among the pairs whose similarity increased the most by applying our transformation: cellphone-telephone, movie- film, book-manuscript or rhythm-cadence, which are either translated to the same word in Spanish (i.e., teléfono and película in the first two cases) or are already very close in the Spanish space. More generally, we found that word pairs which move together the most tend to be semantically very similar and belong to the same domain, e.g., car-bicycle, opera-cinema, or snow-ice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Cross-lingual hypernym discovery</head><p>Modeling hypernymy is a crucial task in NLP, with direct applications in diverse areas such as seman- tic search <ref type="bibr" target="#b15">(Hoffart et al., 2014;</ref><ref type="bibr" target="#b28">Roller and Erk, 2016)</ref>, question answering ( <ref type="bibr" target="#b26">Prager et al., 2008;</ref><ref type="bibr" target="#b45">Yahya et al., 2013</ref>) or textual entailment <ref type="bibr" target="#b9">(Geffet and Dagan, 2005</ref>). Hypernyms, in addition, are the backbone of lexical ontologies ( <ref type="bibr" target="#b47">Yu et al., 2015)</ref>, which are in turn useful for organizing, navigat- ing and retrieving online content ( <ref type="bibr">Bordea et al., 2016</ref>). Thus, we propose to evaluate the contribu- tion of cross-lingual embeddings towards the task of hypernym discovery, i.e., given an input word (e.g., cat), retrieve or discover its most likely (set of) valid hypernyms (e.g., animal, mammal, feline, and so on). Intuitively, by leveraging a bilingual vector space condensing the semantics of two lan- guages, one of them being English, the need for large amounts of training data in the target lan- guage may be reduced.  <ref type="bibr" target="#b12">Hearst, 1992)</ref>, both of which are generally scarcely (if at all) available for lan- guages other than English. Therefore, we re- port experiments with training data only from En- glish (11,779 hyponym-hypernym pairs), and "en- riched" models informed with relatively few train- ing pairs (500, 1k and 2k) from the target lan- guages. Evaluation is conducted with the same metrics as in the original SemEval task, i.e., Mean Reciprocal Rank (MRR), Mean Average Precision (MAP) and Precision at 5 (P@5). These measures explain a model's behavior from complementary prisms, namely how often at least one valid hy- pernym was highly ranked (MRR), and in cases where there is more than one correct hypernym, to what extent they were all correctly retrieved (MAP and P@5). Finally, as in the previous ex- periments, we report comparative results between our proposed models and the two competing base- lines (VecMap and MUSE). As an additional in- formative baseline, we include the highest scoring unsupervised system at the SemEval task for both Spanish and Italian (BestUns), which is based on the distributional models described in <ref type="bibr" target="#b33">Shwartz et al. (2017)</ref>.</p><p>Results The results listed in <ref type="table" target="#tab_4">Table 4</ref>   model-wise comparisons, we observe that our pro- posed alterations of both VecMap and MUSE im- prove their quality in a consistent manner, across most metrics and data configurations. In Italian our proposed model shows an improvement across all configurations. However, in Spanish VecMap emerges as a highly competitive baseline, with our model only showing an improved performance when training data in this language abounds (in this specific case there is an increase from 17.2 to 19.5 points in the MRR metric). This suggests that the fact that the monolingual spaces are closer in our model is clearly beneficial when hybrid train- ing data is given as input, opening up avenues for future work on weakly-supervised learning. Con- cerning the other baseline, MUSE, the contribution of our proposed model is consistent for both lan- guages, again becoming more apparent in the Ital- ian split and in a fully cross-lingual setting, where the improvement in MRR is almost 3 points (from 10.6 to 13.3). Finally, it is noteworthy that even in the setting where no training data from the tar- get language is leveraged, all the systems based on cross-lingual embeddings outperform the best un- supervised baseline, which is a very encouraging result with regards to solving tasks for languages on which training data is not easily accessible or not directly available.</p><p>Analysis A manual exploration of the results obtained in cross-lingual hypernym discovery re- veals a systematic pattern when comparing, for ex- ample, VecMap and our model. It was shown in <ref type="table" target="#tab_4">Table 4</ref> that the performance of our model grad- ually increased alongside the size of the train- ing data in the target language until surpassing VecMap in the most informed configuration (i.e., EN+2k). Specifically, our model seems to show a higher presence of generic words in the output hy- pernyms, which may be explained by these being closer in the space. In fact, out of 1000 candi- date hyponyms, our model correctly finds person 143 times, as compared to the 111 of VecMap, and this systematically occurs with generic types such as citizen or transport. Let us mention, how- ever, that the considered baselines perform re- markably well in some cases. For example, the English-only VecMap configuration (EN), unlike ours, correctly discovered the following hyper- nyms for FrancescMacì a (a Spanish politician and soldier): politician, ruler, leader and person. These were missing from the prediction of our model in all configurations until the most informed one (EN+2k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have shown how to refine bilingual word embeddings by applying a simple transformation which moves cross-lingual synonyms closer to- wards their average representation. Before apply- ing this strategy, we start by aligning the mono- lingual embeddings of the two languages of in- terest. For this initial alignment, we have consid- ered two state-of-the-art methods from the litera- ture, namely VecMap (Artetxe et al., 2017) and MUSE ( <ref type="bibr" target="#b3">Conneau et al., 2018)</ref>, which also served as our baselines. Our approach is motivated by the fact that these alignment methods do not change the structure of the individual monolingual spaces. However, the internal structure of embeddings is, at least to some extent, language-specific, and is moreover affected by biases of the corpus from which they are trained, meaning that after the ini- tial alignment significant gaps remain between the representations of cross-lingual synonyms. We tested our approach on a wide array of datasets from different tasks (i.e., bilingual dictionary in- duction, word similarity and cross-lingual hyper- nym discovery) with state-of-the-art results. This paper opens up several promising avenues for future work. First, even though both lan- guages are currently being treated symmetrically, the initial monolingual embedding of one of the languages may be more reliable than that of the other. In such cases, it may be of interest to replace the vectors µ w,w by a weighted aver- age of the monolingual word vectors. Second, while we have only considered bilingual scenar- ios in this paper, our approach can naturally be ap- plied to scenarios involving more languages. In this case, we would first choose a single target language, and obtain alignments between all the other languages and this target language. To ap- ply our model, we can then simply learn map- pings to predict averaged word vectors across all languages. Finally, it would also be interesting to use the obtained embeddings in downstream ap- plications such as language identification or cross- lingual sentiment analysis, and extend our analy- sis to other languages, with a particular focus on morphologically-rich languages (after seeing our success with Finnish), for which the bilingual in- duction task has proved more challenging for stan- dard cross-lingual embedding models ( <ref type="bibr">Søgaard et al., 2018</ref> Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 451-462, Vancouver, Canada. Association for Computational Linguistics.</p><p>Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intel- ligence (AAAI-18). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mikel</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Monolingual embeddings.</head><label></label><figDesc>The monolingual word embeddings are trained with the Skipgram model from FastText (Bojanowski et al., 2017) on the corpora described above. The dimensionality of the vectors was set to 300, with the default Fast- Text hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparative average similarity between VecMap and MUSE (blue) and our proposed model (red) on the SemEval cross-lingual similarity datasets.</figDesc><graphic url="image-1.png" coords="6,307.28,195.72,218.27,72.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Experimental setting We follow Espinosa- Anke et al. (2016) and learn a (cross-lingual) lin- ear transformation matrix between the hyponym and hypernym spaces, which is afterwards used to predict the most likely (set of) hypernyms, given an unseen hyponym. Training and evalu- ation data come from the SemEval 2018 Shared Task on Hypernym Discovery (Camacho-Collados et al., 2018). Note that current state-of-the-art systems aimed at modeling hypernymy (Shwartz et al., 2016; Bernier-Colborne and Barriere, 2018) combine large amounts of annotated data along with language-specific rules and cue phrases such as Hearst Patterns (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2018b. Unsupervised neural ma- chine translation. In Proceedings of the Sixth Inter- national Conference on Learning Representations. Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209-226. Gabriel Bernier-Colborne and Caroline Barriere. 2018. Crim at semeval-2018 task 9: A hybrid approach to hypernym discovery. In Proceedings of The 12th In- ternational Workshop on Semantic Evaluation, New Orleans, Louisiana. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion of Computational Linguistics, 5(1):135-146. Georgeta Bordea, Els Lefever, and Paul Buitelaar. 2016. Semeval-2016 task 13: Taxonomy extrac- tion evaluation (texeval-2). In Proceedings of the 10th International Workshop on Semantic Evalua- tion (SemEval-2016), pages 1081-1091. Jose Camacho-Collados, Claudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas, Tommaso Pasini, Enrico Santus, Vered Shwartz, Roberto Navigli, and Horacio Saggion. 2018. SemEval-2018 Task 9: Hy- pernym Discovery. In Proceedings of SemEval, New Orleans, LA, United States. Jose Camacho-Collados, Mohammad Taher Pilehvar, Nigel Collier, and Roberto Navigli. 2017. Semeval- 2017 task 2: Multilingual and cross-lingual semantic word similarity. In Proceedings of the 11th Interna- tional Workshop on Semantic Evaluation (SemEval- 2017), pages 15-26. José Camacho-Collados, Mohammad Taher Pilehvar, and Roberto Navigli. 2015. A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets. In Proceedings of the 53rd Annual Meeting of the Association for Com- putational Linguistics and the 7th International Joint Conference on Natural Language Processing -Short Papers, pages 1-7, Beijing, China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Monolingual word similarity results. Pearson (r) and Spearman (ρ) correlation. 

gual vector space, the cross-lingual setting consti-
tutes a straightforward benchmark to test the qual-
ity of bilingual embeddings. 

Experimental setting For monolingual word 
similarity we use the English SimLex-999 (Hill 
et al., 2015), and the language-specific versions 
of SemEval-17 7 (Camacho-Collados et al., 2017), 
WordSim-353 8 (Finkelstein et al., 2002), and RG-
65 (Rubenstein </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Results on the hypernym discovery task.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016. 
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance. 
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages 
2289-2294. 

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. 

</table></figure>

			<note place="foot" n="1"> Very recently, Kementchedjhieva et al. (2018) showed that projecting both monolingual embedding spaces onto a third space (instead of directly onto each other) using a generalized Procrustes analysis facilitates the learning of alignments.</note>

			<note place="foot" n="6"> The results on this task are lower than those reported in Artetxe et al. (2017). This is due to the different corpora and embedding algorithms used to train the monolingual embeddings. In particular, they use corpora including Wikipedia, which is comparable across languages.</note>

			<note place="foot" n="7"> The original datasets of SemEval-17 contained also multiwords, but for consistency we use the version containing single words only. 8 WordSim datasets consist of the similarity re-scoring for several languages of Leviant and Reichart (2015), downloaded from http://leviants.com/ira.leviant/ MultilingualVSMdata.html 9 The WordSim-353 and RG-65 cross-lingual datasets (Camacho-Collados et al., 2015) were downloaded at http: //lcl.uniroma1.it/similarity-datasets/ 10 The English results correspond to the averaged performance of the English fragments of English-Spanish, EnglishItalian and English-German cross-lingual embeddings. 11 The results of the original VecMap in cross-lingual similarity are comparable or better to those reported in Artetxe et al. (2017) on the three datasets used in their evaluation.</note>

			<note place="foot" n="12"> Note that this task is harder than hypernymy detection (Upadhyay et al., 2018). Hypernymy detection is framed as a binary classification task, while in hypernym discovery hypernyms have to be retrieved from the whole vocabulary.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yerai Doval is supported by the Spanish Min-istry of Economy, Industry and Competitive-ness (MINECO) through projects FFI2014-51978-C2-2-R, TIN201785160C21-R and TIN201785160C22-R; the Spanish State Secre-tariat for Research, Development and Innovation (which belongs to MINECO) and the European Social Fund (ESF) under an FPI fellowship <ref type="bibr">(BES2015-073768</ref>) associated to project FFI2014-51978-C2-1-R; and by the Galician Regional Government under project ED431D 2017/12. Jose Camacho-Collados, Luis Espinosa-Anke and Steven Schockaert have been supported by ERC Starting Grant 637277.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spanish Billion Words Corpus and Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cardellino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly easy meta-embedding-computing metaembeddings by averaging source word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="194" to="198" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Does the world look different in different languages?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="202" to="209" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR, Workshop track</title>
		<meeting>ICLR, Workshop track</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supervised distributional hypernym discovery via domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrilovich</forename><surname>Evgeniy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Yossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rivlin</forename><surname>Ehud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solan</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfman</forename><surname>Gadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruppin</forename><surname>Eytan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bilingual embeddings with random walks over multilingual wordnets. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josu</forename><surname>Goikoetxea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UMBC EBIQUITY-CORE: Semantic textual similarity systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lushan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Second Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING 1992</title>
		<meeting>of COLING 1992</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stics: searching with strings, things, and cats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragan</forename><surname>Milchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1247" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalizing procrustes analysis for better bilingual dictionary induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Separated by an un-common language: Towards judgment language informed vector space modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00106</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A strong baseline for learning cross-lingual word embeddings from sentence alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from parallel and nonparallel corpora for cross-language text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Mogadala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="692" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Question answering by predictive annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Open Domain Question Answering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="307" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Radovanovi´cradovanovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjana</forename><surname>Ivanovi´civanovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2487" to="2531" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relations such as hypernymy: Identifying and exploiting hearst patterns in distributional vectors for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2163" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of cross-lingual word embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter H Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Schlechtweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inverted indexing for cross-lingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor Martínez</forename><surname>Zeljko Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johannsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the limitations of unsupervised bilingual dictionary induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03620</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-lingual models of word embeddings: An empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust cross-lingual hypernymy detection using dependency context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="719" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 38th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bilingual distributed word representations from documentaligned comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="953" to="994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust question answering over the web of linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning word meta-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning term embeddings for hypernymy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1390" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
