<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Governing Neural Networks for On-Device Short Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
							<email>sravi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zornitsa Kozareva Google Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Governing Neural Networks for On-Device Short Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="804" to="810"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>804</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep neural networks reach state-of-the-art performance for wide range of natural language processing, computer vision and speech applications. Yet, one of the biggest challenges is running these complex networks on devices such as mobile phones or smart watches with tiny memory footprint and low computational capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn compact projection vectors with local sensitive hashing. The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embed-dings and complex networks with huge parameters. We conduct extensive evaluation on dialog act classification and show significant improvement over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations , while maintaining high accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks are one of the most suc- cessful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing <ref type="bibr" target="#b26">(Sutskever et al., 2014</ref>), speech ( ) and visual recognition tasks ( <ref type="bibr" target="#b12">Krizhevsky et al., 2012</ref>). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more com- plex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the biggest obstacles to deploy deep neural networks on- device such as mobile phones, smart watches and IoT ( <ref type="bibr" target="#b7">Iandola et al., 2016</ref>). The main challenges with developing and deploying deep neural net- work models on-device are (1) the tiny mem- ory footprint, (2) inference latency and (3) sig- nificantly low computational capacity compared to high performance computing systems such as CPUs, GPUs and TPUs on the cloud.</p><p>There are multiple strategies to build lightweight text classification models for on- device. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. How- ever, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques ( <ref type="bibr" target="#b1">Ahmed et al., 2012;</ref><ref type="bibr" target="#b21">Ravi, 2013)</ref> or incorporate deep learning models with graph learning like ( <ref type="bibr" target="#b2">Bui et al., 2017</ref><ref type="bibr" target="#b3">Bui et al., , 2018</ref>, which result in large models but have proven to be extremely powerful for complex language understanding tasks like response com- pletion ( <ref type="bibr" target="#b18">Pang and Ravi, 2012)</ref> and Smart Reply ( <ref type="bibr" target="#b10">Kannan et al., 2016)</ref>.</p><p>In this paper, we propose Self-Governing Neu- ral Networks (SGNNs) inspired by projection net- works <ref type="bibr" target="#b22">(Ravi, 2017)</ref>. SGNNs are on-device deep learning models learned via embedding-free pro- jection operations. We employ a modified ver- sion of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpre- tation and conversational analysis aiming to under- stand the intent of the speaker at every utterance of the conversation and (2) deep learning meth- ods reached state-of-the-art ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016;</ref><ref type="bibr" target="#b11">Khanpour et al., 2016;</ref><ref type="bibr" target="#b27">Tran et al., 2017;</ref><ref type="bibr" target="#b17">Ortega and Vu, 2017</ref>).</p><p>The main contributions of the paper are:</p><p>• Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.</p><p>• Compression technique that effectively cap- tures low-dimensional semantic text repre- sentation and produces compact models that save on storage and computational cost.</p><p>• On the fly computation of projection vectors that eliminate the need for large pre-trained word embeddings or vocabulary pruning.</p><p>• Exhaustive experimental evaluation on dia- log act datasets, outperforming state-of-the- art deep CNN ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref> and RNN variants ( <ref type="bibr" target="#b11">Khanpour et al., 2016;</ref><ref type="bibr" target="#b17">Ortega and Vu, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Self-Governing Neural Networks</head><p>We model the Self-Governing network using a projection model architecture <ref type="bibr" target="#b22">(Ravi, 2017</ref>).</p><p>The projection model is a simple network with dynamically-computed layers that encodes a set of efficient-to-compute operations which can be performed directly on device for inference. The model defines a set of efficient "projection" func- tions P( x i ) that project each input instance x i to a different space Ω P and then performs learning in this space to map it to corresponding outputs y p i . A very simple projection model comprises just few operations where the inputs x i are transformed us- ing a series of T projection functions P 1 , ..., P T followed by a single layer of activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>In this work, we design a Self-Governing Neu- ral Network (SGNN) using multi-layered locality- sensitive projection model. <ref type="figure" target="#fig_0">Figure 1</ref> shows the model architecture of the on-device SGNN net- work. The self-governing property of this network stems from its ability to learn a model (e.g., a clas- sifier) without having to initialize, load or store any feature or vocabulary weight matrices. In this sense, our method is a truly embedding-free ap- proach unlike majority of the widely-used state- of-the-art deep learning techniques in NLP whose performance depends on embeddings pre-trained on large corpora. Instead, we use the projection functions to dynamically transform each input to a low-dimensional representation. Furthermore, we stack this with additional layers and non-linear ac- tivations to achieve deep, non-linear combinations of projections that permit the network to learn complex mappings from inputs x i to outputs y i . An SGNN network is shown below:</p><formula xml:id="formula_0">i p = [P 1 (x i ), ..., P T (x i )] (1) h p = σ(W p · i p + b p ) (2) h t = σ(W t · h t−1 + b t ) (3) y i = softmax(W o · h k + b o ) (4)</formula><p>where, i p refers to the output of projection opera- tion applied to input x i , h p is applied to projec- tion output, h t is applied at intermediate layers of the network with depth k followed by a final softmax activation layer at the top. In a k-layer SGNN, h t , where t = p, p + 1, ..., p + k − 1 refers to the k subsequent layers after the pro- jection layer. W p , W t , W o and b p , b t , b o represent trainable weights and biases respectively. The projection transformations use pre- computed parameterized functions, i.e., they are not trained during the learning process, and their outputs are concatenated to form the hidden units for subsequent operations. Each input text x i is converted to an intermediate feature vector (via raw text features such as skip-grams) followed by projections.</p><formula xml:id="formula_1">x i F − → x i P − → [P 1 (x i ), ..., P T (x i )]<label>(5)</label></formula><p>On-the-fly Computation. The transformation step F dynamically extracts features from the raw input. Text features (e.g., skip-grams) are con- verted into feature-ids f j (via hashing) to gener- ate a sparse feature representation x i of feature-id, weight pairs (f j , w j ) . This intermediate feature representation is passed through projection func- tions P to construct projection layer i p in SGNN. For this last step, a projection vector P k is first constructed on-the-fly using a hash function with feature ids f j in x i and fixed seed as input, then dot product of the two vectors &lt; x i , P k &gt; is com- puted and transformed into binary representation P k ( x i ) using sgn(.) of the dot product. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, both F and P steps are computed on-the-fly, i.e., no word-embedding or vocabulary/feature matrices need to be stored and looked up during training or inference. Instead feature-ids and projection vectors are dynamically computed via hash functions. For intermediate feature weights w j , we use observed counts in each input text and do not use pre-computed statis- tics like idf. Hence the method is embedding-free.</p><p>Model Optimization. The SGNN network is trained from scratch on the task data using a su- pervised loss defined wrt ground truthˆytruthˆ truthˆy i :</p><formula xml:id="formula_2">L(.) = i∈N cross − entropy(y i , ˆ y i ) (6)</formula><p>During training, the network learns to choose and apply specific projection operations P j (via activations) that are more predictive for a given task. The choice of the type of projection ma- trix P as well as representation of the projected space Ω P has a direct effect on computation cost and model size. We leverage an efficient random- ized projection method and use a binary represen- tation {0, 1} d for Ω P . This yields a drastically lower memory footprint both in terms of number and size of parameters.</p><p>Computing Projections. We employ an effi- cient randomized projection method for the pro- jection step. We use locality sensitive hashing (LSH) <ref type="bibr" target="#b4">(Charikar, 2002</ref>) to model the underly- ing projection operations in SGNN. LSH is typi- cally used as a dimensionality reduction technique for clustering <ref type="bibr" target="#b16">(Manning et al., 2008)</ref>. LSH al- lows us to project similar inputs x i or interme- diate network layers into hidden unit vectors that are nearby in metric space. We use repeated bi- nary hashing for P and apply the projection vec- tors to transform the input x i to a binary hash rep- resentation denoted by P k ( x i ) ∈ {0, 1}, where</p><formula xml:id="formula_3">[P k ( x i )] := sgn[ x i , P k ].</formula><p>This results in a d- bit vector representation, one bit corresponding to each projection row P k=1...d .</p><p>The same projection matrix P is used for train- ing and inference. We never need to explicitly store the random projection vector P k since we can compute them on the fly using hash functions over feature indices with a fixed row seed rather than invoking a random number generator. This also permits us to perform projection operations that are linear in the observed feature size rather than the overall feature or vocabulary size which can be prohibitively large for high-dimensional data, thereby saving both memory and computa- tion cost. Thus, SGNN can efficiently model high- dimensional sparse inputs and large vocabulary sizes common for text applications instead of re- lying on feature pruning or other pre-processing heuristics employed to restrict input sizes in stan- dard neural networks for feasible training. The bi- nary representation is significant since this results in a significantly compact representation for the projection network parameters that in turn consid- erably reduces the model size.</p><p>SGNN Parameters. In practice, we employ T dif- ferent projection functions P j=1...T , each result- ing in d-bit vector that is concatenated to form the projected vector i p in Equation 5. T and d vary depending on the projection network param- eter configuration specified for P and can be tuned to trade-off between prediction quality and model size. Note that the choice of whether to use a sin- gle projection matrix of size T · d or T separate matrices of d columns depends on the type of pro- jection employed (dense or sparse). For the in- termediate feature step F in Equation 5, we use skip-gram features (3-grams with skip-size=2) ex- tracted from raw text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training and Inference</head><p>We use the compact bit units to represent the pro- jection in SGNN. During training, the network learns to move the gradients for points that are nearby to each other in the projected bit space Ω P in the same direction. SGNN network is trained end-to-end using backpropagation. Train- ing can progress efficiently with stochastic gradi- ent descent with distributed computing on high- performance CPUs or GPUs.</p><p>Complexity. The overall complexity for SGNN inference, governed by the projection layer, is O(n · T · d), where n is the observed feature size (*not* overall vocabulary size) which is linear in input size, d is the number of LSH bits specified for each projection vector P k , and T is the number of projection functions used in P. The model size (in terms of number of parameters) and memory storage required for the projection inference step is O(T · d · C), where C is the number of hidden units in h p in the multi-layer projection network and typically smaller than T · d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Description</head><p>We conduct our experimental evaluation on two dialog act benchmark datasets.</p><p>• SWDA: Switchboard Dialog Act Corpus ( <ref type="bibr" target="#b5">Godfrey et al., 1992;</ref><ref type="bibr" target="#b9">Jurafsky et al., 1997)</ref> is a popular open domain dialogs corpus be- tween two speakers with 42 dialogs acts.</p><p>• MRDA: ICSI Meeting Recorder Dialog Act Corpus ( <ref type="bibr" target="#b0">Adam et al., 2003;</ref><ref type="bibr" target="#b23">Shriberg et al., 2004</ref>) is a dialog corpus of multiparty meet- ings with 5 tags of dialog acts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>We setup our experimental evaluation, as follows: given a classification task and a dataset, we gen- erate an on-device model. The size of the model can be configured (by adjusting the projection ma- trix P) to fit in the memory footprint of the de- vice, i.e. a phone has more memory compared to a smart watch. For each classification task, we re- port Accuracy on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hyperparameter and Training</head><p>For both datasets we used the following: 2- layer SGNN (P T =80,d=14 × FullyConnected 256 × FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay ( <ref type="bibr" target="#b15">Loshchilov and Hutter, 2016)</ref>. Unlike prior approaches ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016;</ref><ref type="bibr" target="#b17">Ortega and Vu, 2017</ref>) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored. Instead, features are computed on the fly and are dynamically compressed via the pro- jection matrices into projection vectors. These val- ues were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning. Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (Sutskever et al., 2013), run for 1M steps. <ref type="table" target="#tab_1">Tables 2 and 3</ref> show results on the SwDA and MRDA dialog act datasets. Overall, our SGNN model consistently outperforms the baselines and prior state-of-the-art deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We compare our model against a majority class baseline and Naive Bayes classifier ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref>. Our model significantly outper- forms both baselines by 12 to 35% absolute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison against State-of-art Methods</head><p>We also compare our performance against prior work using HMMs ( <ref type="bibr" target="#b24">Stolcke et al., 2000</ref>) and re- cent deep learning methods like CNN ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref>, <ref type="bibr">RNN (Khanpour et al., 2016)</ref> and RNN with gated attention ( <ref type="bibr" target="#b27">Tran et al., 2017)</ref>.</p><p>To the best of our knowledge, ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016;</ref><ref type="bibr" target="#b17">Ortega and Vu, 2017;</ref><ref type="bibr" target="#b27">Tran et al., 2017)</ref> are the latest approaches in dialog act clas- sification, which also reported on the same data splits. Therefore, we compare our research against these works. According to <ref type="bibr" target="#b17">(Ortega and Vu, 2017)</ref>, prior work by <ref type="bibr" target="#b8">(Ji and Bilmes, 2006</ref>) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.</p><p>For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 ac- curacy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref> and RNN vari- ants ( <ref type="bibr" target="#b11">Khanpour et al., 2016;</ref><ref type="bibr" target="#b17">Ortega and Vu, 2017)</ref>. We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing low- dimensional semantic text representations that are useful for text classification applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion on Model Size and Inference</head><p>LSTMs have millions of parameters, while our on-device architecture has just 300K parameters (order of magnitude lower). Most deep learning methods also use large vocabulary size of 10K or higher. Each word embedding is represented as 100-dimensional vector leading to a storage re- quirement of 10, 000 × 100 parameter weights just in the first layer of the deep network. In con- trast, SGNNs in all our experiments use a fixed 1120-dimensional vector regardless of the vocab- ulary or feature size, dynamic computation results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Majority Class (baseline) <ref type="bibr" target="#b17">(Ortega and Vu, 2017)</ref> 33.7 Naive Bayes (baseline) ( <ref type="bibr" target="#b11">Khanpour et al., 2016)</ref> 47.3 HMM ( <ref type="bibr" target="#b24">Stolcke et al., 2000)</ref> 71.0 DRLM-conditional training <ref type="bibr" target="#b8">(Ji and Bilmes, 2006</ref>) 77.0 DRLM-joint training <ref type="bibr" target="#b8">(Ji and Bilmes, 2006)</ref> 74.0 LSTM ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref> 69.9 CNN ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref> 73.1 Gated-Attention&amp;HMM ( <ref type="bibr" target="#b27">Tran et al., 2017)</ref> 74.2 RNN+Attention ( <ref type="bibr" target="#b17">Ortega and Vu, 2017)</ref> 73.8 RNN ( <ref type="bibr" target="#b11">Khanpour et al., 2016)</ref> 80.1 SGNN: Self-Governing Neural Network (ours) 83.1  <ref type="bibr" target="#b17">(Ortega and Vu, 2017)</ref> 59.1 Naive Bayes (baseline) ( <ref type="bibr" target="#b11">Khanpour et al., 2016)</ref> 74.6 Graphical Model ( <ref type="bibr" target="#b8">Ji and Bilmes, 2006)</ref> 81.3 CNN ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref> 84.6 RNN+Attention( <ref type="bibr" target="#b17">Ortega and Vu, 2017)</ref> 84.3 RNN ( <ref type="bibr" target="#b11">Khanpour et al., 2016)</ref> 86.8 SGNN: Self-Governing Neural Network (ours) 86.7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods ( <ref type="bibr" target="#b13">Lee and Dernoncourt, 2016;</ref><ref type="bibr" target="#b11">Khanpour et al., 2016;</ref><ref type="bibr" target="#b17">Ortega and Vu, 2017)</ref>. We introduced a compression technique that effectively captures low-dimensional semantic representation and pro- duces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and ef- ficiently computes the projection vectors on the fly. In the future, we are interested in extend- ing this approach to more natural language tasks. For instance, we built a multilingual SGNN model for customer feedback classification ( <ref type="bibr" target="#b14">Liu et al., 2017)</ref> and obtained 73% on Japanese, close to best performing system on the challenge <ref type="bibr" target="#b19">(Plank, 2017)</ref>. Unlike their method, we did not use any pre-processing, tagging, parsing, pre-trained em- beddings or other resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Self-Governing Neural Network (SGNN) architecture.</figDesc><graphic url="image-1.png" coords="3,162.16,62.81,272.13,281.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>SwDA Dataset Results 

Method 
Acc. 
Majority Class (baseline)</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second).</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Andreas Stolcke, and Chuck Wooters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janin</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5TH SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>the 5TH SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="364" to="367" />
		</imprint>
	</monogr>
	<note>The icsi meeting corpus</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fastex: Hash clustering with exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><forename type="middle">M</forename><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2798" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural graph machines: Learning neural networks using graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramavajjala</surname></persName>
		</author>
		<idno>abs/1703.04818</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural graph learning: Training neural networks using graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramavajjala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing, STOC &apos;02</title>
		<meeting>the Thiry-fourth Annual ACM Symposium on Theory of Computing, STOC &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 IEEE International Conference on Acoustics</title>
		<meeting>the 1992 IEEE International Conference on Acoustics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backoff model training using partially observed data: Application to dialog act tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic detection of discourse structure for speech recognition and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel Martin Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Ess-Dykema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smart reply: Automated response suggestion for email</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balint</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><surname>Lukacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramavajjala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dialogue act classification in domain-independent conversations using a deep recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Khanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishitha</forename><surname>Guntakandla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2012" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="515" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ijcnlp-2017 task 4: Customer feedback analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasufumi</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declan</forename><surname>Groves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP 2017, Shared Tasks</title>
		<meeting>the IJCNLP 2017, Shared Tasks<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neuralbased context representation learning for dialog act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="247" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting the predictability of language: Response completion in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1489" to="1499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">All-in-1 at ijcnlp-2017 task 4: Short text classification with one model for all languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP 2017</title>
		<meeting>the IJCNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Asian Federation of Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tasks</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="143" to="148" />
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable decipherment for machine translation via hash sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Projectionnet: Learning efficient on-device deep networks using neural projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<idno>abs/1708.00630</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The icsi meeting recorder dialog act (mrda) corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Carvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>the 5th SIGdial Workshop on Discourse and Dialogue<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>III-1139-III-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A generative attentional neural network model for dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Quan Hung Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zukerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="529" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
