<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Adaptation Layers for Cross-domain Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><forename type="middle">Yuchen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Adaptation Layers for Cross-domain Named Entity Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2012" to="2022"/>
							<date type="published">October 31-November 4, 2018. 2018. 2012</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent research efforts have shown that neural architectures can be effective in conventional information extraction tasks such as named entity recognition, yielding state-of-the-art results on standard newswire datasets. However, despite significant resources required for training such models, the performance of a model trained on one domain typically degrades dramatically when applied to a different domain, yet extracting entities from new emerging domains such as social media can be of significant interest. In this paper, we empirically investigate effective methods for conveniently adapting an existing, well-trained neural NER model for a new domain. Unlike existing approaches , we propose lightweight yet effective methods for performing domain adaptation for neural models. Specifically, we introduce adaptation layers on top of existing neu-ral architectures, where no retraining using the source domain data is required. We conduct extensive empirical studies and show that our approach significantly outperforms state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) focuses on ex- tracting named entities in a given text while iden- tifying their underlying semantic types. Most earlier approaches to NER are based on conven- tional structured prediction models such as condi- tional random fields (CRF) ( <ref type="bibr" target="#b14">Lafferty et al., 2001;</ref><ref type="bibr" target="#b33">Sarawagi and Cohen, 2004</ref>), relying on hand- crafted features which can be designed based on domain-specific knowledge <ref type="bibr" target="#b44">(Yang and Cardie, 2012;</ref><ref type="bibr" target="#b24">Passos et al., 2014;</ref><ref type="bibr" target="#b19">Luo et al., 2015)</ref>. Re- cently, neural architectures have been shown ef- fective in such a task, whereby minimal feature en- gineering is required ( <ref type="bibr" target="#b15">Lample et al., 2016;</ref><ref type="bibr" target="#b20">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b27">Peters et al., 2017;</ref><ref type="bibr" target="#b18">Liu et al., 2018)</ref>. Domain adaptation, as a special case for transfer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Word Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fine-Tuning with</head><p>Target Data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Training with Source Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Parameter Initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLSTMs</head><p>Training with Random Samples from Source or Target Data learning, aims to exploit the abundant data of well- studied source domains to improve the perfor- mance in target domains of interest ( <ref type="bibr" target="#b23">Pan and Yang, 2010;</ref><ref type="bibr" target="#b42">Weiss et al., 2016)</ref>. There is a growing in- terest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (mul- titask learning), have been proposed for studying the transferrability of neural networks under tasks such as sentence (pair) classification ( <ref type="bibr" target="#b22">Mou et al., 2016</ref>) and sequence labeling ( <ref type="bibr" target="#b46">Yang et al., 2017b</ref>). The INIT method first trains a model using la- beled data from the source domain; next, it ini- tializes a target model with the learned param- eters; finally, it fine-tunes the initialized target model using labeled data from the target domain. The MULT method, on the other hand, simul- taneously trains two models using both source and target data respectively, where some param- eters are shared across the two models during the learning process. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the two ap- proaches based on the BLSTM-CRF (bidirectional LSTM augmented with a CRF layer) architecture for NER. While such approaches make intuitive senses, they also come with some limitations.</p><p>First, these methods utilize shared domain- general word embeddings when performing learn- ing from both source and target domains. This es- sentially assumes there is no domain shift of input feature spaces. However, cases when the two do- mains are distinct (words may contain different se-Bob <ref type="bibr">[B,o,b]</ref> Dylan <ref type="bibr">[D,y,l,a,n]</ref> visited <ref type="bibr">[v,i,s,i,t,e,d]</ref> [S,w,e,d,e,n]</p><p>Sweden CRF Layer I-PER O</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-GPE B-PER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional LSTM Layer</head><p>Bob <ref type="bibr">[B,o,b]</ref> Dylan <ref type="bibr">[D,y,l,a,n]</ref> visited <ref type="bibr">[v,i,s,i,t,e,d]</ref> [S,w,e,d,e,n]</p><p>Sweden CRF Layer I-PER O</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-GPE B-PER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional LSTM Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Adaptation Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Adaptation Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Adaptation Layer</head><p>Figure 2: Our base model (left) and target model (right) where we insert new adaptation layers (highlighted in red).</p><p>mantics across two domains), we believe such an assumption can be weak. Second, existing approaches such as INIT di- rectly augment the LSTM layer with a new out- put CRF layer when learning models for the tar- get domain. One basic assumption involved here is that the model would be able to re-construct a new CRF layer that can capture not only the vari- ation of the input features (or hidden states out- putted from LSTM) to the final CRF layer across two domains, but also the structural dependencies between output nodes in the target output space. We believe this overly restrictive assumption may prevent the model from capturing rich, complex cross-domain information due to the inherent lin- ear nature of the CRF model. Third, most methods involving cross-domain embedding often require highly time-consuming retraining word embeddings on domain-specific corpora. This makes it less realistic in scenarios where source corpora are huge or even inacces- sible. Also, MULT-based methods need retrain- ing on the source-domain data for different tar- get domains. We think this disadvantage of exist- ing methods hinders the neural domain adaptation methods for NER to be practical in real scenarios.</p><p>In this work, we propose solutions to address the above-mentioned issues. Specifically, we ad- dress the first issue at both the word and sentence level by introducing a word adaptation layer and a sentence adaptation layer respectively, bridging the gap between the two input spaces. Similarly, an output adaptation layer is also introduced be- tween the LSTM and the final CRF layer, captur- ing the variations in the two output spaces. Fur- thermore, we introduce a single hyper-parameter that controls how much information we would like to preserve from the model trained on the source domain. These approaches are lightweight, without requiring re-training on data from the source domain. We show through extensive em- pirical analysis as well as ablation study that our proposed approach can significantly improve the performance of cross-domain NER over existing transfer approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Base Model</head><p>We briefly introduce the BLSTM-CRF architec- ture for NER, which serves as our base model throughout this paper. Our base model is the com- bination of two recently proposed popular works for named entity recognition by <ref type="bibr" target="#b15">Lample et al. (2016)</ref> and <ref type="bibr" target="#b20">Ma and Hovy (2016)</ref>. <ref type="figure">Figure 2</ref> illus- trates the BLSTM-CRF architecture.</p><p>Following <ref type="bibr" target="#b15">Lample et al. (2016)</ref>, we de- velop the comprehensive word representations by concatenating pre-trained word embeddings and character-level word representations, which are constructed by running a BLSTM over sequences of character embeddings. The middle BLSTM layer takes a sequence of comprehensive word representations and produces a sequence of hid- den states, representing the contextual information of each token. Finally, following <ref type="bibr" target="#b20">Ma and Hovy (2016)</ref>, we build the final CRF layer by utiliz- ing potential functions describing local structures to define the conditional probabilities of complete predictions for the given input sentence. This architecture is selected as our base model due to its generality and representativeness. We note that several recently proposed models ( <ref type="bibr" target="#b27">Peters et al., 2017;</ref><ref type="bibr" target="#b18">Liu et al., 2018)</ref> are built based on it. As our focus is on how to better transfer such architectures for NER, we include further discus- sions of the model and training details in our sup- plementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>We first introduce our proposed three adaptation layers and describe the overall learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Adaptation Layer</head><p>Most existing transfer approaches use the same domain-general word embeddings for training both source and target models. Assuming that there is little domain shift of input feature spaces, they simplify the problem as homogeneous trans- fer learning <ref type="bibr" target="#b42">(Weiss et al., 2016)</ref>. However, this simplified assumption becomes weak when two domains have apparently different language styles and involve considerable domain-specific terms that may not share the same semantics across the domains; for example, the term "cell" has differ- ent meaning in biomedical articles and product reviews. Furthermore, some important domain- specific words may not be present in the vocab- ulary of domain-general embeddings. As a result, we have to regard such words as out-of-vacabulary (OOV) words, which may be harmful to the trans- fer learning process. <ref type="bibr" target="#b36">Stenetorp et al. (2012)</ref> show that domain- specific word embeddings tend to perform better when used in supervised learning tasks. 1 How- ever, maintaining such an improvement in the transfer learning process is very challenging. This is because two domain-specific embeddings are trained separately on source and target datasets, and therefore the two embedding spaces are het- erogeneous. Thus, model parameters trained in each model are heterogeneous as well, which hin- ders the transfer process. How can we address such challenges while maintaining the improve- ment by using domain-specific embeddings?</p><p>We address this issue by developing a word adaptation layer, bridging the gap between the source and target embedding spaces, so that both input features and model parameters be- <ref type="bibr">1</ref> We also confirm this claim with experiments presented in supplementary materials. come homogeneous across domains. Popular pre-trained word embeddings are usually trained on very large corpora, and thus methods requir- ing re-training them can be extremely costly and time-consuming. We propose a straightforward, lightweight yet effective method to construct the word adaptation layer that projects the learned em- beddings from the target embedding space into the source space. This method only requires some corpus-level statistics from the datasets (used for learning embeddings) to build the pivot lexicon for constructing the adaptation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pivot Lexicon</head><p>A pivot word pair is denoted as (w s , w t ), where w s 2 X S and w t 2 X T . Here, X S and X T are source and target vocabularies. A pivot lexicon P is a set of such word pairs.</p><p>To construct a pivot lexicon, first, motivated by <ref type="bibr" target="#b38">Tan et al. (2015)</ref>, we define P 1 , which con- sists of the ordinary words that have high rela- tive frequency in both source and target domain corpora:</p><formula xml:id="formula_0">P 1 = {(w s , w t )|w s = w t , f(w s ) s , f(w t ) t }, where f (w)</formula><p>is the frequency function that returns the number of occorrence of the word w in the dataset, and s and t are word frequency thresholds 2 . Optionally, we can utilize a customized word-pair list P 2 , which gives map- pings between domain-specific words across do- mains, such as normalization lexicons (Han and Baldwin, 2011; <ref type="bibr" target="#b17">Liu et al., 2012</ref>). The final lexicon is thus defined as P = P 1 [ P 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Projection Learning</head><p>Mathematically, given the pre-trained domain- specific word embeddings V S and V T as well as a pivot lexicon P, we would like to learn a lin- ear projection transforming word vectors from V T into V S . This idea is based on a bilingual word embedding model ( <ref type="bibr" target="#b21">Mikolov et al., 2013</ref>), but we adapt it to this domain adaptation task.</p><p>We first construct two matrices V ⇤ S and V ⇤ T , where the i-th rows of these two matrices are the vector representations for the words in the i-th en- try of P: P (i) = (w i s , w i t ). We use V ⇤i S to denote the vector representation of the word w i s , and sim- ilarly for V ⇤i T and w i t . Next, we learn a transformation matrix Z min- imizing the distances between V ⇤ S and V ⇤ T Z with the following loss function, where c i is the con- fidence coefficient for the entry P (i) , highlighting the significance of the entry:</p><formula xml:id="formula_1">argmin Z |P| X i=1 ci V ⇤i T Z V ⇤i S 2 ,</formula><p>We use normalized frequency ( ¯ f ) and Sørensen- Dice coefficient <ref type="bibr">(Sørensen, 1948)</ref> to describe the significance of each word pair:</p><formula xml:id="formula_2">¯ f (ws) = f (ws) max w 0 2X S f (w 0 ) , ¯ f (wt) = f (wt) max w 0 2X T f (w 0 ) ci = 2 · ¯ f (w i s ) · ¯ f (w i t ) ¯ f (w i s ) + ¯ f (w i t )</formula><p>The intuition behind this scoring method is that a word pair is more important when both words have high relative frequency in both domains. This is because such words are likely more domain- independent, conveying identical or similar se- mantics across these two different domains. Now, the matrix Z exactly gives the weights to the word adaptation layer, which takes in the target domain word embeddings and returns the trans- formed embeddings in the new space. We learn Z with stochastic gradient descent. After learning, the projected new embeddings would be V T Z, which would be used in the subsequent steps as illustrated in <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 3</ref>. With such a word-level input-space transformation, the pa- rameters of the pre-trained source models based on V S can still be relevant, which can be used in subsequent steps.</p><p>We would like to highlight that, unlike many previous approaches to learning cross-domain word embeddings ( <ref type="bibr" target="#b2">Bollegala et al., 2015;</ref><ref type="bibr" target="#b45">Yang et al., 2017a</ref>), the learning of our word adapta- tion layer involves no modifications to the source- domain embedding spaces. It also requires no re- training of the embeddings based on the target- domain data. Such a distinctive advantage of our approach comes with some important practical im- plications: it essentially enables the transfer learn- ing process to work directly on top of a well- trained model by performing adaptation without involving significant re-training efforts. For ex- ample, the existing model could be one that has already gone through extensive training, tuning and testing for months based on large datasets with embeddings learned from a particular domain (which may be different from the target domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Adaptation Layer</head><p>The word adaptation layer serves as a way to bridge the gap of heterogeneous input spaces, but it does so only at the word level and is context in- dependent. We can still take a step further to ad- dress the input space mismatch issue at the sen- tence level, capturing the contextual information in the learning process of such a mapping based on labeled data from the target domain. To this end, we augment a BLSTM layer right after the word adaptation layer (see <ref type="figure">Figure 2)</ref>, and we name it the sentence adaptation layer.</p><p>This layer pre-encodes the sequence of pro- jected word embeddings for each target instance, before they serve as inputs to the LSTM layer in- side the base model. The hidden states for each word generated from this layer can be seen as the further transformed word embeddings captur- ing target-domain specific contextual information, where such a further transformation is learned in a supervised manner based on target-domain an- notations. We also believe that with such a sen- tence adaptation layer, the OOV issue mentioned above may also be partially alleviated. This is because without such a layer, OOV words would all be mapped to a single fixed vector representa- tion -which is not desirable; whereas with such a sentence adaptation layer, each OOV word would be assigned their "transformed" embeddings based on its respective contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Adaptation Layer</head><p>We focus on the problem of performing domain adaptation for NER under a general setup, where we assume the set of output labels for the source and target domains could be different. Due to the heterogeneousness of output spaces, we have to re- construct the final CRF layer in the target models.</p><p>However, we believe solely doing this may not be enough to address the labeling difference prob- lem as highlighted in <ref type="bibr" target="#b11">(Jiang, 2008)</ref> as the two out- put spaces may be very different. For example, in the sentence "Taylor released her new songs", "Taylor" should be labeled as "MUSIC-ARTIST" instead of "PERSON" in some social media NER datasets; this suggests re-classifying with contex- tual information is necessary. In another exam- ple, where we have a tweet "so...#kktny in 30 mins?"; here we should recognize "#kktny" as a CREATIVE-WORK entity, but there is little sim- ilar instances in newswire data, indicating that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Adaptation Layer</head><p>Pre-trained Source Model <ref type="figure">Figure 3</ref>: Overview of our transfer learning process.</p><p>context-aware re-recognition is also needed. How can we perform re-classification and re- recognition with contextual information in the tar- get model? We answer this question by insert- ing a BLSTM output adaptation layer in the base model, right before the final CRF layer, to capture variations in outputs with contextual information. This output adaption layer takes the output hidden states from the BLSTM layer from the base model as its inputs, producing a sequence of new hidden states for the re-constructed CRF layer. Without this layer, the learning process directly updates the pre-trained parameters of the base model, which may lead to loss of knowledge that can be trans- ferred from the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Learning Process</head><p>Figure 3 depicts our overall learning process. We initialize the base model with the parameters from the pre-trained source model, and tune the weights for all layers -including layers from the base model, sentence and output adaptation layers, and the CRF layer. We use different learning rates when updating the weights in different layers us- ing Adam ( <ref type="bibr" target="#b13">Kingma and Ba, 2014)</ref>. In all our ex- periments, we fixed the weights to Z for the word adaptation layers to avoid over-fitting. This al- lows us to preserve the knowledge learned from the source domain while effectively leveraging the limited training data from the target domain.</p><p>Similar to <ref type="bibr" target="#b46">Yang et al. (2017b)</ref>, who utilizes a hyper-parameter for controlling the transferabil- ity, we also introduce a hyper-parameter that serves a similar purpose -it captures the relation between the learning rate used for the base model (↵ base ) and the learning rate used for the adaptation layers plus the final CRF layer ↵ base = · ↵ adapt . If = 0, we fix the learned parameters (from source domain) of the base model completely (Ours-Frozen). If = 1, we treat all the layers equally (Ours-FineTune).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section, we present the setup of our exper- iments. We show our choice for source and target domains, resources for embeddings, the datasets for evaluation and finally the baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Source and Target Domains</head><p>We evaluate our approach with the setting that the source domain is newswire and the target domain is social media. We designed this experimental setup based on the following considerations:</p><p>• Challenging: Newswire is a well-studied do- main for NER and existing neural models perform very well (around 90.0 F1-score ( <ref type="bibr" target="#b20">Ma and Hovy, 2016)</ref>). However, the perfor- mance drop dramatically in social media data (around 60.0 F-score ( <ref type="bibr" target="#b37">Strauss et al., 2016)</ref>).</p><p>• Important: Social media is a rich soil for text mining ( <ref type="bibr" target="#b28">Petrovic et al., 2010;</ref><ref type="bibr">Rosenthal and McKeown, 2015;</ref><ref type="bibr" target="#b39">Wang and Yang, 2015)</ref>, and NER is of significant importance for other information extraction tasks in so- cial media ( <ref type="bibr" target="#b29">Ritter et al., 2011a;</ref><ref type="bibr" target="#b25">Peng and Dredze, 2016;</ref><ref type="bibr" target="#b3">Chou et al., 2016</ref>).</p><p>• Representative: The noisy nature of user gen- erated content as well as emerging entities with novel surface forms make the domain shift very salient ( <ref type="bibr" target="#b8">Finin et al., 2010;</ref><ref type="bibr" target="#b10">Han et al., 2016</ref>). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two do- mains so long as we have the necessary resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Resources for Cross-domain Embeddings</head><p>We utilizes GloVe ( <ref type="bibr" target="#b26">Pennington et al., 2014</ref>) to train domain-specific and domain-general word embeddings from different corpora, denoted as follows: 1) source emb, which is trained on the newswire domain corpus (NewYorkTimes and Dai- lyMail articles); 2) target emb, which is trained on a social media corpus (Archive Team's Twit- ter stream grab 3 ); 3) general emb, which is pre- trained on CommonCrawl containing both formal <ref type="table" target="#tab_3">CO  ON  RI  WN  #train token 204,567 848,220 37,098 46,469  #dev token  51,578  144,319  4,461  16,261  #test token  46,666  49,235  4,730  61,908  #train sent.  14,987  33,908  1,915  2,394  #dev sent.  3,466  5,771  239  1,000  #test sent.  3,684  1,898  240  3,856  #entity type  4  11  10  10   Table 1</ref>: Statistics of the NER datasets. and user-generated content. <ref type="bibr">4</ref> We obtain the inter- section of the top 5K words from source and tar- get vocabularies sorted by frequency to build P 1 . For P 2 , we utilize an existing twitter normaliza- tion lexicon containing 3,802 word pairs( <ref type="bibr" target="#b17">Liu et al., 2012</ref>). More details are in supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NER Datasets for Evaluation</head><p>For the source newswire domain, we use the following two datasets: OntoNotes-nw -the newswire section of OntoNotes 5.0 release dataset (ON) ( <ref type="bibr" target="#b41">Weischedel et al., 2013</ref>) that is publicly available <ref type="bibr">5</ref> , as well as the CoNLL03 NER dataset (CO) ( <ref type="bibr" target="#b32">Sang and Meulder, 2003)</ref>. For the first dataset, we randomly split the dataset into three sets: 80% for training, 15% for development and 5% for testing. For the second dataset, we follow their provided standard train-dev-test split. For the target domain, we consider the following two datasets: Ritter11 (RI) ( <ref type="bibr" target="#b30">Ritter et al., 2011b</ref>) and WNUT16 (WN) (Strauss et al., 2016), both of which are publicly available. The statistics of the four datasets we used in the paper are shown in Ta- ble 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Transfer Approaches</head><p>We present the baseline approaches, which were originally investigated by <ref type="bibr" target="#b22">Mou et al. (2016</ref> MULT: Multi-task learning based transfer method simultaneously trains M S and M T us- ing D S and D T . The parameters of M S and M T excluding their CRF layers are shared during the training process. Both <ref type="bibr" target="#b22">Mou et al. (2016)</ref> and <ref type="bibr" target="#b46">Yang et al. (2017b)</ref> follow <ref type="bibr" target="#b4">Collobert and Weston (2008)</ref> and use a hyper-parameter as the probability of choosing an instance from D S instead of D T to optimize the model parameters. By selecting the hyper-parameter , the multi-task learning process tends to perform better in target domains. Note that this method needs re-training of the source model with D S every time we would like to build a new target model, which can be time-consuming especially when D S is large.</p><p>MULT+INIT: This is a combination of the above two methods. We first use INIT to initial- ize the target model. Afterwards, we train the two models as what MULT does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We primarily focus on the discussion of experi- ments with a particular setup where D S is set to OntoNotes-nw and D T is Ritter11. In the exper- iments, "in-domain" means we only use D T to train our base model without any transfer from the source domain. "" represents the amount of im- provement we can obtain (in terms of F measure) using transfer learning over "in-domain" for each transfer method. The hyper-parameters and are tuned from {0.1, 0.2, ..., 1.0} on the develop- ment set, and we show the results based on the developed hyper-parameters.</p><p>We first conduct the first set of experiments to evaluate performance of different transfer meth- ods under the assumption of homogeneous input spaces. Thus, we utilize the same word embed- dings (general emb) for training both source and target models. Consequently we remove the word adaptation layer (cd emb) in our approach under this setting. The results are listed in <ref type="table" target="#tab_3">Table 2</ref>. As we can observe, the INIT-Frozen method leads to a slight "negative transfer", which is also re- ported in the experiments of <ref type="bibr" target="#b22">Mou et al. (2016)</ref>. This indicates that solely updating the parame- ters of the final CRF layer is not enough for per- forming re-classification and re-recognition of the named entities for the new target domain. The INIT-FineTune method yields better results for it also updates the parameters of the middle LSTM  layers in the base model to mitigate the heteroge- neousness. The MULT and MULT+INIT meth- ods yield higher results, partly due to the fact that they can better control the amount of transfer through tuning the hyper-parameter. Our proposed transfer approach outperforms all these baseline approaches. It not only controls the amount of transfer across the two domains but also explicitly captures variations in the input and output spaces when there is significant domain shift.</p><p>We use the second set of experiments to un- derstand the effectiveness of each method when dealing with heterogeneous input spaces. We use source emb for training source models and tar- get emb for learning the target models. From the results shown in <ref type="table" target="#tab_5">Table 3</ref>, we can find that all the baseline methods degrade when the two word embeddings used for training source mod- els and learning target models are different from each other. The heterogeneousness of input fea- ture space hinders them to better use the infor- mation from source domains. However, with the help of word and sentence adaptation layers, our method achieves better results. The experiment on learning without the word adaptation layer also confirms the importance of such a layer. <ref type="bibr">6</ref> Our results are also comparable to the re- sults when the cross-lingual embedding method of <ref type="bibr" target="#b45">Yang et al. (2017a)</ref> is used instead of the word adaptation layer. However, as we mentioned ear- lier, their method requires re-training the embed- dings from target domain, and is more expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Test</head><p>To investigate the effectiveness of each component of our method, we conduct ablation test based on our full model (F =66.40) reported in <ref type="table" target="#tab_5">Table 3</ref>. We use to denote the differences of the performance between each setting and our model. The results <ref type="bibr">6</ref> We use the approximate randomization test <ref type="bibr" target="#b47">(Yeh, 2000)</ref> for statistical significance of the difference between "Ours" and "MULT+INIT". Our improvements are statistically sig- nificant with a p-value of 0.0033.  of ablation test are shown in <ref type="table" target="#tab_6">Table 4</ref>. We first set to 0 and 1 respectively to investigate the two special variant (Ours-Frozen, Ours-FineTune) of our method. We find they both perform worse than using the optimal (0.6).</p><p>One natural concern is whether our perfor- mance gain is truly caused by the effective ap- proach for cross-domain transfer learning, or is simply because we use a new architecture with more layers (that is built on top of the base model) for learning the target model. To understand this, we carry out an experiment named "w/o transfer" by setting to 1, where we randomly initialize the parameters of the middle BLSTMs in the tar- get model instead of using source model parame- ters. Results show that such a model does not per- form well, confirming the effectiveness of transfer learning with our proposed adaptation layers. Re- sults also confirm the importance of all the three adaptation layers that we introduced. Learning the confidence scores (c i ) and employing the optional P 2 are also helpful but they appear to be playing less significant roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Additional Experiments</head><p>As shown in <ref type="table" target="#tab_7">Table 5</ref>, we conduct some additional experiments to investigate the significance of our improvements on different source-target domains, and whether the improvement is simply because of the increased model expressiveness due to a larger number of parameters. <ref type="bibr">7</ref> We first set the hidden dimension to be the same as the dimension of source-domain word embed- dings for the sentence adaptation layer, which is 200 (I/M-200). The dimension used for the out- put adaptation layer is just half of that of the base BLSTM model. Overall, our model roughly <ref type="bibr">7</ref> In this table, I-200/300 and M-200/300 represent the best performance from {INIT-Frozen, INIT-FineTune} and {MULT, MULT+INIT} respectively after tuning; "in-do." here is the best score of our base model without transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>F =0.0 Ours-Frozen 63.95 -2.45 =1.0 Ours-FineTune 63.40 -3.00 =1.0 w/o transfer 63.26 -3.14 =0.6 w/o using confidence ci 66.04 -0.36 =0.6 w/o using P2 66.11 -0.29 =0.6 w/o word adapt. layer 64.51 -1.89 =0.6 w/o sentence adapt. layer 65.25 -1.15 =0.6 w/o output adapt. layer 64.84 -1.56  involves 117.3% more parameters than the base model. 8 To understand the effect of a larger pa- rameter size, we further experimented with hidden unit size as 300 (I/M-300), leading to a parame- ter size of 213, 750 that is comparable to "Ours" <ref type="bibr">(203,</ref><ref type="bibr">750)</ref>. As we can observe, our approach out- performs these approaches consistently in the four settings. More experiments with other settings can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Target-Domain Data Size</head><p>To assess the effectiveness of our approach when we have different amounts of training data from the target domain, we conduct additional experi- ments by gradually increasing the amount of the target training data from 10% to 100%. We again use the OntoNotes-nw and Ritter11 as D S and D T , respectively. Results are shown in <ref type="figure">Figure 4</ref>. Ex- periments for INIT and MULT are based on the respective best settings used in <ref type="table" target="#tab_7">Table 5</ref>. We find that the improvements of baseline methods tend to be smaller when the target training set becomes larger. This is partly because INIT and MULT do not explicitly preserve the parameters from source models in the constructed target models. Thus, the transferred information is diluted while we train the target model with more data. In contrast, our transfer method explicitly saves the transferred in- formation in the base part of our target model, and we use separate learning rates to help the target model to preserve the transferred knowledge. Sim-  ilar experiments on other datasets are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of the Hyper-parameter</head><p>We present a set of experiments around the hyper- parameter in <ref type="figure" target="#fig_2">Figure 5</ref>. Such experiments over different datasets can shed some light on how to select this hyper-parameter. We find that when the target data set is small (Ritter11), the best are 0.5 and 0.6 respectively for the two source do- mains, whereas when the target data set is larger (WNUT16), the best becomes 0.7 and 0.8. The results suggest that the optimal tends to be rela- tively larger when the target data set is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Domain adaptation and transfer learning has been a popular topic that has been extensively stud- ied in the past few years ( <ref type="bibr" target="#b23">Pan and Yang, 2010)</ref>. For well-studied conventional feature-based mod- els in NLP, there are various classic transfer ap- proaches, such as EasyAdapt <ref type="bibr" target="#b5">(Daumé, 2007)</ref>, instance weighting ( <ref type="bibr" target="#b12">Jiang and Zhai, 2007</ref>) and structural correspondence learning <ref type="bibr" target="#b1">(Blitzer et al., 2006</ref>). Fewer works have been focused on trans- fer approaches for neural models in NLP. <ref type="bibr" target="#b22">Mou et al. (2016)</ref> use intuitive transfer methods (INIT and MULT) to study the transferability of neu- ral network models for the sentence (pair) clas- sification problem; <ref type="bibr" target="#b16">Lee et al. (2017)</ref> utilize the INIT method on highly related datasets of elec- tronic health records to study their specific de- identification problem. <ref type="bibr" target="#b46">Yang et al. (2017b)</ref> use the MULT approach in sequence tagging tasks in- cluding named entity recognition. Following the MULT scheme, <ref type="bibr" target="#b40">Wang et al. (2018)</ref> introduce a label-aware mechanism into maximum mean dis- crepancy (MMD) to explicitly reduce domain shift between the same labels across domains in medi- cal data. Their approach requires the output space to be the same in both source and target domains.</p><p>Note that the scenario in our paper is that the out- put spaces are different in two domains.</p><p>All these existing works do not use domain- specific embeddings for different domains and they use the same neural model for source and target models. However, with our word adapta- tion layer, it opens the opportunity to use domain- specific embeddings. Our approach also addresses the domain shift problem at both input and out- put level by re-constructing target models with our specifically designed adaptation layers. The hyper-parameter in our proposed methods and in MULT both control the knowledge transfer from source domain in the transfer learning pro- cess. While our method works on top of an ex- isting pre-trained source model directly, MULT needs re-training with source domain data each time they train a target model. <ref type="bibr" target="#b6">Fang and Cohn (2017)</ref> add an "augmented layer" before their final prediction layer for cross- lingual POS tagging -which is a simple multi- layer perceptron performing local adaptation for each token separately -ignoring contextual in- formation. In contrast, we employ a BLSTM layer due to its ability in capturing contextual infor- mation, which was recently shown to be crucial for sequence labeling tasks such as NER ( <ref type="bibr" target="#b20">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b15">Lample et al., 2016</ref>). We also notice that a similar idea to ours has been used in the re- cently proposed Deliberation Network ( <ref type="bibr" target="#b43">Xia et al., 2017</ref>) for the sequence generation task, where a second-pass decoder is added to a first-pass de- coder to polish sequences generated by the latter.</p><p>We propose to learn the word adaptation layer in our task inspired by two prior studies. <ref type="bibr" target="#b6">Fang and Cohn (2017)</ref> use the cross-lingual word em- beddings to obtain distant supervision for tar- get languages. <ref type="bibr" target="#b45">Yang et al. (2017a)</ref> propose to re-train word embeddings on target domain by using regularization terms based on the source- domain embeddings, where some hyper-parameter tuning based on down-stream tasks is required. Our word adaptation layer serves as a linear- transformation ( <ref type="bibr" target="#b21">Mikolov et al., 2013)</ref>, which is learned based on corpus level statistics. Although there are alternative methods that also learn a map- ping between embeddings learned from different domains <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b0">Artetxe et al., 2016;</ref><ref type="bibr" target="#b34">Smith et al., 2017)</ref>, such methods usually in- volve modifying source domain embeddings, and thus re-training of the source model based on the modified source embeddings would be required for the subsequent transfer process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a novel, lightweight transfer learn- ing approach for cross-domain NER with neural networks. Our introduced transfer method per- forms adaptation across two domains using adap- tation layers augmented on top of the existing neu- ral model. Through extensive experiments, we demonstrated the effectiveness of our approach, reporting better results over existing transfer meth- ods. Our approach is general, which can be poten- tially applied to other cross-domain structured pre- diction tasks. Future directions include investiga- tions on employing alternative neural architectures such as convolutional neural networks (CNNs) as adaptation layers, as well as on how to learn the optimal value for from the data automatically rather than regarding it as a hyper-parameter. 9</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two existing adaptation approaches for NER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8</head><label></label><figDesc>Base Model: (25 ⇥ 50 + 50 2 ) + (250 ⇥ 200 + 200 2 ) = 93, 750; Ours: (25 ⇥ 50 + 50 2 ) + (200 ⇥ 200 + 200 2 ) + (250 ⇥ 200 + 200 2 ) + (200 ⇥ 100 + 100 2 ) = 203, 750.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F1Figure 5 :</head><label>5</label><figDesc>Figure 4: F1-score vs. amount of target-domain data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>M T and reconstruct the final CRF layer to address the issue of different output spaces. We use the learned parameters of M S to initialize M T excluding the CRF layer. Fi- nally, INIT-FineTune continues training M T with the target-domain training data D T , while INIT- Frozen instead only updates the parameters of the newly constructed CRF layer.</figDesc><table>). Lee 
et al. (2017) explored the INIT method for NER, 
while Yang et al. (2017b) extended the MULT 
method for sequence labeling. 
INIT: We first train a source model M S us-
ing the source-domain training data D S . Next, we 
construct a target model </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparisons of different methods for homo-
geneous input spaces. (D S = ON, D T = RI) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparisons of different methods for hetero-
geneous input spaces. (D S = ON, D T = RI) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparing different settings of our method. 

D S , D T 
in-do. 
I-200 
M-200 
I-300 
M-300 
Ours 
(ON, RI) 
63.37 
+0.41 
+1.44 
+0.43 
+1.48 
+3.03 
(CO, RI) 
+0.23 
+0.81 
+0.22 
+ 0.88 
+1.86 
(ON, WN) 
51.03 
+0.89 
+1.72 
+0.88 
+1.77 
+3.16 
(CO, WN) 
+0.69 
+1.04 
+0.71 
+1.13 
+2.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results of transfer learning methods on dif-
ferent datasets with different number of LSTM units in 
the base model. (I: INIT; M: MULT; 200/300: number 
of LSTM units). 

</table></figure>

			<note place="foot" n="2"> A simple way of setting them is to choose the frequency of the k-th word in the word lists sorted by frequencies.</note>

			<note place="foot" n="3"> https://archive.org/details/twitterstream</note>

			<note place="foot" n="4"> https://nlp.stanford.edu/projects/glove/ 5 https://catalog.ldc.upenn.edu/ldc2013t19</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their thoughtful and constructive comments. Most of this work was done when the first au-thor was visiting Singapore University of Tech-nology and Design (SUTD). This work is sup-ported by DSO grant DSOCL17061, and is par-tially supported by Singapore Ministry of Edu-cation Academic Research Fund (AcRF) Tier 1 SUTDT12015008. <ref type="bibr">9</ref> We make our supplementary material and code available at http://statnlp.org/research/ie.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosted web named entity recognition via tri-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Lung</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Yun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Asian and LowResource Lang. Inf. Process</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model transfer for tagging low-resource languages using a bilingual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotating named entities in twitter data with crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">W</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murnane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Mturk@HLT-NAACL</title>
		<meeting>of Mturk@HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a #twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Results of the WNUT16 named entity recognition shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Domain adaptation in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transfer learning for named-entity recognition with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06273</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A broadcoverage normalization system for social media language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint named entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, and Zaiqing Nie</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How transferable are neural networks in nlp applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5367</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Streaming first story detection with application to twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">2015. I couldn&apos;t agree more: The role of conversational structure in agreement and disagreement detection in online discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGDIAL</title>
		<meeting>of SIGDIAL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on danish commons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorvald</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Skr</title>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Size (and domain) matters: Evaluating semantic word space representations for biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SMBM</title>
		<meeting>of SMBM</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Results of the wnut16 named entity recognition shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bethany</forename><forename type="middle">E</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of W-NUT</title>
		<meeting>of W-NUT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lexical comparison between wikipedia and twitter corpora by using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Label-aware double transfer learning for cross-specialty medical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A survey of transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Journal of Big Data</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extracting opinion expressions with semi-markov conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A simple regularization-based algorithm for learning cross-domain word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
