<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Interestingness with Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Interestingness with Deep Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2" to="13"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a deep semantic similarity model (DSSM), a special type of deep neural networks designed for text analysis, for recommending target documents to be of interest to a user based on a source document that she is reading. We observe, identify, and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents, which we collect from commercial Web browser logs. The DSSM is trained on millions of Web transitions, and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized. The effectiveness of the DSSM is demonstrated using two interestingness tasks: automatic highlighting and contextual entity search. The results on large-scale, real-world da-tasets show that the semantics of documents are important for modeling interest-ingness and that the DSSM leads to significant quality improvement on both tasks, outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tasks of predicting what interests a user based on the document she is reading are fundamental to many online recommendation systems. A recent survey is due to <ref type="bibr">Ricci et al. (2011)</ref>. In this paper, we exploit the use of a deep semantic model for two such interestingness tasks in which document semantics play a crucial role: automatic highlight- ing and contextual entity search.</p><p>Automatic Highlighting. In this task we want a recommendation system to automatically dis- cover the entities (e.g., a person, location, organi- zation etc.) that interest a user when reading a doc- ument and to highlight the corresponding text spans, referred to as keywords afterwards. We show in this study that document semantics are among the most important factors that influence what is perceived as interesting to the user. For example, we observe in Web browsing logs that when a user reads an article about a movie, she is more likely to browse to an article about an actor or character than to another movie or the director.</p><p>Contextual entity search. After identifying the keywords that represent the entities of interest to the user, we also want the system to recommend new, interesting documents by searching the Web for supplementary information about these enti- ties. The task is challenging because the same key- words often refer to different entities, and interest- ing supplementary information to the highlighted entity is highly sensitive to the semantic context. For example, "Paul Simon" can refer to many peo- ple, such as the singer and the senator. Consider an article about the music of Paul Simon and an- other about his life. Related content about his up- coming concert tour is much more interesting in the first context, while an article about his family is more interesting in the second.</p><p>At the heart of these two tasks is the notion of interestingness. In this paper, we model and make use of this notion of interestingness with a deep semantic similarity model (DSSM). The model, extending from the deep neural networks shown recently to be highly effective for speech recogni- tion ( <ref type="bibr" target="#b21">Hinton et al., 2012;</ref><ref type="bibr" target="#b10">Deng et al., 2013</ref>) and computer vision ( <ref type="bibr" target="#b26">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b28">Markoff, 2014</ref>), is semantic because it maps docu- ments to feature vectors in a latent semantic space, also known as semantic representations. The model is deep because it employs a neural net- work with several hidden layers including a spe- cial convolutional-pooling structure to identify keywords and extract hidden semantic features at different levels of abstractions, layer by layer. The semantic representation is computed through a deep neural network after its training by back- propagation with respect to an objective tailored to the respective interestingness tasks. We obtain naturally occurring "interest" signals by observ- ing Web browser transitions, from a source docu- ment to a target document, in Web usage logs of a commercial browser. Our training data is sampled from these transitions.</p><p>The use of the DSSM to model interestingness is motivated by the recent success of applying re- lated deep neural networks to computer vision ( <ref type="bibr">Krizhevshy et al. 2012;</ref><ref type="bibr" target="#b28">Markoff, 2014)</ref>, speech recognition ( <ref type="bibr" target="#b21">Hinton et al. 2012</ref>), text processing ( <ref type="bibr" target="#b8">Collobert et al. 2011</ref>), and Web search ( <ref type="bibr" target="#b24">Huang et al. 2013</ref>). Among them, ( <ref type="bibr" target="#b24">Huang et al. 2013</ref>) is most relevant to our work. They also use a deep neural network to map documents to feature vec- tors in a latent semantic space. However, their model is designed to represent the relevance be- tween queries and documents, which differs from the notion of interestingness between documents studied in this paper. It is often the case that a user is interested in a document because it provides supplementary information about the entities or concepts she encounters when reading another document although the overall contents of the sec- ond documents is not highly relevant. For exam- ple, a user may be interested in knowing more about the history of University of Washington af- ter reading the news about President Obama's visit to Seattle. To better model interestingness, we extend the model of <ref type="bibr" target="#b24">Huang et al. (2013)</ref> in two significant aspects. First, while Huang et al. treat a document as a bag of words for semantic map- ping, the DSSM treats a document as a sequence of words and tries to discover prominent key- words. These keywords represent the entities or concepts that might interest users, via the convo- lutional and max-pooling layers which are related to the deep models used for computer vision ( <ref type="bibr">Krizhevsky et al., 2013</ref>) and speech recognition (Deng et al., 2013a) but are not used in Huang et al.'s model. The DSSM then forms the high-level semantic representation of the whole document based on these keywords. Second, instead of di- rectly computing the document relevance score using cosine similarity in the learned semantic space, as in <ref type="bibr" target="#b24">Huang et al. (2013)</ref>, we feed the fea- tures derived from the semantic representations of documents to a ranker which is trained in a super- vised manner. As a result, a document that is not highly relevant to another document a user is read- ing (i.e., the distance between their derived feature <ref type="bibr">1</ref> We stress here that, although the click signal is available to form a dataset and a gold standard ranker (to be described in vectors is big) may still have a high score of inter- estingness because the former provides useful in- formation about an entity mentioned in the latter. Such information and entity are encoded, respec- tively, by (some subsets of) the semantic features in their corresponding documents. In Sections 4 and 5, we empirically demonstrate that the afore- mentioned two extensions lead to significant qual- ity improvements for the two interestingness tasks presented in this paper.</p><p>Before giving a formal description of the DSSM in Section 3, we formally define the inter- estingness function, and then introduce our data set of naturally occurring interest signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Notion of Interestingness</head><p>Let be the set of all documents. Following <ref type="bibr" target="#b15">Gamon et al. (2013)</ref>, we formally define the inter- estingness modeling task as learning the mapping function:</p><p>: → where the function , is the quantified degree of interest that the user has in the target document ∈ after or while reading the source document ∈ .</p><p>Our notion of a document is meant in its most general form as a string of raw unstructured text. That is, the interestingness function should not rely on any document structure such as title tags, hyperlinks, etc., or Web interaction data. In our tasks, documents can be formed either from the plain text of a webpage or as a text span in that plain text, as will be discussed in Sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We can observe many naturally occurring mani- festations of interestingness on the Web. For ex- ample, on Twitter, users follow shared links em- bedded in tweets. Arguably the most frequent sig- nal, however, occurs in Web browsing events where users click from one webpage to another via hyperlinks. When a user clicks on a hyperlink, it is reasonable to assume that she is interested in learning more about the anchor, modulo cases of erroneous clicks. Aggregate clicks can therefore serve as a proxy for interestingness. That is, for a given source document, target documents that at- tract the most clicks are more interesting than doc- uments that attract fewer clicks 1 .</p><p>We collect a large dataset of user browsing events from a commercial Web browser. Specifi- cally, we sample 18 million occurrences of a user click from one Wikipedia page to another during a one year period. We restrict our browsing events to Wikipedia since its pages tend to contain many anchors (79 on average, where on average 42 have a unique target URL). Thus, they attract enough traffic for us to obtain robust browsing transition data 2 . We group together all transitions originat- ing from the same page and randomly hold out 20% of the transitions for our evaluation data (EVAL), 20% for training the DSSM described in Section 3.2 (TRAIN_1), and the remaining 60% for training our task specific rankers described in Section 3.3 (TRAIN_2). In our experiments, we used different settings for the two interestingness tasks. Thus, we postpone the detailed description of these datasets and other task-specific datasets to Sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Deep Semantic Similarity Model (DSSM)</head><p>This section presents the architecture of the DSSM, describes the parameter estimation, and the way the DSSM is used in our tasks.</p><p>we remove all structural information (e.g., hyperlinks and XML tags) in our documents, except that in the highlighting experiments (Section 4) we use anchor texts to simulate the candidate keywords to be highlighted. We then convert each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The heart of the DSSM is a deep neural network with convolutional structure, as shown in <ref type="figure" target="#fig_0">Figure  1</ref>. In what follows, we use lower-case bold letters, such as , to denote column vectors, to de- note the element of , and upper-case letters, such as , to denote matrices.</p><p>Input Layer . It takes two steps to convert a doc- ument , which is a sequence of words, into a vec- tor representation for the input layer of the net- work: (1) convert each word in to a word vector, and (2) build by concatenating these word vec- tors. To convert a word into a word vector, we first represent by a one-hot vector using a vo- cabulary that contains high frequent words ( 150K in this study). Then, following <ref type="bibr" target="#b24">Huang et al. (2013)</ref>, we map to a separate tri-letter vec- tor. Consider the word "#dog#", where # is a word boundary symbol. The nonzero elements in its tri- letter vector are "#do", "dog", and "og#". We then form the word vector of by concatenating its one-hot vector and its tri-letter vector. It is worth noting that the tri-letter vector complements the one-hot vector representation in two aspects. First, different OOV (out of vocabulary) words can be represented by tri-letter vectors with few colli- sions. Second, spelling variations of the same word can be mapped to the points that are close to each other in the tri-letter space. Although the number of unique English words on the Web is extremely large, the total number of distinct tri- letters in English is limited (restricted to the most frequent 30K in this study). As a result, incorpo- rating tri-letter vectors substantially improves the representation power of word vectors while keep- ing their size small. To form our input layer using word vectors, we first identify a text span with a high degree of relevance, called focus, in using task-specific heuristics (see Sections 4 and 5 respectively). Sec- ond, we form by concatenating each word vec- tor in the focus and a vector that is the summation of all other word vectors, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Since the length of the focus is much smaller than that of its document, is able to capture the con- textual information (for the words in the focus) Web document into plain text, which is white-space to- kenized and lowercased. Numbers are retained and no stem- ming is performed. <ref type="bibr">2</ref> We utilize the May 3, 2013 English Wikipedia dump con- sisting of roughly 4.1 million articles from http://dumps.wiki- media.org. Convolutional Layer . A convolutional layer extracts local features around each word in a word sequence of length as follows. We first generate a contextual vector by concatenating the word vectors of and its surrounding words defined by a window (the window size is set to 3 in this paper). Then, we generate for each word a local feature vector using a tanh activation function and a linear projection matrix , which is the same across all windows in the word se- quence, as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tanh</head><p>, where 1 … (1)</p><p>Max-pooling Layer . The size of the output depends on the number of words in the word se- quence. Local feature vectors have to be com- bined to obtain a global feature vector, with a fixed size independent of the document length, in order to apply subsequent standard affine layers. We design by adopting the max operation over each "time" of the sequence of vectors computed by (1), which forces the network to retain only the most useful, partially invariant local features pro- duced by the convolutional layer:</p><formula xml:id="formula_0">max ,…, u<label>(2)</label></formula><p>where the max operation is performed for each di- mension of across 1, … , respectively.</p><p>That convolutional and max-pooling layers are able to discover prominent keywords of a docu- ment can be demonstrated using the procedure in <ref type="figure">Figure 2</ref> using a toy example. First, the convolu- tional layer of (1) generates for each word in a 5- word document a 4-dimensional local feature vec- tor, which represents a distribution of four topics. For example, the most prominent topic of within its three word context window is the first topic, denoted by 11, and the most prominent topic of is 33. Second, we use max-pooling of <ref type="formula" target="#formula_0">(2)</ref> to form a global feature vector, which rep- resents the topic distribution of the whole docu- ment. We see that 1 and 3 are two promi- nent topics. Then, for each prominent topic, we trace back to the local feature vector that survives max-pooling:</p><formula xml:id="formula_1">1 max ,…, 1 1 3 max ,…, 3 3.</formula><p>Finally, we label the corresponding words of these local feature vectors, and , as keywords of the document. <ref type="figure" target="#fig_1">Figure 3</ref> presents a sample of document snip- pets and their keywords detected by the DSSM ac- cording to the procedure elaborated in <ref type="figure">Figure 2</ref>. It is interesting to see that many names are identified as keywords although the DSSM is not designed explicitly for named entity recognition.</p><p>Fully-Connected Layers and . The fixed sized global feature vector of (2) is then fed to several standard affine network layers, which are stacked and interleaved with nonlinear activation functions, to extract highly non-linear features at the output layer. In our model, shown in <ref type="figure" target="#fig_0">Figure  1</ref>, we have:</p><formula xml:id="formula_2">tanh (3) tanh<label>(4)</label></formula><p>where and are learned linear projection matri- ces.  <ref type="figure">Figure 2</ref>: Toy example of (upper) a 5-word document and its local feature vectors ex- tracted using a convolutional layer, and (bot- tom) the global feature vector of the document generated after max-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the DSSM</head><p>interestingness score. Let ∆ be the difference of their interestingness scores: ∆ , , , where is the interestingness score, computed as the cosine similarity:</p><p>, ≡ sim ,</p><formula xml:id="formula_3">‖ ‖‖ ‖<label>(5)</label></formula><p>where and are the feature vectors of and , respectively, which are generated using the DSSM, parameterized by . Intuitively, we want to learn to maximize ∆. That is, the DSSM is learned to represent documents as points in a hid- den interestingness space, where the similarity be- tween a document and its interesting documents is maximized.</p><p>We use the following logistic loss over ∆ , which can be shown to upper bound the pairwise accuracy:</p><formula xml:id="formula_4">∆; log1 exp∆<label>(6)</label></formula><p>3 In our experiments, we observed better results by sampling more negative training examples (e.g., up to 100) although this makes the training much slower. An alternative approach</p><p>The loss function in (6) has a shape similar to the hinge loss used in SVMs. Because of the use of the cosine similarity function, we add a scaling factor that magnifies ∆ from [-2, 2] to a larger range. Empirically, the value of makes no dif- ference as long as it is large enough. In the exper- iments, we set 10. Because the loss function is differentiable, optimizing the model parameters can be done using gradient-based methods. Due to space limitations, we omit the derivation of the gradient of the loss function, for which readers are referred to related derivations (e.g., <ref type="bibr" target="#b8">Collobert et al. 2011;</ref><ref type="bibr" target="#b24">Huang et al. 2013;</ref><ref type="bibr" target="#b35">Shen et al. 2014</ref>).</p><p>In our experiments we trained DSSMs using mini-batch Stochastic Gradient Descent. Each mini-batch consists of 256 source-target docu- ment pairs. For each source document , we ran- domly select from that batch four target docu- ments which are not paired with as negative training samples 3 . The DSSM trainer is imple- mented using a GPU-accelerated linear algebra li- brary, which is developed on CUDA 5.5. Given the training set (TRAIN_1 in Section 2), it takes approximately 30 hours to train a DSSM as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, on a Xeon E5-2670 2.60GHz machine with one Tesla K20 GPU card.</p><p>In principle, the loss function of <ref type="formula" target="#formula_4">(6)</ref> can be fur- ther regularized (e.g. by adding a term of 2 norm) to deal with overfitting. However, we did not find a clear empirical advantage over the simpler early stop approach in a pilot study, hence we adopted the latter in the experiments in this paper. Our ap- proach adjusts the learning rate during the course of model training. Starting with 1.0, after each epoch (a pass over the entire training data), the learning rate is adjusted as 0.5 if the loss on validation data (held-out from TRAIN_1) is not reduced. The training stops if either is smaller than a preset threshold (0.0001) or the loss on training data can no longer be reduced significantly. In our experiments, the DSSM training typically converges within 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using the DSSM</head><p>We experiment with two ways of using the DSSM for the two interestingness tasks. First, we use the DSSM as a feature generator. The output layer of the DSSM can be seen as a set of semantic fea- tures, which can be incorporated in a boosted tree is to approximate the partition function using Noise Contras- tive Estimation ( <ref type="bibr" target="#b20">Gutmann and Hyvarinen 2010)</ref>. We leave it to future work.</p><p>… the comedy festival formerly known as the us comedy arts festival is a comedy festival held each year in las vegas nevada from its 1985 inception to 2008 . it was held annually at the wheeler opera house and other venues in aspen colorado . the primary sponsor of the festival was hbo with co-sponsorship by based ranker (Friedman 1999) trained discrimina- tively on the task-specific data. Given a source- target document pair , , the DSSM generates 600 features (300 from the output layers and for each and , respectively).</p><note type="other">caesars palace . the primary venue tbs geico insurance twix candy bars and smirnoff vodka hbo exited the festival business in 2007 and tbs became the pri- mary sponsor the festival includes standup comedy performances appearances by the casts of television shows… … bad samaritans is an american comedy series produced by walt becker kelly hayes and ross putman . it premiered on netflix on march 31 2013 cast and char- acters . the show focuses on a community service parole group and their parole officer brian kubach as jake gibson an aspiring professional starcraft player who gets sentenced to 2000 hours of com- munity service for starting a forest fire during his breakup with drew prior to community service he had no real</note><p>Second, we use the DSSM as a direct imple- mentation of the interestingness function . Re- call from Section 3.2 that in model training, we measure the interestingness score for a document pair using the cosine similarity between their cor- responding feature vectors ( and ). Similarly at runtime, we define sim , as (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Highlighting</head><p>Recall from Section 1 that in this task, a system must select most interesting keywords in a doc- ument that a user is reading. To evaluate our mod- els using the click transition data described in Sec- tion 2, we simulate the task as follows. We use the set of anchors in a source document to simulate the set of candidate keywords that may be of in- terest to the user while reading , and treat the text of a document that is linked by an anchor in as a target document . As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, to apply DSSM to a specific task, we need to define the fo- cus in source and target documents. In this task, the focus in s is defined as the anchor text, and the focus in t is defined as the first 10 tokens in t.</p><p>We evaluate the performance of a highlighting system against a gold standard interestingness function which scores the interestingness of an anchor as the number of user clicks on from the anchor in in our data. We consider the ideal se- lection to then consist of the most interesting anchors according to . A natural metric for this task is Normalized Discounted Cumulative Gain (NDCG) (Jarvelin and Kekalainen 2000).</p><p>We evaluate our models on the EVAL dataset described in Section 2. We utilize the transition distributions in EVAL to create three other test sets, following the stratified sampling methodol- ogy commonly employed in the IR community, for the frequently, less frequently, and rarely viewed source pages, referred to as HEAD, TORSO, and TAIL, respectively. We obtain these sets by first sorting the unique source docu- ments according to their frequency of occurrence in EVAL. We then partition the set so that HEAD corresponds to all transitions from the source pages at the top of the list that account for 20% of the transitions in EVAL; TAIL corresponds to the transitions at the bottom also accounting for 20% of the transitions in EVAL; and TORSO corre- sponds to the remaining transitions. source document s and from user session infor- mation in the browser log. The document features include: position of the anchor in the document, frequency of the anchor, and anchor density in the paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>The rankers in Rows 5 to 12 use the NSF and the semantic features computed from source and target documents of a browsing transition. We compare semantic features derived from three dif- ferent sources. The first feature source comes from our DSSMs (DSSM and DSSM_BOW) us- ing the output layers as feature generators as de- scribed in Section 3.3. DSSM is the model de- scribed in Section 3 and DSSM_BOW is the model proposed by <ref type="bibr" target="#b24">Huang et al. (2013)</ref> where documents are view as bag of words (BOW) and the convolutional and max-pooling layers are not used. The two other sources of semantic features are used as a point of comparison to the DSSM. One is a generative semantic model (Joint Transi- tion Topic model, or JTT) ( <ref type="bibr" target="#b15">Gamon et al. 2013</ref>). JTT is an LDA-style model ( <ref type="bibr" target="#b3">Blei et al. 2003</ref>) that is trained jointly on source and target documents linked by browsing transitions. JTT generates a total of 150 features from its latent variables, 50 each for the source topic model, the target topic model and the transition model. The other seman- tic model of contrast is a manually defined one, which we use to assess the effectiveness of auto- matically learned models against human model- ers. To this effect, we use the page categories that editors assign in Wikipedia as semantic features (WCAT). These features number in the multiple thousands. Using features such as WCAT is not a viable solution in general since Wikipedia catego- ries are not available for all documents. As such, we use it solely as a point of comparison against DSSM and JTT.</p><p>We also distinguish between two types of learned rankers: those which draw their features only from the source (src only) document and those that draw their features from both the source and target (src+tar) documents. Although our task setting allows access to the content of both source and target documents, there are practical scenarios where a system should predict what in- terests the user without looking at the target doc- ument because the extra step of identifying a suit- able target document for each candidate concept or entity of interest is computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Results</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, NSF+DSSM, which incor- porates our DSSM, is the overall best performing system across test sets. The task is hard as evi- denced by the weak baseline scores. One reason is the large average number of candidates per page. On HEAD, we found an average of 170 anchors (of which 95 point to a unique target URL). For TORSO and TAIL, we found the average number of anchors to be 94 (52 unique targets) and 41 (19 unique targets), respectively.</p><p>Clearly, the semantics of the documents form important signals for this task: WCAT, JTT, DSSM_BOW, and DSSM all significantly boost the performance over NSF alone. There are two interesting comparisons to consider: (a) manual semantics vs. learned semantics; and (b) deep se- mantic models vs. generative topic models. On (a), we observe somewhat surprisingly that the learned DSSM produces features that outperform the thousands of features coming from manually (editor) assigned Wikipedia category features (WCAT), in all but the TAIL where the two per- form statistically the same. In contrast, features from the generative model (JTT) perform worse than WCAT across the board except on TAIL where JTT and WCAT are statistically tied. On (b), we observe that DSSM outperforms a state- of-the-art generative model (JTT) on HEAD and TORSO. On TAIL, they are statistically indistin- guishable.</p><p>We turn now to inspecting the scenario where features are only drawn from the source document (Rows 1-8 in <ref type="table" target="#tab_1">Table 1</ref>). Again we observe that se- mantic features significantly boost the perfor- mance against NSF alone, however they signifi- cantly deteriorate when compared to using fea- tures from both source and target documents. In this scenario, the manual semantics from WCAT outperform all other models, but with a diminish- ing effect as we move from HEAD through TORSO to TAIL. DSSM is the best performing learned semantic model. Finally, we present the results to justify the two modifications we made to extend the model of <ref type="bibr" target="#b24">Huang et al. (2013)</ref> to the DSSM, as described in Section 1. First, we see in <ref type="table" target="#tab_1">Table 1</ref> that DSSM_BOW, which has the same network struc- ture of Huang et al.'s model, is much weaker than DSSM, demonstrating the benefits of using con- volutional and max-pooling layers to extract se- mantic features for the highlighting task. Second, we conduct several experiments by using the co- sine scores between the output layers of DSSM for and as features (following the procedure in Section 3.3 for using the DSSM as a direct imple- mentation of ). We found that adding the cosine features to NSF+DSSM does not lead to any im- provement. We also combined NSF with solely the cosine features from DSSM (i.e., without the other semantic features drawn from its output lay- ers). But we still found no improvement over us- ing NSF alone. Thus, we conclude that for this task it is much more effective to feed the features derived from DSSM to a supervised ranker than directly computing the interestingness score using cosine similarity in the learned semantic space, as in <ref type="bibr" target="#b24">Huang et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Entity Search</head><p>We construct the evaluation data set for this sec- ond task by randomly sampling a set of documents from a traffic-weighted set of Web documents. In a second step, we identify the entity names in each document using an in-house named entity recog- nizer. We issue each entity name as a query to a commercial search engine, and retain up to the top-100 retrieved documents as candidate target documents. We form for each entity a source doc- ument which consists of the entity text and its sur- rounding text defined by a 200-word window. We define the focus (as in <ref type="figure" target="#fig_0">Figure 1</ref>) in as the entity text, and the focus in as the first 10 tokens in .</p><p>The final evaluation data set contains 10,000 source documents. On average, each source docu- ment is associated with 87 target documents. Fi- nally, the source-target document pairs are labeled in terms of interestingness by paid annotators. The label is on a 5-level scale, 0 to 4, with 4 meaning the target document is the most interesting to the source document and 0 meaning the target is of no interest.</p><p>We test our models on two scenarios. The first is a ranking scenario where interesting docu- ments are displayed to the user. Here, we select the top-ranked documents according to their in- terestingness scores. We measure the performance via NDCG at truncation levels 1 and 3. The sec- ond scenario is to display to the user all interesting results. In this scenario, we select all target docu- ments with an interestingness score exceeding a predefined threshold. We evaluate this scenario using ROC analysis and, specifically, the area un- der the curve (AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>The main results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. Rows 1 to 6 are single model results, where each model is used as a direct implementation of the interest- ingness function . Rows 7 to 9 are ranker results, where is defined as a boosted tree based ranker that incorporates different sets of features ex- tracted from source and target documents, includ- ing the features derived from single models. As in the highlighting experiments, all the machine- learned single models, including the DSSM, are trained on TRAIN_1, and all the rankers are trained on TRAIN_2. <ref type="table" target="#tab_3">Table 2</ref>) is the classic document model <ref type="bibr" target="#b34">(Robertson and Zaragoza 2009)</ref>. It uses the bag-of-words document representation and the BM25 term weighting function. In our set- ting, we define the interestingness score of a doc- ument pair as the dot product of their BM25- weighted term vectors. To verify the importance of using contextual information, we compare two different ways of forming the term vector of a source document. The first only uses the entity text (Row 1). The second (Row 2) uses both the entity text and and its surrounding text in a 200- word window (i.e., the entire source document). Results show that the model using contextual in- formation is significantly better. Therefore, all the other models in this section use both the entity texts and their surrounding text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BM25 (Rows 1 and 2 in</head><p>WTM (Row 3) is our implementation of the word translation model for IR (Berger and Laf- ferty 1999; <ref type="bibr" target="#b16">Gao et al. 2010)</ref>. WTM defines the in- terestingness score as:  where | is the unigram probability of word in , and | is the probability of trans- lating into , trained on source-target docu- ment pairs using EM ( <ref type="bibr" target="#b5">Brown et al. 1993</ref>). The translation-based approach allows any pair of non-identical but semantically related words to have a nonzero matching score. As a result, it sig- nificantly outperforms BM25. BTLM (Row 4) follows the best performing bilingual topic model described in <ref type="bibr" target="#b18">Gao et al. (2011)</ref>, which is an extension of PLSA <ref type="bibr" target="#b23">(Hofmann 1999</ref>). The model is trained on source-target doc- ument pairs using the EM algorithm with a con- straint enforcing a source document and its tar- get document to not only share the same prior topic distribution, but to also have similar frac- tions of words assigned to each topic. BLTM de- fines the interestingness score between s and t as:</p><formula xml:id="formula_5">, ∏ ∑ | | ∈∈</formula><formula xml:id="formula_6">, ∏ ∑ | | ∈∈ .</formula><p>The model assumes the following story of gener- ating from . First, for each topic a word dis- tribution is selected from a Dirichlet prior with concentration parameter . Second, given , a topic distribution is drawn from a Dirichlet prior with parameter . Finally, is generated word by word. Each word is generated by first selecting a topic according to , and then drawing a word from . We see that BLTM models interestingness by taking into account the semantic topic distribution of the entire docu- ments. Our results in <ref type="table" target="#tab_3">Table 2</ref> show that BLTM outperforms WTM by a significant margin in both NDCG and AUC. DSSM (Row 5) outperforms all the competing single models, including the state-of-the-art topic model BLTM. Now, we inspect the difference be- tween DSSM and BLTM in detail. Although both models strive to generate the semantic representa- tion of a document, they use different modeling approaches. BLTM by nature is a generative model. The semantic representation in BLTM is a distribution of hidden semantic topics. Such a dis- tribution is learned using Maximum Likelihood Estimation in an unsupervised manner, i.e., max- imizing the log-likelihood of the source-target document pairs in the training data. On the other hand, DSSM represents documents as points in a hidden semantic space using a supervised learning method, i.e., paired documents are closer in that latent space than unpaired ones. We believe that the superior performance of DSSM is largely due to the fact that the model parameters are discrimi- natively trained using an objective that is tailored to the interestingness task.</p><p>In addition to the difference in training meth- ods, DSSM and BLTM also use different model structures. BLTM treats a document as a bag of words (thus losing some important contextual in- formation such as word order and inter-word de- pendencies), and generates semantic representa- tions of documents using linear projection. DSSM, on the other hand, treats text as a sequence of words and better captures local and global con- text, and generates highly non-linear semantic features via a deep neural network. To further ver- ify our analysis, we inspect the results of a variant of DSSM, denoted as DSSM_BOW (Row 6), where the convolution and max-pooling layers are removed. This model treats a document as a bag of words, just like BLTM. These results demon- strate that the effectiveness of DSSM can also be attributed to the convolutional architecture in the neural network, in addition to being deep and be- ing discriminative.</p><p>We turn now to discussing the ranker results in Rows 7 to 9. The baseline ranker (Row 7) uses 158 features, including many counts and single model scores, such as BM25 and WMT. DSSM (Row 5) alone is quite effective, being close in perfor- mance to the baseline ranker with non-DSSM fea- tures. Integrating the DSSM score computed in (5) as one single feature into the ranker (Row 8) leads to a significant improvement over the baseline. The best performing combination (Row 9) is ob- tained by incorporating the DSSM feature vectors of source and target documents (i.e., 600 features in total) in the ranker.</p><p>We thus conclude that on both tasks, automatic highlighting and contextual entity search, features drawn from the output layers of our deep semantic model result in significant gains after being added to a set of non-semantic features, and in compari- son to other types of semantic models used in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In addition to the notion of relevance as described in Section 1, related to interestingness is also the notion of salience (also called aboutness) ( <ref type="bibr" target="#b15">Gamon et al. 2013;</ref><ref type="bibr" target="#b28">2014;</ref><ref type="bibr">Parajpe 2009;</ref><ref type="bibr" target="#b40">Yih et al. 2006</ref>). Salience is the centrality of a term to the content of a document. Although salience and interesting- ness interact, the two are not the same. For exam- ple, in a news article about President Obama's visit to Seattle, Obama is salient, yet the average user would probably not be interested in learning more about Obama while reading that article.</p><p>There are many systems that identify popular content in the Web or recommend content (e.g., <ref type="bibr" target="#b0">Bandari et al. 2012;</ref><ref type="bibr" target="#b27">Lerman and Hogg 2010;</ref><ref type="bibr" target="#b38">Szabo and Huberman 2010)</ref>, which is closely re- lated to the highlighting task. In contrast to these approaches, we strive to predict what term a user is likely to be interested in when reading content, which may or may not be the same as the most popular content that is related to the current docu- ment. It has empirically been demonstrated in <ref type="bibr" target="#b15">Gamon et al. (2013)</ref> that popularity is in fact a ra- ther poor predictor for interestingness. The task of contextual entity search, which is formulated as an information retrieval problem in this paper, is also related to research on entity resolution ( <ref type="bibr" target="#b37">Stefanidis et al. 2013</ref>).</p><p>Latent Semantic Analysis <ref type="bibr" target="#b9">(Deerwester et al. 1990</ref>) is arguably the earliest semantic model de- signed for IR. Generative topic models widely used for IR include PLSA (Hofmann 1990) and LDA ( <ref type="bibr" target="#b3">Blei et al. 2003)</ref>. Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., <ref type="bibr" target="#b12">Dumais et al. 1997;</ref><ref type="bibr" target="#b18">Gao et al. 2011;</ref><ref type="bibr" target="#b32">Platt et al. 2010;</ref><ref type="bibr" target="#b41">Yih et al. 2011</ref>).</p><p>By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associ- ated features at different levels of abstraction use- ful for a variety of tasks (e.g., <ref type="bibr" target="#b8">Collobert et al. 2011;</ref><ref type="bibr" target="#b21">Hinton et al. 2012;</ref><ref type="bibr" target="#b36">Socher et al. 2012;</ref><ref type="bibr" target="#b26">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b17">Gao et al. 2014</ref>). <ref type="bibr" target="#b22">Hinton and Salakhutdinov (2010)</ref> propose the most origi- nal approach based on an unsupervised version of the deep neural network to discover the hierar- chical semantic structure embedded in queries and documents. <ref type="bibr" target="#b24">Huang et al. (2013)</ref> significantly ex- tends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text pro- cessing, central to our DSSM, was also described in <ref type="bibr" target="#b8">Collobert et al. (2011) and</ref><ref type="bibr" target="#b35">Shen et al. (2014)</ref> but with very different applications. The DSSM described in Section 3 can be viewed as a variant of the deep neural network models used in these previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Modeling interestingness is fundamental to many online recommendation systems. We obtain natu- rally occurring interest signals by observing Web browsing transitions where users click from one webpage to another. We propose to model this "interestingness" with a deep semantic similarity model (DSSM), based on deep neural networks with special convolutional-pooling structure, mapping source-target document pairs to feature vectors in a latent semantic space. We train the DSSM using browsing transitions between docu- ments. Finally, we demonstrate the effectiveness of our model on two interestingness tasks: auto- matic highlighting and contextual entity search. Our results on large-scale, real-world datasets show that the semantics of documents computed by the DSSM are important for modeling interest- ingness and that the new model leads to signifi- cant improvements on both tasks. DSSM is shown to outperform not only the classic document mod- els that do not use (latent) semantics but also state- of-the-art topic models that do not have the deep and convolutional architecture characterizing the DSSM.</p><p>One area of future work is to extend our method to model interestingness given an entire user session, which consists of a sequence of browsing events. We believe that the prior brows- ing and interaction history recorded in the session provides additional signals for predicting interest- ingness. To capture such signals, our model needs to be extended to adequately represent time series (e.g., causal relations and consequences of ac- tions). One potentially effective model for such a purpose is based on the architecture of recurrent neural networks (e.g., <ref type="bibr">Mikolov et al. 2010;</ref><ref type="bibr" target="#b7">Chen and Deng, 2014</ref>), which can be incorporated into the deep semantic model proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Authors</head><p>Yelong Shen (Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA, email: yeshen@microsoft.com).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the network architecture and information flow of the DSSM</figDesc><graphic url="image-1.png" coords="3,76.56,79.55,214.22,209.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>am- bition in life other than to be a pro- fessional gamer and become wealthy overnight like mark zuckerberg as in life his goal during …Figure 3 :</head><label>3</label><figDesc>Figure 3: A sample of document snippets and the keywords (in bold) detected by the DSSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 summarizes</head><label>1</label><figDesc>the results of various models over the three test sets using NDCG at truncation levels 1, 5, and 10. Rows 1 to 3 are simple heuristic baselines. RAND selects random anchors, 1stK selects the first anchors and LastK the last anchors. The other models in Table 1 are boosted tree based rankers trained on TRAIN_2 described in Section 2. They vary only in their features. The ranker in Row 4 uses Non-Semantic Features (NSF) only. These features are derived from the</figDesc><table># 
Models 
HEAD 
TORSO 
TAIL 
@1 
@5 
@10 
@1 
@5 
@10 
@1 
@5 
@10 

src only 

1 
RAND 
0.041 0.062 0.081 0.036 
0.076 
0.109 0.062 0.195 
0.258 
2 
1stK 
0.010 0.177 0.243 0.072 
0.171 
0.240 0.091 0.274 
0.348 
3 
LastK 
0.170 0.022 0.027 0.022 
0.044 
0.062 0.058 0.166 
0.219 
4 
NSF 
0.215 0.253 0.295 0.139 
0.229 
0.282 0.109 0.293 
0.365 
5 
NSF+WCAT 
0.438 0.424 0.463 0.194 
0.290 
0.346 0.118 0.317 
0.386 
6 
NSF+JTT 
0.220 0.302 0.343 0.141 
0.241 
0.295 0.111 0.300 
0.369 
7 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 
0.258 
0.313 0.110 0.299 
0.372 
8 
NSF+DSSM 
0.362 0.386 0.421 0.178 
0.275 
0.330 0.116 0.312 
0.382 

src+tar 
9 
NSF+WCAT 
0.505 0.475 0.501 0.224 
0.304 
0.356 0.129 0.324 
0.391 
10 
NSF+JTT 
0.345 0.380 0.418 0.183 
0.280 
0.332 0.131 0.321 
0.390 
11 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 
0.274 
0.325 0.123 0.311 
0.380 
12 
NSF+DSSM 
0.554 0.524 0.547 0.241 
0.317 
0.367 0.135 0.329 
0.398 

Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO and 
TAIL test sets. Bold indicates statistical significance over all non-shaded results using t-test ( 
0.05). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Contextual entity search task perfor-
mance (NDCG @ K and AUC). * indicates sta-
tistical significance over all non-shaded single 
model results (Rows 1 to 6) using t-test ( 
0.05). # indicates statistical significance over re-
sults in Row 7. ## indicates statistical signifi-
cance over results in Rows 7 and 8. </table></figure>

			<note place="foot">Section 4), our task is to model interestingness between unstructured documents, i.e., without access to any document structure or Web interaction data. Thus, in our experiments,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Johnson Apacible, Pradeep Chilakamarri, Edward Guo, Bernhard Kohlmeier, Xiaolong Li, Kevin Powell, Xinying Song and Ye-Yi Wang for their guidance and valuable dis-cussions. We also thank the three anonymous re-viewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The pulse of news in social media: forecasting popularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Asur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Fundamental Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information retrieval as statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A semantic approach to contextual advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A primal-dual method for training recurrent neural networks constrained by the echo-state property</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New types of deep neural network learning for speech recognition and related applications: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic cross-linguistic information retrieval using latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Letsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting interesting things in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying salient entities in web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clickthroughbased translation models for web search: from word models to phrase models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1139" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>-T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clickthrough-based latent semantic models for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICASSP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS2010)</title>
		<meeting>Int. Conf. on Artificial Intelligence and Statistics (AISTATS2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering binary codes for documents by learning deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">IR evaluation methods for retrieving highly relevant documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kekalainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using a model of social dynamics to predict popularity of news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Computer eyesight gets a lot more accurate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Markoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>New York Times</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning document aboutness from implicit user feedback and document structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paranjpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Translingual document representations from discriminative projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kantor</forename></persName>
		</author>
		<title level="m">Recommender System Handbook</title>
		<editor>P. B.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stefanidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophides</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<title level="m">Entity resolution in the web of data. CIKM&apos;13 Tutorial</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting the popularity of online content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adapting boosting for information retrieval measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="270" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Finding advertising keywords on web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Carvalho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
