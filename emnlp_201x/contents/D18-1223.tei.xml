<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Relational Learning for Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 1980</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>{xwhan, william}@cs.ucsb.edu, yum@us.ibm.com, {shiyu.chang, xiaoxiao.guo}@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Relational Learning for Knowledge Graphs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1980" to="1990"/>
							<date type="published">October 31-November 4, 2018. 2018. 1980</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge graphs (KGs) are the key components of various natural language processing applications. To further expand KGs&apos; coverage , previous studies on knowledge graph completion usually require a large number of training instances for each relation. However, we observe that long-tail relations are actually more common in KGs and those newly added relations often do not have many known triples for training. In this work, we aim at predicting new facts under a challenging setting where only one training instance is available. We propose a one-shot relational learning framework , which utilizes the knowledge extracted by embedding models and learns a matching metric by considering both the learned embed-dings and one-hop graph structures. Empirically , our model yields considerable performance improvements over existing embedding models, and also eliminates the need of retraining the embedding models when dealing with newly added relations. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale knowledge graphs <ref type="bibr" target="#b30">(Suchanek et al., 2007;</ref><ref type="bibr">Vrandeči´Vrandeči´c and Krötzsch, 2014;</ref><ref type="bibr" target="#b1">Bollacker et al., 2008;</ref><ref type="bibr" target="#b0">Auer et al., 2007;</ref><ref type="bibr" target="#b3">Carlson et al., 2010)</ref> represent every piece of information as binary re- lationships between entities, usually in the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question An- swering and Semantic Web.</p><p>Despite KGs' large scale, they are known to be highly incomplete ( <ref type="bibr" target="#b18">Min et al., 2013</ref>  et al., <ref type="bibr" target="#b32">Trouillon et al., 2016;</ref><ref type="bibr" target="#b13">Lao and Cohen, 2010;</ref><ref type="bibr" target="#b21">Neelakantan et al., 2015;</ref><ref type="bibr">Xiong et al., 2017;</ref><ref type="bibr" target="#b5">Das et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018</ref>) have been made to build relational learning models that could infer missing triples by learning from exist- ing ones. These methods explore the statistical in- formation of triples or path patterns to infer new facts of existing relations; and have achieved con- siderable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover com- mon relations in KGs. For more practical scenar- ios, we believe the desired KG completion models should handle two key properties of KGs. First, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a large portion of KG rela- tions are actually long-tail. In other words, they have very few instances. But intuitively, the fewer training triples that one relation has, the more KG completion techniques could be of use. Therefore, it is crucial for models to be able to complete re- lations with limited numbers of triples. However, existing research usually assumes the availability of sufficient training triples for all relations, which limits their usefulness on sparse long-tail relations.</p><p>Second, to capture up-to-date knowledge, real- world KGs are often dynamic and evolving at any given moment. New relations will be added when- ever new knowledge is acquired. If a model can predict new triples given only a small number of examples, a large amount of human effort could be spared. However, to predict target relations, pre- vious methods usually rely on well-learned repre- sentations of these relations. In the dynamic sce- nario, the representations of new relations cannot be sufficiently trained given limited training in- stances, thus the ability to adapt to new relations is also limited for current models.</p><p>In contrast to previous methods, we propose a model that depends only on the entity embed- dings and local graph structures. Our model aims at learning a matching metric that can be used to discover more similar triples given one reference triple. The learnable metric model is based on a permutation-invariant network that effectively en- codes the one-hop neighbors of entities, and also a recurrent neural network that allows multi-step matching. Once trained, the model will be able to make predictions about any relation while exist- ing methods usually require fine-tuning to adapt to new relations. With two newly constructed datasets, we show that our model can achieve consistent improvement over various embedding models on the one-shot link prediction task.</p><p>In summary, our contributions are three-fold:</p><p>• We are the first to consider the long-tail rela- tions in the link prediction task and formulate the problem as few-shot relational learning;</p><p>• We propose an effective one-shot learn- ing framework for relational data, which achieves better performance than various embedding-based methods;</p><p>• We also present two newly constructed datasets for the task of one-shot knowledge graph completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Embedding Models for Relational Learning Various models have been developed to model re- lational KGs in continous vector space and to au- tomatically infer missing links. RESCAL <ref type="bibr" target="#b22">(Nickel et al., 2011</ref>) is one of the earlier work that models the relationship using tensor operations. <ref type="bibr" target="#b2">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D vector space. Following this line of research, more advanced models such as <ref type="bibr">DistMult (Yang et al., 2014</ref>), <ref type="bibr">ComplEx (Trouillon et al., 2016)</ref> and <ref type="bibr">ConvE (Dettmers et al., 2017</ref>) have been proposed. These embedding-based models usually assume enough training instances for all relations and en- tities and do not pay attention to those sparse symbols. More recently, several models <ref type="bibr" target="#b26">(Shi and Weninger, 2017;</ref><ref type="bibr">Xie et al., 2016</ref>) have been pro- posed to handle unseen entities by leveraging text descriptions. In contrast to these approaches, our model deals with long-tail or newly added rela- tions and focuses on one-shot relational learning without any external information, such as text de- scriptions of entities or relations.</p><p>Few-Shot Learning Recent deep learning based few-shot learning approaches fall into two main categories: (1) metric based approaches <ref type="bibr" target="#b11">(Koch, 2015;</ref><ref type="bibr" target="#b34">Vinyals et al., 2016;</ref><ref type="bibr" target="#b27">Snell et al., 2017;</ref><ref type="bibr">Yu et al., 2018)</ref>, which try to learn generalizable metrics and the corresponding matching functions from a set of training tasks. Most methods in this class adopt the general matching framework proposed in deep siamese network <ref type="bibr" target="#b11">(Koch, 2015)</ref>. One example is the Matching Networks (Vinyals et al., 2016), which make predictions by compar- ing the input example with a small labeled support set; (2) meta-learner based approaches <ref type="bibr" target="#b24">(Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b20">Munkhdalai and Yu, 2017;</ref><ref type="bibr" target="#b8">Finn et al., 2017;</ref><ref type="bibr" target="#b14">Li et al., 2017)</ref>, which aim to learn the optimization of model parameters (by either out- putting the parameter updates or directly predict- ing the model parameters) given the gradients on few-shot examples. One example is the LSTM- based meta-learner ( <ref type="bibr" target="#b24">Ravi and Larochelle, 2017)</ref>, which learns the step size for each dimension of the stochastic gradients. Besides the above cate- gories, there are also some other styles of few-shot learning algorithms, e.g. Bayesian Program Induc- tion ( <ref type="bibr" target="#b12">Lake et al., 2015)</ref>, which represents concepts as simple programs that best explain observed ex- amples under a Bayesian criterion.</p><p>Previous few-shot learning research mainly fo- cuses on vision and imitation learning ( <ref type="bibr" target="#b7">Duan et al., 2017)</ref> domains. In the language domain, <ref type="bibr">Yu et al. (2018)</ref> proposed a multi-metric based approach for text classification. To the best of our knowl- edge, this work is the first research on few-shot learning for knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Knowledge graphs G are represented as a collec- tion of triples {(h, r, t)} ⊆ E × R × E, where E and R are the entity set and relation set. The task of knowledge graph completion is to either predict unseen relations r between two existing entities: (h, ?, t) or predict the tail entity t given the head entity and the query relation: (h, r, ?). As our pur- pose is to infer unseen facts for newly added or existing long-tail relations, we focus on the lat- ter case. In contrast to previous work that usually assumes enough triples for the query relation are available for training, this work studies the case where only one training triple is available. To be more specific, the goal is to rank the true tail entity t true higher than other candidate entities t ∈ C h,r , given only an example triple (h 0 , r, t 0 ). The can- didates set is constructed using the entity type con- straint ( <ref type="bibr" target="#b31">Toutanova et al., 2015)</ref>. It is also worth noting that when we predict new facts of the rela- tion r, we only consider a closed set of entities, i.e. no unseen entities during testing. For open-world settings where new entities might appear during testing, external information such as text descrip- tions about these entities are usually required and we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">One-Shot Learning Settings</head><p>This section describes the settings for the training and evaluation of our one-shot learning model.</p><p>The goal of our work is to learn a metric that could be used to predict new facts with one- shot examples. Following the standard one-shot learning settings ( <ref type="bibr" target="#b34">Vinyals et al., 2016;</ref><ref type="bibr" target="#b24">Ravi and Larochelle, 2017)</ref>, we assume access to a set of training tasks. In our problem, each train- ing task corresponds to a KG relations r ∈ R, and has its own training/testing triples:</p><formula xml:id="formula_0">T r = {D train r , D test r }.</formula><p>This task set is often denoted as the meta-training set, T meta−train .</p><p>To imitate the one-shot prediction at evaluation time, there is only one triple</p><formula xml:id="formula_1">(h 0 , r, t 0 ) in each D train r . The D test r = {(h i , r, t i , C h i ,r )</formula><p>} consists of the testing triples of r with ground-truth tail en- tities t i for each query (h i , r), and the correspond- ing tail entity candidates C h i ,r = {t ij } where each t ij is an entity in G. The metric model can thus be tested on this set by ranking the candidate set C h i ,r given the test query (h i , r) and the labeled triple in D train r . We denote an arbitrary ranking-</p><formula xml:id="formula_2">loss function as θ (h i , r, t i |C h i ,r , D train r )</formula><p>, where θ represents the parameters of our metric model. This loss function indicates how well the metric model works on tuple (h i , r, t i , C h i ,r ) while ob- serving only one-shot data from D train r . The ob- jective of training the metric model, i.e. the meta- training objective, thus becomes:</p><formula xml:id="formula_3">min θ ET r   (h i ,r,t i ,C h i ,r )∈D test r θ (hi, r, ti|C h i ,r , D train r ) |D test r |   , (1)</formula><p>where T r is sampled from the meta-training set T meta−train , and |D test r | denotes the number of tu- ples in D test r . Once trained, we can use the model to make predictions on new relations r ∈ R , which is called the meta-testing step in literature. These meta-testing relations are unseen from meta- training, i.e. R ∩ R = φ. Each meta-testing relation r also has its own one-shot training data D train r and testing data D test r , defined in the same way as in meta-training. These meta-testing rela- tions form a meta-test set T meta−test .</p><p>Moreover, we leave out a subset of rela- tions in T meta−train as the meta-validation set T meta−validation . Because of the assumption of one-shot learning, the meta-testing relations do not have validation sets like in the traditional machine learning setting. Otherwise, the metric model will actually see more than one-shot la- beled data during meta-testing, thus the one-shot assumption is violated.</p><p>Finally, we assume that the method has access to a background knowledge graph G , which is a subset of G with all the relations from T meta−train , T meta−validation and T meta−test removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section, we describe the proposed model for similarity metric learning and also the correspond- ing loss function we use to train our model.</p><p>The core of our proposed model is a similar- ity function M((h, t), (h , t )|G ). Thus for any query relation r, as long as there is one known fact (h 0 , r, t 0 ), the model could predict the likelihood of testing triples {(h i , r, t ij )|t ij ∈ C h i ,r }, based on the matching score between each (h i , t ij ) and (h 0 , t 0 ). The implementation of the above match- ing function involves two sub-problems: (1) the representations of entity pairs; and (2) the compar- ison function between two entity-pair representa- tions. Our overall model, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>  deals with the above two problems with two major components respectively:</p><p>• Neighbor encoder <ref type="figure" target="#fig_1">(Figure 2b</ref>), aims at utilizing the local graph structure to better represent enti- ties. In this way, the model can leverage more in- formation that KG provides for every entity within an entity pair.</p><p>• Matching processor <ref type="figure" target="#fig_1">(Figure 2c</ref>), takes the vec- tor representations of any two entity pairs from the neighbor encoder; then performs multi-step matching between two entity-pairs and outputs a scalar as the similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neighbor Encoder</head><p>This module is designed to enhance the represen- tation of each entity with its local connections in knowledge graph.</p><p>Although the entity embeddings from KG em- bedding models <ref type="bibr" target="#b2">(Bordes et al., 2013;</ref><ref type="bibr">Yang et al., 2014</ref>) already have relational information en- coded, previous work ( <ref type="bibr" target="#b21">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b15">Lin et al., 2015a;</ref><ref type="bibr">Xiong et al., 2017)</ref> showed that explicitly modeling the structural patterns, such as paths, is usually beneficial for relationship predic- tion. In view of this, we propose to use a neighbor encoder to incorporate graph structures into our metric-learning model. In order to benefit from the structural information while maintaining the efficiency to easily scale to real-world large-scale KGs, our neighbor encoder only considers enti- ties' local connections, i.e. the one-hop neighbors.</p><p>For any given entity e, its local connections form a set of (relation, entity) tuples. As shown in <ref type="figure" target="#fig_1">Figure 2a</ref>, for the entity Leonardo da Vinci, one of such tuples is (occupation, painter). We refer this neighbor set as as</p><formula xml:id="formula_4">N e = {(r k , e k )|(e, r k , e k ) ∈ G }.</formula><p>The purpose of our neighbor encoder is to encode N e and output a vector as the latent repre- sentation of e. Because this is a problem of encod- ing sets with varying sizes, we hope the encoding function can be (1) invariant to permutations and also (2) insensitive to the size of the neighbor set. Inspired by the results from ( <ref type="bibr" target="#b5">Zaheer et al., 2017)</ref>, we use the following function f that satisfies the above properties:</p><formula xml:id="formula_5">f (N e ) = σ( 1 |N e | (r k ,e k )∈Ne C r k ,e k ).<label>(2)</label></formula><p>where C r k ,e k is the feature representation of a relation-entity pair (r k , e k ) and σ is the acti- vation function. In this paper we set σ = tanh which achieves the best performance on T meta−validation . To encode every tuple (r k , e k ) ∈ N e into C r k ,e k , we first use an embedding layer emb with dimension d (which can be pre-trained using ex- isting embedding-based models) to get the vector representations of r k and e k :</p><formula xml:id="formula_6">v r k = emb(r k ), v e k = emb(e k )</formula><p>Dropout ( <ref type="bibr" target="#b29">Srivastava et al., 2014</ref>) is applied here to the vectors v r k , v e k to achieve better generaliza- tion. We then apply a feed-forward layer to encode the interaction within this tuple:</p><formula xml:id="formula_7">C r k ,e k = W c (v r k ⊕ v e k ) + b c ,<label>(3)</label></formula><p>where W c ∈ R d×2d , b c ∈ R d are parameters to be learned and ⊕ denotes concatenation.  <ref type="table" target="#tab_4">100  150  200  0  200  400  600  800  1000  1200  1400</ref>  To enable batching during training, we manu- ally specify the maximum number of neighbors and use all-zero vectors as "dummy" neighbors. Although different entities have different degrees (number of neighbors), the degree distribution is usually very concentrated, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We can easily find a proper bound as the maximum number of neighbors to batch groups of entities.</p><p>The neighbor encoder module we propose here is similar to the Relational Graph Convolutional Networks ( <ref type="bibr" target="#b25">Schlichtkrull et al., 2017</ref>) in the sense that we also use the shared kernel {W c , b c } to en- code the neighbors of different entities. But un- like their model that operates on the whole graph and performs multiple steps of information prop- agation, we only encode the local graphs of the entities and perform one-step propagation. This enables us to easily apply our model to large-scale KGs such as Wikidata. Besides, their model also does not operate on pre-trained graph embeddings. We leave the investigation of other graph encoding strategies, e.g. ( <ref type="bibr">Xu et al., 2018;</ref><ref type="bibr" target="#b28">Song et al., 2018)</ref>, to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Matching Processor</head><p>Given the neighbor encoder module, now we dis- cuss how we can do effective similarity matching based on our recurrent matching processor. By ap- plying f (N e ) to the reference entity pair (h 0 , t 0 ) and any query entity pair (h i , t ij ), we get two Algorithm 1 One-shot Training 1: Input: 2: a) Meta-training task set Tmeta−training; 3: b) Pre-trained KG embeddings (excluding relation in Tmeta−training); 4: c) Initial parameters θ of the metric model; 5: for epoch = 0:M-1 do 6:</p><p>Shuffle the tasks in T meta−learning 7:</p><p>for Tr in T meta−learning do 8:</p><p>Sample one triple as the reference 9:</p><p>Sample a batch B + of query triples 10:</p><p>Pollute the tail entity of query triples to get B − 11:</p><p>Calculate the matching scores for triple in B + and B − 12:</p><p>Calculate the batch loss L = B 13:</p><p>Update θ using gradient g ∝ L 14:</p><p>end for 15: end for neighbor vectors for each:</p><formula xml:id="formula_8">[f (N h 0 ); f (N t 0 )] and [f (N h i ); f (N t ij )].</formula><p>To get a similarity score that can be used to rank (h i , t ij ) among other candi- dates, we can simply concatenate the f (N h ) and f (N t ) in each pair to form a single pair repre- sentation vector, and calculate the cosine similar- ity between pairs. However, this simple metric model turns out to be too shallow and does not give good performance. To enlarge our model's capacity, we leverage a LSTM-based (Hochre- iter and Schmidhuber, 1997) recurrent "process- ing" block ( <ref type="bibr" target="#b33">Vinyals et al., 2015</ref><ref type="bibr" target="#b34">Vinyals et al., , 2016</ref>) to perform multi-step matching. Every process step is defined as follows:</p><formula xml:id="formula_9">h k+1 , c k+1 = LST M (q, [h k ⊕ s, c k ]) h k+1 = h k+1 + q score k+1 = h k+1 s h k+1 s ,<label>(4)</label></formula><p>where LST M (x, <ref type="bibr">[h, c]</ref>) is a standard LSTM cell with input x, hidden state h and cell state c, and</p><formula xml:id="formula_10">s = f (N h 0 ) ⊕ f (N t 0 ), q = f (N h i ) ⊕ f (N t ij )</formula><p>are the concatenated neighbor vectors of the reference pair and query pair. After K processing steps 2 , we use score K as the final similarity score between the query and support entity pair. For every query (h i , r, ?), by comparing (h i , t ij ) with (h 0 , t 0 ), we can get the ranking scores for every t ij ∈ C h i ,r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Function and Training</head><p>For a query relation r and its reference/training triple (h 0 , r, t 0 ), we collect a group of positive (true) query triples {(h i , r, t + i )|(h i , r, t + i ) ∈ G} and construct another group negative (false) query triples {(h i , r, t − i )|(h i , r, t − i ) ∈ G} by polluting the tail entities. Following previous embedding- based models, we use a hinge loss function to op- timize our model:</p><formula xml:id="formula_11">θ = max(0, γ + score − θ − score + θ ),<label>(5)</label></formula><p>where score + θ and score − θ are scalars calculated by comparing the query triple (h i , r, t + i /t − i ) with the reference triple (h 0 , r, t 0 ) using our metric model, and the margin γ is a hyperparameter to be tuned. For every training episode, we first sam- ple one task/relation T r from the meta-training set T meta−training . Then from all the known triples in  Existing benchmarks for knowledge graph com- pletion, such as FB15k-237 ( <ref type="bibr" target="#b31">Toutanova et al., 2015</ref>) and YAGO3-10 ( <ref type="bibr" target="#b17">Mahdisoltani et al., 2013)</ref> are all small subsets of real-world KGs. These datasets consider the same set of relations during training and testing and often include sufficient training triples for every relation. To construct datasets for one-shot learning, we go back to the original KGs and select those relations that do not have too many triples as one-shot task relations. We refer the rest of the relations as background re- lations, since their triples provide important back- ground knowledge for us to match entity pairs.</p><p>Our first dataset is based on NELL (Mitchell et al., 2018), a system that continuously collects structured knowledge by reading webs. We take the latest dump and remove those inverse relations. We select the relations with less than 500 but more than 50 triples 3 as one-shot tasks. To show that our model is able to operate on large-scale KGs, <ref type="bibr">3</ref> We want to have enough triples for evaluation. we follow the similar process to build another larger dataset based on Wikidata <ref type="bibr">(Vrandeči´Vrandeči´c and Krötzsch, 2014</ref>). The dataset statistics are shown in <ref type="table">Table 1</ref>. Note that the Wiki-One dataset is an order of magnitude larger than any other bench- mark datasets in terms of the numbers of entities and triples. For NELL-One, we use 51/5/11 task relations for training/validation/testing. For Wiki- One, the division ratio is 133:16:34.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>In our experiments, we consider the follow- ing embedding-based methods: RESCAL (Nickel et al., 2011), <ref type="bibr">TransE (Bordes et al., 2013)</ref>, Dist- Mult ( <ref type="bibr">Yang et al., 2014</ref>) and <ref type="bibr">ComplEx (Trouillon et al., 2016)</ref>. For TransE, we use the code released by <ref type="bibr" target="#b16">Lin et al. (2015b)</ref>. For the other models, we have tried the code released by <ref type="bibr" target="#b32">Trouillon et al. (2016)</ref> but it gives much worse results than TransE on our datasets. Thus we use our own implemen- tations based on PyTorch ( <ref type="bibr" target="#b23">Paszke et al., 2017</ref>) for comparison. When evaluating existing embedding models, during training, we use not only the triples of background relations but also all the triples of the training relations and the one-shot training triple of those validation/test relations. However, since the proposed metric model does not require the embeddings of query relations, we only in- clude the triples of the background relations for embedding training. As TransE and DistMult use 1-D vectors to represent entities and relations, they can be directly used in our natching model. While for RESCAL, since it uses matrices to represent relations, we employ mean-pooling over these ma- trices to get 1-D embeddings. For the ComplEx model, we use the concatenation of the real part and imaginary part. The hyperparameters of our model are tuned on the validation task set and can be found in the appendix.</p><p>Apart from the above embedding models, a more recent method <ref type="bibr" target="#b6">(Dettmers et al., 2017</ref>) applies convolution to model relationships and achieves the best performance on several benchmarks. For every query (h, r, ?), their model enumerates the whole entity set to get positive and negative triples for training. We find that this training paradigm takes lots of computational resources when deal- ing with large entity sets and cannot scale to real- world KGs such as Wikidata 4 that have millions  of entities. For the scalability concern, our experi- ments only consider models that use negative sam- pling for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The main results of our methods are shown in Ta- ble 2. We denote our method as "GMatching" since our model is trained to match local graph patterns. We use mean reciprocal rank (MRR) and Hits@K to evaluate different models. We can see that our method produces consistent improve- ments over various embedding models on these one-shot relations. The improvements are even more substantial on the larger Wiki-One dataset.</p><p>To investigate the learning power of our model, we also try to train our metric model with ran- domly initialized embeddings. Surprisingly, al- though the results are worse than the metric mod- els with pre-trained embeddings, they are still su- perior to the baseline embedding models. This suggests that, by incorporating the neighbor enti- ties into our model, the embeddings of many rela- tions and entities actually get updated in an effec- tive way and provide useful information for our model to make predictions on test data. It is worth noting that once trained, our model can be used to predict any newly added relations without fine-tuning, while existing models usu- ally need to be re-trained to handle those newly added symbols. On a large real-world KG, this re-training process can be slow and highly com- putational expensive.</p><p>Remark on Model Selection Given the exis- tence of various KG embedding models, one in- teresting experiment is to incorporate model selec- tion into hyper-parameter tuning and choose the best validation model for testing.</p><p>If we think about comparing KG embedding and metric learning as two approaches, the re- sults from the model selection process can then be used as the "final" measurement for compar- ison. For example, the baseline KG embedding achieves best MRR on Wiki-One with RESCAL (11.9%), so we report the corresponding testing MRR (7.2%) as the final model selection result for KG embedding approach. In this way, at the top half of <ref type="table" target="#tab_4">Table 2</ref>, we select the best KG embedding method according to the validation performance. The results are highlighted with underlines. Sim- ilarly, we select the best metric learning approach at the bottom.</p><p>Our metric-based method outperforms KG em- bedding by a large margin from this perspective as well. Taking MRR as an example, the selected metric model achieves 17.1% on NELL-One and 20.0% on Wiki-One; while the results of KG em- bedding are 9.3% and 7.2%. The improvement is 7.8% and 12.8% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis on Neighbor-Encoder</head><p>As our model leverages entities' local graph struc- tures by encoding the neighbors, here we try to investigate the effect of the neighbor set by re- stricting the maximum number of neighbors. If the size of the true neighbor set is larger than the maximum limit, the neighbors are then selected by random sampling. <ref type="figure">Figure 4</ref> shows the learn- ing curves of different settings. These curves are based on the Hits@10 calculated on the valida- tion set. We see that encoding more neighbors   for every entity generally leads to better perfor- mance. We also observe that the model that en- codes 40 neighbors in maximum actually yields worse performance than the model that only en- codes 30 neighbors. We think the potential reason is that for some entity pairs, there are some local connections that are irrelevant and provide noisy information to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>We conduct ablation studies using the model that achieves the best Hits@10 on the NELL-One dataset. The results are shown in <ref type="table" target="#tab_7">Table 4</ref>. We use Hits@10 on validation and test set for com- parison, as the hyperparameters are selected us- ing this evaluation metric. We can see that both the matching processor 5 and the neighbor encoder play important roles in our model. Another im- portant observation is that the scaling factor 1/N e turns out to be very essential for the neighbor en- coder. Without scaling, the neighbor encoder ac- tually gives worse results compared to the simple embedding-based matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Performance on Different Relations</head><p>When testing various models, we observe that the results on different relations are actually of high variance. <ref type="table" target="#tab_6">Table 3</ref> shows the decomposed results on NELL-One generated by our best metric model (GMatching-ComplEx) and its corresponding em- bedding method. For reference, we also report the embedding model's performance under stan- dard training settings where 75% of the triples (in- stead of only one) are used for training and the rest are used for testing. We can see that rela- tions with smaller candidate sets are generally eas- ier and our model could even perform better than the embedding model trained under standard set- tings. For some relations such as athleteInjured- HisBodypart, their involved entities have very few connections in KG. It is as expected that one-shot learning on these kinds of relations is quite chal- lenging. Those relations with lots of (&gt;3000) can- didates are challenging for all models. Even for embedding model with more training triples, the performance on some relations is still very limited. This suggests that the knowledge graph comple- tion task is still far from being solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper introduces a one-shot relational learn- ing framework that could be used to predict new facts of long-tail relations in KGs. Our model leverages the local graph structure of entities and learns a differentiable metric to match entity pairs. In contrast to existing methods that usually need finetuning to adapt to new relations, our trained model can be directly used to predict any un- seen relation and also achieves much better per- formance in the one-shot setting. Our future work might consider incorporating external text data and also enhancing our model to make better use of multiple training examples in the few-shot learning case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>For the NELL dataset, we set embedding size as 100. For Wikidata, we set the embedding size as 50 for faster training with millions of triples. The embeddings are trained for 1,000 epochs. The other hyperparamters are tuned using the Hits@10 metric 6 on the validation tasks. For matching steps, the optimal setting is 2 for NELL-One and 4 for Wiki-One. For the number of neighbors, we find that the maximum limit 50 works the best for both datasets. For parameter updates, we use Adam ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) with the initial learning rate 0.001 and we half the learning rate after 200k update steps. The margin used in our loss function is 5.0. The dimension of LSTM's hidden size is 200.  <ref type="table">Table 5</ref>: 5-shot experiments on NELL-One. <ref type="bibr">6</ref> The percentage of correct answer ranks within top10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Few-Shot Experiments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The histogram of relation frequencies in Wikidata. There are a large portion of relations that only have a few triples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: a) and b): Our neighbor encoder operating on entity Leonardo da Vinci; c): The matching processor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of entities' degrees (numbers of neighbors) on our two datasets. Since we work on closed-set of entities, we draw the figure by considering the intersection between entities in our background knowledge G and the entities appearing in T meta−train , T meta−validation or T meta−test. Note that all triples in T meta−train , T meta−validation or T meta−test are removed from G. Upper: NELL; Lower: Wikidata.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>T r , we sample one triple as the reference/training triple D train r and a batch of other triples as the positive query/test triples D test r . The detail of the training process is shown in Algorithm 1. Our ex- periments are discussed in the next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>,</head><label></label><figDesc></figDesc><table>Leonardo 
da Vinci 

Occupation 

Painter 

vegetarianism 

Lifestyle 

Milan 

Work location 

Italian 

Ambassador 

Language 

Position held 

a) Local graph of entity Leonardo da Vinci 

... 

Relation: occupation 
Entity: painter 

b) Neighbor Encoder 

... 

... 

LSTM 

Similarity Score 

:  sum 
:  concatenation 
:  cosine similarity 

c) Matching Processor 

(da Vinci,  The Starry Night) (da Vinci, Mona Lisa) 

Reference 
Query 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Link prediction results on validation/test relations. KG embeddings baselines are shown at the top of the 
table and our one-shot learning (GMatching) results are shown at the bottom. Bold numbers denote the best results 
on meta-validation/meta-test. Underline numbers denote the model selection results from all KG embeddings 
baselines, or from all one-shot methods, i.e. selecting the method with the best validation score and reporting the 
corresponding test score. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results decomposed over different relations. " " denotes the results with standard training settings and 
"# Candidates" denotes the size of candidate entity set. 

0 100000 200000 300000 400000 500000 600000 

Traininggsteps 

0.05 

0.10 

0.15 

0.20 

0.25 

0.30 

0.35 

Hits@10 

Learninggcurves 

N=10 
N=20 
N=30 
N=40 
N=50 

Figure 4: The learning curves on NELL-one. Every 
run uses different number of neighbors. The y-axis is 
Hits@10 calculated on all the validation relations. 

Configuration 
Hits@10 

Full Model with ComplEx .308/.313 

w/o Matching Processor 
.266/.269 
w/o Neighbor Encoder 
.248/.296 
w/o Scaling Factor 
.229/.219 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 : Ablation on different components.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> K is a hyperparameter to be tuned.</note>

			<note place="foot" n="4"> On a GPU card with 12GB memory, we fail to run their ConvE model on Wiki-One with batch size 1.</note>

			<note place="foot" n="5"> Matching without Matching Processor is equivalent to matching using simple cosine similarity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by an IBM Faculty Award. We also thank the anonymous reviewers for their useful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06581</idno>
		<title level="m">Variational knowledge graph reasoning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Oneshot imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradly</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Siamese neural networks for oneshot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neverending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kisiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00837</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Meta networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 aaai spring symposium series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<title level="m">Modeling relational data with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03438</idno>
		<title level="m">Open-world knowledge graph completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<title level="m">Prototypical networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A graph-to-sequence model for amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
