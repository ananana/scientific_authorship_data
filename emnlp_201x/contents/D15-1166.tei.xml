<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of at-tentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architec-tures yields a new state-of-the-art result in the WMT&apos;15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale trans- lation tasks such as from English to <ref type="bibr">French (Luong et al., 2015</ref>) and English to German ( <ref type="bibr" target="#b6">Jean et al., 2015)</ref>. NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by <ref type="bibr" target="#b10">Luong et al. (2015)</ref> reads through all the source words until the end-of-sentence symbol &lt;eos&gt; is reached. It then starts emitting one tar- get word at a time, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to general- ize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in stan- dard MT ( <ref type="bibr" target="#b8">Koehn et al., 2003)</ref>. In parallel, the concept of "attention" has gained popularity recently in training neural networks, al- lowing models to learn alignments between dif- ferent modalities, e.g., between image objects and agent actions in the dynamic control problem ( <ref type="bibr" target="#b11">Mnih et al., 2014</ref>), between speech frames and text in the speech recognition task ( <ref type="bibr" target="#b3">Chorowski et al., 2014)</ref>, or between visual features of a picture and its text description in the image caption gen- eration task ( <ref type="bibr" target="#b14">Xu et al., 2015</ref>). In the context of NMT, <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> has successfully ap- plied such attentional mechanism to jointly trans- late and align words. To the best of our knowl- edge, there has not been any other work exploring the use of attention-based architectures for NMT.</p><p>In this work, we design, with simplicity and ef- fectiveness in mind, two novel types of attention-based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time. The former approach resembles the model of ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) but is simpler architec- turally. The latter can be viewed as an interesting blend between the hard and soft attention models proposed in ( <ref type="bibr" target="#b14">Xu et al., 2015)</ref>: it is computation- ally less expensive than the global model or the soft attention; at the same time, unlike the hard at- tention, the local attention is differentiable, mak- ing it easier to implement and train. <ref type="bibr">2</ref> Besides, we also examine various alignment functions for our attention-based models.</p><p>Experimentally, we demonstrate that both of our approaches are effective in the WMT trans- lation tasks between English and German in both directions. Our attentional models yield a boost of up to 5.0 BLEU over non-attentional systems which already incorporate known techniques such as dropout. For English to German translation, we achieve new state-of-the-art (SOTA) results for both WMT'14 and WMT'15, outperforming previous SOTA systems, backed by NMT mod- els and n-gram LM rerankers, by more than 1.0 BLEU. We conduct extensive analysis to evaluate our models in terms of learning, the ability to han- dle long sentences, choices of attentional architec- tures, alignment quality, and translation outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>A neural machine translation system is a neural network that directly models the conditional prob- ability p(y|x) of translating a source sentence, x 1 , . . . , x n , to a target sentence, y 1 , . . . , y m . 3 A basic form of NMT consists of two components: (a) an encoder which computes a representation s for each source sentence and (b) a decoder which generates one target word at a time and hence de- composes the conditional probability as:</p><formula xml:id="formula_0">log p(y|x) = m j=1 log p (y j |y &lt;j , s) (1)</formula><p>A natural choice to model such a decomposi- tion in the decoder is to use a recurrent neural net- work (RNN) architecture, which most of the re- 2 There is a recent work by <ref type="bibr" target="#b5">Gregor et al. (2015)</ref>, which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. <ref type="bibr">3</ref> All sentences are assumed to terminate with a special "end-of-sentence" token &lt;eos&gt;. cent NMT work such as <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b13">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b10">Luong et al., 2015;</ref><ref type="bibr" target="#b6">Jean et al., 2015)</ref> have in common. They, however, dif- fer in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. <ref type="bibr" target="#b7">Kalchbrenner and Blunsom (2013)</ref> used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both <ref type="bibr" target="#b13">Sutskever et al. (2014)</ref> and <ref type="bibr" target="#b10">Luong et al. (2015)</ref> stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. , <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, and <ref type="bibr" target="#b6">Jean et al. (2015)</ref> all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated re- current unit (GRU), for both components. <ref type="bibr">4</ref> In more detail, one can parameterize the proba- bility of decoding each word y j as:</p><formula xml:id="formula_1">p (y j |y &lt;j , s) = softmax (g (h j ))<label>(2)</label></formula><p>with g being the transformation function that out- puts a vocabulary-sized vector. <ref type="bibr">5</ref> Here, h j is the RNN hidden unit, abstractly computed as:</p><formula xml:id="formula_2">h j = f (h j−1 , s),<label>(3)</label></formula><p>where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b13">Sutskever et al., 2014;</ref><ref type="bibr" target="#b10">Luong et al., 2015)</ref>, the source representation s is only used once to initialize the decoder hidden state. On the other hand, in ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b6">Jean et al., 2015)</ref> and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next. In this work, following <ref type="bibr" target="#b13">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b10">Luong et al., 2015)</ref>, we use the stacking LSTM architecture for our NMT systems, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We use the LSTM unit defined in ( . Our training objective is formulated as follows:</p><formula xml:id="formula_3">J t = (x,y)∈D − log p(y|x)<label>(4)</label></formula><p>with D being our parallel training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attention-based Models</head><p>Our various attention-based models are classifed into two broad categories, global and local. These classes differ in terms of whether the "attention" is placed on all source positions or on only a few source positions. We illustrate these two model types in <ref type="figure">Figure 2</ref> and 3 respectively. Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state h t at the top layer of a stacking LSTM. The goal is then to derive a context vector c t that captures rel- evant source-side information to help predict the current target word y t . While these models differ in how the context vector c t is derived, they share the same subsequent steps.</p><p>Specifically, given the target hidden state h t and the source-side context vector c t , we employ a simple concatenation layer to combine the infor- mation from both vectors to produce an attentional hidden state as follows:</p><formula xml:id="formula_4">˜ h t = tanh(W c [c t ; h t ])<label>(5)</label></formula><p>The attentional vector˜hvector˜ vector˜h t is then fed through the softmax layer to produce the predictive distribu- tion formulated as:</p><formula xml:id="formula_5">p(y t |y &lt;t , x) = softmax(W s ˜ h t )<label>(6)</label></formula><p>We now detail how each model type computes the source-side context vector c t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Global Attention</head><p>The idea of a global attentional model is to con- sider all the hidden states of the encoder when de- riving the context vector c t . In this model type, a variable-length alignment vector a t , whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state h t with each source hidden state ¯ h s :</p><formula xml:id="formula_6">a t (s) = align(h t , ¯ h s ) (7) = exp score(h t , ¯ h s ) s ′ exp score(h t , ¯ h s ′ )</formula><p>Here, score is referred as a content-based function for which we consider three different alternatives:</p><formula xml:id="formula_7">score(h t , ¯ h s ) =      h ⊤ t ¯ h s dot h ⊤ t W a ¯ h s general W a [h t ; ¯ h s ] concat (8) y t ˜ h t c t a t h t ¯ h s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global align weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Layer</head><p>Context vector <ref type="figure">Figure 2</ref>: Global attentional model -at each time step t, the model infers a variable-length align- ment weight vector a t based on the current target state h t and all source states ¯ h s . A global context vector c t is then computed as the weighted aver- age, according to a t , over all the source states.</p><p>Besides, in our early attempts to build attention- based models, we use a location-based function in which the alignment scores are computed from solely the target hidden state h t as follows:</p><formula xml:id="formula_8">a t = softmax(W a h t )</formula><p>location <ref type="formula">(9)</ref> Given the alignment vector as weights, the context vector c t is computed as the weighted average over all the source hidden states. <ref type="bibr">6</ref> Comparison to ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) -While our global attention approach is similar in spirit to the model proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, there are several key differences which reflect how we have both simplified and generalized from the original model. First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in <ref type="figure">Figure 2</ref>. <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, on the other hand, use the concatena- tion of the forward and backward source hidden states in the bi-directional encoder and target hid- den states in their non-stacking uni-directional de- coder. Second, our computation path is simpler; we go from h t → a t → c t → ˜ h t then make a prediction as detailed in Eq. (5), Eq. (6), and <ref type="figure">Figure 2</ref>. On the other hand, at any time t, Bah- danau et al. (2015) build from the previous hidden state h t−1 → a t → c t → h t , which, in turn,</p><formula xml:id="formula_9">y t ˜ h t c t a t h t p t ¯ h s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Layer</head><p>Context vector</p><p>Local weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aligned position</head><p>Figure 3: Local attention model -the model first predicts a single aligned position p t for the current target word. A window centered around the source position p t is then used to compute a context vec- tor c t , a weighted average of the source hidden states in the window. The weights a t are inferred from the current target state h t and those source states ¯ h s in the window.</p><p>goes through a deep-output and a maxout layer before making predictions. <ref type="bibr">7</ref> Lastly, <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Attention</head><p>The global attention has a drawback that it has to attend to all words on the source side for each tar- get word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents. To address this deficiency, we propose a local attentional mech- anism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the soft and hard attentional models pro- posed by <ref type="bibr" target="#b14">Xu et al. (2015)</ref> to tackle the image cap- tion generation task. In their work, soft attention refers to the global attention approach in which weights are placed "softly" over all patches in the source image. The hard attention, on the other hand, selects one patch of the image to attend to at a time. While less expensive at inference time, the hard attention model is non-differentiable and re- quires more complicated techniques such as vari- ance reduction or reinforcement learning to train. <ref type="bibr">7</ref> We will refer to this difference again in Section 3.3.</p><p>Our local attention mechanism selectively fo- cuses on a small window of context and is differ- entiable. This approach has an advantage of avoid- ing the expensive computation incurred in the soft attention and at the same time, is easier to train than the hard attention approach. In concrete de- tails, the model first generates an aligned position p t for each target word at time t. The context vec- tor c t is then derived as a weighted average over the set of source hidden states within the window [p t −D, p t +D]; D is empirically selected. 8 Unlike the global approach, the local alignment vector a t is now fixed-dimensional, i.e., ∈ R 2D+1 . We con- sider two variants of the model as below.</p><p>Monotonic alignment (local-m) -we simply set p t = t assuming that source and target sequences are roughly monotonically aligned. The alignment vector a t is defined according to Eq. (7). <ref type="bibr">9</ref> Predictive alignment (local-p) -instead of as- suming monotonic alignments, our model predicts an aligned position as follows:</p><formula xml:id="formula_10">p t = S · sigmoid(v ⊤ p tanh(W p h t )),<label>(10)</label></formula><p>W p and v p are the model parameters which will be learned to predict positions. S is the source sen- tence length. As a result of sigmoid,</p><formula xml:id="formula_11">p t ∈ [0, S].</formula><p>To favor alignment points near p t , we place a Gaussian distribution centered around p t . Specif- ically, our alignment weights are now defined as:</p><formula xml:id="formula_12">a t (s) = align(h t , ¯ h s ) exp − (s − p t ) 2 2σ 2<label>(11)</label></formula><p>We use the same align function as in Eq. <ref type="formula">(7)</ref> and the standard deviation is empirically set as σ = D 2 . It is important to note that p t is a real nummber; whereas s is an integer within the window cen- tered at p t . <ref type="bibr">10</ref> Comparison to ( <ref type="bibr" target="#b5">Gregor et al., 2015</ref>) -have pro- posed a selective attention mechanism, very simi- lar to our local attention, for the image generation task. Their approach allows the model to select an image patch of varying location and zoom. We, instead, use the same "zoom" for all target posi- tions, which greatly simplifies the formulation and still achieves good performance. <ref type="bibr">8</ref> If the window crosses the sentence boundaries, we sim- ply ignore the outside part and consider words in the window. <ref type="bibr">9</ref> local-m is the same as the global model except that the vector at is fixed-length and shorter.</p><p>10 local-p is similar to the local-m model except that we dynamically compute pt and use a Gaussian distribution to modify the original alignment weights align(ht, ¯ hs) as shown in Eq. (11). By utilizing pt to derive at, we can com- pute backprop gradients for Wp and vp. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Input-feeding Approach</head><p>In our proposed global and local approaches, the attentional decisions are made independently, which is suboptimal. Whereas, in standard MT, a coverage set is often maintained during the translation process to keep track of which source words have been translated. Likewise, in atten- tional NMTs, alignment decisions should be made jointly taking into account past alignment infor- mation. To address that, we propose an input- feeding approach in which attentional vectors˜hvectors˜ vectors˜h t are concatenated with inputs at the next time steps as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>. <ref type="bibr">11</ref> The effects of hav- ing such connections are two-fold: (a) we hope to make the model fully aware of previous align- ment choices and (b) we create a very deep net- work spanning both horizontally and vertically.</p><p>Comparison to other work -Bahdanau et al. (2015) use context vectors, similar to our c t , in building subsequent hidden states, which can also achieve the "coverage" effect. However, there has not been any analysis of whether such connections are useful as done in this work. Also, our approach is more general; as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, it can be applied to general stacking recurrent architectures, including non-attentional models. <ref type="bibr" target="#b14">Xu et al. (2015)</ref> propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process. Such a constraint can <ref type="bibr">11</ref> If n is the number of LSTM cells, the input size of the first LSTM layer is 2n; those of subsequent layers are n. also be useful to capture the coverage set effect in NMT that we mentioned earlier. However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of our models on the WMT translation tasks between English and Ger- man in both directions. newstest2013 (3000 sen- tences) is used as a development set to select our hyperparameters. Translation performances are reported in case-sensitive BLEU ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>) on newstest2014 (2737 sentences) and new- stest2015 (2169 sentences). Following ( <ref type="bibr" target="#b10">Luong et al., 2015)</ref>, we report translation quality using two types of BLEU: (a) tokenized 12 BLEU to be com- parable with existing NMT work and (b) NIST 13 BLEU to be comparable with WMT results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>All our models are trained on the WMT'14 train- ing data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both lan- guages. Words not in these shortlisted vocabular- ies are converted into a universal token &lt;unk&gt;.</p><p>When training our NMT systems, following ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b6">Jean et al., 2015</ref>), we fil- ter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embed- dings. We follow ( <ref type="bibr" target="#b13">Sutskever et al., 2014;</ref><ref type="bibr" target="#b10">Luong et al., 2015</ref>) in training NMT with similar set- tings: (a) our parameters are uniformly initialized in [−0.1, 0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learning rate schedule is employed -we start with a learning rate of 1; after 5 epochs, we begin to halve the learning rate ev- ery epoch, (d) our mini-batch size is 128, and (e) the normalized gradient is rescaled whenever its norm exceeds 5. Additionally, we also use dropout for our LSTMs as suggested by . For dropout models, we train for 12 epochs and start halving the learning rate after 8 epochs.</p><p>Our code is implemented in MATLAB. When <ref type="bibr">12</ref> All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl. <ref type="bibr">13</ref> With the mteval-v13a script as per WMT guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Ppl BLEU Winning WMT'14 system -phrase-based + large LM ( <ref type="bibr" target="#b1">Buck et al., 2014)</ref> 20.7 Existing NMT systems RNNsearch ( <ref type="bibr" target="#b6">Jean et al., 2015)</ref> 16.5 RNNsearch + unk replace ( <ref type="bibr" target="#b6">Jean et al., 2015)</ref> 19.0 RNNsearch + unk replace + large vocab + ensemble 8 models ( <ref type="bibr" target="#b6">Jean et al., 2015)</ref> 21.6 Our NMT systems Base 10.6 11.3 Base + reverse 9.9 12.6 (+1.3) Base + reverse + dropout 8.1 14.0 (+1.4) Base + reverse + dropout + global attention (location) 7.3 16.8 (+2.8) Base + reverse + dropout + global attention (location) + feed input 6.4 18.1 (+1.3) Base + reverse + dropout + local-p attention (general) + feed input 5.9 19.0 (+0.9) Base + reverse + dropout + local-p attention (general) + feed input + unk replace 20.9 (+1.9) Ensemble 8 models + unk replace 23.0 (+2.1) <ref type="table">Table 1</ref>: WMT'14 English-German results -shown are the perplexities (ppl) and the tokenized BLEU scores of various systems on newstest2014. We highlight the best system in bold and give progressive improvements in italic between consecutive systems. local-p referes to the local attention with predictive alignments. We indicate for each attention model the alignment score function used in pararentheses.</p><p>running on a single GPU device Tesla K40, we achieve a speed of 1K target words per second. It takes 7-10 days to completely train a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">English-German Results</head><p>We compare our NMT systems in the English- German task with various other systems. These include the winning system in WMT'14 <ref type="bibr" target="#b1">(Buck et al., 2014</ref>), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. For end-to-end neu- ral machine translation systems, to the best of our knowledge, ( <ref type="bibr" target="#b6">Jean et al., 2015</ref>) is the only work ex- perimenting with this language pair and currently the SOTA system. We only present results for some of our attention models and will later ana- lyze the rest in Section 5. As shown in <ref type="table">Table 1</ref>, we achieve progressive improvements when (a) reversing the source sen- tence, +1.3 BLEU, as proposed in <ref type="bibr" target="#b13">(Sutskever et al., 2014</ref>) and (b) using dropout, +1.4 BLEU. On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional sys- tem of <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> (row RNNSearch). When (d) using the input-feeding approach, we seize another notable gain of +1.3 BLEU and out- perform their system. The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improve- ment of +0.9 BLEU on top of the global attention model. It is interesting to observe the trend pre- viously reported in ( <ref type="bibr" target="#b10">Luong et al., 2015</ref>) that per- plexity strongly correlates with translation quality. In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which al- ready includes known techniques such as source reversing and dropout.</p><p>The unknown replacement technique proposed in ( <ref type="bibr" target="#b10">Luong et al., 2015;</ref><ref type="bibr" target="#b6">Jean et al., 2015</ref>) yields another nice gain of +1.9 BLEU, demonstrating that our attentional models do learn useful align- ments for unknown works. Finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able to achieve a new SOTA result of 23.0 BLEU, outperforming the existing best system (Jean et al., 2015) by +1.4 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>BLEU SOTA -NMT + 5-gram rerank (MILA) 24.9 Our ensemble 8 models + unk replace 25.9 <ref type="table">Table 2</ref>: WMT'15 English-German results - NIST BLEU scores of the existing WMT'15 SOTA system and our best one on newstest2015.</p><p>Latest results in WMT'15 -despite the fact that our models were trained on WMT'14 with slightly less data, we test them on newstest2015 to demon- strate that they can generalize well to different test sets. As shown in <ref type="table">Table 2</ref>, our best system es-System Ppl. BLEU WMT'15 systems SOTA -phrase-based (Edinburgh)</p><p>29.2 NMT + 5-gram rerank (MILA)</p><p>27.6 Our NMT systems Base (reverse) 14.3 16.9 + global (location) 12.7 19.1 (+2.2) + global (location) + feed 10.9 20.1 (+1.0) + global (dot) + drop + feed 9.7 22.8 (+2.7) + global (dot) + drop + feed + unk 24.9 (+2.1) <ref type="table">Table 3</ref>: WMT'15 German-English results - performances of various systems (similar to Ta- ble 1). The base system already includes source reversing on which we add global attention, dropout, input feeding, and unk replacement.</p><p>tablishes a new SOTA performance of 25.9 BLEU, outperforming the existing best system backed by NMT and a 5-gram LM reranker by +1.0 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">German-English Results</head><p>We carry out a similar set of experiments for the WMT'15 translation task from German to En- glish. While our systems have not yet matched the performance of the SOTA system, we never- theless show the effectiveness of our approaches with large and progressive gains in terms of BLEU as illustrated in <ref type="table">Table 3</ref>. The attentional mech- anism gives us +2.2 BLEU gain and on top of that, we obtain another boost of up to +1.0 BLEU from the input-feeding approach. Using a better alignment function, the content-based dot product one, together with dropout yields another gain of +2.7 BLEU. Lastly, when applying the unknown word replacement technique, we seize an addi- tional +2.1 BLEU, demonstrating the usefulness of attention in aligning rare words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We conduct extensive analysis to better understand our models in terms of learning, the ability to han- dle long sentences, choices of attentional archi- tectures, and alignment quality. All models con- sidered here are English-German NMT systems tested on newstest2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning curves</head><p>We compare models built on top of one another as listed in <ref type="table">Table 1</ref> Mini−batches Test cost basic basic+reverse basic+reverse+dropout basic+reverse+dropout+globalAttn basic+reverse+dropout+globalAttn+feedInput basic+reverse+dropout+pLocalAttn+feedInput <ref type="figure">Figure 5</ref>: Learning curves -test cost (ln perplex- ity) on newstest2014 for English-German NMTs as training progresses.</p><p>proach and the local attention model also demon- strate their abilities in driving the test costs lower.</p><p>The non-attentional model with dropout (the blue + curve) learns slower than other non-dropout models, but as time goes by, it becomes more ro- bust in terms of minimizing test errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Translating Long Sentences</head><p>We follow ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) to group sen- tences of similar lengths together and compute a BLEU score per group. As demonstrated in <ref type="figure" target="#fig_3">Fig- ure 6</ref>, our attentional models are more effective than the other non-attentional model in handling long sentences: the translation quality does not de- grade as sentences become longer. Our best model (the blue + curve) outperforms all other systems in all length buckets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Choices of Attentional Architectures</head><p>We examine different attention models (global, local-m, local-p) and different alignment func- tions (location, dot, general, concat) as described in Section 3. Due to limited resources, we can- not run all the possible combinations. However,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Ppl BLEU Before After unk global (location)</p><p>6.4 18.1 19.3 (+1.2) global (dot) 6.1 18.6 20.5 (+1.9) global (general) 6.1 17.3</p><formula xml:id="formula_13">19.1 (+1.8) local-m (dot) &gt;7.0 x x local-m (general)</formula><p>6.2 18.6 20.4 (+1.8) local-p (dot)</p><p>6.6 18.0 19.6 (+1.9) local-p (general)</p><p>5.9 19 20.9 (+1.9) <ref type="table">Table 4</ref>: Attentional Architectures -perfor- mances of different attentional models. We trained two local-m (dot) models; both have ppl &gt; 7.0.</p><p>results in <ref type="table">Table 4</ref> do give us some idea about dif- ferent choices. The location-based function does not learn good alignments: the global (location) model can only obtain a small gain when perform- ing unknown word replacement compared to using other alignment functions. 14 For content-based functions, our implementation of concat does not yield good performances and more analysis should be done to understand the reason. <ref type="bibr">15</ref> It is interest- ing to observe that dot works well for the global attention and general is better for the local atten- tion. Among the different models, the local atten- tion model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Alignment Quality</head><p>A by-product of attentional models are word align- ments. While ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> visualized alignments for some sample sentences and ob- served gains in translation quality as an indica- tion of a working attention model, no work has as- sessed the alignments learned as a whole. In con- trast, we set out to evaluate the alignment quality using the alignment error rate (AER) metric. Given the gold alignment data provided by RWTH for 508 English-German Europarl sen- tences, we "force" decode our attentional models to produce translations that match the references. We extract only one-to-one alignments by select- ing the source word with the highest alignment <ref type="bibr">14</ref> There is a subtle difference in how we retrieve align- ments for the different alignment functions. At time step t in which we receive yt−1 as input and then compute ht, at, ct, and˜htand˜ and˜ht before predicting yt, the alignment vector at is used as alignment weights for (a) the predicted word yt in the location-based alignment functions and (b) the input word yt−1 in the content-based functions. <ref type="bibr">15</ref> With concat, the perplexities achieved by different mod- els are 6.7 (global), 7.1 (local-m), and 7.1 (local-p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AER global (location) 0.39 local-m (general) 0.34 local-p (general) 0.36 ensemble 0.34 Berkeley Aligner 0.32 <ref type="table">Table 6</ref>: AER scores -results of various models on the RWTH English-German alignment data.</p><p>weight per target word. Nevertheless, as shown in <ref type="table">Table 6</ref>, we were able to achieve AER scores com- parable to the one-to-many alignments obtained by the Berkeley aligner ( <ref type="bibr" target="#b9">Liang et al., 2006</ref>). <ref type="bibr">16</ref> We also found that the alignments produced by local attention models achieve lower AERs than those of the global one. The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observa- tion that AER and translation scores are not well correlated <ref type="bibr" target="#b4">(Fraser and Marcu, 2007)</ref>. Due to space constraint, we can only show alignment visualiza- tions in the arXiv version of our paper. <ref type="bibr">17</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sample Translations</head><p>We show in <ref type="table">Table 5</ref> sample translations in both directions. It it appealing to observe the ef- fect of attentional models in correctly translat- ing names such as "Miranda Kerr" and "Roger Dow". Non-attentional models, while producing sensible names from a language model perspec- tive, lack the direct connections from the source side to make correct translations.</p><p>We also observed an interesting case in the second English-German example, which requires translating the doubly-negated phrase, "not in- compatible". The attentional model correctly produces "nicht . . . unvereinbar"; whereas the non-attentional model generates "nicht vereinbar", meaning "not compatible". <ref type="bibr">18</ref> The attentional model also demonstrates its superiority in trans- lating long sentences as in the last example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose two simple and effec- tive attentional mechanisms for neural machine English-German translations src Orlando Bloom and Miranda Kerr still love each other ref Orlando Bloom und Miranda Kerr lieben sich noch immer best Orlando Bloom und Miranda Kerr lieben einander noch immer . base Orlando Bloom und Lucas Miranda lieben einander noch immer . src ′′ We ′ re pleased the FAA recognizes that an enjoyable passenger experience is not incompatible with safety and security , ′′ said Roger Dow , CEO of the U.S. Travel Association . ref " Wir freuen uns , dass die FAA erkennt , dass ein angenehmes Passagiererlebnis nicht im Wider- spruch zur Sicherheit steht " , sagte Roger Dow , CEO der U.S. Travel Association . best ′′ Wir freuen uns , dass die FAA anerkennt , dass ein angenehmes ist nicht mit Sicherheit und Sicherheit unvereinbar ist ′′ , sagte Roger Dow , CEO der US -die . base ′′ Wir freuen unsüberuns¨unsüber die &lt;unk&gt; , dass ein &lt;unk&gt; &lt;unk&gt; mit Sicherheit nicht vereinbar ist mit Sicherheit und Sicherheit ′′ , sagte Roger Cameron , CEO der US -&lt;unk&gt; . German-English translations src In einem Interview sagte Bloom jedoch , dass er und Kerr sich noch immer lieben . ref However , in an interview , Bloom has said that he and Kerr still love each other . best In an interview , however , Bloom said that he and Kerr still love . base However , in an interview , Bloom said that he and Tina were still &lt;unk&gt; . src Wegen der von Berlin und der Europäischen Zentralbank verhängten strengen Sparpolitik in Verbindung mit der Zwangsjacke , in die die jeweilige nationale Wirtschaft durch das Festhal- ten an der gemeinsamen Währung genötigt wird , sind viele Menschen der Ansicht , das Projekt Europa sei zu weit gegangen ref The austerity imposed by Berlin and the European Central Bank , coupled with the straitjacket imposed on national economies through adherence to the common currency , has led many people to think Project Europe has gone too far . best Because of the strict austerity measures imposed by Berlin and the European Central Bank in connection with the straitjacket in which the respective national economy is forced to adhere to the common currency , many people believe that the European project has gone too far . base Because of the pressure imposed by the European Central Bank and the Federal Central Bank with the strict austerity imposed on the national economy in the face of the single currency , many people believe that the European project has gone too far . <ref type="table">Table 5</ref>: Sample translations -for each example, we show the source (src), the human translation (ref), the translation from our best model (best), and the translation of a non-attentional model (base). We italicize some correct translation segments and highlight a few wrong ones in bold.</p><p>translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time. We test the effectiveness of our mod- els in the WMT translation tasks between En- glish and German in both directions. Our local attention yields large gains of up to 5.0 BLEU over non-attentional models that already incorpo- rate known techniques such as dropout. For the English to German translation direction, our en- semble model has established new state-of-the-art results for both WMT'14 and WMT'15.</p><p>We have compared various alignment functions and shed light on which functions are best for which attentional models. Our analysis shows that attention-based NMT models are superior to non- attentional ones in many cases, for example in translating names and handling long sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neural machine translation-a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt; marks the end of a sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Input-feeding approach-Attentional vectors˜hvectors˜ vectors˜h t are fed as inputs to the next time steps to inform the model about past alignment decisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>It is pleasant to observe in Fig- ure 5 a clear separation between non-attentional and attentional models. The input-feeding ap-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Length Analysis-translation qualities of different systems as sentences become longer.</figDesc></figure>

			<note place="foot" n="1"> All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt.</note>

			<note place="foot" n="4"> They all used a single RNN layer except for the latter two works which utilized a bidirectional RNN for the encoder. 5 One can provide g with other inputs such as the currently predicted word yj as in (Bahdanau et al., 2015).</note>

			<note place="foot" n="6"> Eq. (9) implies that all alignment vectors at are of the same length. For short sentences, we only use the top part of at and for long sentences, we ignore words near the end.</note>

			<note place="foot" n="16"> We concatenate the 508 sentence pairs with 1M sentence pairs from WMT and run the Berkeley aligner. 17 http://arxiv.org/abs/1508.04025 18 The reference uses a more fancy translation of &quot;incompatible&quot;, which is &quot;im Widerspruch zu etwas stehen&quot;. Both models, however, failed to translate &quot;passenger experience&quot;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We gratefully acknowledge support from a gift from Bloomberg L.P. and the support of NVIDIA Corporation with the donation of Tesla K40 GPUs. We thank Andrew Ng and his group as well as the Stanford Research Computing for letting us use their computing resources. We thank Rus-sell Stewart for helpful discussions on the models. Lastly, we thank Quoc Le, Ilya Sutskever, Oriol Vinyals, Richard Socher, Michael Kayser, Jiwei Li, Panupong Pasupat, Kelvin Gu, members of the Stanford NLP Group and the annonymous review-ers for their valuable comments and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent NN: first results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring word alignment quality for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="303" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
