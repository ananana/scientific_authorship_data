<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Lidong</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
							<email>wguo@linkedin.com, hangli.hl@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">&amp;apos;</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ark</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Technologies</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ‡ AI Lab</orgName>
								<orgName type="institution" key="instit2">Tencent Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization *</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2081" to="2090"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When people recall and digest what they have read for writing summaries, the important content is more likely to attract their attention. Inspired by this observation , we propose a cascaded attention based unsupervised model to estimate the salience information from the text for compressive multi-document summariza-tion. The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of output vectors , we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important informa- tion of a set of documents describing a topic 1 <ref type="bibr" target="#b20">(Luhn, 1958;</ref><ref type="bibr" target="#b6">Edmundson, 1969;</ref><ref type="bibr" target="#b9">Goldstein et al., 2000;</ref><ref type="bibr" target="#b8">Erkan and Radev, 2004b;</ref><ref type="bibr" target="#b31">Wan et al., 2007;</ref><ref type="bibr" target="#b27">Nenkova and McKeown, 2012)</ref>. Considering the procedure of summary writing by humans, when people read, they will remember and forget part * The work described in this paper is supported by grants from the Research and Development Grant of Huawei Tech- nologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Re- gion, China (Project Code: 14203414).</p><p>1 A topic represents a real event, e.g., "AlphaGo versus Lee Sedol". of the content. Information which is more impor- tant may make a deep impression easily. When people recall and digest what they have read to write summaries, the important information usu- ally attracts more attention (the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other per- ceivable information 2 ) since it may repeatedly ap- pears in some documents, or be positioned in the beginning paragraphs.</p><p>In the context of multi-document summariza- tion, to generate a summary sentence for a key as- pect of the topic, we need to find its relevant parts in the original documents, which may attract more attention. The semantic parts with high atten- tion weights plausibly represent and reconstruct the topic's main idea. To this end, we propose a cascaded neural attention model to distill salient information from the input documents in an un- supervised data reconstruction manner, which in- cludes two components: reader and recaller. The reader is a gated recurrent neural network (LSTM or GRU) based sentence sequence encoder which can map all the sentences of the topic into a global representation, with the mechanism of remember- ing and forgetting. The recaller decodes the global representation into significantly fewer diversified vectors for distillation and concentration. A cas- caded attention mechanism is designed by incor- porating attentions on both the hidden layer (dense distributed representation of a sentence) and the output layer (sparse bag-of-words representation of summary information). It is worth noting that the output vectors of the recaller can be viewed as word salience, and the attention matrix can be used as sentence salience. Both of them are auto- matically learned by data reconstruction in an un-supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are in- tegrated into a phrase-based optimization frame- work for compressive summary generation.</p><p>In fact, the notion of "attention" has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b21">Luong et al., 2015)</ref>. However, very few previous works employ attention mechanism to tackle MDS. <ref type="bibr" target="#b28">Rush et al. (2015)</ref> and <ref type="bibr" target="#b26">Nallapati et al. (2016)</ref> employed attention-based sequence- to-sequence (seq2seq) framework only for sen- tence summarization. <ref type="bibr" target="#b10">Gu et al. (2016)</ref>, <ref type="bibr" target="#b3">Cheng and Lapata (2016)</ref>, and <ref type="bibr" target="#b26">Nallapati et al. (2016)</ref> also utilized seq2seq based framework with attention modeling for short text or single document sum- marization. Different from their works, our frame- work aims at conducting multi-document summa- rization in an unsupervised manner.</p><p>Our contributions are as follows: <ref type="formula" target="#formula_0">(1)</ref> We pro- pose a cascaded attention model that captures salient information in different semantic represen- tations. (2) The attention weights are learned au- tomatically by an unsupervised data reconstruc- tion framework which can capture the sentence salience. By adding sparsity constraints on the number of output vectors of the recaller, we can generate condensed vectors which can be treated as word salience; (3) We thoroughly investigate the performance of combining different attention architectures and cascaded structures. Experimen- tal results on some benchmark data sets show that our framework achieves better performance than the state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Our framework has two phases, namely, in- formation distillation for finding salient words/sentences, and compressive summary generation. For the first phase, our cascaded neu- ral attention model consists of two components: reader and recaller as shown in <ref type="figure">Figure 1</ref>. The reader component reads in all the sentences in the document set corresponding to the topic/event. The information distillation happens in the re- caller component where only the most important information is preserved. Precisely, the recaller outputs fewer vectors s than that of the input Enc Dec <ref type="figure">Figure 1</ref>: Our cascaded attention based unsuper- vised information distillation framework. X is the original input sentence sequence of a topic. H i is the hidden vectors of sentences. "Enc" and "Dec" represent the RNN-based encoding and decoding layer respectively. c g is the global representation for the whole topic. A h and A o are the distilled attention matrices for the hidden layer and the out- put layer respectively, representing the salience of sentences. H o is the output hidden layer. s 1 and s 2 are the distilled condensed vectors representing the salience of words. Note that they are neither origin inputs nor golden summaries. sentences x for the reader.</p><p>After the learning of the neural attention model finishes, the obtained salience information will be used in the second phase for compressive sum- mary generation. This phase consists of two com- ponents: (i) the coarse-grained sentence compres- sion component which can filter the trivial infor- mation based on the output vectors S from the neural attention model; (ii) the unified phrase- based optimization method for summary genera- tion in which the attention matrix A o is used to conduct fine-grained compression and summary construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Modeling for Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Reader</head><p>In the reader stage, for each topic, we extract all the sentences X = {x 1 , x 2 , . . . , x m } from the set of input documents corresponding to a topic and generate a sentence sequence with length m. The sentence order is the same as the original order of the documents. Then the reader reads the whole sequence sentence by sentence. We employ the bag-of-words (BOW) representation as the initial semantic representation for sentences. Assume that the dictionary size is k, then x i ∈ R k .</p><p>Sparsity is one common problem for the BOW representation, especially when each vector is gen- erated from a single sentence. Moreover, down- stream algorithms might suffer from the curse of dimensionality. To solve these problems, we add a hidden layer H v (v for input layer) which is a densely distributed representation above the input layer as shown in <ref type="figure">Figure 1</ref>. Such distributed rep- resentation can provide better generalization than BOW representation in many different tasks ( <ref type="bibr" target="#b13">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013)</ref>. Specif- ically, the input hidden layer will project the input sentence vector x j to a new space R h according to Equation 1. Then we obtain a new sentence se- quence</p><formula xml:id="formula_0">H v = [h v 1 , h v 2 , . . . , h v m ]. h v j = tanh(W v xh x j + b v h )<label>(1)</label></formula><p>where W v xh and b v h are the weight and bias respec- tively. The superscript v means that the variables are from the input layer.</p><p>While reading the sentence sequence, the reader should have the ability of remembering and for- getting. Therefore, we employ the RNN models with various gates (input gate, forget gate, etc.) to imitate the remembering and forgetting mech- anism. Then the RNN based neural encoder (the third layer in <ref type="figure">Figure 1</ref>) will map the whole embed- ding sequence to a single vector c g which can be regarded as a global representation for the whole topic. Let t be the index of the sequence state for the sentence x t , the hidden unit h e t (e for encoder RNN) of the RNN encoder can be computed as:</p><formula xml:id="formula_1">h e t = f (h e t−1 , h v t )<label>(2)</label></formula><p>where the RNN f (·) computes the current hidden state given the previous hidden state h e t−1 and the sentence embedding h v t . The encoder generates hidden states {h e t } over all time steps. The last state {h e m } is extracted as the global representa- tion c g for the whole topic. The structure for f (·) can be either an LSTM (Hochreiter and Schmid- huber, 1997) or GRU ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Recaller</head><p>The recaller stage is a reverse of the reader stage, but it outputs less number of vectors in S as shown in <ref type="figure">Figure 1</ref>. Given the global representation c g , the past hidden state h d t−1 (d for decoder RNN) from the decoder layer, an RNN based decoder gener- ates several hidden states according to:</p><formula xml:id="formula_2">h d t = f (h d t−1 , c g )<label>(3)</label></formula><p>We use c g to initialize the first decoder hidden state. The decoder will generate several hidden states {h d t } over pre-defined time steps. Then, similar to the reader stage, we add an output hid- den layer after the decoder layer:</p><formula xml:id="formula_3">h o t = tanh(W o hh h d t + b o h ) (4)</formula><p>where W o hh and b o h are the weight and bias respec- tively for the projection from h d t to h o t . Finally, the output layer maps these hidden vectors to the con- densed vectors S = [s 1 , s 2 , . . . , s n ], Each output vector s t has the same dimension k as the input BOW vectors and is obtained as follows:</p><formula xml:id="formula_4">s t = σ(W hs h o t + b s )<label>(5)</label></formula><p>For the purpose of distillation and concentration, we restrict n to be very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Cascaded Attention Modeling</head><p>Salience estimation for words and sentences is a crucial component in MDS, especially in the un- supervised summarization setting. We propose a cascaded attention model for information distil- lation to tackle the salience estimation task for MDS. We add attention mechanism not only in the hidden layer, but also in the output layer. By this cascaded attention model, we can capture the salience of sentences from two different and com- plementary vector spaces. One is the embedding space that provides better generalization, and the other one is the BOW vector space that captures more nuanced and subtle difference.</p><p>For each output hidden state h o t , we align it with each input hidden state h v i by an attention vector a h t,i ∈ R m (recall that m is the number of input sentences). a h t,i is derived by comparing h o t with each input sentence hidden state h v i :</p><formula xml:id="formula_5">a h t,i = exp(score(h o t , h v i )) i exp(score(h o t , h v i ))<label>(6)</label></formula><p>where score(·) is a content-based function to cap- ture the relation between two vectors. Several dif- ferent formulations can be used as the function score(·) which will be elaborated later. Based on the alignment vectors {a h t,i }, we can create a context vector c h t by linearly blending the sentence hidden states {h v i }:</p><formula xml:id="formula_6">c h t = i a h t,i h v i<label>(7)</label></formula><p>Then the output hidden state can be updated based on the context vector. Let˜hLet˜ Let˜h o t = h o t , then update the original state according to the following operation:</p><formula xml:id="formula_7">h o t = tanh(W a ch c h t + W a hh˜h hh˜ hh˜h o t )<label>(8)</label></formula><p>The alignment vector a h t,i captures which sentence should be attended more in the hidden space when generating the condensed representation for the whole topic.</p><p>Besides the attention mechanism on the hidden layer, we also directly add attention on the out- put BOW layer which can capture more nuanced and subtle difference information from the BOW vector space. The hidden attention vector a h t,i is integrated with the output attention by a weight</p><formula xml:id="formula_8">λ a ∈ [0, 1]: ¯ a o t,i = exp(score(s t , x i )) i exp(score(s t , x i ))<label>(9)</label></formula><formula xml:id="formula_9">a o t,i = λ a ¯ a o t,i + (1 − λ a )a h t,i<label>(10)</label></formula><p>The output context vector is computed as:</p><formula xml:id="formula_10">c o t = i a o t,i x i<label>(11)</label></formula><p>To update the output vector s t in Equation 5, we develop a different method from that of the hidden attentions. Specifically we use a weighted combi- nation of the context vectors and the original out- puts with λ c ∈ [0, 1]. Let˜sLet˜ Let˜s t = s t , then the updated s t is:</p><formula xml:id="formula_11">s t = λ c c o t + (1 − λ c )˜ s t<label>(12)</label></formula><p>The parameters λ a and λ c can also be learned dur- ing training. There are several different alternatives for the function score(·):</p><formula xml:id="formula_12">score(ht, hs) =    ht T hs dot ht T W hs tensor v T tanh(W [ht; hs]) concat<label>(13)</label></formula><p>Considering their behaviors as studied in (Luong et al., 2015), we adopt "concat" for the hidden attention layer, and "dot" for the output attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Unsupervised Learning</head><p>By minimizing the loss owing to using the con- densed output vectors to reconstruct the original input sentence vectors, we are able to learn the so- lutions for all the parameters as follows.</p><formula xml:id="formula_13">min Θ 1 2m m i=1 x i − n j=1 s j a o j,i 2 2 + λ s S 1 (14)</formula><p>where Θ denotes all the parameters in our model. In order to penalize the unimportant terms in the output vectors, we put a sparsity constraint on the rows of S using l 1 -regularization, with the weight λ s as a scaling constant for determining its relative importance. Let ¯ s be the magnitude vector computed from the columns in S (S ∈ R n×k ). Once the train- ing is finished, each dimension of the vector ¯ s can be regarded as the word salience score. Accord- ing to Equation 14, s i ∈ S is used to reconstruct the original sentence space X, and n m (the number of sentences in X is much more than the number of vectors in S) Therefore a large value in ¯ s means that the corresponding word contains important information about this topic and it can serve as the word salience.</p><p>Moreover, the output layer attention matrix A o can be regarded as containing the sentence salience information. Note that each output vec- tor s i is generated based on the cascaded atten- tion mechanism. Assume that a o i = A o i,: ∈ R m is the attention weight vector for s i . According to Equation 9, a large value in a o i conveys a meaning that the corresponding sentence should contribute more when generating s i . We also use the magni- tude of the columns in A o to represent the salience of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Compressive Summary Generation Phase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Coarse-grained Sentence Compression</head><p>Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods ( <ref type="bibr" target="#b14">Li et al., 2013;</ref><ref type="bibr" target="#b32">Wang et al., 2013;</ref><ref type="bibr" target="#b35">Yao et al., 2015)</ref>. Our coarse-grained sentence compression jointly con- siders word salience obtained from the neural at- tention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncrit- ical information from the word level to the clause level, which include news headers such as "BEI- JING, Nov. 24 (Xinhua) -", intra-sentential at- tribution such as ", police said Thursday", ", he said", etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience score (&lt; ) will be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Phrase-based Optimization for Summary Construction</head><p>After coarse-grained compression on each single sentence as described above, we design a uni- fied optimization method for summary generation. We refine the phrase-based summary construction model in <ref type="figure" target="#fig_3">(Bing et al., 2015</ref>) by adjusting the goal as compressive summarization. We consider the salience information obtained by our neural atten- tion model and the compressed sentences in the coarse-grained compression component. Based on the parsed constituency tree for each input sentence as described in Section 2.3.1, we extract the noun-phrases (NPs) and verb-phrases (VPs). The salience S i of a phrase P i is defined as:</p><formula xml:id="formula_14">S i = { t∈P i tf (t)/ t∈T opic tf (t)} × a i<label>(15)</label></formula><p>where a i is the salience of the sentence containing P i . tf (t) is the frequency of the concept t (uni- gram/bigram) in the whole topic. Thus, S i inherits the salience of its sentence, and also considers the importance of its concepts. The overall objective function of our optimiza- tion formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:</p><formula xml:id="formula_15">max{ i α i S i − i&lt;j α ij (S i + S j )R ij } (16)</formula><p>where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is the co-occurrence indicator and the similarity of a pair of phrases (P i , P j ) respectively. The similarity is calculated by the Jaccard Index based method. Specifically, this objective maximizes the salience score of the selected phrases as indicated by the first term, and penalizes the selection of similar phrase pairs.</p><p>In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework such as sentence generation con- straint: Let β k denote the selection indicator of the sentence x k . If any phrase from x k is selected, β k = 1. Otherwise, β k = 0. For generating a compressed summary sentence, it is required that if β k = 1, at least one NP and at lease one VP of the sentence should be selected. It is expressed as:</p><formula xml:id="formula_16">∀P i ∈ x k , α i ≤ β k ∧ i α i ≥ β k ,<label>(17)</label></formula><p>Other constraints include sentence number, sum- mary length, phrase co-occurrence, etc. For de- tails, please refer to <ref type="bibr" target="#b22">McDonald (2007)</ref>, <ref type="bibr" target="#b34">Woodsend and Lapata (2012)</ref>, and . The objective function and constraints are lin- ear. Therefore the optimization can be solved by existing ILP solvers such as the simplex algo- rithm ( <ref type="bibr" target="#b5">Dantzig and Thapa, 2006</ref>). In the imple- mentation, we use a package called lp solve 3 .</p><p>In the post-processing, the phrases and sen- tences in a summary are ordered according to their natural order if they come from the same docu- ment. Otherwise, they are ordered according to the timestamps of the corresponding documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings</head><p>For text processing, the input sentences are repre- sented as BOW vectors with dimension k. The dictionary is created using unigrams and named entity terms. The word salience threshold used in sentence compression is 0.005. For the neu- ral network framework, we set the hidden size as 500. All the neural matrix parameters W in hidden layers and RNN layers are initialized from a uni- form distribution between [−0.1, 0.1]. Adadelta (Schmidhuber, 2015) is used for gradient based optimization. Gradient clipping is adopted by scaling gradients then the norm exceeded a thresh- old of 10. The maximum epoch number in the op- timization procedure is 200. We limit the num- ber of distilled vectors n = 5. The attention cas- caded parameter λ a and λ c can be learned by our model. The sparsity penalty λ s in Equation 14 is We use ROUGE score as our evaluation metric <ref type="bibr" target="#b18">(Lin, 2004</ref>) with standard options. F-measures of ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE- SU4 (R-SU4) are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effect of Existing Salience Models and Different Attention Architectures</head><p>We quantitatively evaluate the performance of dif- ferent variants on the dataset of TAC 2010. The experimental results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Note that the summary generation phase for different methods are the same, and only the salience es- timation methods are different. Commonly used existing methods for salience estimation include: concept weight (CW) (  and sparse coding (SC) ( <ref type="bibr" target="#b2">Li et al., 2015)</ref>. As men- tioned in Section 2.2.3, there are several alterna- tives for the attention scoring function score(·): dot, tensor, and concat. Moreover, we also de- sign experiments to show the benefit of our cas- caded attention mechanism versus the single atten- tion method. AttenC denotes the cascaded atten- tion mechanism. AttenH and AttenO represent the attention only on the hidden layer or the output layer respectively without cascaded combination. Among all the methods, the cascaded attention model with dot structure achieves the best perfor- mance. The effect of different RNN models, such as LSTM and GRU, is similar. However, there are less parameters in GRU resulting in improve- ments for the efficiency of training. Therefore, we choose AttenC-dot-gru as the attention structure of our framework in the subsequent experiments. Moreover, the results without coarse-grained sen-  show that the compression can indeed improve the sumamriza- tion performance.</p><note type="other">0.373 0.066 0.125 MDS-Sparse 0.340 0.052 0.107 DSDR 0.377 0.073 0.117 RA-MDS 0.391 0.081 0.136 ABS-Phrase 0.392 0.082 0.137 C-Attention 0.393* 0.087* 0.141*</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results of Compressive MDS</head><p>We compare our system C-Attention with sev- eral unsupervised summarization baselines and state-of-the-art models. Random baseline se- lects sentences randomly for each topic. Lead baseline ( <ref type="bibr" target="#b33">Wasson, 1998)</ref>   <ref type="table" target="#tab_1">Table 2, Table 3, and Table 4</ref>, our system achieves the best results on all the ROUGE metrics. The reasons are as follows: <ref type="formula" target="#formula_0">(1)</ref> The attention model can directly capture the salient sentences, which are obtained by minimizing the global data reconstruction error; (2) The cascaded structure of attentions can jointly consider the embedding vector space and bag-of-words vector space when conducting the estimation of sentence salience; (3) The coarse-grained sentence com- pression based on distilled word salience, and the fine-grained compression via phrase-based unified optimization framework can generate more con- cise and salient summaries. It is worth noting that PKUTM used a Wikipedia corpus for providing domain knowledge. The system SWING ( <ref type="bibr" target="#b25">Min et al., 2012</ref>) is the best system for TAC 2011. Our results are not as good as SWING. The reason is that SWING employs category-specific features and requires supervised training. These features help them select better category-specific content for the summary. In contrast, our model is basi- cally unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linguistic Quality Evaluation</head><p>The linguistic quality of summaries generated by ABS-Phrase, PKUTM, and our model from 20 topics of TAC 2011 is evaluated using the five lin- guistic quality questions on grammaticality (Q1), non-redundancy (Q2), referential clarity (Q3), fo- cus (Q4), and coherence (Q5) in Document Un- derstanding Conferences (DUC). A Likert scale with five levels is employed with 5 being very good with 1 being very poor. A summary was blindly evaluated by three assessors on each ques- tion. The results are given in <ref type="table" target="#tab_4">Table 5</ref>. PKUTM is an extractive method that picks the original sen- tences, hence it achieves higher score in Q1 gram- maticality. ABS-Phrase is an abstractive method and can generate new sentences by merging differ-  Grammaticality of our compression-based frame- work is better than ABS-Phrase, but not as good as PKUTM. However, our framework performs the best on some other metrics such as Q2 (non- redundancy) and Q4 (focus). The reason is that our framework can compress and remove some uncritical and redundancy content from the orig- inal sentences, which leads to better performance on Q2 and Q4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study: Distilled Word Salience</head><p>As mentioned above, the output vectors S in our neural model contain the distilled word salience information. In order to show the performance of word salience estimation, we select 3 topics (events) from different categories of TAC 2011: "Finland Shooting", "Heart Disease", and "Hiv In- fection Africa". For each topic, we sort the dic- tionary terms according to their salience scores, and extract the top-10 terms as the salience esti- mation results as shown in <ref type="table" target="#tab_5">Table 6</ref>. We can see that the top-10 terms reveal the most important in- formation of each topic. For the topic "Finland Shooting", there is a sentence from the golden summary "A teenager at a school in Finland went on a shooting rampage Wednesday, November 11, 2007, killing 8 people, then himself." It is obvious that the top-10 terms from <ref type="table" target="#tab_5">Table 6</ref> can capture this main point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study: Attention-based Sentence Salience</head><p>In our model, the distilled attention matrix A o can be treated as sentence salience estimation. Let a be the magnitude of the columns in A o and a ∈ R m . a i represents the salience of the sentence x i . We collect all the attention vectors for 8 topics of TAC 2011, and display them as an image as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The x-axis represents the sentence id (we show at most 100 sentences), and the y-axis repre- sents the topic id. The gray level of pixels in the image indicates different salience scores, where dark represents a high salience score and light represents a small score. Note that different top- ics seem to hold different ranges of salience scores because they have different number of sentences, i.e. m. According to Equation 9, topics contain- ing more sentences will distribute the attention to more units, therefore, each sentence will get a rela- tively smaller attention weight. But this issue does not affect the performance of MDS since different topics are independently processed. In <ref type="figure" target="#fig_3">Figure 2</ref>, there are some chunks in each topic (see Topic 3 as an example) having higher atten- tion weights, which indeed automatically captures one characteristic of MDS: sentence position is an important feature for news summarization. As observed by several previous studies <ref type="bibr" target="#b25">Min et al., 2012)</ref>, the sentences in the be- ginning of a news document are usually more im- portant and tend to be used for writing model sum- maries. Manual checking verified that those high- attention chunks correspond to the beginning sen- tences. Our model is able to automatically capture this information by assigning the latter sentences in each topic lower attention weights. <ref type="table">Table 7</ref> shows the summary of the topic "Hawkins Robert Van Maur" in TAC 2011. The summary contains four sentences, which are all compressed with different compression ratio. Some uncrit- ical information is excluded from the summary sentences, such as "police said Thursday" in S2, "But" in S3, and "he said" in S4. In addition, the VP "killing eight people" in S2 is also excluded since it is duplicate with the phrase "killed eight people" in S3. Moreover, from the case we can find that the compression operation did not harm the linguistic quality. <ref type="table">Table 7</ref>: The summary of the topic "Hawkins Robert Van Maur".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Summary Case Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1:</head><p>The young gunman who opened fire at a mall busy with holiday shoppers appeared to choose his victims at random, according to police[, but a note he left behind hinted at a troubled life]. S2: The teenage gunman who went on a shooting rampage in a department store, [killing eight peo- ple,] may have smuggled an assault rifle into the mall underneath clothing[, police said Thursday]. S3: <ref type="bibr">[But]</ref> police said it was Hawkins who went into an Omaha shopping mall on Wednesday and began a shooting rampage that killed eight people. S4: Mall security officers noticed Hawkins briefly enter the Von Maur department store at Omaha's Westroads Mall earlier Wednesday[, he said].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>According to different machine learning paradigms, summarization models can be divided into supervised framework and unsuper- vised framework. Some previous works have been proposed based on unsupervised models. For example, <ref type="bibr" target="#b23">Mihalcea and Tarau (2004)</ref> and <ref type="bibr" target="#b7">Erkan and Radev (2004a</ref> Some recent works utilize attention modeling based recurrent neural networks to tackle the task of single-document summarization. <ref type="bibr" target="#b28">Rush et al. (2015)</ref> proposed a sentence summarization frame- work based on a neural attention model using a supervised sequence-to-sequence neural machine translation model. <ref type="bibr" target="#b10">Gu et al. (2016)</ref> combined a copying mechanism with the seq2seq framework to improve the quality of the generated summaries. <ref type="bibr" target="#b26">Nallapati et al. (2016)</ref> also employed the typi- cal attention modeling based seq2seq framework, but utilized a trick to control the vocabulary size to improve the training efficiency. However, few previous works employ attention mechanism to tackle the unsupervised MDS problem. In con- trast, our attention-based framework can gener- ate summaries for multi-document summarization settings in an unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a cascaded neural attention based un- supervised salience estimation method for com- pressive multi-document summarization. The at- tention weights for sentences and salience values for words are both learned by data reconstruction in an unsupervised manner. We thoroughly inves- tigate the performance of combining different at- tention architectures and cascaded structures. Ex- perimental results on some benchmark data sets show that our framework achieves good perfor- mance compared with the state-of-the-art meth- ods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Both DUC 2006 and DUC 2007 are used in our evaluation. DUC 2006 and DUC 2007 con- tain 50 and 45 topics respectively. Each topic has 25 news documents and 4 model summaries. The length of the model summary is limited to 250 words. TAC: We also use TAC 2010 and TAC 2011 in our experiments. TAC 2011 is the latest standard summarization benchmark data set and it contains 44 topics. Each topic falls into one of 5 predefined event categories and contains 10 related news documents and 4 model summaries. TAC 2010 is used as the parameter tuning data set of our TAC evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ranks the news chrono- logically and extracts the leading sentences one by one. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004a) esti- mate sentence salience by applying the PageRank algorithm to the sentence graph. PKUTM (Li et al., 2011) employs manifold-ranking for sen- tence scoring and selection; ABS-Phrase (Bing et al., 2015) generates abstractive summaries us- ing phrase-based optimization framework. Three other unsupervised methods based on sparse cod- ing are also compared, namely, DSDR (He et al., 2012), MDS-Sparse (Liu et al., 2015), and RA- MDS (Li et al., 2015). As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) estimated sentence salience by applying the PageRank algorithm to the sentence graph. He et al. (2012), Liu et al. (2015), Li et al. (2015) and Song et al. (2017) employed sparse coding techniques for finding the salient sentences as summaries. Li et al. (2017) conducted salience estimation jointly considering reconstructions on several different vector spaces generated by a variational auto-ecoder framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization for sentence attention.</figDesc><graphic url="image-3.png" coords="9,100.88,65.35,384.61,75.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Comparisons on TAC 2010</head><label>1</label><figDesc></figDesc><table>System 
R-1 
R-2 
R-SU4 
CW 
0.353 0.092 
0.123 
SC 
0.346 0.083 
0.116 
AttenC-tensor-gru 
0.339 0.078 
0.115 
AttenC-concat-gru 0.353 0.089 
0.121 
AttenC-dot-lstm 
0.352 0.089 
0.121 
AttenH-dot-gru 
0.348 0.086 
0.119 
AttenO-dot-gru 
0.348 0.085 
0.118 
AttenC-dot-gru 
0.359 0.092 0.124 
(w\o coarse-comp) 0.351 0.089 
0.122 

0.001. Our neural network based framework is im-
plemented using Theano (Bastien et al., 2012) on 
a single GPU of Tesla K80. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Results on DUC 2006.</head><label>2</label><figDesc></figDesc><table>System 
R-1 
R-2 
R-SU4 
Random 
0.280 
0.046 
0.088 
Lead 
0.308 
0.048 
0.087 
LexRank 
0.360 
0.062 
0.118 
TextRank 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on DUC 2007. 

System 
R-1 
R-2 
R-SU4 
Random 
0.302 
0.046 
0.088 
Lead 
0.312 
0.058 
0.102 
LexRank 
0.378 
0.075 
0.130 
TextRank 
0.403 
0.083 
0.144 
MDS-Sparse 
0.353 
0.055 
0.112 
DSDR 
0.398 
0.087 
0.137 
RA-MDS 
0.408 
0.097 
0.150 
ABS-Phrase 
0.419 
0.103 
0.156 
C-Attention 
0.423* 
0.107* 
0.161* 

tence compression (Section 2.3.1) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Results on TAC 2011.</head><label>4</label><figDesc></figDesc><table>System 
R-1 
R-2 
R-SU4 
Random 
0.303 
0.045 
0.090 
Lead 
0.315 
0.071 
0.103 
LexRank 
0.313 
0.060 
0.102 
TextRank 
0.332 
0.064 
0.107 
PKUTM 
0.396 
0.113 
0.148 
ABS-Phrase 
0.393 
0.117 
0.148 
RA-MDS 
0.400 
0.117 
0.151 
C-Attention 
0.400* 
0.121* 
0.153* 

* Statistical significance tests show that our method is better 
than the best baselines. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : Evaluation of linguistic quality.</head><label>5</label><figDesc></figDesc><table>System 
Q1 Q2 Q3 Q4 Q5 AVG 
ABS-Phrase 3.75 3.38 3.75 3.35 3.12 3.47 
PKUTM 
4.13 3.45 3.83 3.33 2.92 3.53 
Ours 
3.96 3.50 3.79 3.50 3.25 3.60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Top-10 terms extracted from each topic 
according to the word salience 

Topic 1 
Topic 2 
Topic 3 
school 
heart 
HIV 
shooting 
disease 
Africa 
Auvinen 
study 
circumcision 
Finland 
risk 
study 
police 
test 
infection 
video 
blood 
trial 
Wednesday 
red 
woman 
gunman 
telomere 
drug 
post 
level 
health 

ent phrases, which decreases the grammaticality. 
</table></figure>

			<note place="foot" n="2"> https://en.wikipedia.org/wiki/Attention (Apr., 2017)</note>

			<note place="foot" n="3"> http://lpsolve.sourceforge.net/5.5/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5590</idno>
		<title level="m">Theano: new features and speed improvements</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive multidocument summarization via phrase selection and merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1587" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1724" to="1734" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Fethi Bougares Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Linear programming 1: introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New methods in automatic extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold P Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lexpagerank: Prestige in multi-document text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-document summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACLANLPWorkshop</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Document summarization based on data reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="620" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PKUTM participation in TAC2011</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Li</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reader-aware multi-document summarization via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1270" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Salience estimation via variational auto-encoders for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3497" to="3503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-document summarization based on two-level sparse representation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Peter Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of research and development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting category-specific information for multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><forename type="middle">Kan</forename><surname>Ziheng Lin Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lim</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<biblScope unit="page" from="2093" to="2108" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="379" to="389" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarizing answers in non-factoid community questionanswering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongya</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manifold-ranking based topic-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2903" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
	<note>Radu Florian, and Claire Cardie</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using leading text for news summaries: Evaluation results and implications for commercial summarization applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Wasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1364" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiple aspect summarization using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compressive document summarization via sparse optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1376" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
