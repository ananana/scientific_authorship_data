<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sanskrit Sandhi Splitting using seq2(seq) 2 seq2(seq) 2 seq2(seq) 2</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">IBM Research Anush Sankaran IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neelamadhav</forename><surname>Gantayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">IBM Research Anush Sankaran IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Panwar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">IBM Research Anush Sankaran IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Mani</surname></persName>
							<email>sentmani@in.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">IBM Research Anush Sankaran IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sanskrit Sandhi Splitting using seq2(seq) 2 seq2(seq) 2 seq2(seq) 2</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4909" to="4914"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4909</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In Sanskrit, small words (morphemes) are combined to form compound words through a process known as Sandhi. Sandhi splitting is the process of splitting a given compound word into its constituent morphemes. Although rules governing word splitting exists in the language, it is highly challenging to identify the location of the splits in a compound word. Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low accuracy as the same compound word might be broken down in multiple ways to provide syntactically correct splits. In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s) with 95% accuracy, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5% accuracy, outperforming the state-of-art by 20%. Additionally , we show the generalization capability of our deep learning model, by showing competitive results in the problem of Chinese word segmentation, as well.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rahul Aralikatte</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In Sanskrit, small words (morphemes) are combined to form compound words through a process known as Sandhi. Sandhi split- ting is the process of splitting a given com- pound word into its constituent morphemes. Although rules governing word splitting ex- ists in the language, it is highly challenging to identify the location of the splits in a com- pound word. Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low accuracy as the same compound word might be broken down in multiple ways to provide syntactically correct splits.</p><p>In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the lo- cation of the split(s) with 95% accuracy, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5% accuracy, outperforming the state-of-art by 20%. Addi- tionally, we show the generalization capability of our deep learning model, by showing com- petitive results in the problem of Chinese word segmentation, as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Compound word formation in Sanskrit is gov- erned by a set of deterministic rules follow- ing a well-defined structure described in P¯ an . ini's As . t . ¯ adhy¯ ay¯ ı, a seminal work on Sanskrit grammar. The process of merging two or more morphemes to form a word in Sanskrit is called Sandhi and the process of breaking a compound word into its con- stituent morphemes is called Sandhi splitting. In Japanese, Rendaku ('sequential voicing') is sim- ilar to Sandhi. For example, 'origami' consists of 'ori' (paper) + 'kami' (folding), where 'kami' changes to 'gami' due to Rendaku. Learning the process of sandhi splitting for San- skrit could provide linguistic insights into the for- mation of words in a wide-variety of Dravidian languages. From an NLP perspective, automated learning of word formations in Sanskrit could provide a framework for learning word organiza- tion in other Indian languages, as <ref type="bibr">well (Bharati et al., 2006</ref>). In literature, past works have ex- plored sandhi splitting <ref type="bibr">(Gillon, 2009) (Kulkarni and</ref><ref type="bibr" target="#b8">Shukl, 2009)</ref>, as a rule based problem by ap- plying the rules from As . t . ¯ adhy¯ ay¯ ı in a brute force manner. Consider the example in <ref type="figure" target="#fig_0">Figure 1</ref> illus- trating the different possible splits of a compound word paropak¯ arah . . While the correct split is para + upak¯ arah . , other forms of splits such as, para + apa + k¯ arah . are syntactically possible while se- mantically incorrect <ref type="bibr">1</ref> . Thus, knowing all the rules of splitting is insufficient and it is essential to iden- tify the location(s) of split(s) in a given compound word.</p><p>In this research, we propose an approach for au-tomated generation of split words by first learn- ing the potential split locations in a compound word. We use a deep bi-directional character RNN encoder and two decoders with attention, seq2(seq) 2 seq2(seq) 2 seq2(seq) 2 . The accuracy of our approach on the benchmark dataset for split location prediction is 95% and for split words prediction is 79.5% re- spectively. To the best of our knowledge, this is the first research work to explore deep learning tech- niques for the problem of Sanskrit Sandhi split- ting, along with producing state-of-art results. Ad- ditionally, we show the performance of our pro- posed model for Chinese word segmentation to demonstrate the model's generalization capability.</p><p>2 seq2(seq) 2 seq2(seq) 2 seq2(seq) 2 : Model Description</p><p>In this section, we present our double decoder model to address the Sandhi splitting problem. We first outline the issues with basic deep learning ar- chitectures and conceptually highlight the advan- tages of the double decoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Issues with standard architectures</head><p>Consider an example of splitting a sequence abcdefg as abcdx + efg. The primary task is to identify d as the split location. Further, for a given location d in the character sequence, the algorithm should take into account (i) the context of char- acter sequence abc, (ii) the immediate previous character c, (iii) the immediate succeeding char- acter e, to make an effective split. For such se- quence learning problems, RNNs have become the most popular deep learning model ( <ref type="bibr" target="#b11">Pascanu et al., 2013</ref><ref type="bibr" target="#b14">) (Sak et al., 2014</ref>. A basic RNN encoder-decoder model <ref type="bibr" target="#b3">(Cho et al., 2014</ref>) with LSTM units <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>), similar to a machine transla- tion model, was trained initially. The compound word's characters is fed as input to the encoder and is translated to a sequence of characters repre- senting the split words ('+' symbol acts as a sep- arator between the generated split words). How- ever, the model did not yield adequate perfor- mance as it encoded only the context of the char- acters that appeared before the potential split lo- cation(s). Though we tried making the encoder bi-directional (referred to as B-RNN), the model's performance only improved marginally. Adding global attention (referred to as B-RNN-A) to the decoder enabled the model to attend to the charac- ters surrounding the potential split location(s) and improved the split prediction performance, mak- ing it comparable with some of the best perform- ing tools in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Double Decoder RNN (DD-RNN) model</head><p>The critical part of learning to split compound words is to correctly identify the location(s) of the split(s). Therefore, we added a two decoders to our bi-directional encoder-decoder model: (i) lo- cation decoder which learns to predict the split lo- cations and (ii) character decoder which generates the split words. A compound word is fed into the encoder character by character. Each character's embedding x i is passed to the encoders LSTM units. There are two LSTM layers which encode the word, one in forward direction and the other backward. The encoded context vector e i is then passed to a global attention layer.</p><p>In the first phase of training, only the loca- tion decoder is trained and the character decoder is frozen. The character embeddings are learned from scratch in this phase along with the attention weights and other parameters. Here, the model learns to identify the split locations. For example, if the inputs are the embeddings for the compound word prots¯ ahah . , the location decoder will gener- ate a binary vector [0, 0, 1, 0, 0, 0, 0, 0, 0] which in- dicates that the split occurs between the third and fourth characters. In the second phase, the loca- tion decoder is frozen and the character decoder is trained. The encoder and attention weights are allowed to be fine-tuned. This decoder learns the underlying rules of Sandhi splitting. Since the at- tention layer is already pre-trained to identify po- tential split locations in the previous phase, the character decoder can use this context and learn to split the words more accurately. For example, for the same input word prots¯ ahah . , the character decoder will generate [p, r, a, +, u, t, s, ¯ a, h, a, h . ] as the output. Here the character o is split into two characters a and u.</p><p>In both the training phases, we use negative log likelihood as the loss function. Let X be the se- quence of the input compound word's characters and Y be the binary vector which indicates the lo- cation of the split(s) in the first phase and the true target sequence of characters which form the split words in the second phase. If Y = y 1 , y 2 , ..., y n , then the loss function is defined as: We evaluate the DD-RNN and compare it with other tools and architectures in Section 4.</p><formula xml:id="formula_0">loss = − |Y | i=1 log P (y i |y i−1 , · · · , y 1 , X)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation details</head><p>The architecture of the DD-RNN is shown in <ref type="figure" target="#fig_1">Fig- ure 2</ref>. We used a character embedding size of 128. The bi-directional encoder and the two decoders are 2 layers deep with 512 LSTM units in each layer. A dropout layer with p = 0.3 is applied after each LSTM layer. The entire network is im- plemented in Torch 2 .</p><p>Of the 71, 747 words in our benchmark dataset, we randomly sampled 80% of the data for training our models. The remaining 20% was used for test- ing. We used stochastic gradient descent for opti- mizing the model parameters with an initial learn- ing rate of 1.0. The learning rate was decayed by a factor of 0.5 if the validation perplexity did not improve after an epoch. We used a batch size of 64 and trained the network for 10 epochs on four Tesla K80 GPUs. This setup remains the same for all the experiments we conduct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Existing Datasets and Tools</head><p>In this section, we briefly introduce various San- skirt Sandhi datasets and splitting tools available in literature. We also discuss the tools' drawbacks and the major challenges faced while creating such tools.</p><p>Datasets: The UoH corpus, created at the Uni- versity of Hyderabad 3 contains 113, 913 words and their splits. This dataset is noisy with typ- ing errors and incorrect splits. The recent Sand- hiKosh corpus <ref type="bibr">(Shubham Bhardwaj, 2018</ref>) is a set of 13, 930 annotated splits. We combine these datasets and heuristically prune them to finally get 71, 747 words and their splits. The pruning is done by considering a data point to be valid only if the compound word and it's splits are present in a standard Sanskrit dictionary <ref type="bibr" target="#b10">(Monier-Williams, 1970</ref>). We use this as our benchmark dataset and run all our experiments on it.</p><p>Tools: There exist multiple Sandhi splitters in the open domain such as (i) JNU splitter <ref type="bibr" target="#b13">(Sachin, 2007)</ref>, (ii) UoH splitter ( <ref type="bibr" target="#b9">Kumar et al., 2010)</ref> and (iii) INRIA sanskrit reader companion <ref type="bibr">(Huet, 2003) (Goyal and</ref><ref type="bibr" target="#b5">Huet, 2013)</ref>. Though each tool addresses the splitting problem in a specialized way, the general principle remains constant. For a given compound word, the set of all rules are applied to every character in the word and a large potential candidate list of word splits is obtained. Then, a morpheme dictionary of Sanskrit words is used with other heuristics to remove infeasible word split combinations. However, none of the approaches address the fundamental problem of identifying the location of the split before apply- ing the rules, which will significantly reduce the number of rules that can be applied, hence result- ing in more accurate splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Results</head><p>We evaluate the performance of our DD-RNN model by: (i) comparing the split prediction ac- curacy with other publicly available sandhi split- ting tools, (ii) comparing the split prediction accuracy with other standard RNN architectures such as RNN, B-RNN, and B-RNN-A, and (iii) comparing the location prediction accuracy with the RNNs used for Chinese word segmentation (as they only predict the split locations and do not learn the rules of splitting)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with publicly available tools</head><p>The tools discussed in Section 3 take a compound word as input and provide a list of all possible splits as output (UoH and INRIA splitters pro- vide weighted lists). Initially, we compared only the top prediction in each list with the true out- put. This gave a very low precision for the tools as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Therefore, we relaxed this constraint and considered an output to be correct if the true split is present in the top ten predictions of the list. This increased the precision of the tools as shown in <ref type="figure" target="#fig_3">Figure 4</ref> and <ref type="table">Table 1</ref>.</p><p>Even though DD-RNN generates only one out- put for every input, it clearly out-performs the other publicly available tools by a fair margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with standard RNN architectures</head><p>To compare the performance of DD-RNN with other standard RNN architectures, we trained the following three models to generate the split predictions on our benchmark dataset: (i) uni- directional encoder and decoder without attention (RNN), (ii) bi-directional encoder and decoder without attention (B-RNN), and (iii) bi-directional encoder and decoder with attention (B-RNN-A) As seen from the middle part of <ref type="table">Table 1</ref>, the DD-RNN performs much better than the other ar- chitectures with an accuracy of 79.5%. It is to be noted that B-RNN-A is the same as DD-RNN without the location decoder. However, the ac- curacy of DD-RNN is 14.7% more than that the B-RNN-A and consistently outperforms B-RNN- A on almost all word lengths ( <ref type="figure" target="#fig_4">Figure 5</ref>). This in- dicates that the attention mechanism of DD-RNN has learned to better identify the split location(s) due to its pre-training with the location decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with similar works</head><p>( <ref type="bibr" target="#b12">Reddy et al., 2018</ref>) propsed a seq2seq model using RNN with attention to tackle the Sandhi problem. Their model is similar to B-RNN-A and is outperformed by our proposed DD-RNN by˜6by˜ by˜6.47%. We also compared our proposed DD- RNN with a uni-directional LSTM with a depth of 4 (Chen et al., 2015b) (LSTM-4) and a Gated Recursive Neural Network with a depth of 5 (Chen et al., 2015a) (GRNN-5). These models were used to get state of the art results for Chinese word segmentation and their source code is made avail- able online. <ref type="bibr">4</ref> Since these models can only predict the location(s) of the split(s) and cannot gener- ate the split words themselves, we used the loca- tion prediction accuracy as the metric. We trained these models on our benchmark dataset and the results are shown in <ref type="table">Table 1</ref>. DD-RNN's pre- cision is 35.3% and 40.3% better than LSTM-4 and GRNN-5 respectively. Conversely, we trained the DD-RNN for the Chinese word segmenta- tion task to test the generalizability of the model. Since there are no morphological changes during segmentation in Chinese, the character decoder is redundant and the model collapses to simple seq2seq. We used the PKU dataset which is also used in <ref type="bibr" target="#b2">(Chen et al., 2015b</ref><ref type="bibr" target="#b1">) &amp; (Chen et al., 2015a</ref> and obtained an accuracy of 64.25% which is com- parable to the results of other standard models.</p><p>To summarize, we have used our benchmark dataset to compare the DD-RNN model with ex- isting publicly available Sandhi splitting tools, other RNN architectures and models used for Chi- nese word segmentation task. Among the exist- ing tools, the INRIA splitter gives the highest split prediction accuracy of 59.9%. Among the stan- dard RNN architectures, B-RNN-A performs the best with a split prediction accuracy of 69.3%. LSTM-4 performs the best among the Chinese word segmentation models with a location predic- tion accuracy of 70.2%. DD-RNN outperforms all the models both in location and split predictions with 95% and 79.5% accuracies, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Research Impact</head><p>This work can be foundational to other Sanskrit based NLP tasks. Let us consider translation as an example. In Sanskrit, arbitrary number of words can be joined together to form a compound word. Literary works, especially from the Vedic era of- ten contain words which are a concatenation of three or more simpler words. Presence of such compound words will increase the vocabulary size exponentially and hinder the translation process. However, as a pre-processing step, if all the com- pound words are split before training a translation model, the number of unique words in the vocabu- lary reduces which will ease the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this research, we propose a novel double de- coder RNN architecture with attention for Sanskrit Sandhi splitting. Learning such a model would provide further insights into the fundamental lin- guistic word formation rules of the language. A deep bi-directional encoder is used to encode the character sequence of a Sanskrit word. Using this encoded context vector, a location decoder is first used to learn the location(s) of the split(s). Then the character decoder is used to generate the split words. We evaluate the performance of the pro- posed approach on the benchmark dataset in com- parison with other publicly available tools, stan- dard RNN architectures and with prior work which tackle similar problems in other languages. As fu- ture work, we intend to tackle the harder Samasa problem which requires semantic information of a word in addition to the characters' context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different possible splits for the word paropak¯ arah. and prots¯ ahah. , provided by a standard Sandhi splitter.</figDesc><graphic url="image-1.png" coords="1,326.28,247.06,180.25,119.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The bi-directional encoder and decoders with attention</figDesc><graphic url="image-2.png" coords="3,72.00,62.81,453.54,256.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top-1 split prediction accuracy comparison of different publicly available tools with DD-RNN</figDesc><graphic url="image-3.png" coords="4,72.00,62.81,218.27,163.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Split prediction accuracy comparison of different publicly available tools (Top-10) with DD-RNN (Top-1)</figDesc><graphic url="image-4.png" coords="4,307.28,62.81,218.27,163.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Split prediction accuracy comparison of different variations of RNN on words of different lengths</figDesc><graphic url="image-5.png" coords="5,73.14,62.81,216.00,162.00" type="bitmap" /></figure>

			<note place="foot" n="1"> Different syntactic splits given by one of the popular Sandhi splitters: https://goo.gl/0M5CPS and https://goo.gl/JHnpJw</note>

			<note place="foot" n="2"> http://torch.ch/</note>

			<note place="foot" n="3"> Available at: http://sanskrit.uohyd.ac.in/ Corpus/</note>

			<note place="foot" n="4"> https://github.com/FudanNLP</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a wide coverage sanskrit morphological analyser: A practical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshar</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Amba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheeba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First National Symposium on Modelling and Shallow Parsing of Indian Languages</title>
		<imprint>
			<publisher>IIT-Bombay</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tagging Classical Sanskrit Compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">S</forename><surname>Gillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Completeness analysis of a sanskrit reader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Sanskrit Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="130" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards computational processing of sanskrit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Language Processing (ICON)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sanskrit morphological analyser: Some issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amba</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devanand</forename><surname>Shukl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian Linguistics</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amba</forename><surname>Kulkarni</surname></persName>
		</author>
		<title level="m">Sanskrit Compound Processor</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sanskrit-English Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monier</forename><surname>Monier-Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<title level="m">How to construct deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Building a word segmenter for sanskrit overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishnu Dutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<idno>abs/1802.06185</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sandhi splitter and analyzer for sanskrit (with reference to ac sandhi). M. Phil. degree at SCSS, JNU (submitted</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar Sachin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Françoise</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sandhikosh: A benchmark corpus for evaluating sanskrit sandhi tools</title>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Language Resources and Evaluation</title>
		<editor>Rahul Garg Sumeet Agarwal Shubham Bhardwaj, Neelamadhav Gantayat</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
