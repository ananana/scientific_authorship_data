<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
							<email>{junczys,romang}@amu.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="institution">Adam Mickiewicz University in Pozna´nPozna´n ul</orgName>
								<address>
									<addrLine>Umultowska 87</addrLine>
									<postCode>61-614</postCode>
									<settlement>Pozna´nPozna´n</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Adam Mickiewicz University in Pozna´nPozna´n ul</orgName>
								<address>
									<addrLine>Umultowska 87</addrLine>
									<postCode>61-614</postCode>
									<settlement>Pozna´nPozna´n</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1546" to="1556"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we study parameter tuning towards the M 2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M 2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M 2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning out-performs all previously published results for the CoNLL-2014 test set by a large margin (46.37% M 2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M 2 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical machine translation (SMT), especially the phrase-based variant, is well established in the field of automatic grammatical error correction (GEC) and systems that are either pure SMT or incorporate SMT as system components occupied top positions in GEC shared tasks for different languages.</p><p>With the recent paradigm shift in machine trans- lation towards neural translation models, neural encoder-decoder models are expected to appear in the field of GEC as well, and first published results ( <ref type="bibr" target="#b29">Xie et al., 2016</ref>) already look promising. As it is the case in classical bilingual machine translation research, these models should be compared against strong SMT baselines. Similarly, system combina- tions of SMT with classifier-based approaches <ref type="bibr" target="#b26">(Rozovskaya and Roth, 2016</ref>) suffer from unnecessarily weak MT base systems which make it hard to assess how large the contribution of the classifier pipelines really is. In this work we provide these baselines.</p><p>During our experiments, we find that a bare-bones phrase-based system outperforms the best published results on the CoNLL-2014 test set by a significant margin only due to a task-specific parameter tun- ing when being trained on the same data as previous systems. When we further investigate the influence of well-known SMT-specific features and introduce new features adapted to the problem of GEC, our fi- nal systems outperform the best reported results by 8% M 2 , moving the state-of-the-art results for the CoNLL-2014 test set from 41.75% M 2 to 49.49%.</p><p>The paper is organized as follows: section 2 describes previous work, the CoNLL-2014 shared tasks on GEC and follow-up papers. Our main con- tributions are presented in sections 3 and 4 where we investigate the interaction of parameter tuning towards the M 2 metric with task-specific dense and sparse features. Especially tuning for sparse fea- tures is more challenging than initially expected, but we describe optimizer hyper-parameters that make sparse feature tuning with M 2 feasible. Section 5 reports on the effects of adding a web-scale n-gram language model to our models.</p><p>Scripts and models used in this paper are available from https://github.com/grammatical/ baselines-emnlp2016 to facilitate repro- ducibility of our results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>While machine translation has been used for GEC in works as early as <ref type="bibr" target="#b0">Brockett et al. (2006)</ref>, we start our discussion with the CoNLL-2014 shared task ( ) where for the first time an unre- stricted set of errors had to be fully corrected. Previ- ous work, most notably during the <ref type="bibr">CoNLL sharedtask 2013</ref>, concentrated only on five selected errors types, but machine translation approaches ( <ref type="bibr" target="#b30">Yoshimoto et al., 2013;</ref><ref type="bibr" target="#b31">Yuan and Felice, 2013</ref>) were used as well.</p><p>The goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use additional publicly available data.</p><p>The corrected system outputs were evalu- ated blindly using the MaxMatch (M 2 ) metric <ref type="bibr" target="#b7">(Dahlmeier and Ng, 2012)</ref>. Thirteen system sub- missions took part in the shared task. Among the top-three positioned systems, two submissions - CAMB <ref type="bibr" target="#b10">(Felice et al., 2014</ref>) and AMU <ref type="bibr">(JunczysDowmunt and Grundkiewicz, 2014</ref>) 1 -were par- tially or fully based on SMT. The second system, CUUI <ref type="bibr" target="#b27">(Rozovskaya et al., 2014</ref>), was a classifier- based approach, another popular paradigm in GEC.</p><p>After the shared task,  pub- lished work on GEC systems combinations. They combined the output from a classification-based system and a SMT-based system using MEMT <ref type="bibr" target="#b12">(Heafield and Lavie, 2010)</ref>, reporting new state-of- the-art results for the CoNLL-2014 test set. <ref type="bibr" target="#b29">Xie et al. (2016)</ref> presented a neural network- based approach to GEC. Their method relies on a character-level encoder-decoder recurrent neural network with an attention mechanism. They use data from the public Lang-8 corpus and combine their model with an n-gram language model trained on web-scale Common Crawl data.</p><p>More recent results are  and <ref type="bibr" target="#b14">Hoang et al. (2016)</ref> which also rely on MT systems with new features (a feed-forward neural translation model) and n-best list re-ranking meth- ods. However, most of the improvement over the CoNLL-2014 shared task of these works stems from using the parameter tuning tools we introduced in Junczys-Dowmunt and <ref type="bibr" target="#b16">Grundkiewicz (2014)</ref>.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> we give a graphical overview of the published results for the CoNLL-2014 test set in comparison to the results we will discuss in this work. Positions marked with (r) use only restricted data which corresponds to the data set used by . Positions with (u) make use of web-scale data, this corresponds to the resources used in <ref type="bibr" target="#b29">Xie et al. (2016)</ref>. We marked the participants of the CoNLL-2014 shared task as unrestricted as some participants made use of Common Crawl data or Google n-grams. The visible plateau for results prior to this work seem to confirm our claims about missing strong baselines.</p><p>Rozovskaya and Roth (2016) introduce a SMT- classifier pipeline with state-of-the-art results. Un- fortunately, these results are reported for a training set that is not publicly available (data crawled from the Lang-8 website) <ref type="bibr">2</ref> . <ref type="figure" target="#fig_1">Figure 2</ref> compares our results for this resource to <ref type="bibr" target="#b26">Rozovskaya and Roth (2016)</ref>. See Section 6 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dense feature optimization</head><p>Moses comes with tools that can tune parameter vec- tors according to different MT tuning metrics. Prior work used Moses with default settings: minimum error rate training <ref type="bibr" target="#b24">(Och, 2003)</ref> towards BLEU ( <ref type="bibr" target="#b25">Papineni et al., 2002</ref>). BLEU was never designed for grammatical error correction; we find that directly optimizing for M 2 works far better. <ref type="bibr">2</ref> We shared this resource that has been crawled by us for use in Junczys-Dowmunt and Grundkiewicz (2014) privately with <ref type="bibr" target="#b26">Rozovskaya and Roth (2016)</ref>, but originally were not planning to report results for this resource in the future. We now provide a comparison to <ref type="bibr" target="#b26">Rozovskaya and Roth (2016)</ref>, but discourage any further use of this unofficial data due to reproducibility issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tuning towards M 2</head><p>The M 2 metric <ref type="bibr" target="#b7">(Dahlmeier and Ng, 2012</ref>) is an F- Score, based on the edits extracted from a Leven- shtein distance matrix. For the CoNLL-2014 shared task, the β-parameter was set to 0.5, putting two times more weight on precision than on recall.</p><p>In Junczys-Dowmunt and Grundkiewicz (2014) we have shown that tuning with BLEU is counter- productive in a setting where M 2 is the evaluation metric. For inherently weak systems this can result in all correction attempts to be disabled, MERT then learns to disallow all changes since they lower the similarity to the reference as determined by BLEU. Systems with better training data, can be tuned with BLEU without suffering this "disabling" effect, but will reach non-optimal performance. However, Su- santo et al. <ref type="formula">(2014)</ref> tune the feature weights of their two SMT-based systems with BLEU on the CoNLL- 2013 test set and report state-of-the-art results.</p><p>Despite tuning with M 2 , in Junczys-Dowmunt and Grundkiewicz (2014) we were not able to beat sys- tems that did not tune for the task metric. We re- investigated these ideas with radically better results, re-implemented the M 2 metric in C++ and added it as a scorer to the Moses parameter optimization framework. Due to this integration we can now tune parameter weights with MERT, PRO or Batch Mira. The inclusion of the latter two enables us to experi- ment with sparse features.</p><p>Based on Clark et al. (2011) concerning the ef- fects of optimizer instability, we report results aver- aged over five tuning runs. Additionally, we com- pute parameter weight vector centroids as suggested by <ref type="bibr" target="#b3">Cettolo et al. (2011)</ref>. They showed that param- eter vector centroids averaged over several tuning runs yield similar to or better than average results and reduce variance. We generally confirm this for M 2 -based tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dense features</head><p>The standard features in SMT have been chosen to help guiding the translation process. In a GEC set- ting the most natural units seem to be minimal edit operations that can be either counted or modeled in context with varying degrees of generalization. That way, the decoder can be informed on several levels source phrase target phrase LD D I S a short time . short term only .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">1 1 1 a situation</head><p>into a situation 1 0 1 0 a supermarket . a supermarket . 0 0 0 0 a supermarket . at a supermarket 2 1 1 0 able unable 1 0 0 1 <ref type="table">Table 1</ref>: Word-based Levenshtein distance (LD) fea- ture and separated edit operations (D = deletions, I = insertions, S = substitutions) of abstraction how the output differs from the input. <ref type="bibr">3</ref> In this section we implement several features that try to capture these operation in isolation and in context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Stateless features</head><p>Our stateless features are computed during trans- lation option generation before decoding, model- ing relations between source and target phrases. They are meant to extend the standard SMT-specific MLE-based phrase and word translation probabili- ties with meaningful phrase-level information about the correction process.</p><p>Levenshtein distance. In Junczys-Dowmunt and Grundkiewicz (2014) we use word-based Leven- shtein distance between source and target as a trans- lation model feature, <ref type="bibr" target="#b10">Felice et al. (2014)</ref> indepen- dently experiment with a character-based version.</p><p>Edit operation counts. We further refine Leven- shtein distance feature with edit operation counts. Based on the Levenshtein distance matrix, the num- bers of deletions, insertions, and substitutions that transform the source phrase into the target phrase are computed, the sum of these counts is equal to the original Levenshtein distance (see <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Stateful features</head><p>Contrary to stateless features, stateful features can look at translation hypotheses outside their own span and take advantage of the constructed target context. The most typical stateful features are language mod- els. In this section, we discuss LM-like features over edit operations. <ref type="bibr">3</ref> We believe this is important information that currently has not yet been mastered in neural encoder-decoder approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Sentences  Word-class language model. The monolingual Wikipedia data has been used create a 9-gram word- class language model with 200 word-classes pro- duced by word2vec ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref>. This fea- tures allows to capture possible long distance depen- dencies and semantical aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Test Data</head><p>The training data provided in both shared tasks is the NUS Corpus of Learner English (NUCLE) ( <ref type="bibr" target="#b8">Dahlmeier et al., 2013)</ref>. NUCLE consists of 1,414 essays written by Singaporean students who are non- native speakers of English. The essays cover top- ics, such as environmental pollution, health care, etc. The grammatical errors in these essays have been hand-corrected by professional English teachers and annotated with one of the 28 predefined error type. Another 50 essays, collected and annotated sim- ilarly as NUCLE, were used in both CoNLL GEC shared tasks as blind test data. The CoNLL-2014 test set has been annotated by two human annota- tors, the CoNLL-2013 by one annotator. Many par- ticipants of CoNLL-2014 shared task used the test set from 2013 as development set for their systems.</p><p>As mentioned before, we report main results us- ing similar training data as . We refer to this setting that as the "resticted-data set- ting" (r). Parallel data for translation model train- ing is adapted from the above mentioned NUCLE corpus and the publicly available Lang-8 corpus ( <ref type="bibr" target="#b19">Mizumoto et al., 2012</ref>), this corpus is distinct from the non-public web-crawled data described in Sec- tion 6. Uncorrected sentences serve as source data, corrected counterparts as target data. For language modeling, the target language sentences of both par- allel resources are used, additionally we extract all text from the English Wikipedia. Phrase-based SMT makes it ease to scale up in terms of training data, especially in the case of n- gram language models. To demonstrate the ease of data integration we propose an "unrestricted setting" (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of the shared task submis- sions, and later in <ref type="bibr" target="#b29">Xie et al. (2016)</ref>. We use Common Crawl data made-available by <ref type="bibr" target="#b2">Buck et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiments</head><p>Our system is based on the phrase-based part of the statistical machine translation system Moses ( <ref type="bibr" target="#b17">Koehn et al., 2007)</ref>. Only plain text data is used for lan- guage model and translation model training. Ex- ternal linguistic knowledge is introduced during pa- rameter tuning as the tuning metric relies on the error annotation present in NUCLE. The transla- tion model is built with the standard Moses training script, word-alignment models are produced with MGIZA++ ( <ref type="bibr" target="#b11">Gao and Vogel, 2008)</ref>, we restrict the word alignment training to 5 iterations of Model 1 and 5 iterations of the HMM-Model. No reorder- ing models are used, the distortion limit is set to 0, effectively prohibiting any reordering. All sys- tems use one 5-gram language model that has been estimated from the target side of the parallel data available for translation model training. Another 5- gram language model trained on Wikipedia in the restricted setting or on Common Crawl in the unre- stricted case.</p><p>Systems are retuned when new features of any type are added. We first successfully reproduce re- sults from  for BLEU-based tuning on the CoNLL-2013 test set as the devel- opment set <ref type="figure" target="#fig_2">(Fig. 3a)</ref>  </p><note type="other">instability on their experiments remains unclear.</note><p>Even with BLEU-based tuning, we can see signifi- cant improvements when replacing Levenshtein dis- tance with the finer-grained edit operations, and an- other performance jump with additional stateful fea- tures. The value range of the different tuning runs for the last feature set includes the currently best- performing system (Xie et al. <ref type="formula">(2016)</ref> with 40.56%), but the result for the averaged centroid are inferior.</p><p>Tuning directly with M 2 <ref type="figure" target="#fig_2">(Fig. 3b)</ref> and averag- ing weights across five iterations, yields between 40.66% M 2 for a vanilla Moses system and 42.32% for a system with all described dense features. Re- sults seen to be more stable. Averaging weight vec- tors across runs to produce the final vector seems like a fair bet. Performance with the averaged weight vectors is either similar to or better than the average number for five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Larger development sets</head><p>No less important than choosing the correct tun- ing metric is a good choice of the development set. Among MT researches, there is a number of more or less well known truths about suitable develop- ment sets for translation-focused settings: usually they consist of between 2000 and 3000 sentences, they should be a good representation of the testing data, sparse features require more sentences or more references, etc. Until now, we followed the seem- ingly obvious approach from  to tune on the CoNLL-2013 test set. The CoNLL-2013 test set consists of 1380 sentences, which might be barely enough for a translation-task, and it is unclear how to quantify it in the context of grammar correc- tion. Furthermore, calculating the error rate in this set reveals that only 14.97% of the tokens are part of an erroneous fragment, for the rest, input and refer- ence data are identical. Intuitively, this seems to be very little significant data for tuning an SMT system.</p><p>We therefore decide to take advantage of the en- tire NUCLE data as a development set which so far has only been used as translation model train- ing data. NUCLE consist of more than 57,000 sen- tences, however, the error rate is significantly lower than in the previous development set, only 6.23%. We adapt the error rate by greedily removing sen- tences from NUCLE until an error rate of ca. 15% is reached, 23381 sentences and most error annota- tions remain. We further divide the data into four folds. Each folds serves as development set for pa- rameter tuning, while the three remaining parts are treated as translation model training data. The full Lang-8 data is concatenated with is NUCLE train- ing set, and four models are trained. Tuning is then performed four times and the resulting four parame- ter weight vectors are averaged into a single weight vector across folds. We repeat this procedure again five times which results in 20 separate tuning steps. Results on the CoNLL-2014 test set are obtained us- ing the full translation model with a parameter vec- tor average across five runs. The CoNLL-2013 test set is not being used for tuning and can serve as a second test set.</p><p>As can be seen in <ref type="figure" target="#fig_2">Fig. 3c</ref>, this procedure sig- nificantly improves performance, also for the bare- bones set-up (41.63%). The lower variance between iterations is an effect of averaging across folds.</p><p>It turns out that what was meant to be a strong baseline, is actually among the strongest systems re- ported for this task, outperformed only by the fur- ther improvements over this baseline presented in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sparse Features</head><p>We saw that introducing finer-grained edit opera- tions improved performance. The natural evolution of that idea are features that describe specific cor- rection operations with and without context. This can be accomplished with sparse features, but tun- ing sparse features according to the M 2 metric poses unexpected problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Optimizing for M 2 with PRO and Mira</head><p>The MERT tool included in Moses cannot handle parameter tuning with sparse feature weights and one of the other optimizers available in Moses has to be used. We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets <ref type="table" target="#tab_4">(Table 3)</ref>.</p><p>Experiments with Mira hyper-parameters allowed to counter these effects. We first change the   background BLEU approximation method in Batch Mira to use model-best hypotheses (--model-bg) which seems to produce more satisfactory results. Inspecting the tuning process, however, reveals problems with this setting, too. <ref type="figure" target="#fig_5">Figure 4</ref> documents how instable the tuning process with Mira is across iterations. The best result is reached after only three iterations. In a setting with sparse features this would result in only a small set of weighted sparse features.</p><p>After consulting with one of the authors of Batch- Mira, we set the background corpus decay rate to 0.001 (-D 0.001), resulting in a sentence-level approximation of M 2 . Mira's behavior seems to sta- bilize across iterations. At this point it is not quite clear why this is required. While PRO's behav- ior is more sane during tuning, results on the test sets are subpar. It seems that no comparable hyper- parameter settings exist for PRO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sparse edit operations</head><p>Our sparse edit operations are again based on the Levenshtein distance matrix and count specific edits that are annotated with the source and target tokens that took part in the edit. For the following erro- neous/corrected sentence pair we generate sparse features that model contextless edits (matches are omitted):</p><formula xml:id="formula_0">subst(Then,Hence)=1 insert(,)=1 subst(comes, surfaces)=1 del(out)=1</formula><p>and sparse features with one-sided left or right or two-sided context: &lt;s&gt;_subst(Then,Hence)=1 <ref type="formula">1 2 3 4 5 6 7 8 9 10 11 12 13 14</ref>  All sparse feature types are added on-top of our best dense-features system. When using sparse fea- tures with context, the contextless features are in- cluded. The context annotation comes from the er- roneous source sentence, not from the corrected tar- get sentence. We further investigate different source factors: elements taking part in the edit operation or appearing in the context can either be word forms (factor 0) or word classes (factor 1). As before for dense features we average sparse feature weights across folds and multiple tuning runs. <ref type="figure">Figure 5</ref> summarizes the results for our sparse feature experiments. On both test sets we can see significant improvements when including edit- based sparse features, the performance increases even more when source context is added. The CoNLL-2013 test set contains annotations from only one annotator and is strongly biased towards high </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adding a web-scale language model</head><p>Until now we restricted our experiments to data used by . However, systems from the CoNLL-2014 were free to use any publicly available data, for instance in <ref type="bibr" target="#b16">Junczys-Dowmunt and Grundkiewicz (2014)</ref>, we made use of an n-gram lan- guage model trained from Common Crawl. <ref type="bibr" target="#b29">Xie et al. (2016)</ref> reach the best published result for the task (before this work) by integrating a similar n-gram language model with their neural approach.</p><p>We filter the English resources made available by <ref type="bibr" target="#b2">Buck et al. (2014)</ref> with cross-entropy filtering <ref type="bibr" target="#b20">(Moore and Lewis, 2010</ref>) using the corrected NU- CLE corpus as seed data. We keep all sentence with a negative cross-entropy score and compute a 5- gram KenLM (Heafield, 2011) language model with heavy pruning. This step produces roughly 300G of compressed text and a manageable 21G binary model (available for download). Bryant and Ng (2015) extended the CoNLL-2014 test set with additional annotations from two to ten annotators. We report results for this valuable re- source (column 2014-10) as well. <ref type="bibr">4</ref> According to <ref type="bibr" target="#b1">Bryant and Ng (2015)</ref>, human annotators seem to reach on average 72.58% M 2 which can be seen as an upper-bound for the task. In this work, we made a large step towards this upper-bound. <ref type="bibr">Prec</ref>   <ref type="table">Table 5</ref>: Previous best systems trained with non- public (np) error-corrected data for comparison with Rozovskaya and Roth (2016) denoted as R&amp;R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">More error-corrected data</head><p>As mentioned before, Rozovskaya and Roth (2016) trained their systems on crawled data from the Lang- 8 website that has been collect by us for our submis- sion to the CoNLL-2014 shared task. Since this data has not been made officially available, we treat it as non-public. This makes it difficult to put their results in relation with previously published work, but we can at least provide a comparison for our systems. As our strongest MT-only systems trained on pub- lic data already outperform the pipelined approaches from <ref type="bibr" target="#b26">Rozovskaya and Roth (2016)</ref>, it is unsurprising that adding more error-corrected parallel data results in an even wider gap <ref type="table">(Table 5</ref>). We can assume that this gap would persist if only public data had been used. Although these are the highest reported results for the CoNLL-2014 shared task so far, we think of them as unofficial results and refer to <ref type="table" target="#tab_5">Table 4</ref> as our final results in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Despite the fact that statistical machine translation approaches are among the most popular methods in automatic grammatical error correction, few papers that report results for the CoNLL-2014 test set seem to have exploited its full potential. An important as- pect when training SMT systems that one needs to tune parameters towards the task evaluation metric seems to have been under-explored. We have shown that a pure SMT system actu- ally outperforms the best reported results for any paradigm in GEC if correct parameter tuning is per- formed. With this tuning mechanism available, task- specific features have been explored that bring fur- ther significant improvements, putting phrase-based SMT ahead of other approaches by a large margin. None of the explored features require complicated pipelines or re-ranking mechanisms. Instead they are a natural part of the log-linear model in phrase- based SMT. It is therefore quite easy to reproduce our results and the presented systems may serve as new baselines for automatic grammatical error cor- rection. Our systems and scripts have been made available for better reproducibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison with previous work on the CoNLL-2014 task, trained on publicly available data. Dashed lines mark results for our baseline systems with restricted (r) and unrestricted (u) data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison with Rozovskaya and Roth (2016) using the non-public Lang-8 data set. Here (r) means no web-scale monolingual resources, (u) includes Google 1T n-grams or CommonCrawl.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on the CoNLL-2014 test set for different optimization settings (5 runs for each system) and different feature sets, the "All dense" entry includes OSM, the word class language model, and edit operations). The small circle marks results for averaged weights vectors and is chosen as the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>MERT</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Err: Then a new problem comes out . Cor: Hence , a new problem surfaces .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results per iteration on development set (4-th NUCLE fold) subst(Then,Hence)_a=1 Hence_insert(,)=1 insert(,)_a=1 problem_subst(comes, surfaces)=1 subst(comes, surfaces)_out=1 comes_del(out)=1 del(out)_.=1 &lt;s&gt;_subst(Then,Hence)_a=1 Hence_insert(,)_a=1 problem_subst(comes, surfaces)_out=1 comes_del(out)_.=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>Figure 5: Results on the CoNLL-2013 and CoNLL-2014 test set for different sparse features sets precision which might explain the greater instability. It appears that sparse features with context where surface forms and word-classes are mixed allow for the best fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parallel (above line) and monolingual train-
ing data. 

Operation Sequence Model. Durrani et al. (2013) 
introduce Operation Sequence Models in Moses. 
These models are Markov translation models that 
in our setting can be interpreted as Markov edition 
models. Translations between identical words are 
matches, translations that have different words on 
source and target sides are substitutions; insertions 
and deletions are interpreted in the same way as for 
SMT. Gaps, jumps, and other operations typical for 
OSMs do not appear as we disabled reordering. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>using similar training data. Repeated tuning places the scores reported by Su- santo et al. (2014) for their SMT-ML combinations (37.90 -39.39) within the range of possible values for a purely Moses-based system without any spe- cific features (35.19 -38.38) or with just the Leven- shtein distance features (37.46 -40.52). Since Su- santo et al. (2014) do not report results for multiple tuning steps, the extend of influence of optimizer</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Tuning with different optimizers with dense 
features only, results are given for the CoNLL-2013 
and CoNLL-2014 test set 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 summarizes the best results reported in</head><label>4</label><figDesc></figDesc><table>this paper for the CoNLL-2014 test set (column 
2014) before and after adding the Common Crawl 
n-gram language model. The vanilla Moses base-
line with the Common Crawl model can be seen as a 
new simple baseline for unrestricted settings and is 
ahead of any previously published result. The com-
bination of sparse features and web-scale monolin-
gual data marks our best result, outperforming pre-
viously published results by 8% M 2 using similar 
training data. While our sparse features cause a re-
spectable gain when used with the smaller language 
model, the web-scale language model seems to can-
cel out part of the effect. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Best results in restricted setting with added unrestricted language model for original (2014) and 
extended (2014-10) CoNLL test set (trained with public data only). 

System 
Prec. Recall 
M 2 

R&amp;R (np) 
60.17 25.64 47.40 

Best dense (np) 53.56 29.59 46.09 
+CCLM 
61.74 30.51 51.25 

Best sparse (np) 58.57 27.11 47.54 
+CCLM 
63.52 30.49 52.21 

</table></figure>

			<note place="foot" n="1"> Junczys-Dowmunt and Grundkiewicz (2014) is our own contribution and introduced many of the concepts discussed in this work, but seemingly to little effect during the task. Later analysis revealed that our submission had an incorrectly filtered language model that was missing many possible entries. Our original system without this deficiency would have achieved results around 44% M 2 already in 2014. This discovery triggered an intensive reanalysis of our shared task system with significantly new conclusions presented in this work. We apologize for supplying these results so late, as this seems to have halted progress in the field for nearly two years.</note>

			<note place="foot" n="4"> See Bryant and Ng (2015) for a re-assessment of the CoNLL-2014 systems with this extended test set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Colin Cherry for his help with Batch Mira hyper-parameters and Kenneth Heafield for many helpful comments and discussions. This work was partially funded by the Polish National Science Centre (Grant No. 2014/15/N/ST6/02330) and by Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as neces-sarily representing the official policies or endorse-ments, either expressed or implied, of Facebook.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Correcting ESL errors using phrasal SMT techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from fully automatic high quality grammatical error correction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="697" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the Common Crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference</title>
		<meeting>the Language Resources and Evaluation Conference<address><addrLine>Reykjavík, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3579" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Methods for smoothing the optimizer instability in SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit XIII: the Thirteenth Machine Translation Summit</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural network translation models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT &apos;11</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT &apos;11<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The NUS Corpus of Learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can Markov Models Over Minimal Translation Units Help PhraseBased SMT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="399" to="405" />
		</imprint>
	</monogr>
	<note>ACL (2)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>Helen Yannakoudakis, and Ekaterina Kochmar. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel implementations of word alignment tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering, Testing, and Quality Assurance for Natural Language Processing</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="27" to="36" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT &apos;11</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation, WMT &apos;11<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploiting n-best hypotheses to improve an smt approach to grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task)</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task)<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar, Alexandra Constantin, and Evan Herbst</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The effect of learner corpus size in grammatical error correction of ESL writings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers, ACLShort &apos;10</title>
		<meeting>the ACL 2010 Conference Short Papers, ACLShort &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The CoNLL2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the 17th Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
	</analytic>
	<monogr>
		<title level="m">Shared Task)</title>
		<meeting><address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno>Au- gust 7-12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Long Papers. The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Illinois-Columbia system in the CoNLL-2014 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">System combination for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Hendy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tou Hwee</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="951" to="962" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural language correction with character-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1603.09727</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">NAIST at 2013 CoNLL grammatical error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ippei</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensuke</forename><surname>Mitsuzawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the 17th Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Constrained grammatical error correction using statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the 17th Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
