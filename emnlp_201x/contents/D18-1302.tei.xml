<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Gender Bias in Abusive Language Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2799</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Ho</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Gender Bias in Abusive Language Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2799" to="2804"/>
							<date type="published">October 31-November 4, 2018. 2018. 2799</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, &quot;You are a good woman&quot; was considered &quot;sexist&quot; when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation , and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic detection of abusive language is an im- portant task since such language in online space can lead to personal trauma, cyber-bullying, hate crime, and discrimination. As more and more peo- ple freely express their opinions in social media, the amount of textual contents produced every day grows almost exponentially, rendering it difficult to effectively moderate user content. For this rea- son, using machine learning and natural language processing (NLP) systems to automatically detect abusive language is useful for many websites or social media services.</p><p>Although many works already tackled on train- ing machine learning models to automatically de- tect abusive language, recent works have raised concerns about the robustness of those systems. <ref type="bibr" target="#b9">Hosseini et al. (2017)</ref> have shown how to easily cause false predictions with adversarial examples in Google's API, and  show that classifiers can have unfair biases toward certain groups of people.</p><p>We focus on the fact that the representations of abusive language learned in only supervised learn- ing setting may not be able to generalize well enough for practical use since they tend to over- fit to certain words that are neutral but occur fre- quently in the training samples. To such classi- fiers, sentences like "You are a good woman" are considered "sexist" probably because of the word "woman."</p><p>This phenomenon, called false positive bias, has been reported by . They further defined this model bias as unintended, "a model contains unintended bias if it performs bet- ter for comments containing some particular iden- tity terms than for comments containing others."</p><p>Such model bias is important but often unmea- surable in the usual experiment settings since the validation/test sets we use for evaluation are al- ready biased. For this reason, we tackle the is- sue of measuring and mitigating unintended bias. Without achieving certain level of generalization ability, abusive language detection models may not be suitable for real-life situations.</p><p>In this work, we address model biases specific to gender identities (gender bias) existing in abu- sive language datasets by measuring them with a generated unbiased test set and propose three re- duction methods: (1) debiased word embedding, (2) gender swap data augmentation, (3) fine-tuning with a larger corpus. Moreover, we compare the effects of different pre-trained word embeddings and model architectures on gender bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>So far, many efforts were put into defining and constructing abusive language datasets from dif- ferent sources and labeling them through crowd-sourcing or user moderation ( <ref type="bibr" target="#b16">Waseem and Hovy, 2016;</ref><ref type="bibr" target="#b15">Waseem, 2016;</ref><ref type="bibr" target="#b8">Founta et al., 2018;</ref><ref type="bibr" target="#b17">Wulczyn et al., 2017</ref>). Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system ( <ref type="bibr" target="#b0">Badjatiya et al., 2017;</ref><ref type="bibr" target="#b13">Park and Fung, 2017;</ref><ref type="bibr" target="#b14">Pavlopoulos et al., 2017)</ref>. However, these works do not explicitly address any model bias in their models.</p><p>Addressing biases in NLP models/systems have recently started to gain more interest in the re- search community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. <ref type="bibr" target="#b3">Bolukbasi et al. (2016)</ref> is one of the first works to point out the gender stereotypes inside <ref type="bibr">word2vec (Mikolov et al., 2013)</ref> and propose an algorithm to correct them. <ref type="bibr" target="#b4">Caliskan et al. (2017)</ref> also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embed- dings contain problematic bias toward gender or race.  is one of the first works that point out existing "unintended" bias in abu- sive language detection models. <ref type="bibr" target="#b10">Kiritchenko and Mohammad (2018)</ref> compare 219 sentiment analy- sis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those sys- tems. <ref type="bibr" target="#b19">Zhao et al. (2018)</ref> shows the effectiveness of measuring and correcting gender biases in co- reference resolution tasks. We later show how we extend a few of these works into ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sexist Tweets (st)</head><p>This dataset consists of tweets with sexist tweets collected from Twitter by searching for tweets that contain common terms pertaining to sexism such as "feminazi." The tweets were then annotated by experts based on criteria founded in critical race theory. The original dataset also contained a rel- atively small number of "racist" label tweets, but we only retain "sexist" samples to focus on gen- der biases. <ref type="bibr" target="#b16">Waseem and Hovy (2016)</ref>; <ref type="bibr" target="#b15">Waseem (2016)</ref>, the creators of the dataset, describe "sex- ist" and "racist" languages as specific subsets of abusive language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Measuring Gender Biases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>Gender bias cannot be measured when evaluated on the original dataset as the test sets will follow the same biased distribution, so normal evaluation set will not suffice. Therefore, we generate a sep- arate unbiased test set for each gender, male and female, using the identity term template method proposed in . The intuition of this template method is that given a pair of sentences with only the identity terms different (ex. "He is happy" &amp; "She is happy"), the model should be able to generalize well and output same prediction for abusive lan- guage. This kind of evaluation has also been per- formed in SemEval 2018: Task 1 Affect In Tweets ( <ref type="bibr" target="#b10">Kiritchenko and Mohammad, 2018)</ref> to measure the gender and race bias among the competing sys- tems for sentiment/emotion analysis.</p><p>Using the released code 1 of , we generated 1,152 samples (576 pairs) by filling the templates with common gender identity pairs (ex. male/female, man/woman, etc.). We created templates ( <ref type="table" target="#tab_1">Table 2</ref>) that contained both neutral and offensive nouns and adjectives inside the vocabu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Templates</head><p>You are a (adjective) (identity term).</p><p>(verb) (identity term). Being (identity term) is (adjective) I am (identity term) I hate (identity term)   <ref type="table" target="#tab_2">Table 3</ref>) to retain balance in neutral and abusive samples.</p><p>For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equal- ity differences proposed in  which aggregates the difference between the over- all false positive/negative rate and gender-specific false positive/negative rate. False Positive Equal- ity Difference (FPED) and False Negative Equal- ity Difference (FNED) are defined as below, where T = {male, f emale}.</p><formula xml:id="formula_0">F P ED = t∈T |F P R − F P R t | F N ED = t∈T |F N R − F N R t |</formula><p>Since the classifiers output probabilities, equal er- ror rate thresholds are used for prediction decision.</p><p>While the two AUC scores show the perfor- mances of the models in terms of accuracy, the equality difference scores show them in terms of fairness, which we believe is another dimension for evaluating the model's generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classifica- tion: Convolutional Neural Network <ref type="figure">(CNN)</ref>   3. α-GRU: hidden dimension=256 (bidirec- tional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embed- ding Size=300, Dropout=0.3</p><p>We also compare different pre-trained embed- dings, <ref type="bibr">word2vec (Mikolov et al., 2013</ref>) trained on Google News corpus, <ref type="bibr">FastText (Bojanowski et al., 2017)</ref>) trained on Wikipedia corpus, and randomly initialized embeddings (random) to ana- lyze their effects on the biases. Experiments were run 10 times and averaged. <ref type="table" target="#tab_4">Tables 4 and 5</ref> show the bias measurement exper- iment results for st and abt, respectively. As expected, pre-trained embeddings improved task performance. The score on the unbiased generated test set (Gen. ROC) also improved since word em- beddings can provide prior knowledge of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results &amp; Discussions</head><p>However, the equality difference scores tended to be larger when pre-trained embeddings were   Results on abt. The false nega- tive/positive equality difference is significantly smaller than the st used, especially in the st dataset. This confirms the result of <ref type="bibr" target="#b3">Bolukbasi et al. (2016)</ref>. In all ex- periments, direction of the gender bias was to- wards female identity words. We can infer that this is due to the more frequent appearances of fe- male identities in "sexist" tweets and lack of neg- ative samples, similar to the reports of . This is problematic since not many NLP datasets are large enough to reflect the true data distribution, more prominent in tasks like abusive language where data collection and annotation are difficult.</p><p>On the other hand, abt dataset showed sig- nificantly better results on the two equality dif- ference scores, of at most 0.04. Performance in the generated test set was better because the mod- els successfully classify abusive samples regard- less of the gender identity terms used. Hence, we can assume that abt dataset is less gender-biased than the st dataset, presumably due to its larger size, balance in classes, and systematic collection method.</p><p>Interestingly, the architecture of the models also influenced the biases. Models that "attend" to certain words, such as CNN's max-pooling or α- GRU's self-attention, tended to result in higher false positive equality difference scores in st dataset. These models show effectiveness in catch- ing not only the discriminative features for clas- sification, but also the "unintended" ones causing the model biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Reducing Gender Biases</head><p>We experiment and discuss various methods to re- duce gender biases identified in Section 4.3.  <ref type="table">Table 6</ref>: Results of bias mitigation methods on st dataset. 'O' indicates that the corresponding method is applied. See Section 5.3 for more anal- ysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>Debiased Word Embeddings (DE) ( <ref type="bibr" target="#b3">Bolukbasi et al., 2016)</ref> proposed an algorithm to correct word embeddings by removing gender stereotyp- ical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their ef- fectiveness in our task.</p><p>Gender Swap (GS) We augment the training data by identifying male entities and swapping them with equivalent female entities and vice-versa. This simple method removes correlation between gender and classification decision and has proven to be effective for correcting gender biases in co- reference resolution task ( <ref type="bibr" target="#b19">Zhao et al., 2018</ref>).</p><p>Bias fine-tuning (FT) We propose a method to use transfer learning from a less biased corpus to re- duce the bias. A model is initially trained with a larger, less-biased source corpus with a same or similar task, and fine-tuned with a target corpus with a larger bias. This method is inspired by the fact that model bias mainly rises from the imbal- ance of labels and the limited size of data samples.</p><p>Training the model with a larger and less biased dataset may regularize and prevent the model from over-fitting to the small, biased dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Debiased word2vec <ref type="bibr" target="#b3">Bolukbasi et al. (2016)</ref> is compared with the original word2vec ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) for evaluation. For gender swapping data augmentation, we use pairs identified through crowd-sourcing by <ref type="bibr" target="#b19">Zhao et al. (2018)</ref>. After identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove fi- nal softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset.</p><p>Based on this criterion and results from Section 4.3, we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments. <ref type="table">Table 6</ref> shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gen- der swapping significantly reduced both the equal- ity difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbi- ased test set. The latter improvement shows that the model significantly reduced errors on the un- biased set in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results &amp; Discussion</head><p>To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differ- ences by 98% &amp; 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of "attending" model ar- chitectures on biases as discussed in Section 4.3. On the other hand, using the three methods to- gether improved both generated unbiased set per- formance and equality differences, but had the largest decrease in the original performance.</p><p>All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks <ref type="bibr">(abusive &amp; sexist)</ref>. However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important "unbiased" features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Work</head><p>We discussed model biases, especially toward gen- der identity terms, in abusive language detection. We found out that pre-trained word embeddings, model architecture, and different datasets all can have influence. Also, we found our proposed methods can reduce gender biases up to 90-98%, improving the robustness of the models.</p><p>As shown in Section 4.3, some classification performance drop happens when mitigation meth- ods. We believe that a meaningful extension of our work can be developing bias mitigation meth- ods that maintain (or even increase) the classifica- tion performance and reduce the bias at the same time. Some previous works ( <ref type="bibr" target="#b1">Beutel et al.;</ref><ref type="bibr" target="#b18">Zhang et al., 2018</ref>) employ adversarial training methods to make the classifiers unbiased toward certain variables. However, those works do not deal with natural language where features like gender and race are latent variables inside the language. Al- though those approaches are not directly compa- rable to our methods, it would be interesting to ex- plore adversarial training to tackle this problem in the future.</p><p>Although our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like ac- curacy or AUC. The idea of improving models by measuring and correcting gender bias is still un- familiar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be ex- tended to other identity problems like racial and to different tasks like sentiment analysis by follow- ing similar steps, and we hope to work on this in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example of templates used to generated 
an unbiased test set. 

Type 
Example Words 
Offensive 
disgusting, filthy, nasty, 
rude, horrible, terrible, aw-
ful, worst, idiotic, stupid, 
dumb, ugly, etc. 
Non-offensive 
help, love, respect, believe, 
congrats, hi, like, great, 
fun, nice, neat, happy, 
good, best, etc. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Example of offensive and non-offensive 
verbs &amp; adjectives used for generating the unbi-
ased test set. 

lary (See </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on st. False negative/positive 
equality differences are larger when pre-trained 
embedding is used and CNN or α-RNN is trained 

and Fung, 2017), Gated Recurrent Unit (GRU) 
(Cho et al., 2014), and Bidirectional GRU with 
self-attention (α-GRU) (Pavlopoulos et al., 2017), 
but with a simpler mechanism used in Felbo et al. 
(2017). Hyperparameters are found using the val-
idation set by finding the best performing ones in 
terms of original AUC scores. These are the used 
hyperparameters: 

1. CNN: Convolution layers with 3 filters 
with the size of [3,4,5], feature map 
size=100, Embedding Size=300, Max-
pooling, Dropout=0.5 

2. GRU: hidden dimension=512, Maximum Se-
quence Length=100, Embedding Size=300, 
Dropout=0.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/conversationai/ unintended-ml-bias-analysis</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work is partially funded by ITS/319/16FP of Innovation Technology Commission, HKUST, and 16248016 of Hong Kong Research Grants</head><p>Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for hate speech detection in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinkesh</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="759" to="760" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<title level="m">Data decisions and theoretical implications when adversarially learning fair representations. FAT/ML 2018: 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Issue 1</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring and mitigating unintended bias in text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Iyad Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale crowdsourcing and characterization of twitter abusive behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antigoni-Maria</forename><surname>Founta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Djouvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Despoina</forename><surname>Chatzakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Leontiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Stringhini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athena</forename><surname>Vakali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Michael Sirivianos, and Nicolas Kourtellis</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deceiving google&apos;s perspective api built for detecting toxic comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreeram</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Examining gender and race bias in two hundred sentiment analysis systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th</title>
		<meeting>the 7th</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Joint Conference on Lexical and Computational Semantics(*SEM)</title>
		<meeting><address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One-step and twostep classification for abusive language detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji Ho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALW1: 1st Workshop on Abusive Language Online to be held at the annual meeting of the Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeper attention to abusive user content moderation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1135" />
		</imprint>
	</monogr>
	<note>Prodromos Malakasiotis, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first workshop on NLP and computational social science</title>
		<meeting>the first workshop on NLP and computational social science</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hateful symbols or hateful people? predictive features for hate speech detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL student research workshop</title>
		<meeting>the NAACL student research workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ex machina: Personal attacks seen at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1391" to="1399" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hu Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI/ACM Conference on Ethics and Society(AIES)</title>
		<meeting>AAAI/ACM Conference on Ethics and Society(AIES)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
