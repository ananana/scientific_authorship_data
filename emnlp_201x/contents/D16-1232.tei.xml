<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonparametric Bayesian Models for Spoken Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Wakabayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsukuba University</orgName>
								<address>
									<addrLine>1-2 Kasuga</addrLine>
									<postCode>305-8550</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johane</forename><surname>Takeuchi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<addrLine>Ltd. 8-1 Honcho</addrLine>
									<postCode>351-0188</postCode>
									<region>Saitama</region>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Funakoshi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<addrLine>Ltd. 8-1 Honcho</addrLine>
									<postCode>351-0188</postCode>
									<region>Saitama</region>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikio</forename><surname>Nakano</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Honda Research Institute</orgName>
								<address>
									<addrLine>Ltd. 8-1 Honcho</addrLine>
									<postCode>351-0188</postCode>
									<region>Saitama</region>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonparametric Bayesian Models for Spoken Language Understanding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2144" to="2152"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a new generative approach for semantic slot filling task in spoken language understanding using a nonparamet-ric Bayesian formalism. Slot filling is typically formulated as a sequential labeling problem , which does not directly deal with the posterior distribution of possible slot values. We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an infinite set of slot values. We demonstrate that this approach significantly improves slot estimation accuracy compared to the existing sequential labeling algorithm.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken language understanding (SLU) refers to the challenge of recognizing a speaker's intent from a natural language utterance, which is typically de- fined as a slot filling task. For example, in the ut- terance "Remind me to call John at 9am tomorrow", the specified information {"time": "9am tomor- row"} and {"subject": "to call John"} should be extracted. The term slot refers to a variable such as the time or subject that is expected to be filled with a value provided through the user's utterance.</p><p>The slot filling task is typically formulated as a sequential labeling problem as shown in <ref type="figure">Figure 1</ref>. This labeling scheme naturally represents the recog- nition of arbitrary phrases that appear in the tran- scription of an utterance. Formally speaking, when we assume a given set of slots {s 1 , ..., s M } and de- note the corresponding slot values by {v s 1 , ..., v s M } where v s i ∈ V s i , the domain of each slot value V s i is an infinite set of word sequences. In this paper, we use the term arbitrary slot filling task to refer to this implicit problem statement, which inherently under- lies the sequential labeling formulation.</p><p>In contrast, a different line of work has explored the case where V s i is provided as a finite set of possi- ble values that can be handled by a backend system <ref type="bibr" target="#b7">(Henderson, 2015)</ref>. We refer to this type of task as a categorical slot filling task. In this case, the slot filling task is regarded as a classification problem that explicitly considers a value-based prediction, as shown in <ref type="figure">Figure 2</ref>. From this point of view, we can say that a distribution of slot values is actually con- centrated in a small set of typical phrases, even in the arbitrary slot filling task, because users basically know what kind of function is offered by the system.</p><p>To reflect this observation, in this paper we ex- plore the value-based formulation approach for arbi- trary slot filling tasks. Unlike the sequential labeling formulation, which is basically position-based label prediction, our method directly estimates the poste- rior distribution over an infinite set of possible val- ues for each slot V s i . The distribution is represented by using a Dirichlet process <ref type="bibr" target="#b3">(Gershman and Blei, 2012)</ref>, which is a nonparametric Bayesian formal- ism that generates a categorical distribution for any space. We demonstrate that this approach improves estimation accuracy in the arbitrary slot filling task compared with conventional sequential labeling ap- proach.</p><p>The rest of this paper is organized as follows. In Section 2, we review the existing approaches for categorical and arbitrary slot filling tasks and intro-duce related work. In Section 3, we present our nonparametric Bayesian formulation, the hierarchi- cal Dirichlet process slot model (HDPSM), which directly models an infinite set of slot values. On the basis of the HDPSM, we develop a generative utter- ance model that allows us to compute the posterior probability of slot values in Section 4. In Section 5, we introduce a two-stage slot filling algorithm that consists of a candidate generation step and a candi- date ranking step using the proposed model. In Sec- tion 6, we show the experimental results for multiple datasets in different domains to demonstrate that the proposed algorithm performs better than the base- line sequential labeling method. We conclude in Section 7 with a brief summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The difference between the categorical and arbitrary slot filling approaches has not been explicitly dis- cussed in a comparative manner to date. In this sec- tion, we review existing work for both approaches.</p><p>For the categorical slot filling approach, various algorithms that directly model the distribution of slot values have been proposed, including generative models <ref type="bibr" target="#b20">(Williams, 2010)</ref>, maximum entropy linear classifiers ( <ref type="bibr" target="#b14">Metallinou et al., 2013)</ref>, and neural net- works ( <ref type="bibr" target="#b16">Ren et al., 2014</ref>). However, none of these models are applicable for predicting a variable that ranges over an infinite set, and it is not straightfor- ward to extend them suitably. In particular, a dis- criminative approach is not applicable for arbitrary slot filling tasks because it requires a fixed finite set of slot values to take statistics.</p><p>The arbitrary slot filling approach is a natural application of shallow semantic parsing <ref type="bibr" target="#b4">(Gildea, 2002)</ref>, which is naturally formulated as a sequen- tial labeling problem. Various sequential labeling algorithms have been applied to this task, including support vector machines, conditional random fields (CRF) ( <ref type="bibr" target="#b11">Lafferty et al., 2001;</ref><ref type="bibr" target="#b6">Hahn et al., 2011)</ref>, and deep neural networks <ref type="bibr" target="#b13">(Mesnil et al., 2015;</ref><ref type="bibr" target="#b22">Xu and Sarikaya, 2013)</ref>. <ref type="bibr" target="#b19">Vukotic et al. (2015)</ref> reported that the CRF is still the most accurate, rapid, and stable method among them. Because the focus of this pa- per is arbitrary slot filling tasks, we use CRFs as our baseline method.</p><p>In this paper, we apply nonparametric Bayesian models <ref type="bibr" target="#b3">(Gershman and Blei, 2012</ref>) to represent the distribution over arbitrary phrases for each slot. The effectiveness of this phrase modeling approach has been examined in various applications including morphological analysis <ref type="bibr" target="#b5">(Goldwater et al., 2011</ref>) and infinite vocabulary topic models <ref type="bibr">(Zhai and Boydgraber, 2013</ref>). Our method can be regarded as an application of this idea, although it is not straight- forward to integrate it with the utterance generation process, as we explain later.</p><p>Consequently, our proposed method is catego- rized as a generative approach. There are many ad- vantages inherent in generative approaches that have been examined, including unsupervised SLU ( <ref type="bibr" target="#b0">Chen et al., 2015)</ref>, automatic feature extraction ( <ref type="bibr" target="#b18">Tur et al., 2013)</ref>, and integration with syntactic modeling ( <ref type="bibr" target="#b12">Lorenzo et al., 2013)</ref>. Another convenient prop- erty of generative models is that prior knowledge can be integrated in an intuitive way <ref type="bibr" target="#b15">(Raymond et al., 2006</ref>). This often leads to better performance with less training data compared with discriminative models trained completely from scratch ( <ref type="bibr" target="#b8">Komatani et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Dirichlet Process Slot Model</head><p>In this section, we present a nonparametric Bayesian formulation that directly models the distribution over an infinite set of possible values for each slot. Let S = {s 1 , ..., s M S } be a given set of slots and M S be the number of slots. We define each slot s i as a random variable ranging over an infinite set of letter sequences V , which is represented as follows:</p><formula xml:id="formula_0">V = {b 1 , ..., b L |b ι ∈ C, L ≥ 0}</formula><p>where C is a set of characters including the blank character and any other character that potentially ap- pears in the transcription of an utterance. Conse- quently, we regard the set of slots S as also being a random variable that ranges over V M S . The objec- tive of this section is to develop the formulation of the probabilistic distribution p(S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dirichlet Process</head><p>We apply the Dirichlet process (DP) to model both the distribution for an individual slot p i (s i ) and the joint distribution p(S). In this subsection, we review the definition and key properties of DP with general notation for the target distribution G over the do- main X . In the DP for the prior of p i (s i ) that is described in Section 3.2, the domain X corresponds to a set of slot values V , e.g., "fen ditton", "new chesterton", and None. In the DP for p(S) presented in Section 3.3, X indicates a set of tuples of slot val- ues V M S , e.g., ("restaurant", "new chesterton", "fast food") and ("restaurant", "fen ditton", None). The DP is a probabilistic distribution over the dis- tribution G. DP is parameterized by α 0 and G 0 , where α 0 &gt; 0 is a concentration parameter and G 0 is a base distribution over X . If G is drawn from DP (α 0 , G 0 ) (i.e., G ∼ DP (α 0 , G 0 )), then the fol- lowing Dirichlet distributed property holds for any partition of X denoted by {A 1 , ..., A L }:</p><formula xml:id="formula_1">(G(A 1 ), ..., G(A L )) ∼ Dir(α(A 1 ), ..., α(A L ))</formula><p>where α(A) = α 0 G 0 (A), which is known as the base measure of DP. <ref type="bibr" target="#b1">Ferguson (1973)</ref> proved an important property of a posterior distribution of repeated i.i.d. sam- ples</p><formula xml:id="formula_2">x 1:N = {x 1 , ..., x N } drawn from G ∼ DP (α 0 , G 0 ).</formula><p>Consider a countably infinite set of atoms φ = {φ 1 , φ 2 , ...} that are independently drawn from G 0 . Let c i ∈ N be the assignment of an atom for sample x i , which is generated by a sequen- tial draw with the following conditional probability:</p><formula xml:id="formula_3">p(c N +1 = k|c 1:N ) = n k N +α 0 k ≤ K α 0 N +α 0 k = K + 1</formula><p>where n k is the number of times that the kth atom appears in c 1:N and K is the number of different atoms in c 1:N . Given the assignment c 1:N , the pre- dictive distribution of x N +1 ∈ X is represented in the following form:</p><formula xml:id="formula_4">P (x N +1 = θ|c 1:N , φ 1:K , α 0 , G 0 ) = K k=1 n k N + α 0 δ(φ k , θ) + α 0 N + α 0 G 0 (θ)</formula><p>The base distribution possibly generates an iden- tical value for different atoms, such as (φ 1 = "fen ditton", φ 2 = "new chesterton", φ 3 = "fen ditton"). The assignment c i is an auxiliary variable to indi- cate which of these atoms is assigned to the ith data point x i ; when x i = "fen ditton", c i can be 1 or 3. The posterior distribution above depends on the fre- quency of atom n k , not on the frequency of θ itself. The atoms φ and the assignment c are latent vari- ables that should be determined at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Individual Slot Model</head><p>First we formulate the distribution for an individ- ual slot as</p><formula xml:id="formula_5">p i (s i ) ∼ DP (α 0 i , G 0 i ) where G 0 i is a base distribution over the set of phrases V . 1 We define G 0</formula><p>i as a generative model that consists of two-step generation: generation of the phrase length 0 ≤ L i ≤ L max using a categorical distribution and generation of a letter sequence s 1:L i using an n-gram model, as follows:</p><formula xml:id="formula_6">L i ∼ Categorical(λ i ) s ι i ∼ p(s ι i |s ι−n+1:ι−1 i , η i )</formula><p>where λ i and η i are parameters for the categorical distribution and the n-gram model for slot s i , respec- tively. This explicit modeling of the length helps avoid the bias toward shorter phrases and leads to a better distribution, as reported by Zhai and Boyd- graber (2013). We define G 0 i as a joint distribution of these models:</p><formula xml:id="formula_7">G 0 i (s 1:L i i ) = p(L i |λ i ) L i ι=1 p(s ι i |s ι−n+1:ι−1 i , η i ) (1)</formula><p>G 0 i potentially generates an empty phrase of L i = 0 to express the case that the slot value v s i is not provided by an utterance. Therefore, the distribu- tion p i (s i ) can naturally represent the probability of None, which is shown in <ref type="figure">Figure 2</ref>.</p><p>We consider prior distributions of the parame- ters λ i and η i to treat the n-gram characteristics of each slot in a fully Bayesian manner. p(λ) is given as a L max -dimensional symmetric Dirichlet distri- bution with parameter a. We also define the |C|- dimensional symmetric Dirichlet distributions with parameter b for each n-gram context, since given the context p(s ι i |s ι−n+1:ι−1 i , η i ) is just a categorical dis- tribution that ranges over C. Consider we observe N phrases s i for slot i. Let n L iι be the number of phrases that have length ι and n γ ih be the number of times that letter s ι = h appears after context s ι−n+1:ι−1 = γ. The predictive probability of a phrase is represented as follows:</p><formula xml:id="formula_8">G 0 i (s 1:L i i |s i ) = n L iι + b N + bC L i ι=1 n γ is ι i + a c n γ ic + a L max l=1 n L il</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generative Model for a Set of Slot Values</head><p>A naive definition of the joint distribution p(S) is a product of all slot probabilities M S i=1 p i (s i ) for making an independence assumption. However, the slot values are generally correlated with each other <ref type="bibr" target="#b0">(Chen et al., 2015)</ref>. To obtain more accurate dis- tribution, we formulate p(S) using another DP that recognizes a frequent combination of slot values, as p(S) ∼ DP (α 1 , G 2 ) where G 2 is a base distribu- tion over V M S . We apply the naive independence assumption to G 2 as follows:</p><formula xml:id="formula_9">G 2 (S) = M S i=1 p i (s i )</formula><p>The whole generation process of S involves two- layered DPs that share atoms among them. In this sense, this generative model is regarded as a hierar- chical Dirichlet process ( <ref type="bibr" target="#b17">Teh et al., 2005)</ref>.</p><p>Let</p><formula xml:id="formula_10">G 1 i (s i ) = p i (s i )</formula><p>and G 3 (S) = p(S) for con- sistent notations. In summary, we define the hierar- chical Dirichlet process slot model (HDPSM) as a generative model that has the following generation process.</p><formula xml:id="formula_11">G 1 i ∼ DP (α 0 i , G 0 i ) G 3 ∼ DP (α 1 , G 2 ) S ∼ G 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference of HDPSM</head><p>In a slot filling task, observations of S 1:T = {S 1 , ..., S T } are available as training data. The in- ference of HDPSM refers to the estimation of λ, η and the atom assignments for each DP.</p><p>We formulate the HDPSM in a form of the Chi- nese restaurant franchise process, which is one of the explicit representations of hierarchical DPs obtained by marginalizing out the base distributions. <ref type="bibr" target="#b17">Teh et al. (2005)</ref> </p><note type="other">presents a Gibbs sampler for this repre- sentation, which involves a repetitive resampling of atoms and assignment. In our method, we prefer to adopt a single pass inference, which samples the as- signment for each observation only once. Our pre- liminary experiments showed that the quality of in- ference is not affected because S is observed unlike the settings in Teh et al. (2005).</note><p>We denote the atoms and the atom assignment in the first level DP DP (α 1 , G 2 ) by φ 1 and c 1 1:N , re- spectively. The posterior probability of atom assign- ment for a new observation S N +1 is represented as follows:</p><formula xml:id="formula_12">p(c 1 N +1 = k|c 1 1:N , φ 1 , S N +1 ) ∝ n 1 k δ(φ 1 k , S N +1 ) k ≤ K α 1 G 2 (S N +1 ) k = K + 1</formula><p>where n 1 k is the number of times that the kth atom appears in c 1 1:N and K is the number of different atoms in c 1 1:N . φ 0 i and c 0 i1:K denote the atoms and the assignment in the second level DPs DP (α 0 i , G 0 i ). The second level DPs assign atoms to each first level atom φ 1 k , i.e. the second level atom φ 0 it is generated only when a new atom is assigned for S N +1 at the first level. The posterior probability of atom assignment at the second level is:</p><formula xml:id="formula_13">p(c 0 iK+1 = t|c 0 i1:K , φ 0 i , s N +1i ) ∝ n 0 it δ(φ 0 it , S N +1 ) t ≤ T i α 0 i G 0 (S N +1 ) t = T i + 1</formula><p>where n 0 it is the number of times that the tth atom appears in c 0 i1:K and T i is the number of different atoms in c 0 i1:K . The single pass inference procedure is presented in Algorithm 1. Given the atoms φ and the as- signments c, the predictive distribution of S N +1 = Algorithm 1 Single pass inference of HDPSM Input: A set of observations S 1:N 1: Set empty list to c 1 and c 0</p><formula xml:id="formula_14">i 2: for d = 1 to N do 3: k ∼ p(c 1 d = k|c 1 1:d−1 , φ 1 , S d )</formula><p>4:</p><p>if k = K + 1 then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for i = 1 to M S do 6:</p><formula xml:id="formula_15">t i ∼ p(c 0 iK+1 = t i |c 0 i1:K , φ 0 i , s di ) 7: if t i = T i + 1 then 8:</formula><p>Update n L i and n γ i with s di</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>end if </p><formula xml:id="formula_16">P (S N +1 |c, φ) = K k=1 n 1 k N + α 1 δ(φ 1 k , S N +1 ) (2) + α 1 N + α 1 M S i=1 P (s N +1i |c 0 i , φ 0 i ) P (s N +1i |c 0 i , φ 0 i ) = T i t=1 n 0 it K + α 0 i δ(φ 0 it , s N +1i ) + α 0 i K + α 0 i G 0 i (s N +1i |φ 0 i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Model for an Utterance</head><p>We present a generative utterance model to derive a slot estimation algorithm given utterance u. <ref type="figure">Figure 3</ref> presents the basic concept of our generative model. In the proposed model, we formulate the distribution of slot values as well as the distribution of non-slot parts. In <ref type="figure">Figure 3</ref>, the phrases "hi we're in um" and "and we need a" should be removed to identify the slot information. We call these non-slot phrases as functional fillers because they more or less have a function to convey information. Identifying the set of non-slot phrases is equivalent to identifying the set of slot phrases. Therefore, we define a generative model of functional fillers in the same way as the slot values.</p><p>!"#$%&amp;'%#"(#)*#+%(#,"-.(#/(,#$%#(%%,#/#'%01/)'/(1!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!"#$2#'%01/)'/(1# %&amp;$%2#+%(#,"-.(# '(()2#!"#$%</head><p>3-%'/(4%2! 5.(1%(1#67.12!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>*$+,--,-+2#!"#$%&amp;'%#"(#)*# .,))/$2#/(,#$%#(%%,#/# $-),-+2#!"#$!</head><p>8)(49.(/7#8"77%'2! <ref type="figure">Figure 3</ref>: The proposed generative utterance model. We at- tempt to find the best combination of the slot parts and the non- slot parts (i.e., functional filler parts) by using this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Functional Filler</head><p>We assume an utterance u is a concatenation of slot values S and functional fillers F . A functional filler is represented as a phrase that ranges over V . To derive the utterance model, we first formulate a gen- erative model for functional fillers.</p><p>In our observation, the distribution of the func- tional filler depends on its position in an utterance. For example, utterances often begin with typical phrases such as "Hello I'm looking for ..." or "Hi please find ...", which can hardly ever appear at other positions. To reflect this observation, we introduce a filler slot to separately model the functional fillers based on a position feature. Specifically, we define three filler slots: beginning filler f 1 , which precedes any slot value, ending filler f 3 , which appears at the end of an utterance, and middle filler f 2 , which is inserted between slot values. We use the term con- tent slot to refer to S when we intend to explicitly distinguish it from a filler slot.</p><p>Let F = {f 1 , f 2 , f 3 } be a set of filler slots and M F = 3 be the number of filler slots. Each slot f i is a random variable ranging over V and F is a ran- dom variable over V M F . These notations for filler slots indicate compatibility to a content slot, which suggests that we can formulate F using HDPSMs, as follows:</p><formula xml:id="formula_17">H 1 i ∼ DP (β 0 i , H 0 i ) H 3 ∼ DP (β 1 , H 2 ) F ∼ H 3</formula><p>where H 0 i is an n-gram-based distribution over V that is defined in an identical way to (1) and <ref type="figure">Figure 4</ref>: Graphical model of the utterance model. <ref type="figure">Figure 4</ref> presents the graphical model of our utter- ance model. We assume that an utterance u is built with phrases provided by S and F . Therefore, the conditional distribution p(u|S, F ) basically involves a distribution over the permutation of these slot val- ues with two constraints: f 1 is placed first and f 3 has to be placed last. In our formulation, we simply adopt a uniform distribution over all possible permu- tations.</p><formula xml:id="formula_18">H 2 (F ) = M F i=1 H 1 i (F ). G 0 G 1 S u H 0 H 1 F M S M F G 3 H 3 D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Utterance Model</head><p>For training the utterance model, we assume that a set of annotated utterances is available. Each train- ing instance consists of utterance u and annotated slot values S. Given u and S, we assume that the functional fillers F can be uniquely identified. For the example in <ref type="figure">Figure 3</ref>, we can identify the sub- sequence in u that corresponds to each content slot value of "restaurant" and "fen ditton". This match- ing result leads to the identification of filler slot val- ues. Consequently, a triple (u, S, F ) is regarded as an observation. Because the HDPSMs of the content slot and of the filler slot are conditionally indepen- dent given S and F , we can separately apply Algo- rithm 1 to train each HDPSM.</p><p>For slot filling, we examine the posterior proba- bility of content slot values S given u, which can be reformed as follows:</p><formula xml:id="formula_19">P (S|u) ∝ F P (u|S, F )P (S)P (F )</formula><p>In this equation, we can remove the summation of F because filler slot values F are uniquely identified regarding u and S in our assumption. Additionally, we approximately regard P (u|S, F ) as a constant if u can be built with S and F . By using these assump- tions, the posterior probability is reduced to the fol- lowing formula:  where F in this formula is fillers identified given u and S. Consequently, the proposed method attempts to find the most likely combination of the slot val- ues and the non-slot phrases, since all words in an utterance have to belong to either of them. By using trained HDPSM (i.e., the posterior given all training data), P (S) and P (F ) can be computed by (2).</p><formula xml:id="formula_20">P (S|u) ∝ P (S)P (F ) (3) !"# ! ! ! "#$%&amp;$ '#$%&amp;$ '#$%&amp;$ ! ! ! ! "#()*&amp; $%&amp; ! ! ! ! "#$%&amp;$ '#$%&amp;$ ! ! ! ! "#()*&amp; '(&amp; ! ! ! ! "#$%&amp;$ '#$%&amp;$ ! ! "#+,,-'#+,,- ! ./ 0&amp;1%&amp; /2 34 +&amp;2 -/((,2 $2-0&amp; 2&amp;&amp;- $ %&amp;5($3%$2( !"# $%&amp;$ 346+&amp;26-/((,2<label>()*</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Candidate Generation</head><p>For estimating slot values given u, we adopt a can- didate generation approach <ref type="bibr" target="#b21">(Williams, 2014</ref>) that leverages another slot filling algorithm to enumer- ate likely candidates. <ref type="bibr">2</ref> Specifically, we assume a candidate generation function g(u) that generates N candidates {S 1 , ..., S N } regarding u. Our slot fill- ing algorithm computes the posterior probability by (3) for each candidate slot S j and takes the candi- date that has the highest posterior probability. In this estimation process, our utterance model works as a secondary filter that covers the error of the primary analysis. <ref type="figure" target="#fig_0">Figure 5</ref> provides an example of candidate gener- ation by using a sequential labeling algorithm with IOB tags. The subsequences to which the O tag is assigned can be regarded as functional fillers. The values for each filler slot are identified depending on the position of the subsequence, as the figure shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the performance of the proposed gener- ative model with an experiment using the algorithm name #utterances #slots max. diversity <ref type="table" target="#tab_3">DSTC  1,441  6  55  Weather  1,442  3  191   Table 1</ref>: Datasets in the experiment. Max. diversity refers to the maximum number of value types that are taken by a slot. described in Section 5. We adopt a conditional ran- dom field (CRF) as a candidate generation algorithm that generates N -best estimation as candidates. For the CRF, we apply commonly used features includ- ing unigram and bigram of the surface form and part of speech of the word. We used CRF++ 3 as the CRF implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset</head><p>The performance of our method is evaluated using two datasets from different languages, as summa- rized in <ref type="table">Table 1</ref>. The first dataset is provided by the third Dialog State Tracking Challenge <ref type="bibr" target="#b7">(Henderson, 2015)</ref>, hereafter referred to as the DSTC corpus.</p><p>The DSTC corpus consists of dialogs in the tourist information domain. In our experiment, we use the user's first utterance in each dialog, which typically describes the user's query to the system. Utterances without any slot information are excluded. We man- ually modified the annotated slot values into "as- is form" to allow a sequential labeling method to extract the ground-truth values. This identification process can be done in a semi-automatic manner that involves no expert knowledge. We apply the part of speech tagger in NLTK 4 for the CRF application. The second dataset is a weather corpus consisting of user utterances in an in-house corpus of human- machine dialogues in the weather domain. It con- tains 1,442 questions spoken in Japanese. In this corpus, the number of value types for each slot is higher than DSTC, which indicates a more challeng- ing task. We applied the Japanese morphological analyzer MeCab ( <ref type="bibr" target="#b9">Kudo et al., 2004</ref>) to segment the Japanese text into words before applying CRF.</p><p>For both datasets, we examine the effect of the amount of available annotated utterances by varying the number of training data in <ref type="bibr">25, 50, 75, 100, 200, 400, 800, all.</ref> #train CRF best HDP N = 5 HDP N = 300 25 0.560 0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics</head><p>The methods are compared in terms of slot estima- tion accuracy. Let n c be the number of utterances for which the estimated slot S and the ground-truth slotˆSslotˆ slotˆS are perfectly matched, and let n e be the num- ber of the utterances including an estimation error. The slot estimation accuracy is simply calculated as nc nc+ne . All evaluation scores are calculated as the average of 10-fold cross validation. We also conduct a binomial test to examine the statistical significance of the improvement in the proposed algorithm com- pared to the CRF baseline. <ref type="table" target="#tab_3">Tables 2 and 3</ref> present the slot estimation accu- racy for the DSTC corpus and the Japanese weather corpus, respectively. The baseline (CRF best) is a method that takes only one best output of CRF for slot estimation. HDP with N = 5 and N = 300 is the proposed method, where N is the number of candidates generated by the CRF candidate genera-utterance estimation by CRF best estimation by HDP N = 5 im looking for a restaurant that type:restaurant (*) type:restaurant,food:fast food serves fast food i want a moderate restaurant in area:new chesterton, area:new chesterton, the new chesterton area type:restaurant, type:restaurant, food:moderate (*) pricerange:moderate im looking for a cheap chine pricerange:cheap,type:restaurant, pricerange:cheap,type:restaurant, chinese takeaway restaurant food:chinese takeaway food:chine chinese takeaway (*) tor. The asterisks (*) beside the HDP accuracy in- dicate the statistical significance against CRF best, which is tested using the binomial test. Results show that our proposed method performs significantly better than CRF. Especially when the amount of training data is limited, the proposed method outperforms the baseline. This property is attractive for practical speech recognition systems that offer many different functions. Accurate recog- nition at an early stage of development allows a practitioner to launch a service that results in quickly collecting hundreds of speech examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>Since we use the CRF as a candidate generator, we expect that the CRF N-best can rank the correct answer higher in the candidate list. In fact, the top five candidates cover almost all of the correct an- swers. Therefore, the result in the comparison of N = 5 and N = 300 suggests the stability of the proposed method against the mostly noisy 295 can- didates. Because the proposed algorithm makes no use of the original ranking order, N = 300 is a harder condition in which to identify the correct an- swer. Nevertheless, the result shows that the drop in the performance is limited; the accuracy is still sig- nificantly better than the baseline. This result sug- gests that the proposed method is less dependent on the performance of the candidate generator. <ref type="table" target="#tab_5">Table 4</ref> presents some examples of the slot val- ues estimated by CRF best and HDP with N = 5 for the condition where the number of training utter- ances is 800. The first two are samples where CRF best failed to predict the correct values. These er- rors are attributed to infrequent sequential patterns caused by the less trained expressions "that serves fast food" and "moderate restaurant" because CRF is a position-based classifier. The value-based for- mulation allows the model to learn that the phrase "fast food" is more likely to be a food name than to be a functional filler and to reject the candidate.</p><p>The third example in <ref type="table" target="#tab_5">Table 4</ref> shows an error using HDP, which extracted "chine chinese take- away" which includes a reparandum of disfluency ( <ref type="bibr" target="#b2">Georgila et al., 2010)</ref>. This error can be attributed to the fact that this kind of disfluency resembles the true slot value, which leads to a higher probability of "chine" in the food slot model compared to in the functional filler model. Regarding this type of error, preliminary application of a disfluency detec- tion method ( <ref type="bibr" target="#b23">Zayats et al., 2016</ref>) is promising for improving accuracy.</p><p>The execution time for training the proposed HDP utterance model with 1297 training data in the Japanese weather corpus was about 0.3 seconds. This is a good performance since the CRF training takes about 5.5 seconds. Moreover, the training of the proposed HDP model is scalable and works in an online manner because it is a single pass algorithm. When we have a very large number of training ex- amples, the bottleneck is the CRF training, which requires scanning the whole dataset repeatedly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed an arbitrary slot fill- ing method that directly deals with the posterior probability of slot values by using nonparametric Bayesian models. We presented a two-stage method that involves an N-best candidate generation step, which is typically done using a CRF. Experimental results show that our method significantly improves recognition accuracy. This empirical evidence sug- gests that the value-based formulation is a promis- ing approach for arbitrary slot filling tasks, which is worth exploring further in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Candidate generation using sequential labeling algorithm. The figure shows the case of N = 3.</figDesc><graphic url="image-5.png" coords="6,314.79,59.40,223.63,108.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Slot estimation accuracy for the DSTC corpus. The 

asterisk (*) indicates that the accuracy is statistically significant 

compared against CRF best (p &lt; 0.005). 

#train CRF best HDP N = 5 HDP N = 300 
25 
0.327 
0.452* 
0.480* 
50 
0.379 
0.488* 
0.499* 
75 
0.397 
0.504* 
0.522* 
100 
0.418 
0.501* 
0.512* 
200 
0.493 
0.526* 
0.531* 
400 
0.512 
0.551* 
0.549* 
800 
0.533 
0.555* 
0.554* 
1297 
0.546 
0.560* 
0.554 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Slot estimation accuracy for the Japanese weather cor-

pus. An asterisk (*) indicates statistical significance against 

CRF best (p &lt; 0.01). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Examples of estimated slot values for the condition of #train is 800. An asterisk (*) indicates misrecognition. 

</table></figure>

			<note place="foot" n="1"> Note that the subscript i for s, p, α 0 and G 0 indicates the slot type such as &quot;type&quot;, &quot;area&quot; and &quot;food&quot; in Figure 2.</note>

			<note place="foot" n="2"> The direct inference of the generative utterance model is a topic for near future work. The MCMC method will circumvent the difficulty of searching the entire candidate space.</note>

			<note place="foot" n="3"> https://taku910.github.io/crfpp/ 4 http://www.nltk.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Bayesian Analysis of Some Nonparametric Problems. The Annual of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="209" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-Domain Speech Disfluency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual SIGDIAL Meeting on Discourse and Dialogue</title>
		<meeting>Annual SIGDIAL Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A tutorial on Bayesian nonparametric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2335" to="2382" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing stochastic approaches to spoken language understanding in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Dinarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lehnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1569" to="1583" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine Learning for Dialog State Tracking: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Machine Learning in Spoken Language Processing</title>
		<meeting>Workshop on Machine Learning in Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic Allocation of Training Data for Rapid Prototyping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunori</forename><surname>Komatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Katsumaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikio</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Funakoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hiroshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computational Linguistics</title>
		<meeting>International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Applying Conditional Random Fields to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Japanese Morphological Analysis</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language essing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised structured semantic inference for spoken dialog reservation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandra</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Cerisara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual SIGDIAL Meeting on Discourse and Dialogue</title>
		<meeting>Annual SIGDIAL Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative state tracking for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the use of finite state transducers for semantic interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Béchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Damnati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="288" to="304" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Markovian discriminative modeling for dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual SIGDIAL Meeting on Discourse and Dialogue</title>
		<meeting>Annual SIGDIAL Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent Semantic Modeling for Slot Filling in Conversational Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is it Time to Switch to Word Embedding and Recurrent Neural Networks for Spoken Language Understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedran</forename><surname>Vukotic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental partition recombination for efficient tracking of multiple dialog states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Web-style ranking and SLU combination for dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual SIGDIAL Meeting on Discourse and Dialogue</title>
		<meeting>Annual SIGDIAL Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular CRF for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03209</idno>
		<title level="m">Disfluency Detection using a Bidirectional LSTM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online Latent Dirichlet Allocation with Infinite Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
