<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CALCS: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
							<email>syavuz@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
							<email>chungchengc@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<email>yonghui@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CALCS: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3708" to="3718"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3708</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Maximum-likelihood estimation (MLE) is one of the most widely used approaches for training structured prediction models for text-generation based natural language processing applications. However, besides exposure bias, models trained with MLE suffer from wrong objective problem where they are trained to maximize the word-level correct next step prediction, but are evaluated with respect to sequence-level discrete met-rics such as ROUGE and BLEU. Several variants of policy-gradient methods address some of these problems by optimizing for final discrete evaluation metrics and showing improvements over MLE training for downstream tasks like text summarization and machine translation. However, policy-gradient methods suffers from high sample variance, making the training process very difficult and unstable. In this paper, we present an alternative direction towards mitigating this problem by introducing a new objective (CALCS) based on a differentiable surrogate of longest common subsequence (LCS) measure that captures sequence-level structure similarity. Experimental results on abstractive summarization and machine translation validate the effectiveness of the proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, deep neural networks have achieved state-of-the-art results in various tasks including computer vision, natural language processing, and speech processing. Specifically, neural text gen- eration models, central focus of this work, have led to great progress in central downstream NLP tasks like text summarization, machine transla- tion, and image captioning. For example, the abstractive summarization task, which has previ- ously not been the popular choice for text sum- * Work done while interning at Google Brain. marization due to lack of appropriate text gener- ation methods, has gained revived attention with the success of neural sequence-to-sequence mod- els ( <ref type="bibr" target="#b30">Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015)</ref>. There has been several recent work with an im- pressive progress on this task including ( <ref type="bibr" target="#b27">Rush et al., 2015;</ref><ref type="bibr" target="#b21">Nallapati et al., 2016;</ref><ref type="bibr" target="#b19">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr" target="#b31">Tan et al., 2017;</ref>. Machine trans- lation is another central field in NLP where the emergence of neural sequence-to-sequence mod- els has enabled viable alternative approaches ( <ref type="bibr" target="#b18">Luong et al., 2015;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b7">Cho et al., 2014;</ref><ref type="bibr" target="#b30">Sutskever et al., 2014</ref>) to challenge tradi- tional phrase-based methods ( <ref type="bibr" target="#b15">Koehn et al., 2003</ref>).</p><p>Most of the recent existing works on neural text generation are based on variants of sequence-to- sequence models with attention ( <ref type="bibr" target="#b3">Bahdanau et al., 2015</ref>) trained with Maximum-likelihood estima- tion (MLE) with teacher forcing. As <ref type="bibr" target="#b25">Ranzato et al. (2016)</ref> points out in a previous work, these models have two major drawbacks. First, they are trained to maximize the probability of correct next word given the entire sequence of previous ground truth words. While, at test time, the models need to generate the entire sequence by feeding its own predictions at previous time steps. This discrep- ancy is called exposure bias and hurts the perfor- mance as the model is never exposed to its own predictions during training. The second drawback, called wrong objective, is due yet another discrep- ancy between training and testing. It refers to the critique ( <ref type="bibr" target="#b25">Ranzato et al., 2016</ref>) that MLE-trained models tend to have suboptimal performance as they are trained to maximize a convenient objec- tive (i.e., maximum likelihood of word-level cor- rect next step prediction) rather than a desirable sequence-level objective that correlates better with the common discrete evaluation metrics such as ROUGE ( <ref type="bibr" target="#b16">Lin and Och, 2004</ref>) for summarization, BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>) for translation, and word error rate for speech recognition, not log- likelihood. On the other hand, training models that directly optimize for such discrete metrics as objective is hard due to non-differentiable nature of the corresponding loss functions <ref type="bibr" target="#b26">(Rosti et al., 2011</ref>). To address these issues, <ref type="bibr" target="#b25">Ranzato et al. (2016)</ref> introduces an incremental learning recipe that uses a hybrid loss function combining <ref type="bibr">REINFORCE (Williams, 1992)</ref> and cross-entropy. Re- cently, <ref type="bibr" target="#b24">Paulus et al. (2018)</ref> also explored com- bining maximum-likelihood and policy gradient training for text summarization.</p><p>Towards sequence level optimization, previous works ( <ref type="bibr" target="#b25">Ranzato et al., 2016;</ref><ref type="bibr" target="#b24">Paulus et al., 2018</ref>) employ reinforcement learn- ing (RL) with a policy-gradient approach which works around the difficulty of differentiating the reward function by using it as a weight. How- ever, REINFORCE is known to suffer from high sample variance and credit assignment problems which makes the training process difficult and un- stable besides resulting in models that are hard to reproduce <ref type="bibr" target="#b12">(Henderson et al., 2018)</ref>.</p><p>In this paper, we propose an alternative ap- proach for sequence-level training with longest common subsequence (LCS) metric that measures the sequence-level structure similarity between two sequences. We essentially introduce a con- tinuous approximation to the discrete LCS met- ric which can be directly optimized against using standard gradient-based methods. Our proposed approach has the advantage of being able to di- rectly optimize for a surrogate reward as opposed to using the exact reward only as a weight as in RL-inspired works. Hence, it provides a viable alternative perspective to policy-gradient methods for side stepping the non-differentiability with re- spect to the exact reward. In addition, it simultenu- ously combats the exposure bias problem through exposing the model to its own predictions while computing our approximation to LCS metric.</p><p>To this end, we introduce a new learning recipe that incorporates the aformentioned continuous approximation to LCS metric (CALCS) as an ad- ditional objective on top of maximum-likelihood loss in existing neural text generation models. We evaluate the proposed approach on abstrac- tive text summarization and machine translation tasks. To this end, we use recently introduced pointer-generator network ( <ref type="bibr" target="#b28">See et al., 2017</ref>) and transformer ( <ref type="bibr" target="#b33">Vaswani et al., 2017)</ref> as underlying baselines for summarization and machine transla- tion, respectively. More precisely, we start from a pre-trained baseline model with cross-entropy loss, and continue training the model to optimize for the proposed differentiable objective based on CALCS. Using this recipe, we conduct various experiments on CNN/Daily Mail ( <ref type="bibr" target="#b13">Hermann et al., 2015;</ref><ref type="bibr" target="#b21">Nallapati et al., 2016</ref>) summarization and WMT 2014 English-to-German machine transla- tion tasks. Experimental results validate the effec- tiveness of the proposed approach on both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Continuously Approximating Longest Common Subsequence Metric</head><p>In this work, we explore the potential use of longest common subsequence (LCS) metric from an algorithmic point of view to address the afore- mentioned wrong objective and exposure bias problems. LCS metric measures a sequence-level structure similarity between discrete sequences by identifying longest co-occurring in sequence n- grams and it has been shown to correlate well with human judgments for downstream text generation tasks ( <ref type="bibr" target="#b16">Lin and Och, 2004)</ref>. To this end, we pro- pose a way to continuously approximate LCS met- ric and use this differentiable approximation as the objective to train text generation models rather than the exact LCS measure, which is hard to op- timize for due to non-differentiability of the cor- responding loss function. Although such differ- entiable approximation provides a unique advan- tage from modeling and optimization perspective, the difficulty of controlling its tightness might be a potential drawback in terms of its applicability. In this section, we will first introduce our proposed approximation to LCS metric, and then provide a natural way to control its tightness. Consider a sequence generation problem condi- tioned on an input sequence x = (x 1 , x 2 , . . . , x n ) and let y = (y 1 , y 2 , . . . , y m ) denote its corre- sponding ground-truth output sequence. Let</p><formula xml:id="formula_0">f (x, Θ) = z = (z 1 , z 2 , . . . , z k )</formula><p>denote hypothesis sequence obtained by greedy decoding from a generic encoder-decoder archi- tecture for input sequence x, where Θ represents model parameters. Also, let p 1 , p 2 , . . . , p k be the probability distributions over vocabulary V at de- coding time steps from which z 1 , z 2 , . . . , z k are generated via argmax operator, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CALCS</head><p>In this section, we define our approach to con- tinuously approximate the longest common sub- sequence measure (LCS), which is an unnormal- ized version of ROUGE-L metric ( <ref type="bibr" target="#b16">Lin and Och, 2004</ref>) (See Appendix B) that is commonly used for performance evaluation of text summarization models. The main intuition behind our approach is to relax the common necessity for hard inferences while computing discrete metrics by instead com- paring discrete tokens in a soft way. Towards this end, we start by defining LCS metric. Definition 1. Given two sequences y and z of to- kens, longest common subsequence LCS(y, z) is defined as the longest sequence of tokens that ap- pear left-to-right (but not necessarily in a contigu- ous block) in both sequences.</p><p>The most common and intuitive solution for computing longest common subsequence is via dynamic programming. We will briefly revisit this here as it will be useful in terms of both recall and notational convenience while describing our sur- rogate LCS measure. Let r i,j denote the longest common subsequence between prefix sequences y [:i] = (y 1 , y 2 , . . . , y i ) and z [:j] = (z 1 , z 2 , . . . , z j ) of y and z, respectively. A dynamic programming solution is given by</p><formula xml:id="formula_1">r i,j =      0 if i = 0 or j = 0 r i−1,j−1 + 1 if y i = z j max(r i−1,j , r i,j−1 ) o/w.</formula><p>(1) r i,j for all i = 1, 2, . . . , m and j = 1, 2, . . . , k.</p><p>It can be computed in mk iterations using the for- mula in Eqn 1. After computing 2D dynamic pro- gramming matrix r, we obtain LCS(y, z) = r m,k .</p><p>Towards removing the dependence on hard in- ference for computing LCS, we now define our approximation to longest common subsequence, which we call CALCS. At high-level, the idea is to continuously relax the original LCS mea- sure. To this end, we leverage output probabil- ity distributions p 1 , p 2 , . . . , p k as soft predictions to refine the dynamic programming formulation for original LCS. More precisely, we recursively define soft longest common subsequence s i,j be- tween prefixes y <ref type="bibr">[:i]</ref> and z <ref type="bibr">[:j]</ref> in analogous to r i,j as follows:</p><formula xml:id="formula_2">s i,j = p (y i ) j (s i−1,j−1 + 1) + (2) (1 − p (y i ) j ) max(s i−1,j , s i,j−1 ) (3)</formula><p>for i, j &gt; 0 and s i,0 = s 0,j = 0, where p (y i ) j de- note the probability of generating y i at j-th decod- ing step. Intuitively, CALCS replaces the hard to- </p><formula xml:id="formula_3">ken comparison 1 [y i = z j ] in</formula><p>Although the proposed approximation is a natu- ral way of relaxing/extending the hard binary com- parison of discrete tokens, it is not clear how tight the approximation is, which is established in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">On the Tightness of Approximation</head><p>In this section, we first discuss the tightness of the proposed approximation, and then provide a natu- ral way of controlling it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Bounding the Approximation Error</head><p>We now present a bound on the approximation error of the proposed CALCS compared to the original LCS measure. Characterization of this bound will enable us to theoretically argue about the feasibilty of using the proposed surrogate re- ward function for our objective as well as control- ling its tightness.</p><p>LCS measure is intrinsically monotonic by defi- nition. We start by a lemma that establishes a sim- ilar monotonicity property for CALCS. Lemma 1.</p><p>[Monotonicity] The following two in- equalities</p><formula xml:id="formula_5">s i,j ≤ s i,j+1 ≤ s i,j + 1 s i,j ≤ s i+1,j ≤ s i,j + 1</formula><p>hold for all 0 ≤ i &lt; m and 0 ≤ j &lt; k.</p><p>Proof. See Appendix A for the proof.</p><p>Having established a certain monotonicity prop- erty for CALCS, we will discuss its approximation error to the original LCS measure. Let</p><formula xml:id="formula_6">δ i,j = s i,j − r i,j<label>(5)</label></formula><p>denote the approximation error of CALCS to LCS measure between generated prefix sequence y <ref type="bibr">[:i]</ref> and the ground-truth prefix z <ref type="bibr">[:j]</ref> .</p><formula xml:id="formula_7">Lemma 2. Let P i,j = {(0, 0), (i 1 , j 1 ), . . . , (i q−1 , j q−1 ), (i q , j q )}</formula><p>de- note the path of dynamic programming algorithm for LCS ending at (i, j) = (i q , j q ) cell of m × k grid. Then,</p><formula xml:id="formula_8">|δ i,j | &lt; δ i q−1 ,j q−1 + (1 − max(p j )) (6)</formula><p>where max(p j ) = max{p</p><formula xml:id="formula_9">(1) j , p (2) j , . . . , p (|V |) j }.</formula><p>Proof. We will establish the proof by investigating two cases and combining them. CASE 1: z j = y i . In this case, we have</p><formula xml:id="formula_10">r i,j = 1 + r i−1,j−1<label>(7)</label></formula><p>and</p><formula xml:id="formula_11">(i q−1 , j q−1 ) = (i − 1, j − 1)<label>(8)</label></formula><p>by 1. Using Eq. 7, we get</p><formula xml:id="formula_12">δ i,j = s i,j − r i,j = 1 − p (y i ) j max (s i−1,j , s i,j−1 ) + p (y i ) j (s i−1,j−1 + 1) − (1 + r i−1,j−1 ) = (s i−1,j−1 − r i−1,j−1 ) + 1 − p (y i ) j max(s i−1,j , s i,j−1 ) − (1 + s i−1,j−1 )</formula><p>Using the definition of δ and triangle inequality, we get</p><formula xml:id="formula_13">|δ i,j | ≤ |δ i−1,j−1 | + 1 − p (y i ) j (1 + s i−1,j−1 ) − max (s i−1,j , s i,j−1 ) ≤ |δ i−1,j−1 | + 1 − p (y i ) j<label>(9)</label></formula><p>where inequality 9 follows from the monotonicity established by Lemma 1. Moreover, z j = y i implies p (y i ) j = max(p j ) be- cause z is generated by greedy decoding. Plugging this in Eq. 9 and using Eq. 8, we can immediately conclude that</p><formula xml:id="formula_14">|δ i,j | &lt; δ i q−1 ,j q−1 + (1 − max(p j ))<label>(10)</label></formula><p>CASE 2: z j = y i . By definition 1, we have r i,j = max (r i−1,j , r i,j−1 ) .</p><p>Using this identity, we obtain</p><formula xml:id="formula_15">δ i,j = s i,j − r i,j = 1 − p (y i ) j max (s i−1,j , s i,j−1 ) + p (y i ) j (s i−1,j−1 + 1) − max (r i−1,j , r i,j−1 ) = p (y i ) j [(1 + s i−1,j−1 ) − max (s i−1,j , s i,j−1 )] + [max (s i−1,j , s i,j−1 ) − max (r i−1,j , r i,j−1 )]</formula><p>Applying triangle inequality on the last equation above, we get</p><formula xml:id="formula_16">|δ i,j | ≤ p (y i ) j |(1 + s i−1,j−1 ) − max (s i−1,j , s i,j−1 )| + |max (s i−1,j , s i,j−1 ) − max (r i−1,j , r i,j−1 )| ≤ p (y i ) j |(1 + s i−1,j−1 ) − max (s i−1,j , s i,j−1 )| + max (|s i−1,j − r i−1,j | , |s i,j−1 − r i,j−1 |) (11) = p (y i ) j |(1 + s i−1,j−1 ) − max (s i−1,j , s i,j−1 )| + max (|δ i−1,j | , |δ i,j−1 |) ≤ p (y i ) j + max (|δ i−1,j | , |δ i,j−1 |)<label>(12)</label></formula><p>where inequality 12 follows from again the mono- tonicity of s[·, ·], and inequality 11 follows from the following identity that holds true for all real numbers a, b, c, d ≥ 0</p><formula xml:id="formula_17">|max(a, b) − max(c, d)| ≤ max(|a − c| , |b − d|)</formula><p>Moreover, since z j = y i , we know that p</p><formula xml:id="formula_18">(y i ) j = max(p j ), which implies p (y i ) j ≤ 1 − max(p i ).<label>(13)</label></formula><p>Combining 11 and 13 completes the proof for this case. Finally, two cases investigated above to- gether establish the proof of Lemma 2.</p><p>Lemma 2 leads to the following important corollary. <ref type="figure">j 1 )</ref>, . . . , (i q , j q )} be the path of dynamic programming algorithm for LCS ending at (i, j) = (i q , j q ) cell of m × k grid. Then,</p><formula xml:id="formula_19">Corollary 1. Let P i,j = {(0, 0), (i 1 ,</formula><formula xml:id="formula_20">|δ i,j | ≤ q w=1 (1 − max(p jw ))<label>(14)</label></formula><p>Proof. Applying Lemma 2 iteratively and using δ 0,0 = 0, we get</p><formula xml:id="formula_21">|δ i,j | ≤ δ i q−1 ,j q−1 + (1 − max(p jq )) δ i q−1 ,j q−1 ≤ δ i q−2 ,j q−2 + (1 − max(p j q−1 )) δ i q−2 ,j q−2 ≤ δ i q−3 ,j q−3 + (1 − max(p j q−2 )) . . . |δ i 1 ,j 1 | ≤ |δ 0,0 | + (1 − max(p j 1 )) |δ 0,0 | ≤ 0</formula><p>Summing (q + 1)-many inequalities above side by side and cancelling out the same terms appearing on both sides of the resulting inequality establishes the proof of corollary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Controlling the Tightness of</head><p>Approximation Corollary 1 hints for a natural way of control- ling the tightness of approximation CALCS by ex- ploiting the peakedness of model's softmax output probability distributions. More precisely, upper bound on the approximation error is represented as a sum of 1 − max(p j )'s, hence the more peaked the model's output probability distributions on av- erage, the smaller the approximation error we are guaranteed by the established bounds.</p><p>We exploit this property to control the tight- ness of approximation by making a modification to computation of the proposed CALCS measure. Formally, let l 1 , l 2 , . . . , l k denote the unnormal- ized logits of the model output before applying softmax to obtain probabilities p 1 , p 2 , . . . , p k at decoding time steps, respectively. Hence,</p><formula xml:id="formula_22">p (i) j = exp(l (i) j ) i exp(l (i) j )<label>(15)</label></formula><p>Recall that CALCS is computed using p j 's. Using peaked softmax, we can obtain more peaked prob- ability distributions without causing any change in the actual generated sequence z via greedy decod- ing. This is simply because the order of probabil- ities for corresponding vocabulary words will not change, only the probability disribution p j will get more peaked. So, we define peaked softmax oper- ator with hyperparameter α as</p><formula xml:id="formula_23">p (i) j (α) = exp(l (i) j /α) i exp(l (i) j /α)<label>(16)</label></formula><p>By Corollary 1, |δ i,j | → 0 as α → 0 for CALCS measure computed with p j (α). One can further attempt to use Corollary 1 as a guide to pinpoint a range of α values to force the approximation error within certain desired limits. We will use α as a hyperparameter in this work.</p><p>Corollary 1 is also useful for alternative ways of controlling the tightness of approximation such as incurring penalty for high-entropy output prob- ability distributions or simply penalizing the max- imum output probability values less than a desired threshold (that explicitly controls the tightness of the approximation). We leave such options of con- trolling the approximation error for future work.</p><p>With the guidance of Corollary 1 and peaked softmax in Eq. 16, we conclude that CALCS es- tablishes a promising approximation for LCS mea- sure. In the next section, we introduce a new ob- jective function using CALCS as a continuously differentiable reward to be directly maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sequence Level Optimization via CALCS</head><p>In this section, we describe how to leverage CALCS to define a loss function for sequence level optimization. For notational consistency, we will use f (x, Θ) to denote an encoder-decoder archi- tecture that takes an input sequence x and out- puts a sequence of tokens z = (z 1 , z 2 , . . . , z m ) via greedy decoding from corresponding probabil- ity distributions p 1 , p 2 , . . . , p m at each step.</p><p>For a pair of input sequence x and its corre- sponding ground-truth output sequence y, we de- fine</p><formula xml:id="formula_24">J CALCS (x, y; Θ) = − log CALCS(y, f (x, Θ)) |y|<label>(17)</label></formula><p>as the loss function for a sample (x, y) based on the CALCS, where |y| denote the length of se- quence y. It is important to note here that while computing probability distribution p t at decoding step t, we feed model's own prediction z t−1 at the previous time step to fight exposure bias.</p><p>It is important to observe here that J CALCS (x, y; Θ) is differentiable in p 1 , p 2 , . . . , p k by definition and each p i is differentiable in model parameters Θ. Hence, J CALCS (x, y; Θ) is differ- entiable in model parameters Θ, which allows us to directly optimize the network parameters with respect to LCS metric. The bound we established on the approximation error and our proposed strategy to control it theoretically ensures the feasibility of using the introduced loss function J CALCS to optimize for LCS metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we first briefly revisit the pointer- generator ( <ref type="bibr" target="#b28">See et al., 2017</ref>) and transformer ( <ref type="bibr" target="#b33">Vaswani et al., 2017</ref>) networks that are used as the underlying baselines in our experiments. Sub- sequently, we describe how the proposed objective function and its variants are used to train new sum- marization and machine translation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Models</head><p>Pointer-Generator Network. We use pointer- generator network (See et al., 2017) as our base- line sequence-to-sequence model for text sum- marization. It is essentially a hybrid between sequence-to-sequence model with attention (Bah- danau et al., 2015) and a pointer network ( ) that supports two decoding modes, copying and generating, via a soft switch mecha- nism. This enables the model to copy a word from the input sequence based on the attention distribu- tion. On each decoding time step t, the decoder LSTM is fed the word embedding of the previous word, and computes a decoder state s t , an atten- tion distribution a t over the words of input article, and a probability P vocab (w) of generating word w for summary from output vocabulary V , which is then softly combined with the copy mode's proba- bility distribution P copy (w) via soft switch proba- bility p gen ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>  where a t i indicates the attention probability on i- th word of the input article. The whole network is then trained end-to-end with the negative log- likelihood loss function of</p><formula xml:id="formula_25">J PG (x, y; Θ) = − 1 |y| |y| t=1 log(p (yt) t )</formula><p>for a sample article-summary pair (x, y) where Θ denote the learnable model parameters. It is im- portant to note here that we do not use the cov- erage mechanism introduced by the original work ( <ref type="bibr" target="#b28">See et al., 2017)</ref> to prevent the potential repetition problem in the summaries generated by the model.</p><p>Transformer Network. For machine translation, we use the transformer network ( <ref type="bibr" target="#b33">Vaswani et al., 2017)</ref>, which is a recently published model that achieved state-of-the-art results on WMT 2014 English-to-German MT task with less computa- tional time owing to its highly parallelizable ar- chitecture. The core idea behind this model is to use stacked self-attention mechanisms along with point-wise, fully connected layers for both en- coder and decoder to represent its input and out- put. For the sake of brevity, we refer the reader to ( <ref type="bibr" target="#b33">Vaswani et al., 2017</ref>) for further details regard- ing the architecture. Similar to previously defined loss functions, let J TF (x, y; Θ) denote the per- example loss function of transformer networks for an input-output translation pair (x, y) where Θ is again indicating the learnable model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Variants and Training</head><p>Let {(x (l) , y (l) )} N l=1 denote the set of training ex- amples, where x (l) 's are input sequences, and y (l) 's are their corresponding ground-truth output sequences. Before optimizing for the introduced objective J CALCS , we first train the corresponding baseline network by minimizing</p><formula xml:id="formula_26">J {PG,TF} (Θ) = 1 N N l=1</formula><p>J {PG,TF} (x, y; Θ).</p><p>Unlike J CALCS , loss functions J {PG,TF} for base- line models are computed by teacher forcing, feed- ing the previous ground-truth word at each de- coding step. We will denote the baseline models by POINTGEN for pointer-generator network and TRANSFORMER for transformer network.</p><p>To optimize for the proposed objective J CALCS , we initialize the model parameters Θ from the pre- trained baseline network and continue training the model by minimizing the joint loss</p><formula xml:id="formula_27">J(Θ) = λJ CALCS (Θ) + (1 − λ)J {PG,TF} (Θ) (18) J CALCS (Θ) = 1 N N l=1 J CALCS (x, y; Θ)<label>(19)</label></formula><p>where λ is a hyperparameter controlling the bal- ance between the two losses. During the training with the joint loss, we compute J CALCS (x, y; Θ), defined in Eq. 17, by performing |y|-many decoding steps as a simple strategy to pre- vent the model from gaming the training objec- <ref type="bibr" target="#b21">Nallapati et al., 2016)</ref> 35.46 32.65 w/o coverage ( <ref type="bibr" target="#b28">See et al., 2017)</ref> 36.44 33.42 w/ coverage ( <ref type="bibr" target="#b28">See et al., 2017)</ref> 39.53 36.38 LEAD-3 baseline ( <ref type="bibr" target="#b28">See et al., 2017)</ref> 40.34 36.57 RL ( <ref type="bibr" target="#b24">Paulus et al., 2018)</ref> 41.16 39.08 ML + RL ( <ref type="bibr" target="#b24">Paulus et al., 2018)</ref> 39 tive by generating longer and longer hypothe- ses instead of incurring an additional length penalty. We will refer to the resulting model trained with the loss function in Eq. 18 as {POINTGEN, TRANSFORMER}+CALCS depend- ing on the baseline model.</p><formula xml:id="formula_28">Model ROUGE-1 ROUGE-L (</formula><note type="other">.87 36.90 Our Models POINTGEN* 39.11 26.97** POINTGEN*+SS 39.33 26.94** POINTGEN*+SS+CALCS 40.37 29.18**</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We numerically evaluate the proposed method on two sequence generation benchmarks: abstrac- tive document-summarization and machine trans- lation. We compare the results of the proposed method against the recently proposed strong base- line models ( <ref type="bibr" target="#b28">See et al., 2017</ref>) for summarization and and ( <ref type="bibr" target="#b33">Vaswani et al., 2017</ref>) for machine trans- lation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Abstractive Summarization</head><p>We use a modified version of the CNN/Daily Mail dataset ( <ref type="bibr" target="#b13">Hermann et al., 2015</ref>) that is first used for summarization by <ref type="bibr" target="#b21">(Nallapati et al., 2016)</ref>. How- ever, we follow the processing script provided by For training our baseline model, we use single layer LSTM encoder (bi-directional) and decoder with hidden dimensions of 512 and 1024, respec- tively. We use a vocabulary of 50k words for both source and target. Following the original paper, we also do not pre-train word embeddings, which are learned with the rest of model parameters during training. We use the Adam ( <ref type="bibr" target="#b14">Kingma and Ba, 2015)</ref> optimizer with a learning rate of 0.00001 for train- ing. We pre-train the baseline model for 20k steps by applying greedy scheduled sampling ( ) with fixed ground-truth feeding prob- ability of 75%. Once the baseline model training is complete, we start optimizing for CALCS objec- tive as described in the previous section. Also, we set λ = 1.0 and α = 1.0, which are tuned on the development set.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we report our main results on the summarization task. POINTGEN+SS refers to the baseline model trained with scheduled sampling. POINTGEN+SS+CALCS corresponds our model trained with CALCS starting from POINTGEN+SS model. Experimental results demonstrate that training with our proposed objective provides an improvement of 2.2 points in ROUGE-L score. This also provides empirical evidence to justify that our approximate CALCS effectively captures what the original LCS metric is supposed to mea- sure, recalling ROUGE-L is a normalized LCS. The reason why ROUGE-L scores of our models are lower than previously reported is that we eval- uate ROUGE-L score by taking the entire sum- mary as a single sequence instead of splitting it into sentences, which is also the way we compute CALCS objective during the model training pro- cess. The main motivation behind this approach is to encourage the model to preserve the sentence order within a summary, and evaluate its perfor- mance in the same way. We consider the capability of preserving the order across produced sentences as an important attribute a multi-sentence sum- marization model should have in terms of read- ability and fluency of its generated summaries as a whole. When POINTGEN*+SS and POINT- GEN*+SS+CALCS are evaluated by splitting the generated summaries into sentences, their cor- responding ROUGE-L scores become 35.38 and 35.12, respectively. We also observe a nice side- improvement of 1.0 point in ROUGE-1 score over the baseline, which achieves a comparable per- formance with the long-overdue LEAD-3 base- line score. It might also be comparable to the recently reported state-of-the-art ROUGE-1 re- sult on CNN/DailyMail dataset by <ref type="bibr" target="#b24">Paulus et al. (2018)</ref> as they used a different dataset processing pipeline, which makes it difficult to directly com- pare with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model BLEU</head><p>GNMT (  24.61 GNMT+RL (  24.60 TRANSFORMER ( <ref type="bibr" target="#b33">Vaswani et al., 2017)</ref> 27.3 WEIGHTED TRANSFORMER ( <ref type="bibr" target="#b0">Ahmed et al., 2018)</ref> 28.4 TRANSFORMER* 27.6 TRANSFORMER*+CALCS 27.8 <ref type="table">Table 2</ref>: Machine translation results on WMT 2014 English- to-German task. TRANSFORMER* corresponds to our train- ing of the original model in ( <ref type="bibr" target="#b33">Vaswani et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Translation</head><p>We also evaluate our sequence-level training ap- proach on the WMT 2014 English-to-German ma- chine translation task, which contains 4.5M pairs of sentences. To train our baseline transformer model, we closely follow the small model in the original transformer paper ( <ref type="bibr" target="#b33">Vaswani et al., 2017)</ref>. We use a vocabulary of size 32k. Our encoder and decoder consist of N = 6 identical layers each. Following the notation in the original paper, we set the other parameters as d model = 512, d ff = 2048, h = 8, P drop = 0.1. We set λ = 0.3 and α = 1.0, which are tuned on the development set.</p><p>In <ref type="table">Table 2</ref>, we show our empirical results on machine translation task. Our first observation is that our trained baseline transformer network achieves a better performance than the one re- ported in the original paper ( <ref type="bibr" target="#b33">Vaswani et al., 2017</ref>) by 0.3 BLEU score, which might be solely due to hyperparameter tuning. More importantly, we observe that training with our proposed CALCS objective leads to noticeable 0.2 BLEU point im- provements over the baseline, which further rein- forces our confidence in effectiveness of our pro- posed sequence-level training approach and its ap- plicability to other sequence prediction tasks. It is also interesting to note that optimizing for LCS metric via its continuous approximation leads to improvements in evaluation with another discrete metric BLEU. On the other had, optimizing for the exact discrete metric BLEU via reinforcement learning strategy may not improve the evaluation performance in BLEU as reported by ( . As a final remark, we would like to note that our proposed approach is orthogonal to ad- vancements in more expressive and powerful ar- chitecture designs. Hence it has the potential to provide further improvements over the recently proposed models such as WEIGHTED TRANS- FORMER ( <ref type="bibr" target="#b0">Ahmed et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Text Summarization. Before the successful ap- plication of neural generative models, most of the existing works on text summarization ( <ref type="bibr" target="#b10">Dorr et al., 2003;</ref><ref type="bibr" target="#b11">Durrett et al., 2016</ref>) have focused on ex- tractive methods. While some of the early ap- proaches have used a rich set of heuristic rules or sparse features to select textual units to include in the summary, more recent works <ref type="bibr" target="#b6">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b20">Nallapati et al., 2017</ref>) leverage neu- ral models to select words and sentences from the original text. With the emergence of sequence- to-sequence models <ref type="bibr" target="#b30">(Sutskever et al., 2014</ref>) and large-scale datasets like CNN/Daily Mail <ref type="bibr" target="#b13">(Hermann et al., 2015;</ref><ref type="bibr" target="#b21">Nallapati et al., 2016) and</ref><ref type="bibr">NYT (Paulus et al., 2018)</ref>, abstractive summariza- tion of longer text have become a more feasi- ble and popular task. Several recent approaches have been proposed to tackle abstractive summa- rization problem, where <ref type="bibr" target="#b21">Nallapati et al. (2016)</ref> exploits hierarchical encoders, <ref type="bibr" target="#b28">See et al. (2017)</ref> proposes pointer-generator network and cover- age mechanism to overcome OOV and repetition problems, <ref type="bibr" target="#b31">Tan et al. (2017)</ref> introduces a graph- based attention mechanism and hierarchical beam search strategy, and ( <ref type="bibr" target="#b24">Paulus et al., 2018</ref>) pro- poses to optimize for ROUGE metric via rein- forcement learning. Although impressive progress has been achieved for sentence-level summariza- tion, attempts on abstractive document summa- rization task are still in early stages where the sim- ple LEAD-3 baseline performance is only very re- cently matched <ref type="bibr" target="#b24">(Paulus et al., 2018)</ref>. Neural Machine Translation. With the re- cent success of encoder-decoder architectures <ref type="bibr" target="#b30">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015)</ref>, neural machine translation systems has gained a a lot of attention both from academia ( <ref type="bibr" target="#b7">Cho et al., 2014;</ref><ref type="bibr" target="#b18">Luong et al., 2015;</ref><ref type="bibr" target="#b17">Luong and Manning, 2016</ref>) and industry ( <ref type="bibr" target="#b33">Vaswani et al., 2017;</ref><ref type="bibr" target="#b0">Ahmed et al., 2018</ref>) over statistical machine translation, which has been the domi- nating translation paradigm for years. Most of these works has focused more on enhancing the architecture design aspect to tackle with various challenges such as different attention mechanisms ( <ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b18">Luong et al., 2015)</ref>, a character-level decoder <ref type="bibr" target="#b9">(Chung et al., 2016</ref>), a translation coverage mechanism ( <ref type="bibr" target="#b32">Tu et al., 2016)</ref>, and so on. However, only very recently, a few works ( <ref type="bibr" target="#b25">Ranzato et al., 2016;</ref><ref type="bibr" target="#b22">Norouzi et al., 2016;</ref><ref type="bibr" target="#b29">Shen et al., 2016;</ref><ref type="bibr">Bahdanau et al., 2017;</ref><ref type="bibr" target="#b39">Zhukov and Kretov, 2017;</ref><ref type="bibr" target="#b5">Casas et al., 2018)</ref> have investigated sequence-level op- timization by training to maximize BLEU score. Neural Sequence Generation with RL. Most neural sequence generation models are trained with the objective of maximizing the probability of the next correct word. However, this results in a major discrepancy between training and test settings of these models because they are trained with cross-entropy loss at word-level, but evalu- ated based on sequence-level discrete metrics such as ROUGE ( <ref type="bibr" target="#b16">Lin and Och, 2004</ref>) or BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>). On the other hand, directly op- timizing for such evaluation metrics is hard due to non-differentiable nature of the exact objec- tive <ref type="bibr" target="#b26">(Rosti et al., 2011</ref>). Recent works ( <ref type="bibr" target="#b25">Ranzato et al., 2016;</ref><ref type="bibr">Bahdanau et al., 2017;</ref><ref type="bibr" target="#b24">Paulus et al., 2018)</ref> address the difficulty of differentiating with respect to rewards based on such discrete metrics using variants of reinforce- ment learning. These methods essentially pro- pose to mitigate the problem by optimizing the reward weighted log-likelihood of the hypothesis sequences generated by the model distribution. In this paper, we propose an alternative solution to tackle this problem by introducing a differentiable approximation to exact LCS metric that can be di- rectly optimized by standard gradient-based meth- ods without RL, while still addressing the expo- sure bias problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work we explored an alternative approach for training text generation models with sequence- level optimization to combat wrong objective and exposure bias problems. We introduced a new ob- jective function based on a continuous approxima- tion of LCS metric that measures sequence-level structure similarity between sentences. We ap- plied our proposed approach to CNN/Daily Mail dataset for long document summarization and WMT 2014 English-to-German machine transla- tion task. By extending the objectives of strong neural baseline models with our proposed objec- tive, we empirically demonstrated its effective- ness on these two tasks. Our proposed approach suggests a promising alternative to policy-gradient methods to side step the difficulty of differentiat- ing w.r.t reward function while directly optimizing for sequence-level discrete metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Eq. 1 with the prob- ability p (y i ) j as in Eq. 2. Interpreting the proba- bility p (y i ) j as a continuous relaxation of discrete comparison operator 1 [y i = z j ], s i,j establishes a natural continuous approximation to r i,j . Similar to LCS, after iteratively filling up s i,j matrix, we define CALCS(y, z) = s m,k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>=</head><label></label><figDesc>p gen P vocab (w) + (1 − p gen )P copy (w) and P copy (w) = {i:w i =w} a t i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>See et al., 2017) to obtain non-anonymized ver- sion of the data that contains 287,226 training pairs, 13,368 validation pairs, and 11,490 test pairs of news articles (781 tokens on average) and their corresponding ground-truth summaries (56 tokens on average). We refer the reader to (See et al., 2017) for further details of the difference of their version from (Nallapati et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>ROUGE F1 results on CNN/Daily Mail summa-

rization dataset. Our reimplementation of POINTGEN* cor-
responds to w/o coverage (See et al., 2017). ** sign near 
ROUGE-L results reported for our models indicates a differ-
ence in our ROUGE-L evaluation as explained below. 

</table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Lemma 1</head><p>[Monotonicity] The following two inequalities s i,j ≤ s i,j+1 ≤ s i,j + 1 s i,j ≤ s i+1,j ≤ s i,j + 1 hold for all 0 ≤ i &lt; m and 0 ≤ j &lt; k.</p><p>Proof. We will prove this lemma by induction on i + j.</p><p>Base Case: i + j = 0. In this case, we have</p><p>hold.</p><p>We will now prove that the inequalities of in- ductive hypothesis hold for i + j = l + 1.</p><p>We will start by showing s i,j+1 ≥ s i,j . By defi- nition, we have</p><p>≥ p</p><p>where inequality 24 follows from the definition of max operator, and inequality 25 follows from in- duction assumption 20 because (i − 1) + j = l. Hence, final inequality 26 establishes the proof of s i,j+1 ≥ s i,j . Now, we will show that s i,j+1 ≤ s i,j + 1 holds.</p><p>Again by definition, we have</p><p>where inequalities 30 and 32 follow from inequal- ities 20 and 21 of inductive step as (i − 1) + j = l.</p><p>Note that 26 and 32 completes the proof of s i,j ≤ s i,j+1 ≤ s i,j + 1 for i + j = l + 1. Fol- lowing similar arguments, one can easily establish the correctness of s i,j ≤ s i+1,j ≤ s i,j + 1 for i+j = l+1, which completes the proof of Lemma by induction.</p><p>B Definition of Rouge-L Definition 2. ROUGE-L is a discrete similarity metric that takes into account sentence level struc- ture similarity by identifying longest co-occurring in-sequence n-grams automatically via longest common subsequence measure. Formally, given two sequences y and z of tokens, we define ROUGE-L(y, z) as the harmonic mean of preci- sion LCS(y,z) k and recall LCS(y,z) m based on LCS measure, where k = |z| and m = |y|.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<pubPlace>Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Actor-Critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A differentiable bleu loss. analysis and first results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Costa-Juss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A R</forename><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the 6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hedge trimmer: A parse-and-trim approach to headline generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 03 on Text Summarization Workshop</title>
		<meeting>the HLT-NAACL 03 on Text Summarization Workshop</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning-based single-document summarization with compression and anaphoricity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hybrid Word-Character approach to open vocabulary neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Expected bleu training for graphs: Bbn system description for wmt11 system combination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antti-Veikko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coveragebased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shazeer</forename><surname>Noam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norouzim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Googles neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fur</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differentiable lower bound for expected BLEU score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kretov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Conversational AI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
