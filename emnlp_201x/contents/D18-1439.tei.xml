<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Keyphrase Generation with Correlation Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Science and Technology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
							<email>zhaoyan@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<title level="a" type="main">Keyphrase Generation with Correlation Constraints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4057" to="4066"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4057 ♥ The author is supported by AdeptMind Scholarship</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we study automatic keyphrase generation. Although conventional approaches to this task show promising results, they neglect correlation among keyphrases, resulting in duplication and coverage issues. To solve these problems, we propose a new sequence-to-sequence architecture for keyphrase generation named CorrRNN, which captures correlation among multiple keyphrases in two ways. First, we employ a coverage vector to indicate whether the word in the source document has been summarized by previous phrases to improve the coverage for keyphrases. Second, preceding phrases are taken into account to eliminate duplicate phrases and improve result coherence. Experiment results show that our model significantly outperforms the state-of-the-art method on benchmark datasets in terms of both accuracy and diversity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A keyphrase is a piece of text that is able to sum- marize a long document, organize contents and highlight important concepts, like "virtual orga- nizations" in <ref type="table">Table 1</ref>. It provides readers with a rough understanding of a document without going through its content, and has many potential appli- cations, such as information retrieval, text summa- rization and document classification.</p><p>Keyphrase can be categorized into present keyphrase which appears in a source document, and absent keyphrase that does not appear in the document. Conventional approaches extract im- portant text spans as candidate phrases and rank them as keyphrases <ref type="bibr" target="#b10">(Hulth, 2003;</ref><ref type="bibr" target="#b19">Medelyan et al., 2008;</ref><ref type="bibr" target="#b15">Liu et al., 2011;</ref><ref type="bibr" target="#b32">Wu et al., 2015;</ref><ref type="bibr" target="#b30">Wang et al., 2016)</ref>, that show promising results on the present keyphrases but cannot handle absent keyphrases. * Corresponding Author Title: Resolving conflict and inconsistency in norm regulated virtual orga- nizations. Abstract: Norm governed virtual organizations define, govern and fa- cilitate coordinated resource sharing and problem solving in societies of agents. With an explicit account of norms, openness in virtual organiza- tions can be achieved new components, designed by various parties, can be seamlessly accommodated. We focus on virtual organizations realised as multi agent systems ... Ground truth: virtual organizations; multi agent systems; agent; norm conflict; conflict prohibition; norm inconsistency; ... Predicted keyphrases: virtual organizations; multi agent systems; arti- ficial intelligence; inter agent; multi agent; action delegation; software agents; resource sharing; grid services; agent systems; <ref type="table">Table 1</ref>: The example shows the duplication and coverage issues of state-of-the-art model. The phrases in red are dupli- cate, and the underlined parts in the source document are not covered by the predicted results, while they are summarized by "norm conflict" and "norm inconsistency" in the golden list.</p><p>To predict absent keyphrases, generative meth- ods have been proposed by . The approach employs a sequence-to-sequence (Seq2Seq) framework <ref type="bibr" target="#b25">(Sutskever et al., 2014</ref>) with a copy mechanism ( <ref type="bibr" target="#b7">Gu et al., 2016</ref>) to encourage rare word generation, in which the encoder com- presses the text into a dense vector and the de- coder generates a phrase with a Recurrent Neural Network (RNN) language model, achieving state- of-the-art performance. Since a document corre- sponds to multiple keyphrases, the approach di- vides it into multiple document-keyphrase pairs as training instances. This approach, however, neglects the correlation among target keyphrases since it does not model the one-to-many rela- tionship between the document and keyphrases. Therefore, keyphrase prediction only depends on the source document, and ignores the keyphrases which have been generated. As a consequence, the generated keyphrases suffer from duplication is- sue and coverage issue. A duplication issue is de- fined as at least two phrases expressing the same meaning, hindering readers from obtaining more information from keyphrases. For example, three keyphrases have an identical meaning in <ref type="table">Table 1,</ref> including "multi agent systems", "multi agent" and "agent systems". A coverage issue means some key points in the document are not covered by the keyphrases, such as "norm conflict" and "norm inconsistency" in <ref type="table">Table 1</ref>.</p><p>To mitigate such issues, we mimic human be- havior in terms of how to assign keyphrases for an arbitrary document. Given a document in <ref type="table">Table 1</ref>, an annotator will read it and generate keyphrases according to his understanding of the content, like "virtual organizations", "multi agent systems". After that, instead of generating du- plicate phrases like "agent systems" and "multi agent", the annotator will review the document and preceding keyphrases, then generate a phrase like "norm conflict" to cover topics that have not been summarized by previous phrases. The itera- tion does not stop until all of a document's topics are covered by keyphrases.</p><p>We propose a new sequence-to-sequence archi- tecture CorrRNN, capable of capturing correla- tion among keyphrases. Notably, correlation con- straints in this paper are defined as keyphrases that should cover all topics in the source docu- ment and differ from each other. Specifically, we employ a coverage mechanism ( <ref type="bibr" target="#b27">Tu et al., 2016)</ref> to memorize which parts in the source document have been covered by previous phrases. In this way, the document coverage is modeled explicitly, enabling the generated keyphrases to cover more topics. Furthermore, we propose a review mech- anism that considers the previous keyphrases in the generation process, in order to avoid the rep- etition in the final results. Concretely, the review mechanism explicitly models the correlation be- tween the keyphrases that have been generated and the keyphrase that is going to be generated with a novel architecture. It extends the existing Seq2Seq model and captures the one-to-many relationship in keyphrase generation. Augmented with the cov- erage mechanism and the review mechanism, Cor- rRNN does not only inherit the advantages of the Seq2Seq model, but also improves the coverage and diversity in the generation process. We test our model on three benchmark datasets. The results show that our model outperforms state- of-the art methods by a large margin, demonstrat- ing the effectiveness of the correlation constraints. In addition, our model is better than heuristic rules on improving diversity, since it instills the correla- tion knowledge to the model in an end-to-end fash- ion.</p><p>Our contributions in this paper are three-fold: (1) the proposal of modeling the one-to-many cor- relation for keyphrase generation, (2) the proposal of a new architecture CorrRNN for keyphrase gen- eration, and (3) empirical verification of the effec- tiveness of CorrRNN on public datasets.</p><p>In the remainder of this paper, we will first re- view the related work in Section 2, then we elab- orate on the proposed model in Section 3. After that, we list the experiment settings in Section 4, results and discussion follow in Section 5. Finally, the conclusion and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>How to assign keyphrases to a long document is a fundamental task, that has been studied inten- sively in previous works. Existing methods can be categorized into two groups: extraction based and generation based methods.</p><p>The former group extracts important keyphrases in a document which consists of two phases. The first phase is to construct a set of phrase candidates with heuristic methods, such as extracting impor- tant n-grams <ref type="bibr" target="#b10">(Hulth, 2003;</ref><ref type="bibr" target="#b19">Medelyan et al., 2008;</ref><ref type="bibr" target="#b10">Hulth, 2003;</ref><ref type="bibr" target="#b24">Shang et al., 2017)</ref> and selecting text chunks with certain postags ( <ref type="bibr" target="#b15">Liu et al., 2011;</ref><ref type="bibr" target="#b30">Wang et al., 2016;</ref><ref type="bibr" target="#b13">Le et al., 2016;</ref><ref type="bibr" target="#b14">Liu et al., 2015)</ref>. The second phase is to rank the candidates with machine learning methods. Specifically, some re- searchers ( <ref type="bibr" target="#b10">Hulth, 2003;</ref><ref type="bibr" target="#b18">Medelyan et al., 2009;</ref><ref type="bibr" target="#b5">Gollapalli and Caragea, 2014</ref>) formulate the keyphrase extraction as a supervised classification problem, while oth- ers apply unsupervised approaches ( <ref type="bibr" target="#b21">Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b6">Grineva et al., 2009;</ref><ref type="bibr" target="#b17">Liu et al., 2009</ref><ref type="bibr" target="#b16">Liu et al., , 2010</ref><ref type="bibr" target="#b33">Zhang et al., 2013;</ref><ref type="bibr" target="#b1">Bougouin et al., 2013</ref><ref type="bibr" target="#b2">Bougouin et al., , 2016</ref> on this task. Besides, Tomokiyo and Hurst (2003) employ two statistical language models to measure the informativeness for phrases. <ref type="bibr" target="#b15">Liu et al. (2011)</ref> use a word alignment model to learn translation probabilities between the words in doc- uments and the words in keyphrases, which allevi- ates the problem of vocabulary gaps.</p><p>The latter group, generative methods, assigns keyphrases to a document with natural language generation techniques, and is capable of generat- ing absent keyphrases. Owing to the development of neural networks ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>),  apply an encoder-decoder framework <ref type="bibr" target="#b25">(Sutskever et al., 2014</ref>) with a copy mechanism ( <ref type="bibr" target="#b7">Gu et al., 2016)</ref> to this task, achieving state-of- the-art performance.</p><p>Our work is a generation based approach. The main difference of our model is that we consider the correlation among keyphrases. Our model proposes a new review mechanism to enhance keyphrase diversity, while employs a coverage mechanism that has proven effective for summa- rization ( <ref type="bibr" target="#b23">See et al., 2017</ref>) and machine translation ( <ref type="bibr" target="#b27">Tu et al., 2016</ref>) to guarantee keyphrase coverage. Some previous works on keyword extraction have already exploited the correlation problem with a re-rank strategy <ref type="bibr" target="#b8">(Habibi and Popescu-Belis, 2013;</ref><ref type="bibr" target="#b22">Ni et al., 2012</ref>). In contrast, we model the correla- tion in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KeyphraseGenerationwithCorrelation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formalization</head><p>Suppose that we have a data set</p><formula xml:id="formula_0">D = {x i , p i } N i=1</formula><p>, where x i is a source text,</p><formula xml:id="formula_1">p i = {p i,j } M i</formula><p>j=1 is the keyphrase set of x i , and N is the number of documents. Both the source text and target keyphrase are word sequences, donated as</p><formula xml:id="formula_2">x i = (x i 1 , x i 2 , ..., x i T ) and p i,j = (y i,j 1 , y i,j 2 , ..., y i,j L i ) respectively.</formula><p>T and L i are the length of word sequences of x i and p i,j . Prior work aims to maximize the probability of N i=1 M i j=1 P (p i,j |x i ), while our model considers keyphrase correlation to address coverage and du- plication issues by maximizing the probability of</p><formula xml:id="formula_3">N i=1 M i j=1 P (p i,j |x i , p i,l&lt;j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Seq2Seq Model with Copy Mechanism</head><p>A Seq2Seq model <ref type="bibr" target="#b25">(Sutskever et al., 2014</ref>) is employed as backbone in this paper. The en- coder converts the variable-length input sequence x = (x 1 , x 2 , ..., x T ) into a set of hidden represen- tation h = (h 1 , h 2 , ..., h T ) by iterating along time t with the following equation:</p><formula xml:id="formula_4">ht = f (xt, ht−1) (1)</formula><p>where f is a non-linear function. Then the context vector c is computed as a weighted sum of hidden representation set h through an attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), which next acts as the representation of the whole input x at time step t.</p><formula xml:id="formula_5">ct = T j=1 αtjhj (2)</formula><p>where α tj is a coefficient which measures the match degree between the inputs around position j and the output at position t. With the context vector c t , decoder generates variable-length word sequence step by step, the generative process which is known as a language model:</p><formula xml:id="formula_6">st = f (yt−1, st−1, ct) (3) pg(yt|y&lt;t, x) = g(yt−1, st, ct)<label>(4)</label></formula><p>where s t denotes the hidden state of the decoder at time t. y t is the predicted word from vocabulary based on the largest probability after g(.). Unfortunately, pure generative mode cannot generate any keyphrase (e.g. noun, entity) which contains out-of-vocabulary words. Thus we in- corporate a copy mechanism ( <ref type="bibr" target="#b7">Gu et al., 2016)</ref> into the encoder-decoder model to predict out-of- vocabulary words by selecting appropriate words from source text. After incorporation, the prob- ability of predicting a new word consists of two parts:</p><formula xml:id="formula_7">p(yt|y&lt;t, x) = pg(yt|y&lt;t, x) + pc(yt|y&lt;t, x) (5) pc(yt|y&lt;t, x) = 1 Z j:x j =y t e σ(h T j Wc)[y t−1 ;s t ;c t] , yt ∈ X (6)</formula><p>where p g and p c denote the probability of gen- erating and coping. X is the set of unique words in source sequence x, σ is a non-linear function. W c ∈ R is a learned parameter matrix. Z is the sum for normalization. For more details, please see (Gu et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Correlation</head><p>Keyphrases should cover more topics and dif- fer from each other, while previous work ( ) ignored this correlation among mul- tiple keyphrases, resulting in duplication and cov- erage issues. In this part, we focus on capturing the one-to-many correlation to alleviate above is- sues. On the one hand, we employ a coverage mechanism ( <ref type="bibr" target="#b27">Tu et al., 2016</ref>) that diversifies atten- tion distributions to improve the topic coverage of keyphrases. On the other hand, we propose a re- view mechanism which makes use of contextual information of previous phrases (already gener- ated) to avoid duplicate generation. For better dis- play of the proposed model, the overall framework is illustrated in <ref type="figure">Figure 1</ref> and the detailed process is described in Algorithm 1.</p><formula xml:id="formula_8">0 E c 0 h 1 h 0 s 2 h 3 h 4 h 5 h 6 h 1 E c 0 : p D 0 c 0 0 0 E a 1 E a 0 0+ E a 0+ E a 0 1 0+ + E E a a 0+ + E E +a + 0 D a 1 : p D 1 c 1 D a 0 1 s s 0 s 1 s 0 0 s 0 1 s 0 2 s 1 0 s 1 1 s 1 2 s C C C S S 2 E a</formula><p>Figure 1: The overall framework structure. Note that pi indicates a keyphrase (e.g. p0 ="neural network"), s i indicates the hidden state set of phrase pi, coverage vector C and target-side review context S update and transmit along the process of decoding multiple keyphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Coverage Mechanism</head><p>As is well known, multiple keyphrases usually correspond to multiple different positions of the source text (see <ref type="figure">Figure 1</ref>), the positions of words that have already been summarized should not be focused again since the attention mechanism au- tomatically focuses on important areas of source text. To overcome the coverage issue, we incorpo- rate a coverage mechanism ( <ref type="bibr" target="#b27">Tu et al., 2016)</ref> into our model, which diversifies the attention distri- butions of multiple keyphrases to make sure more important areas in the source document are at- tended and summarized into keyphrases.</p><p>Concretely, we maintain a coverage vector c t which is the sum of attention distributions over all previous decoder time steps. Intuitively, c t repre- sents the degree of coverage that those words in the source text have received from the attention mechanism so far.</p><formula xml:id="formula_9">c t = t−1 i=0 a i<label>(7)</label></formula><p>Note that c 0 is a zero vector since no word in source text has be covered. Later, the coverage vector c t is an extra input for the attention mechanism, then the source context set h is read and weight averaged into a contextual representation c E t by the attention mechanism with a coverage vector, with Eqn.(2) transforming into Eqn.(8) as follows:</p><formula xml:id="formula_10">c E t = T j=1 α tj h j ; α tj = exp(e E tj ) T k=1 exp(e E tk )</formula><p>;</p><formula xml:id="formula_11">e E tj = v T tanh(W h hj + Wsst−1 + wcc t j + battn) (8)</formula><p>where E is the encoder and w c is a learned pa- rameter with the same length as v.</p><p>With the coverage vector, the attention mecha- nism's decision for choosing where in source text to focus next is informed by a reminder of its previous decisions, which ensures that the atten- tion mechanism avoids repeatedly attending to the same locations in the source text more easily, thus generated phrases cover more topics in the source document.</p><p>Algorithm 1 Training procedure of the proposed model. for each Pi = (y i 1 , y i 2 , ..., y i T i ) ∈ (P1, ..., PM ) do 6: init s i 0 ; 7:</p><p>for t = 1; t &lt;= T i ; t + + do 8:</p><formula xml:id="formula_12">c iE t , a i t = attention(H, s i t−1 , C) ; 9: c iD t = attention(S, s i t−1 ) ; 10: s i t = ϕD(y i t−1 , s i t−1 , c iE t , c iD t ) ; 11: S = S ∪ s i t−1 ; 12: C = C + a i t ; 13:</formula><p>end for 14:</p><p>compute loss for Pi; 15:</p><p>end for 16:</p><p>compute gradient and update; 17: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Review Mechanism</head><p>Considering human behavior on assigning keyphrases that review previous phrases to avoid duplicate assignment, we construct a target side review context set which contains contextual in- formation of generated phases. The target context with an attention mechanism can make use of con- textual information of generated phrases to help predict the next phrase, which we call the review mechanism.</p><p>Like source context c E t described above, on the target side, the target context is defined as s t = {s 1 , s 2 , ..., s t−1 }, which is the collection of hidden states before time step t. When decoding the word at t-th step, s t is used to inform an extra contextual representation, thus target side attentive contexts are integrated into c D t :</p><formula xml:id="formula_13">c D t = t−1 j=1 β tj s j ; β tj = exp(e D tj ) t−1 k=1 exp(e D tk )</formula><p>;</p><formula xml:id="formula_14">e D tj = v T tanh(W h sj + Wsst−1)<label>(9)</label></formula><p>Afterwards, c D t is provided as an extra input to derive the hidden state s t and later the probability distribution for choosing t-th word . The target context gets updated consequently as s t+1 = s t ∪ {s t } in the decoding progress.</p><formula xml:id="formula_15">st = f (yt−1, st−1, c E t , c D t )<label>(10)</label></formula><formula xml:id="formula_16">pg(yt|y&lt;t, x) = g(yt−1, st, c E t , c D t )<label>(11)</label></formula><formula xml:id="formula_17">pc(yt|y&lt;t, x) = 1 Z j:x j =y t e σ(h T j Wc)[y t−1 ;s t ;c E t ;c D t ] , yt ∈ X<label>(12)</label></formula><p>Eqn.(10), Eqn. <ref type="formula" target="#formula_16">(11)</ref> and Eqn. <ref type="formula" target="#formula_17">(12)</ref> are trans- formed from Eqn.(3), Eqn.(4) and Eqn.(6) respec- tively. More mathematical details are displayed below to make Eqn.(10) more clear:</p><formula xml:id="formula_18">rt = σ(Wryt−1 + Urst−1 + C E r c E t + C D r c D t ); zt = σ(Wzyt−1 + Uzst−1 + C E z c E t + C D z c D t ); st = tanh(W yt−1 + U [rt • st−1] + C E c E t + C D c D t ); st = (1 − zt) • st−1 + zt • st<label>(13)</label></formula><p>where E, D indicate the encoder and decoder, W , U , C are learned parameters of the model, σ is a sigmoid function, • indicates an element-wise product.</p><p>With the contextual information of previous phrases, review mechanism ensures next predicted phrase less duplication and topic coherence. So far, we transmit and update the coverage vector and review context along the multi-target phrase decoding process to improve the coverage and di- versity of keyphrases. We denote our model with coverage only and review only as CorrRNN C and CorrRNN R , and empirically compare them in ex- periments. The objectives are to minimize the neg- ative log-likelihood of the target words, given a data sample with source text x and corresponding phrases set p = {p i } M i=0 , loss is calculated as fol- lows:</p><formula xml:id="formula_19">loss = − 1 M M i=0 T i t=0 log(p(y i t |y i &lt;t , x, pj,j&lt;i))<label>(14)</label></formula><p>4 Experiment Settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>In the preprocessing phase, we follow (  to preprocess the text with tokeniza- tion, lowercasing, and digit replacement to ensure fairness. Each article consists of one source text and corresponding multiple keyphrases, and the source text is the concatenation of its title and ab- stract. We set the max number of target phrases to 10 for an article in consideration of the de- vice memory, thus those with more than 10 tar- get phrases are cut into multiple articles. Finally, we have 558830 articles (text-keyphrases pair) for training.</p><p>In the training phase, we choose a bidirectional GRU for the encoder and another forward GRU for the decoder. The top 50000 frequent words are chosen as the vocabulary, the dimension of word embeddings is set to 150, the value of embedding is randomly initialized with uniform distribution in [-0.1, 0.1], and the dimension of the hidden lay- ers is set to 300. Adam is adopted to optimize the model with initial learning rate=10 −4 , gradient clipping=0.1 and dropout rate=0.5. The training is stopped once the loss on the validation set stops dropping for several iterations.</p><p>In the generation phase, we use beam search to generate multiple phrases. The beam depth is set to 6 and the beam size is set to 200. Source code will be released at https://github.com/ nanfeng1101/s2s-kg.   <ref type="bibr">Semeval-2010</ref><ref type="bibr" target="#b11">(Kim et al., 2010</ref>): 288 arti- cles are collected from the ACM Digital Li- brary. 100 of them are used for test data and the rest are added to the training set. -Krapivin ( <ref type="bibr" target="#b12">Krapivin et al., 2008)</ref>: This dataset contains 2304 papers. The first 400 papers in alphabetical order are used for eval- uation and the rest are added to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Models</head><p>We compare our model with extractive and generative approaches. The extractive baselines include Tf-idf, TextRank (Mihalcea and <ref type="table">Tarau</ref> To demonstrate the effectiveness of end-to-end learning, we compare CorrRNN to CopyRNN with post-processing. In post-precessing, we only keep the first appearence of keyphrase in duplicate predictions, duplication means that a phrase is a substring of another. The baseline can be seen as heuristic rules for improving the diversity, denoted as CopyRNN F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>For a fair comparison, we evaluate the exper- iment results on present keyphrases and absent keyphrases separately, because extractive meth- ods cannot generate absent keyphrases. Follow- ing , we employ F1-measure for present keyphrases and recall for absent keyphrases. Here, we use F1@K and R@K to denote the F1 and recall score in the top K keyphrases. Note that we use Porter Stemmer for preprocessing to determine whether the two keyphrases are identical.</p><p>Furthermore, α-NDCG, which is widely used to measure the diversity of keyphrase generation (Habibi and Popescu-Belis, 2013) and information retrieval ( <ref type="bibr" target="#b3">Clarke et al., 2008)</ref>, is adopted to eval- uate the diversity of the generative methods, de- noted as N@K. α is a trade-off between relevance 1 https://github.com/adrien-bougouin/KeyBench and diversity in α-NDCG, which is set to equal weights of 0.5 according to <ref type="bibr">Habibi and PopescuBelis (2013)</ref>. The higher α-NDCG is, the more diverse the results are. We re-implement Copy- RNN with the source code 2 provided by the au- thors in order to evaluate it on the α-NDCG met- ric.</p><formula xml:id="formula_20">α-N DCG[k] = DCG[k] DCG [k] ; DCG[k] = k j=1 G[j]/log 2 (j + 1); G[k] = m i=1 J(d k , i)(1 − α) r i,k−1</formula><p>where α is a parameter, m denotes the number of target phrases, k denotes the number of pre- dicted phrases. J(d k , i) = 0 or 1, which indi- cates whether the k-th predicted phrase is relevant to the i-th target phrase, and r i,k−1 indicates how many predicted phrases are relevant to the i-th tar- get phrase before the k-th predicted phrase. Note that relevance here is defined as whether the word set of a keyphrase is a subset of another keyphrase (e.g. "multi agent" vs "multi agent system").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Present Phrase Prediction</head><p>Present phrase prediction is also known as keyphrase extraction in prior studies. We evalu- ate how well our model performs on this common task. The results are shown in <ref type="table" target="#tab_1">Tables 2 and 3</ref>, which list the performance of the top 5 and top 10 results.</p><p>In terms of F1-measure, CorrRNN and Copy- RNN outperform other baselines by a large mar- gin, indicating the effectiveness of RNN with a copy mechanism. As we consider the correla- tion among multiple phrases, the overall results of CorrRNN are better than CopyRNN signifi- cantly (t−test with p &lt; 0.01). This is mainly be- cause CorrRNN alleviates the duplication and cov- erage issues in existing methods, with more cor- rect phrases boosted in the top 10 results. Heuritic baseline CopyRNN F is even worse than Copy- RNN, indicating that the heuristic rules may hurt the performance of generative approaches. It also proven that it is a better way to model the correla- tion among keyphrases in an end-to-end fashion.</p><p>Regarding α-NDCG, CorrRNN and its variants surpass other methods, demonstrating that incor- porating correlation constraints can improve both relevance and diversity. As the heuristic rules in- fluence the relevance of CopyRNN, CopyRNN F performs a little better than CopyRNN on the α-NDCG.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Absent Phrase Prediction</head><p>We evaluate the performance of generative methods within the recall of the top 10 results, which is shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generalization Ability</head><p>As described above, CorrRNN performed well on scientific publications. In this part, we con- struct our experiments on news domain to see if the proposed model works when transferring to a different domain with unfamiliar texts. We adopted the popular news article dataset: DUC- 2001 ( <ref type="bibr" target="#b29">Wan and Xiao, 2008)</ref> for our experiments. The dataset contains 308 news articles with 2488 manually assigned keyphrases, and each article consists of 740 words on average, which is com- pletely different from the datasets we used above (see <ref type="table" target="#tab_6">Table 5</ref>).  We directly applied CorrRNN, which is trained on scientific publications, on predicting phrases for news articles without any adaptive adjustment. Experiment results from <ref type="bibr" target="#b9">(Hasan and Ng, 2010)</ref>,  and our experiments are shown in <ref type="table" target="#tab_8">Table 6</ref>, from which we can see that the pro- posed model CorrRNN can extract a consider- able portion of keyphrases correctly from unfa- miliar texts. It outperforms TextRank ( <ref type="bibr" target="#b21">Mihalcea and Tarau, 2004</ref> When transferring to news domain, the vocab- ulary changes a lot, more unknown words oc- cur, and the correlation also may not applicable, the model can still capture positional and syntac- tic features within the text to predict phrases de- spite the different text type and length. The ex- periment verifies the generalization ability of our model, thus we have good reasons to believe that our model has a great potential to be generalized to more domains after sufficient training.  Source text: a a support t vector method for optimizing average precision . suppo ort ve n . . machine r method m ecto ve or learning is commonly used to improve ranked a achin ne e e n in arn lea d retrieval systems . due to computational difficulties , few l ems syste s w learning due to ms . ng techniques have been developed to directly optimize for mean average precision ( map ) , despite its widespread use in evaluating such systems . existing approaches optimizing map either do not find a globally optimal solution , or are computationally expensive . in contrast , we present a general ationally ral svm exp p nsiv nally y vm m learning ve . in pensiv ng g algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of map .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source text: a support t vector method for optimizing</head><p>Source average xt: a su e tex ge e precision . port ve supp n . . <ref type="table">machine   r method m  ecto  ve or  learning is commonly used  to improve ranked   a achin ne e  n  in  arn  le ea  d retrieval systems . due to  computational difficulties , few   l  ems  syste s  w learning   due to  s .  g techniques  have been developed to directly optimize for   hniques  or mean  have  average   n develo  been  ge e e e e precision (   ed to  elop  n ( ( map )</ref> , despite its widespread use in evaluating such systems . existing approaches optimizing ng such h ng map either do not find a globally optimal solution , or are computationally expensive . in contrast , we present a general ationally ral svm exp p nsiv ally y vm learning ve . in ensiv ng g algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of oba ally o opt of f f f f f f map .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source text: a support t vector method for optimizing Source average</head><p>xt: a supp ce tex ge e e e e e e precision . po ort v supp n . . . machine r ethod m ecto e or learning is commonly used to improve ranked a achin ne e e n in arn lea d retrieval systems . due to computational difficulties , few al ems syste s w learning due to ms . ng techniques have been developed to directly optimize for hniques or mean have average n develo been ge e precision ( ed to d lope n ( ( map ) , despite its widespread use in evaluating such systems . existing approaches optimizing ting uat su uc ng map either do not find a globally optimal solution , or are computationally expensive . in contrast , we present a general ationally ral l svm exp p nsiv nally y vm m learning ve . in pensiv ng g algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of oba ally o opti of f f f f f f f map .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source text: a support t vector method for optimizing</head><p>Source average xt: a suppo e tex ge precision <ref type="table">.   o ort ecto  ve o  ppo  . machine   ethod  m  or  fo  or m  learning is commonly  used to improve ranked   hin  ac n ne arnin  e  le  ed retrieval systems . due to  computational difficulties , few   val  ev  ystem  sy  w learning   . due to  ms  em  ng techniques  have been developed to directly optimize for   chniques  or mean  have  average   develo  been  ge e precision (   ed to d  lope  n ( ( map )</ref>   <ref type="figure">Figure 2</ref>: Visualization, deeper shading denotes higher value. Note that yellow shading and green shading indicate coverage vector and review attention respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Model Ablation</head><p>We investigate the effect of coverage mecha- nism and review mechanism in our model with CorrRNN C and CorrRNN R respectively, shown in <ref type="table" target="#tab_1">Tables 2, 3</ref> and 4. It is clear that both the cover- age mechanism and review mechanism are helpful for improving the coverage and diversity of pre- dicted phrases. We note the inconsistency of ab- late models in our experiments. First, no ablate model achieves the best performance on all of the test datasets, the full model CorrRNN gets better perfermance on present phrase prediction, while CorrRNN C seems better than the others in ab- sent phrase prediction. As the present phrases are the majority, the full model CorrRNN can achieve best overall performance in actual use. Second, proposed models perform better on dataset NUS and SemEval than Kravipin. This may be due to the difference of assignment quality among test datasets, keyphrase assignments with higher cov- erage and higher diversity benefiting more from our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Visualization</head><p>In <ref type="figure">Figure 2</ref>, we visualize the coverage vector and review attention with an example to further clarify how our model works. Due to space limita- tion, we only visualize top5 phrases in the exam- ple, they are already enough to support our anal- ysis. For coverage vector, we can see that source attention transfers along the changes of coverage vector. At the first, only relevant words of "ma- chine learning" are attended. After that, the cov- erage vector informs attention mechanism to at- tend other positions instead of repetitive atten- tion, that promotes the generation of later phrases like "average precision". After the last one be- ing generated, it is clear that coverage vector ba- sically covers all topics of the source document, including "machine learning", "mean average pre- cision", "svm" and "ranked retrieval systems". As for the review attention, it's clear that contextual information of all previous phrases are attended for generating the last phrase "ranked retrieval sys- tems", which verifies that the review mechanism helps to alleviate duplication and ensure coherence of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Comparison with Heuristic Rules</head><p>We design the baseline CopyRNN F with post- processing to explore whether heuristic rules can alleviate duplication and coverage issues. It is clear from Tables 2, 3 and 4 that the experi- ment results are negative. Compared to our model CorrRNN, heuristic rules can't address duplica- tion and coverage issues fundamentally. We offer two explanations for these observations. Firstly, heuristic rules can only handle those phrases which have already been generated in results, that shows no help for enabling phrases to cover more topics in source text. Secondly, although duplica- tion can be reduced by heuristic rules forcibly, the remaining phrases are not guaranteed to be cor- rect, thus it hurts the accuracy badly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Complexity Analysis</head><p>According to our observations during experi- ment phase, the <ref type="bibr">CopyRNN (Meng et al., 2017)</ref> model has 78835750 network parameters, while CorrRNN owns 94886750 parameters because of the incorporation of coverage mechanism (few) and review mechanism (most). How- ever, benefiting from the consideration of one- to-many relationship, and correlation constraints among keyphrases, our model CorrRNN not only achieves better performance but also converges faster than CopyRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Case Study</head><p>As shown in <ref type="table" target="#tab_10">Table 7</ref>, we compare the phrases generated by CorrRNN and CopyRNN on an ex- ample article. Compared to CopyRNN, CorrRNN generates one more correct present phrase "voip" Title: Deployment issues of a voip conferencing system in a virtual conferencing environment. Abstract: Real time services have been supported by and large on circuitswitched networks. Recent trends favour services ported on packet switched networks. For audio conferencing, we need to consider many issues scalability, quality of the conference application, floor control and load on the clients servers to name a few. In this paper, we describe an audio service framework designed to provide a virtual conferencing environment (vce). The system is designed to accommodate a large number of end users speaking at the same time and spread across the internet. The framework is based on conference servers DIGIT , which facilitate the audio handling, while we exploit the sip capabilities for signaling purposes. Client selection is based on a recent quantifier called loudness number that helps mimic a physical face to face conference. We deal with deployment issues of the proposed solution both in terms of scalability and interactivity, while explaining the techniques we use to reduce the traffic. We have implemented a conference server (cs) application on a campus wide network at our institute. Present Phrase: CopyRNN: deployment; virtual conferencing; real time; distributed systems; virtual conferencing environment; client server; conference server; distributed applications; audio conferencing; floor control; CorrRNN: voip; virtual conferencing; voip conferencing; audio conferencing; audio service; real time services; real time; distributed systems; conference server; virtual conferencing environment; Absent Phrase: CopyRNN: quality of service; distributed conferencing; virtual environments; internet conferencing; conference conferencing; load balancing; packet conferencing; real time systems; distributed computing; virtual server; CorrRNN: real time systems; real time voip; voip service; real time audio; wireless networks; conference conferencing; real time communications; packet conferencing; audio communication; quality of service; and one more correct absent phrase "real time au- dio" respectively, which covers two important top- ics, while CopyRNN loses these key points. More- over, four "conferencing (noun)" phrases are gen- erated by CopyRNN, including "distributed con- ferencing", "internet conferencing", "conference conferencing" and "packet conferencing", which hinders readers from obtaining more information, while CorrRNN only has two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose a new Seq2Seq ar- chitecture that models correlation among multiple keyphrases in an end-to-end fashion by incorpo- rating a coverage mechanism and a review mech- anism. Comprehensive empirical studies demon- strate that our model can alleviate duplication and coverage issues effectively and improve diversity and coverage for keyphrase generation. To the best of our knowledge, this is the first use of encoder- decoder model for keyphrase generation in an one- to-many way. Our future work will focus on two areas: investigation on multi-document keyphrase generation, and incorporation of structure or syn- tax information in keyphrase generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Require:</head><label></label><figDesc>The train corpus set D; The encoder ϕE; The de- coder ϕD; The attention function attention; 1: for each (X, (P1, ..., PM )) ∈ D do 2: compute source hidden states H = ϕE(X); 3: init target review context S = φ; 4: init coverage vector C = 0; 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Following Meng et al.</head><label></label><figDesc>(2017), we train our model on the KP20k dataset (Meng et al., 2017), which contains articles collected from various on- line digital libraries. The dataset has 527,830 arti- cles for training and 20000 articles for validation. We evaluate our model on three benchmark datasets which are widely adopted in previous works, with the details described below: -NUS (Vijayakumar et al., 2016): It contains 211 papers with author-assigned keyphrases, all of which we use as test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>, 2004 )</head><label>2004</label><figDesc>, SingleRank (Wan and Xiao, 2008), Ex- pandRank (Wan and Xiao, 2008), TopicRank 1 (Bougouin et al., 2013), KEA (Witten et al., 1999) and Maui (Medelyan et al., 2009). The genera- tive baselines include RNN and CopyRNN (Meng et al., 2017). In these baselines, the first five are unsupervised and the later four are supervised. We set up all the baselines following optimal settings in (Hasan and Ng, 2010), (Bougouin et al., 2013) and (Meng et al., 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>), KeyCluster (Liu et al., 2009), TopicRank (Bougouin et al., 2013) and CopyRNN (Meng et al., 2017), but it falls behind the other three baselines because the test domain changes. The model should perform better if it is trained on news dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : The F1 performance on present phrase prediction.</head><label>2</label><figDesc></figDesc><table>From top to bottom, baselines are listed as unsupervised and 
supervised. 

Model 
NUS 
N@5 N@10 

SemEval 
N@5 N@10 

Krapivin 
N@5 N@10 
CopyRNN 
0.740 0.713 
0.682 0.667 
0.622 0.625 
CopyRNN F 
0.743 0.720 
0.692 0.681 
0.635 0.657 
CorrRNN C 
0.781 0.747 
0.728 0.694 
0.649 0.642 
CorrRNN R 
0.770 0.742 
0.718 0.692 
0.669 0.657 
CorrRNN 
0.771 0.752 
0.752 0.720 
0.659 0.647 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The α-NDCG performance on present phrase pre-

diction. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>We can see that both CopyRNN and CorrRNN outperform RNN although the improvement is not as much as in present phrase prediction. It indicates that the copy mechanism is very helpful for predicting ab- sent phrases. We can also see that CopyRNN and CorrRNN are comparable in terms of recall, but CorrRNN is better on diversity, proving that our model can address the duplicate issue in keyphrase generation.</figDesc><table>Model 
NUS 
R@10 N@10 

SemEval 
R@10 N@10 

Krapivin 
R@10 N@10 
RNN 
0.050 N/A 
0.041 N/A 
0.095 N/A 
CopyRNN 
0.058 0.213 
0.043 0.228 
0.113 0.162 
CopyRNN F 
0.057 0.216 
0.043 0.233 
0.112 0.164 
CorrRNN C 
0.064 0.215 
0.041 0.231 
0.121 0.168 
CorrRNN R 
0.054 0.223 
0.041 0.250 
0.103 0.163 
CorrRNN 
0.059 0.229 
0.041 0.243 
0.108 0.166 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The recall and α-NDCG performance on absent 

phrase prediction. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>The average text length of test datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance on DUC-2001. CopyRNN and Cor-

rRNN are supervised, and they are trained on scientific pub-
lications but evaluated on news. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>, despite its widespread use in evaluating such systems . existing approaches optimizing</figDesc><table>ting 
uat 
su uc 
ng map either do not find a globally optimal 
solution , or are computationally expensive . in 
contrast , we present a general 

ationally 
ral l svm 

exp p nsi 
nally y 
vm m learning 

ve . in 
pensiv 
ng g g g algorithm 
that efficiently finds a globally optimal solution to a 
straightforward relaxation of 

oba ally o opti 
of f f f f f f f map . 

Source text: a support t vector method for optimizing 
Source 
average 

xt: a supp 
e tex 
ge e e precision . 

po ort v 
supp 
machine 

r method m 
ecto 
ve or 
learning is commonly used 
to improve 

cision n . . m m 
e ranked 

in ne e e 
n ar 
lea 
a ach 
d retrieval 

g is mm 
com 
in 
rn 
al systems . due to 
computational difficulties , few 

al 
ems 
syste s 
w learning 

due to 
ms . 
ng techniques 
have been developed to directly optimize for 

chniques 
or mean 
have 
average 

n develo 
been 
ge e precision ( 

ed to d 
lope 
n ( map ) , despite its widespread use 
in evaluating such 

map m ) , desp 
h systems . existing approaches 
optimizing 

ting 
uat 
su uc 
ng map either do not find a globally optimal 
solution , or are computationally expensive . in 
contrast , we present a general 

ationally 
ral l svm 

exp p nsiv 
nally y 
vm learning 

ve . in 
pensiv 
ng g algorithm 
that efficiently finds a globally optimal solution to a 
straightforward relaxation of 

oba ally o opt 
of f f f f f f f f f f map . 

1. 1. . machine ne e learning 
2. 2. . average ge e precision 
3. 
3. . support ort t t t t t t t vector machines 
4. 
4. . svm 
5. ranked retrieval systems 
Keyphrases: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Top10 phrases generated by CopyRNN and CorrRNN. Phrases in bold are correct, and phrases underlined are dupli- cate.</figDesc><table></table></figure>

			<note place="foot" n="2"> https://github.com/memray/seq2seq-keyphrase</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topicrank: Graph-based topic ranking for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bougouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Daille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keyphrase annotation with graph co-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bougouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Daille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 26th COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2945" to="2955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Buttcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackinnon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>SIGIR</publisher>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain-specific keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig G Nevill-Manning</forename><surname>Gutwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting keyphrases from research papers using citation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Gollapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1629" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting key terms from noisy and multitheme documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Grineva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Grinev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lizorkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diverse keyword extraction from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Habibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="651" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 23rd COLING</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2003</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olena</forename><surname>Su Nam Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th SemEval</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Large dataset for keyphrases extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikalai</forename><surname>Krapivin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Autayeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Marchese</surname></persName>
		</author>
		<idno>DISI-09-055</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised keyphrase extraction: Introducing new kinds of words to keyphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tho Thi Ngoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Le</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Shimazu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="665" to="671" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining quality phrases from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction by bridging vocabulary gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
	<note>15th CoNLL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction via topic decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
	<note>EMNLP 2010</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering to find exemplar terms for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>EMNLP 2009</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human-competitive tagging using automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2009</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topic indexing with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI WikiAI workshop</title>
		<meeting>the AAAI WikiAI workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep keyphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="404" to="411" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extracting keyphrase set with high diversity and coverage using structural svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingtian</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th APWeb</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="122" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1702.04457</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A language model approach to keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Tomokiyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2003</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno>abs/1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Single document keyphrase extraction using neighborhood knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ptr: Phrase-based topical ranking for automatic keyphrase extraction in scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="120" to="128" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Kea: Practical automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Ian H Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig G Nevill-Manning</forename><surname>Gutwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining query subtopics from questions in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="339" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wordtopic-multirank : A new method for automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lian&amp;apos;en</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
