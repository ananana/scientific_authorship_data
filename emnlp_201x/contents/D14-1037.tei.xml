<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph-based Approach for Contextual Text Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">¸</forename><surname>A˘ Gıl Sönmez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bogazici University Bebek</orgName>
								<address>
									<postCode>34342</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan¨ozgür</forename><surname>Arzucan¨</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bogazici University Bebek</orgName>
								<address>
									<postCode>34342</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan¨ozg</forename><forename type="middle">Arzucan¨ozg¨</forename><surname>Arzucan¨ozgür</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bogazici University Bebek</orgName>
								<address>
									<postCode>34342</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Graph-based Approach for Contextual Text Normalization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="313" to="324"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools. Text normalization, which corresponds to restoring the non-standard words to their canonical forms, provides a solution to this challenge. We introduce an unsupervised text normalization approach that utilizes not only lexical, but also con-textual and grammatical features of social text. The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus. The graph encodes the relative positions of the words with respect to each other, as well as their part-of-speech tags. The lexical features are obtained by using the longest common sub-sequence ratio and edit distance measures to encode the surface similarity among words, and the double metaphone algorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text. Our results show that it achieves state-of-the-art F-score performance on standard datasets. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social text, which has been growing and evolving steadily, has its own lexical and grammatical fea- tures ( <ref type="bibr" target="#b5">Choudhury et al., 2007;</ref><ref type="bibr" target="#b9">Eisenstein, 2013)</ref>. lol meaning laughing out loud, xoxo meaning kiss- ing, 4u meaning for you are among the most com- monly used examples of this jargon. In addition, these informal expressions in social text usually take many different lexical forms when generated by different individuals <ref type="bibr" target="#b9">(Eisenstein, 2013)</ref>. The limited accuracies of the Speech-to-Text (STT) tools in mobile devices, which are increasingly be- ing used to post messages on social media plat- forms, along with the scarcity of attention of the users result in additional divergence of so- cial text from more standard text such as from the newswire domain. Tools such as spellchecker and slang dictionaries have been shown to be in- sufficient to cope with this challenge long time ago <ref type="bibr" target="#b25">(Sproat et al., 2001</ref>). In addition, most Nat- ural Language Processing (NLP) tools including named entity recognizers and dependency parsers generally perform poorly on social text ( <ref type="bibr" target="#b24">Ritter et al., 2010)</ref>.</p><p>Text normalization is a preprocessing step to restore non-standard words in text to their origi- nal (canonical) forms to make use in NLP applica- tions or more broadly to understand the digitized text better ( <ref type="bibr" target="#b12">Han and Baldwin, 2011</ref>). For exam- ple, talk 2 u later can be normalized as talk to you later or similarly enormoooos, enrmss and enour- mos can be normalized as enormous. Other exam- ples of text messages from Twitter and their corre- sponding normalized forms are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The nor- malization task restores the OOV words to their In Vocabulary (IV) forms. Social text is contin- uously evolving with new words and named en- tities that are not in the vocabularies of the sys- tems <ref type="bibr" target="#b14">(Hassan and Menezes, 2013</ref>). Therefore, not every OOV word (e.g. iPhone, WikiLeaks or tok-Hav guts to say wat u desire.. Dnt beat behind da bush!! And 1 mre thng no mre say y r people's man!! Have guts to say what you desire.. Don't beat behind the bush!! And one more thing no more say you are people's man!! There r sm songs u don't want 2 listen 2 yl walking cos when u start dancing ppl won't knw y.</p><p>There are some songs you don't want to listen to while walking because when you start dancing people won't know why. enizing) should be considered for normalization. The OOV tokens that should be considered for normalization are referred to as ill-formed words.</p><p>Ill-formed words can be normalized to different canonical words depending on the context of the text. For example, let's consider the two examples in <ref type="table" target="#tab_0">Table 1</ref>. "y" is normalized as "you" in the first one and as "why" in the second one.</p><p>In this paper, we propose a graph-based text normalization method that utilizes both contex- tual and grammatical features of social text. The contextual information of words is modeled by a word association graph that is created from a large social media text corpus. The graph repre- sents the relative positions of the words in the so- cial media text messages and their Part-of-Speech (POS) tags. The lexical similarity features among the words are modeled using the longest common subsequence ratio and edit distance that encode the surface similarity and the double metaphone algorithm that encodes the phonetic similarity. The proposed approach is unsupervised, which is an important advantage over supervised systems, given the continuously evolving language in the social media domain. The same OOV word may have different appropriate normalizations depend- ing on the context of the input text message. Re- cently proposed dictionary-based text normaliza- tion systems perform dictionary look-up and al- ways normalize the same OOV word to the same IV word regardless of the context of the input text ( <ref type="bibr" target="#b13">Han et al., 2012;</ref><ref type="bibr" target="#b14">Hassan and Menezes, 2013</ref>). On the other hand, the proposed approach does not only make use of the general context information in a large corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by <ref type="bibr" target="#b4">Brill and Moore (2000)</ref>. They proposed a novel noisy channel model for spell checking based on string to string edits. Their model depended on probabilistic mod- eling of sub-string transformations. <ref type="bibr" target="#b27">Toutanova and Moore (2002)</ref> improved this ap- proach by extending the error model with phonetic similarities over words. Their approach is based on learning rules to predict the pronunciation of a single letter in the word depending on the neigh- bouring letters in the word. <ref type="bibr" target="#b5">Choudhury et al. (2007)</ref> developed a super- vised Hidden Markov Model based approach for normalizing Short Message Service (SMS) texts. They proposed a word for word decoding ap- proach and used a dictionary based method to normalize commonly used abbreviations and non- standard usage (e.g. "howz" to "how are" or "aint" to "are not"). <ref type="bibr" target="#b8">Cook and Stevenson (2009)</ref> extended this model by introducing an unsuper- vised noisy channel model. Rather than using one generic model for all word formations as in <ref type="bibr" target="#b5">(Choudhury et al., 2007)</ref>, they used a mix- ture model in which each different word formation type is modeled explicitly.</p><p>The limitations of these methods were that they did not consider contextual features and assumed that tokens have unique normalizations. In the text normalization task several OOV tokens are am- biguous and without contextual information it is not possible to build models that can disambiguate transformations correctly. <ref type="bibr" target="#b0">Aw et al. (2006)</ref> proposed a phrase-based statis- tical machine translation (MT) model for the text normalization task. They defined the problem as translating the SMS language to the English lan- guage and based their model on two submodels: a word based language model and a phrase based lexical mapping model (channel model). Their system also benefits from the input context and they argue that the strength of their model is in its ability to disambiguate mapping as in "2" → "two" or "to", and "w" → "with" or "who". Mak- ing use of the whole conversation, this is the clos- est approach to ours in the sense of utilizing con- textual sensitivity and coverage. <ref type="bibr" target="#b21">Pennell and Liu (2011)</ref> on the other hand, pro- posed a character level MT system, that is robust to new abbreviations. In their two phased system, a character level trained MT model is used to pro- duce word hypotheses and a trigram LM is used to choose a hypothesis that fits into the input context.</p><p>The MT based models are supervised models, a drawback of which is that they require anno- tated data. Annotated training data is not readily available and is difficult to create especially for the rapidly evolving social media text <ref type="bibr" target="#b28">(Yang and Eisenstein, 2013)</ref>.</p><p>More recent approaches handled the text nor- malization task by building normalization lexi- cons. <ref type="bibr" target="#b12">Han and Baldwin (2011)</ref> developed a two phased model, where they only consider the ill- formed OOV words for normalization. First, a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, pre- fix sub-string, suffix sub-string and longest com- mon subsequence (LCS), as well as context sup- port metrics. Chrupala (2014) on the other hand achieved lower word error rates without using any lexical resources.</p><p>Gouws et al. (2011) investigated the distinct contributions of features that are highly depended on user-centric information such as the geologi- cal location of the users and the twitter client that the tweet is received from. Using such user-based contextual metrics they modelled the transforma- tion distributions across populations. <ref type="bibr" target="#b17">Liu et al. (2012)</ref> proposed a broad coverage nor- malization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity. They try to improve the per- formance of the top n normalization candidates by integrating human perspective modeling. <ref type="bibr" target="#b28">Yang and Eisenstein (2013)</ref> introduced an unsu- pervised log linear model for text normalization. Their joint statistical approach uses local context based on language modeling and surface similar- ity. Along with dictionary based models, Yang and Eisenstein's model have obtained a significant im- provement on the performance of text normaliza- tion systems.</p><p>Another relevant study is conducted by <ref type="bibr" target="#b14">Hassan and Menezes (2013)</ref>, who generated a normaliza- tion lexicon using Markov random walks on a con- textual similarity lattice that they created using 5- gram sequences of words. The best normaliza- tion candidates are chosen using the average hit- ting time and lexical similarity features. Context of a word in the center of a 5-gram sequence is de- fined by the other words in the 5-gram. Even if one word is not the same, the context is considered to be different. This is a relatively conservative way for modeling the prior contexts of words. In our model, we filtered candidate words based on their grammatical properties and let each neighbouring token to contribute to the prior context of a word, which leads to both a higher recall and a higher precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this paper, we propose a graph-based approach that models both contextual and lexical similar- ity features among an ill-formed OOV word and candidate IV words. An input text is first prepro- cessed by tokenizing and Part-Of-Speech (POS) tagging. If the text contains an OOV word, the normalization candidates are chosen by making use of the contextual features, which are extracted from a pre-generated directed word association graph, as well as lexical similarity features. Lexi- cal similarity features are based on edit distance, longest common subsequence ratio, and double metaphone distance. In addition, a slang dictio- nary 1 is used as an external resource to enrich the normalization candidate set. The details of the approach are explained in the following sub- sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>After tokenization, the next step in the pipeline is POS tagging each token using a POS tagger specifically designed for social media text. Unlike the regular POS taggers designed for well-written newswire-like text, social media POS taggers pro- vide a broader set of tags specific to the peculiari- ties of social text ( <ref type="bibr" target="#b20">Owoputi et al., 2013;</ref><ref type="bibr" target="#b10">Gimpel et al., 2011)</ref>. Using this extended set of tags we can identify tokens such as discourse markers (e.g. rt for retweets, cont. for a tweet whose content fol- lows up in the coming tweet) or URLs. This en- ables us to model better the context of the words in social media text. A sample preprocessed sentence is shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>As shown in <ref type="table">Table 2</ref>, after preprocessing, each token is assigned a POS tag with a confidence score between 0 and 1 2 . Later, we use these confi- dence scores in calculating the edge weights in our context graph. Note that even though the words w and beatiful are misspelled, they are tagged cor- rectly by the tagger, with lower confidence scores though.  <ref type="table">Table 2</ref>: Sample POS tagger output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph construction</head><p>Contextual information of words is modeled through a word association graph created by us- ing a large corpus of social media text. The graph encodes the relative positions of the POS tagged words in the text with respect to each other. Af- ter preprocessing, each text message in the corpus is traversed in order to extract the nodes and the edges of the graph. A node is defined with four properties: id, oov, freq and tag. The token itself is the id field. The freq property indicates the node's frequency count in the dataset. The oov field is set to True if the token is an OOV word. Following the prior work by Han and Baldwin, (2011) we used the GNU Aspell dictionary (v0.60.6) to determine whether a word is OOV or not. We also edited the output of Aspell dictionary to accept letters other than "a" and "i" as OOV words. A portion of the graph that covers parts of the sample sentence in <ref type="table" target="#tab_2">Table 3</ref> is shown in <ref type="figure">Figure 1</ref>. In the created word association graph, each node is a unique set of a token and its POS tag. This helps us to identify the candidate IV words for a given OOV word by considering not only lexical and contextual similarity, but also gram- matical similarity in terms of POS tags. For ex- ample, if the token smile has been frequently seen as a Noun or a Verb, and not in other forms in the dataset (e.g. <ref type="table" target="#tab_3">Table 4</ref>), this provides evidence that it is not a good normalization candidate for an OOV token that has been tagged as a Pronoun. On the 2 CMU Ark Tagger (v0.3.2) <ref type="figure">Figure 1</ref>: Portion of the word association graph for part of the sample sentence in  An edge is created between two nodes in the graph, if the corresponding word pair (i.e. to- ken/POS pair) are contextually associated. Two words are considered to be contextually associated if they satisfy the following criteria:</p><p>• The two words co-occur within a maximum word distance of t distance in a text message in the corpus.</p><p>• Each word has a minimum frequency of t f requency in the corpus.</p><p>The directionality of the edges is based on the sequence of words in the text messages in the cor- pus. In other words, an edge between two nodes is directed from the earlier seen token towards the later seen token in a message. For example, <ref type="figure" target="#fig_0">Figure 2</ref> shows the edges that would be derived Let'sL startV thisD morningN wP aD beatifulA smileN .C <ref type="table" target="#tab_2">Table 3</ref>: Sample tokenized, POS tagged sentence (L: nominal+verbal, V: verb, D: determiner, N: noun, P: Preposition, A: adjective, C: punctuation).</p><p>from a text including the phrase "with a beautiful smile". The direction (from,to) and the distance together represent a unique triplet. For each pair of nodes with a specific distance there is an edge with a positive weight, if the two nodes are con- textually associated. Each co-occurrence of two contextually associated nodes increases the weight of the edge between them with an average of the nodes' POS tag confidence scores in the text mes- sage considered. If we are to expand the graph with the example phrase "with a beautiful smile", the weight of the edge with distance 2 from the node with|P to the node smile|N would increase by (0.9963 + 0.9712)/2, since the confidence score of the POS tag for the token with is 0.9963 and the confidence score of the POS tag of the token smile is 0.9712 as shown in <ref type="table">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-based Contextual Similarity</head><p>Our graph-based contextual similarity method is based on the assumption that an IV word that is the canonical form of an OOV word appears in the same context with the corresponding OOV word. In other words, the two nodes in the graph share several neighbors that co-occur within the same distances to the corresponding two words in social media text. We also assume that an OOV word and its canonical form should have the same POS tag.</p><p>Given an input text for normalization, the next step after preprocessing is finding the normaliza- tion candidates for each OOV token in the input text. For each ill-formed OOV token o i in the in- put text, first the list of tokens that co-occur with For each neighbor node n j in NL(o i ), the word association graph is traversed, and the edges from or to the node n j are extracted. The resulting edge list EL(o i ) has edges in the form of (n j , c k ) or (c k , n j ), where c k is a candidate canonical form of the OOV word o i . Here the neighbor node n j can be an OOV node, but the candidate node c k is chosen among the IV nodes. The edges in EL(o i ) are fil- tered by the relative distance of n j to o i as given in the NL(o i ). Any edge between n j and c k , whose distance is not the same as the distance between n j and o i is removed.</p><p>In addition to distance based filtering, POS tag based filtering is also performed on the edges in EL(o i ). Each candidate node should have the same POS tag with the corresponding OOV token. For the OOV token o i that has the POS tag T i , all the edges that include candidates with a tag other than T i are removed from the edge list EL(o i ). <ref type="figure" target="#fig_2">Figure 3</ref> represents a portion from the graph where the neighbors and candidates of the OOV node "beatiful" are shown. In the sample sentence in <ref type="table" target="#tab_2">Table 3</ref> there are two OOV tokens to be normal- ized, o 1 = w and o 2 = beatiful. The neighbor list of o 2 , NL(o 2 ) includes n 1 = w, n 2 = a and n 3 = smile. For each neighbor in NL(o 2 ), the can- didate nodes (c 1 = broken, c 2 = nice, c 3 = new, c 4 = beautiful, c 5 = big, c 6 = best, c 7 = great) are extracted. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, there are 11 lines representing the edges between the neighbors of the OOV token and the candidate nodes. These are representative edges in EL(o 2 ). Each member of the edge list has the same tag (A for Adjective) as the OOV node "beatiful" and the same distance to the corresponding neighbor node of the OOV node.</p><p>Each edge in EL(o i ) consists of a neighbor node n j , a candidate node c k and an edge weight edgeWeight(n j , c k ). The edge weight represents the likelihood or the strength of association be- tween the neighbor node n j and the candidate node c k . As described in the previous section the edge weights are computed based on the frequency of co-occurrence of two tokens, as well as the con- fidence scores of their POS tags.</p><p>The edge weights of the edges in EL(o 2 ) are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The edges that are connected to the OOV neighbor "w" have smaller edge weights such as 3, 5, and 26. On the other hand, the edges that are connected to common words have higher weights. For example, the weight of the edge be- tween the nodes "a" and "new" is 24388. This indicates that they are more common words, and frequently co-occur in the same form ("a new"). Although this edge weight metric is reasonable for identifying the most likely canonical form for the OOV word o i , it has the drawback of favoring words with high frequencies like common words or stop words. Therefore, to avoid overrated words and get contextually related candidates, we nor- malize the edge weight edgeWeight(n j , c k ) with the frequency of the candidate node c k as shown in Equation 1.</p><p>Equation 1 provides a metric that captures con- textual similarity based on binary associations. In order to achieve a more comprehensive contex- tual coverage, a contextual similarity feature is built based on the sum of the binary association scores of several neighbors. As shown in Equa- tion 2, for a candidate node c k the total edge weight score is the sum of the normalized edge weight scores EWNorm(n j , c k ), which are the edge weights coming from the different neighbors of the OOV token o i . We expect this contextual similarity feature to favor and identify the candi- dates which are (i) related to many neighbors, and (ii) have a high association score with each neigh- bor.</p><p>EW N orm(nj, c k ) = edgeW eight(nj, c k )/f req(c k )</p><formula xml:id="formula_0">(1) EW Score(oi, c k ) = EL(o i ) EW N orm(nj, c k )<label>(2)</label></formula><p>Our word association graph includes both OOV and IV tokens, and our OOV detection depends on the spellchecker which fails to identify some OOV tokens that have the same spelling with an IV word. In order to propose better canonical forms, the frequencies of the normalization candidates in the social media corpus have also been incorpo- rated to the contextual similarity feature. Nodes with higher frequencies lead to tokens that are in their most likely grammatical forms.</p><p>The final contextual similarity of the token o i and the candidate c k is the weighted sum of the total edge weight score and the frequency score of the candidate (see Equation 3). The frequency score of the candidate is a real number between 0 and 1. It is proportional to the frequency of the candidate with respect to the frequencies of the other candidates in the corpus. Since the total edge weight score is our primary contextual resource, we may want to favor edge weight scores. We give the frequency score a weight 0 ≤ β ≤ 1 to be able to limit its effect on the total contextual similarity score.</p><formula xml:id="formula_1">contSimScore(oi, c k ) = EW Score(oi, c k ) + β * f reqScore(c k )<label>(3)</label></formula><p>Hereby, we have the candidate list CL(o i ) for the OOV token o i that includes all the unique can- didates in EL(o i ) and their contextual similarity scores calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Lexical Similarity</head><p>Following the prior work in ( <ref type="bibr" target="#b12">Han and Baldwin, 2011;</ref><ref type="bibr" target="#b14">Hassan and Menezes, 2013)</ref>, our lexical similarity features are based on edit distance <ref type="bibr" target="#b16">(Levenshtein, 1966)</ref>, double metaphone (phonetic edit distance) <ref type="bibr" target="#b23">(Philips, 2000)</ref>, and a similarity function (simCost) ( <ref type="bibr" target="#b7">Contractor et al., 2010)</ref> which is de- fined as the ratio of the Longest Common Sub- sequence Ratio (LCSR) <ref type="bibr" target="#b19">(Melamed, 1999</ref>) of two words and the Edit Distance (ED) between their skeletons (Equations 4 and 5), where the skeleton of a word is obtained by removing its vowels.</p><formula xml:id="formula_2">LCSR(oj, c k ) = LCS(oj, c k )/maxLength(oj, c k ) (4) simCost(oj, c k ) = LCSR(oj, c k )/ED(oj, c k ) (5)</formula><p>Following the tradition that is inspired from ( <ref type="bibr" target="#b15">Kaufmann and Kalita, 2010)</ref>, before lexical similarity calculations, any repetitions of characters three or more times in OOV tokens are reduced to two (e.g. goooood is reduced to good). Then, the edit distance, phonetic edit distance, and simCost between each candidate in CL(o i ) and the OOV token o i are calculated. Edit distance and phonetic edit distance are used to filter the candidates. Any candidate in CL(o i ) with an edit distance greater than t edit and phonetic edit distance greater than t phonetic to o i is removed from the candidate list CL(o i ).</p><formula xml:id="formula_3">lexSimScore(oi, c k ) = simCost(oi, c k ) + λ * editScore(oi, c k )<label>(6)</label></formula><p>For the remaining candidates, the total lexical similarity score (Equation 6) is calculated using simCost and edit distance score 3 . Similar to con- textual similarity score, here we have one main lexical similarity feature and one minor lexical similarity feature. The major lexical similarity feature is simCost, whereas the edit distance score is the minor feature. We assigned a weight 0 ≤ λ ≤ 1 to the edit distance score to be able to lower its contribution while calculating the total lexical similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">External Score</head><p>Since some social media text messages are ex- tremely short and contain several OOV words, they do not provide sufficient context, i.e., IV neighbors, to enable the extraction of good candi- dates from the word association graph. Therefore, we extended the candidate list obtained through contextual similarity as described in the previous section, by including all the tokens in the word as- sociation graph that satisfy the edit distance and <ref type="bibr">3</ref> an approximate string comparison measure (between 0.0 and 1.0) using the edit distance https://sourceforge.net/projects/febrl/ phonetic edit distance criteria. We also incorpo- rated candidates from external resources, in other words from a slang dictionary and a transliteration table of numbers and pronouns. If a candidate oc- curs in the slang dictionary or in the transliteration table as a correspondence to its OOV word, it is assigned an external score of 1, otherwise it is as- signed an external score of 0.</p><p>The transliterations were first used by <ref type="bibr" target="#b11">(Gouws et al., 2011</ref>). Besides the token and its transliter- ation we also use its POS tag information, which was not available in their system.</p><p>The external score favors the well known inter- pretations of common OOV words. However, un- like the dictionary based methodologies, our sys- tem does not return the corresponding unabbrevi- ated word in the slang dictionary or in the translit- eration table directly. Only an external score gets assigned and the candidate still needs to com- pete with other candidates which may have higher contextual similarities and one of those contextu- ally more similar candidates may be returned as the correct normalization instead of the candidate found equivalent to the OOV word in the slang dic- tionary (or in the transliteration table).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Overall Scoring</head><p>As shown in Equation 7, the final score of a can- didate IV token c k for an OOV token o i is the sum of its lexical similarity score, contextual similarity score and external score with respect to o i .</p><formula xml:id="formula_4">candScore(oi, c k ) = lexSimScore(oi, c k ) + contSimScore(oi, c k ) + externalScore(oi, c k )<label>(7)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We used the LexNorm1.1 (LN) dataset (Han and Baldwin, 2011) and Pennell and Liu (2014)'s tri- gram dataset to evaluate our proposed approach. LexNorm1.1 contains 549 tweets with 1184 manu- ally annotated ill-formed OOV tokens. It has been used by recent text normalization studies for eval- uation, which enables us to directly compare our performance results with results obtained by the recent previous work ( <ref type="bibr" target="#b12">Han and Baldwin, 2011;</ref><ref type="bibr" target="#b21">Pennell and Liu, 2011;</ref><ref type="bibr" target="#b13">Han et al., 2012;</ref><ref type="bibr" target="#b17">Liu et al., 2012;</ref><ref type="bibr" target="#b14">Hassan and Menezes, 2013;</ref><ref type="bibr" target="#b28">Yang and Eisenstein, 2013;</ref><ref type="bibr" target="#b6">Chrupala, 2014)</ref>. The trigram dataset is an SMS-like corpus collected from twit- ter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Generation</head><p>We used a large corpus of social media text to con- struct our word association graph. We extracted 1.5 GB of English tweets from Stanford's 476 mil- lion Twitter Dataset ( <ref type="bibr" target="#b29">Yang and Leskovec, 2011</ref>). The language identification of tweets was per- formed by using the langid.py Python library ( <ref type="bibr" target="#b18">Lui and Baldwin, 2012;</ref><ref type="bibr" target="#b3">Baldwin and Lui, 2010)</ref>. CMU Ark Tagger (v0.3.2), which is a social me- dia specific POS tagger achieving an accuracy of 95% over social media text <ref type="bibr" target="#b20">(Owoputi et al., 2013;</ref><ref type="bibr" target="#b10">Gimpel et al., 2011)</ref>, is used for tokenizing and POS tagging the tweets. We used the twitter tagset which includes some extra POS tags specific to so- cial media including URLs and emoticons, Twit- ter hashtags (#), and twitter at-mentions (@). We made use of these social media specific tags to dis- ambiguate some OOV tokens.</p><p>After tokenization, we removed the tokens that were POS tagged as mention (e.g. @brendon), discourse marker (e.g. RT), URL, email address, emoticon, numeral, and punctuation. The remain- ing tokens are used to build the word association graph. After constructing the graph we only kept the nodes with a frequency greater than 8. For the performance related reasons, the relatedness thresholds t distance and t f requency were chosen as 3 and 8, respectively. The resulting graph contains 105428 nodes and 46609603 edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Candidate Set Generation</head><p>While extending the candidate set with lexical fea- tures we use t edit ≤ 2 ∨ t phonetic ≤ 1 to keep up with the settings in ( <ref type="bibr" target="#b12">Han and Baldwin, 2011</ref>). In other words, IV words that are within 2 char- acter edit distance or 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates. The values for the λ and β parameters in Equations 3 and 6 are set to 0.5. We did not tune these pa- rameters for optimized performance. We selected the value of 0.5 in order to give less weight (half weight) to our minor contextual and lexical simi- larity features compared to the major ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Normalization Candidates</head><p>Most of the prior work assume perfect detection of ill-formed words during test set decoding ( <ref type="bibr" target="#b17">Liu et al., 2012;</ref><ref type="bibr" target="#b12">Han and Baldwin, 2011;</ref><ref type="bibr" target="#b21">Pennell and Liu, 2011;</ref><ref type="bibr" target="#b28">Yang and Eisenstein, 2013)</ref>. To be able to compare our results with studies that do not assume that ill-formed words have been pre- identified <ref type="bibr" target="#b6">(Chrupala, 2014;</ref><ref type="bibr" target="#b14">Hassan and Menezes, 2013;</ref><ref type="bibr" target="#b13">Han et al., 2012</ref>) we used our graph and built a dictionary to identify the ill-formed words.</p><p>Following <ref type="bibr" target="#b12">Han and Baldwin (2011)</ref> and <ref type="bibr" target="#b28">Yang and Eisenstein (2013)</ref>, we created a dictionary by choosing the nodes in our graph that have a fre- quency property higher than 20. Filtering this dic- tionary of 49657 words using GNU Aspell dictio- nary (v0.60.6) we produced a set of 26773 "in- vocabulary" (IV) words. In our second setup our system does not attemp to normalize the words in this set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results and Analysis</head><p>In this paper we introduced a new contextual ap- proach for text normalization. The lexical similar- ity score described in Section 3.4 and the external score described in Section 3.5 depend on the work of <ref type="bibr" target="#b12">Han and Baldwin (2011)</ref>. With small changes made to the previously proposed method we took it as a baseline in our study.</p><p>As contextual layer we proposed two metrics extracted from the word association graph. The first one depends on the total edge weights be- tween candidates and OOV neighbours, the sec- ond one is based on the frequencies of the candi- dates in the corpus.</p><p>As the evaluation metrics we used precision, recall, and F-Measure. Precision calculates the proportion of correctly normalized words among the words for which we produced a normaliza- tion. Recall shows the amount of correct nor- malizations over the words that require normal- ization (ill-formed OOV words). The main metric that we consider while evaluating the performance of our system is F-Measure which is the harmonic mean of precision and recall.</p><p>We investigated the impact of lexSimScore and externalScore seperately on both datasets (Ta- ble 5).</p><p>Using only lexSimScore the sys- tem achieved an F-measure of 28.24% on the LexNorm1.1 dataset and 38.70% on the Trigram dataset, which shows that lexical similarity alone is not enough for a good normalization system. However, the externalScore which is the layer that is more aware of the Internet jargon, along with some social text specific rule based transliterations performs better than expected on both datasets. Mixing these two layers we reach our baseline that is adopted from ( <ref type="bibr" target="#b12">Han and Baldwin, 2011</ref>). This baseline setup obtained an F-measure of 77.12% on LexNorm1.1, which is slightly better than the result (75.30%) reported by the original system of Han and Baldwin (2011).</p><p>The results obtained by our proposed Contex- tual Word Association Graph (CWA-Graph) sys- tem on the LexNorm1.1 and trigram datasets, as well as the results of recent studies that used the same datasets for evaluation are presented in Ta- ble 5. The ill-formed words are assumed to have been pre-identified in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dataset  <ref type="table">Table 5</ref>: Results obtained when ill-formed words are assumed to have been pre-identified in ad- vance.</p><p>Our CWA-Graph approach achieves the best F- measure (82.24%) and precision (85.50%) among the recent previous studies. The high precision value is obtained without compromising much from recall (79.22%). Our recall is the second best among others. The F-score (82.09%) obtained by Yang and Eisenstein (2013)'s system is close to ours and the second best F-score, which on the other hand, has a lower precision.</p><p>Without any modification to our system or to the parameters, we were able to improve the re- sults obtained by Pennell and Liu (2011) on the trigram SMS-like dataset. The trigram nature of the dataset resulted in input texts which are (short thus) very limited with regard to contextual infor- mation. Nevertheless, our system achieved 72.8% F-Measure using this contextual information even though it is limited.</p><p>Along the systems (presented in <ref type="table">Table 5</ref>) that assume ill-formed tokens have been pre-identified perfectly by an oracle, there are also systems that are not based on this assumption but contain ill- formed word identification components <ref type="bibr" target="#b13">(Han et al., 2012;</ref><ref type="bibr" target="#b14">Hassan and Menezes, 2013;</ref><ref type="bibr" target="#b6">Chrupala, 2014)</ref>. We used the method described in Section 4.4 to identify the candidate tokens for normaliza- tion. <ref type="table">Table 6</ref> shows our results compared with the results of other systems that perform ill-formed word detection prior to normalization. We could label 1141 tokens correctly as ill-formed among 1184 ill-formed tokens. We achieved a word error rate (WER) of 2.6%, where Chrupala (2014) re- ported 4.8% and Han et al. <ref type="formula" target="#formula_0">(2012)</ref>   <ref type="table">Table 6</ref>: Results obtained without assuming that ill-formed words have been pre-identified.</p><p>As shown in <ref type="table">Table 5</ref> some systems have equal precision and recall values <ref type="bibr" target="#b28">(Yang and Eisenstein, 2013;</ref><ref type="bibr" target="#b12">Han and Baldwin, 2011;</ref><ref type="bibr" target="#b21">Pennell and Liu, 2011)</ref>. Those systems normalize all ill-formed words. On the other hand, our system does not return a normalization, if there are no candidates that are lexically similar, grammatically correct, and contextually close enough. For this reason, we managed to achieve a higher precision com- pared to the other systems. Our system returns a normalization candidate for an OOV word only if it achieves a similarity score (contextual, lexical, external, or some degree of each feature) above a threshold value. The default threshold used in the system is set equal to the maximum score that can be obtained by lexical features. Thus, we only re- trieve candidates that obtain a non-zero contextual similarity score (conSimScore). The results shown at  <ref type="bibr" target="#b14">and Menezes, 2013)</ref>. The precision of the normal- ization system can be set (e.g. as high, medium, low) depending on the application where it will be used.</p><p>Our motivation behind introducing the λ and β parameters was to investigate the importance 321 conSimScore &gt; Precision Recall F-measure    of the minor features compared to our major fea- tures (described in Sections 3.3 and 3.4). For the experiments reported in <ref type="table">Tables 5, 6</ref>, 7 and 8 we set the λ and β values to 0.5. We did not tune these pa- rameters for optimized performance. Rather, our aim was to give less weight (half weight) to the minor features compared to the major ones. To analyze the effects of the lambda and beta param- eters, we plotted the performance of the system on the LexNorm1.1 data set by varying their values (see <ref type="figure">Figure 4</ref>). It is shown that for λ and β values greater than 0.3 the performance of the system is quite robust. The F-score varies between 80.4% and 82.9%.</p><p>Figure 4: The effect of λ and β on the system per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present an unsupervised graph- based approach for contextual text normalization. The task of normalization is highly dependent on understanding and capturing the dynamics of the informal nature of social text. Our word associ- ation graph is built using a large unlabeled social media corpus. It helps to derive contextual analy- sis on both clean and noisy data.</p><p>It is important to emphasize the difference be- tween corpus based contextual information and contextual information of the input text (input con- text). Most recent unsupervised systems for text normalization only make use of corpus based con- text information. However, this approach is led by statistical information. In other words, it finds which IV word the OOV word is commonly nor- malized to, regardless of the context of the OOV word in the input text message. A major strength of our approach is that it utilizes both corpus based contextual information and input based contextual information. We use corpus based statistical infor- mation to connect/associate the words in the con- textual word association graph. On the other hand, the neighbors of an OOV word in the input text provide us input based context information. Using input context to find normalizations helps us iden- tify the correct normalization, even if it is not the statistically dominant one.</p><p>We compared our approach with the recent social media text normalization systems and achieved state-of-the-art precision and F-measure scores. We reported our results on two datasets. The first one is the standard text normalization dataset (Lexnorm1.1) derived from Twitter. Our results on this dataset showed that our system can serve as a high precision text normalization sys- tem which is highly preferable as an NLP pre- processing step. The second dataset we tested our approach is a SMS-like trigram dataset. The tests showed that the proposed system can perform good on SMS data as well.</p><p>The system does not require a clean corpus or an annotated corpus. The contextual word asso- ciation graph can be built by using the publicly available social media text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample nodes and edges from the word association graph.</figDesc><graphic url="image-1.png" coords="5,71.95,369.99,218.41,121.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>o</head><label></label><figDesc>i in the input text and their positional distances to o i are extracted. This list is called the neighbor list of token o i , i.e., NL(o i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A portion of the graph that includes the OOV token "beatiful", its neighbors and the candidate nodes that each neighbor is connected to. Thick lines show the edge list with relative weights.</figDesc><graphic url="image-6.png" coords="6,71.88,63.57,218.74,192.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>0</head><label>0</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Sample tweets and their normalized forms.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>(d: dis-
tance, w: edge weight). 

other hand, smile can be a good candidate for a 
Noun or a Verb OOV token, if it is lexically and 
contextually similar to it. 

node id freq 
oov 
tag 
smile 
3 
False A 
smile 
3403 False N 
smile 
2796 False V 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The different nodes in the word associ-
ation graph representing the token smile tagged 
with different POS tags. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>reported 6.6% WER on the Lexnorm1.1 dataset.</figDesc><table>Method 
Dataset Precision Recall F-measure 

Han et al. (2012) 
LN 
70.00 
17.90 
28.50 
Hassan and Menezes (2013) 
LN 
85.37 
56.40 
69.93 
CWA-Graph 
LN 
85.87 
76.52 
80.92 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 and</head><label>7</label><figDesc>Table 8 demonstrate that CWA- Graph can obtain even higher values of precision by increasing the percentage of contextual context of candidates. It achieved 94.1% precision on the LexNorm1.1 dataset, where the highest precision reported at the same recall level is 85.37% (Hassan</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Comparison of results for different 
threshold values on LexNorm1.1, the setup we 
have used for our other experiments is shown in 
bold. 

conSimScore &gt; Precision Recall F-measure 

0 
77.2 
68.8 
72.8 
0.1 
80.9 
65.8 
72.6 
0.2 
84.2 
60.8 
70.6 
0.3 
87.6 
54.6 
67.3 
0.4 
89.5 
47.1 
61.7 
0.5 
90.8 
42.1 
57.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Comparison of results for different 
threshold values on trigram dataset, the setup we 
have used for our other experiments is shown in 
bold. 

</table></figure>

			<note place="foot" n="1"> http://www.noslang.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Phrase-based Statistical Model for SMS Text Nor322</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiti</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<orgName type="collaboration">malization</orgName>
		</author>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language Identification: The Long and the Short of the Matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Improved Error Model for Noisy Channel Spelling Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Investigation and Modeling of the Structure of Texting Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalizing tweets with edit scripts and recurrent neural embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="680" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Cleansing of Noisy Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Faruquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Unsupervised Model for Text Message Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What to Do About Bad Language on the Internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics : Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="359" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congxing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<title level="m">Contextual Bearing on Linguistic Variation in Social Media. Proceedings of the Workshop on Languages in Social Media</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lexical Normalisation of Short Text Messages: Makn Sens a #Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Social Text Normalization Using Contextual Graph Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Syntactic Normalization of Twitter Messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Natural Language Processing</title>
		<meeting>the 8th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binary Codes Capable of Correcting Deletions, Insertions and Reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir Iosifovich Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Normalization System for Social Media Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Langid.Py: An Off-the-shelf Language Identification Tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bitext Maps and Alignment via Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Melamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="130" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics : Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A CharacterLevel Machine Translation Approach for Normalization of SMS Abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="974" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Normalization of informal text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="256" to="277" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Double Metaphone Search Algorithm. C/C++ Users Journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Philips</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-06" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="38" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Normalization of Non-Standard Words</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pronunciation Modeling for Improved Spelling Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Log-Linear Model for Unsupervised Text Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods on Natural Language Processing</title>
		<meeting>the Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Patterns of Temporal Variation in Online Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forth International Conference on Web Search and Web Data Mining</title>
		<meeting>the Forth International Conference on Web Search and Web Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
