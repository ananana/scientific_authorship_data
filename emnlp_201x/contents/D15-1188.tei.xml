<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reverse-engineering Language: A Study on the Semantic Compositionality of German Compounds</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
							<email>corina.dima@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Research Center 833</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reverse-engineering Language: A Study on the Semantic Compositionality of German Compounds</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpus-estimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vector space models of language like the ones presented in <ref type="bibr" target="#b4">(Collobert et al., 2011b;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013;</ref><ref type="bibr" target="#b15">Pennington et al., 2014</ref>) create good representations for the individual words of a lan- guage. However, the words in a language can be combined into infinitely many distinct, well- formed phrases and sentences. Creating meaning- ful, reusable representations for such longer word sequences is still an open problem.</p><p>In this paper we focus on building represen- tations for syntactic units just above the word level, by exploring compositional models for com- pounds. Bauer (2001) defines a compound as "a lexical unit made up of two or more elements, each of which can function as a lexeme indepen- dent of the other(s) in other contexts" (e.g. apple tree). The vast majority of compounds are com- positional, i.e. we can understand the meaning of the compound if we know the meaning of its con- stituent words. We would like to equip the vector space model with a composition function able to construct a composite representation for apple tree from the representations of apple and tree. The composite representation should ideally be indis- tinguishable from its observed representation, i.e. the representation learned directly by the language model if the compound is part of the dictionary.</p><p>We situate our investigations in the context of the German language, a language where com- pounds represent an important fraction of the vo- cabulary. <ref type="bibr" target="#b1">Baroni et al. (2002)</ref> analyzed the 28 mil- lion words German APA news corpus and discov- ered that compounds account for 47% of the word types but only 7% of the overall token count, with 83% of compounds having a corpus frequency of 5 or lower. The high productivity of the compound- ing process makes the compositional approach the most tractable way to create meaningful represen- tations for all the compounds that have been or will be coined by the speakers of the German language.</p><p>German compounds have a strategic advantage for our study: they are generally written as a contiguous word, irrespective of how many con- stituents they have. Our example English com- pound, apple tree, translates into the German com- pound Apfelbaum, with the head Baum "tree" and the modifier Apfel "apple". Because the com- pound is written as a single word, we can di- rectly learn the representations for the compound and for its constituents. Given a large dataset of German compounds together with their immedi- ate constituents, and the corresponding distributed representations for each of the individual words, one can try to reverse-engineer the compounding process and learn the parameters of a function that combines the representation of the constituents into the representation of the compound. More formally, we are interested in learning a compo- sition function f such that</p><formula xml:id="formula_0">c comp = f (m obs , h obs )</formula><p>where c comp ∈ R n is the composite representa- tion of the compound and m obs , h obs ∈ R n are the observed representations of its modifier and its head. The function should minimize J, the mean squared error between the composite (c comp ) and the observed (c obs ) representations of the |C| com- pounds in the training set:</p><formula xml:id="formula_1">J = |C| i=1 1 n n j=1 (c comp ij − c obs ij ) 2</formula><p>Several compositionality models have already been proposed in the literature ( <ref type="bibr" target="#b12">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b20">Socher et al., 2012)</ref>. In this paper we evaluate several of the proposed composition functions and also present a new composition model which outper- forms all previous models on a dataset of German compounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word Representations and Compounds Dataset</head><p>We trained 4 vector space language models for German (with 50, 100, 200 and 300 dimensions respectively) using the GloVe package <ref type="bibr" target="#b15">(Pennington et al., 2014</ref>) and a 10 billion token raw-text corpus extracted from the DECOW14AX corpus <ref type="bibr" target="#b17">(Schäfer, 2015)</ref>. We use a vocabulary of 1,029,270 (1M) words, obtained by selecting all the words with a minimum frequency of 100 (the full vocab- ulary had 50M unique words). We used the default GloVe training parameters, the only modifications being the use of a symmetric context when con- structing the co-occurence matrix (10 words to the left and to the right of the target word) and training each model for 15 iterations. All the vector spaces were normalized to the L2-norm, first across fea- tures then across samples using scikit-learn (Pe- dregosa et al., 2011). The German compounds dataset used in the ex- periments is a subset of the 54759 compounds available in GermaNet 9.0 1 . The compounds in the list were automatically split and manu- ally post-corrected ( <ref type="bibr" target="#b8">Henrich and Hinrichs, 2011)</ref>. Each entry in the list is a triple of the form (com- pound, modifier, head). We filtered the entries in the list, keeping only those where all three words have a minimum frequency of 500 in the support corpus used to create the vector space represen- tations. The reason for the filtering step is that a "well-learned" representation (based on a suffi- ciently large number of contexts) should allow for a more accurate reconstruction than a representa- tion based only on a few contexts. The filtered dataset contains 34497 entries. This dataset was</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">12 ways to Represent A Compound</head><p>We adopt a notation similar to the one introduced in <ref type="bibr" target="#b12">(Mitchell and Lapata, 2010)</ref>, where the compos- ite representation p is the result of applying a com- position function f to the vectors u and v. In this study we tested the following composition func- tions:</p><p>1. p = v, the second constituent of the com- pound 2. p = u, the first constituent of the compound</p><formula xml:id="formula_2">3. p = u v, component-wise vector multipli- cation 4. p = (u · u)v + (λ − 1)(u · v)u, dilation 5. p = 0.5u + 0.5v, vector addition 6. p = λu + βv, weighted vector addition,</formula><p>where the λ and β are estimated using the training set. Models 1 through 6 were intro- duced in (Mitchell and Lapata, 2010). 7. p = U v, where v ∈ R n is the vectorial representation of the head word (given) and U ∈ R n×n is a matrix representation for the modifier, estimated with the help of the train- ing data. The model estimates one matrix for each word that is used as a modifier. Referred to as alm in ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010)</ref> and as Lexfunc in ( <ref type="bibr" target="#b6">Dinu et al., 2013b</ref>).</p><formula xml:id="formula_3">8. p = M 1 u + M 2 v, where M 1 , M 2 ∈ R n×n</formula><p>are two matrices that modify the first and the second constituent vectors, respectively. In contrast to the previous model, this model es- timates just one matrix for all the modifiers and one matrix for all the head words. Ref- ered to as EAM in ( <ref type="bibr" target="#b22">Zanzotto et al., 2010</ref>) and as Fulladd in ( <ref type="bibr" target="#b6">Dinu et al., 2013b</ref>).</p><formula xml:id="formula_4">9. p = g(W [u; v]), where: [u; v] ∈ R 2n×1</formula><p>is the concatenation of the individual word vectors; W ∈ R n×2n is a global matrix that: (i) com- bines the individual dimensions of the con- catenated input vector [u; v]; (ii) brings the composite representation back into the R n×1 space; g is an element-wise function, in our experiments the hyperbolic tangent tanh. In- troduced in <ref type="bibr" target="#b18">(Socher et al., 2010)</ref>. <ref type="bibr">et al., 2012)</ref>, it is a generalization of model 7. Each word is represented using an R n×n ma- trix and a R n vector. The vectors are given, while the matrices are estimated using the training data. Referred to as Fulllex in ( <ref type="bibr" target="#b6">Dinu et al., 2013b</ref>). 11. p = u u + v v , the additive mask model (Addmask) and 12. p = g(W [u u ; v v ]), the global ma- trix mask model (Wmask), both presented in subsection 3.1. Models 1 through 8 were tested using the im- plementations available in the DISSECT toolkit ( <ref type="bibr" target="#b5">Dinu et al., 2013a</ref>). As a side note, the Lex- func implementation in DISSECT does not pro- duce a composite representation for 11.5% of the our test data, where a word does not appear as a modifier during training. Therefore, we reimple- mented the Lexfunc model and solved the missing training material problem by initializing the ma- trix for all the words in the dictionary with I + , the identity matrix plus a small amount of Gaus- sian noise. This type of initialization was proposed by <ref type="bibr" target="#b20">(Socher et al., 2012)</ref>, and allows the model to back-off to the model p = v when there is no data to estimate the parameters of the modifier matrix. We also reimplemented models 9 and 10, which were used in ( <ref type="bibr" target="#b18">Socher et al., 2010;</ref><ref type="bibr" target="#b20">Socher et al., 2012)</ref>, as the existing implementations are part of a more complex recursive architecture aimed at constructing representations for full sentences.</p><formula xml:id="formula_5">10. p = g(W [V u; U v]). Introduced in (Socher</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The mask models</head><p>The newly introduced mask models build upon the idea that when a word w enters a composition pro- cess, there is some variation in its meaning de- pending on whether it is the first or the second el- ement of the composition. Think, for instance, of the compounds company car and car factory. In the first case, car has its primary denotation, that of a road vehicle. In the second case, what mat- ters more about the car is its product aspect, the fact that it is an "artifact produced in a factory". A good representation of the word car should encode both aspects. Likewise, a good composition model should be able to select from the individual word representations only those aspects that are relevant for the composition process.</p><p>We want to give the composition model the pos- sibility to deal with these slight sense variations, so we train, for each word in the dictionary, two masks, one for the case when it is the first word in the composition process and one for when it is the second word. The masks of the word w rep- resented by u ∈ R n are two vectors u , u ∈ R n . The mask vectors are initialized with a vector of all ones, 1, and estimated with the help of the train- ing data. Each time w is the first word in the com- position process, it is represented as the element- wise multiplication of the vector u and the mask u , u u . When w is the second word in the composition, it is represented by the element-wise multiplication of u and the mask u , u u .</p><p>It is important to note that the initial vector rep- resentations remain fixed during the learning pro- cess. The learning process only affects the mask vectors. The composite representation of a com- pound like car factory is obtained by combin- ing the masked representations, u car u car and v f actory v f actory . We tried two different combi- nation methods: (i) p = u u + v v , called Addmask (model 11), where the masked represen- tations are combined via component-wise addi- tion, and (ii) p = g(W [u u ; v v ]), called Wmask (model 12), where the combination of the masked representations is made via a global ma- trix W ∈ R n×2n and a nonlinearity g (tanh), sim- ilar to model 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementing composition models</head><p>Models 7, 9 and 10 and the mask models were im- plemented using neural network architectures in the Torch7 library <ref type="bibr" target="#b3">(Collobert et al., 2011a</ref>). We use the mean squared error as a training criterion, and optimize all models using Adagrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) and a mini-batch of 100 samples. The hyperparameters were chosen by testing different parameter values and evaluating their performance on the dev set. To avoid overfitting we used early stopping <ref type="bibr" target="#b16">(Prechelt, 1998)</ref>. All the implemented models keep the input vectors fixed during the composition process.</p><p>Training the mask models entails estimating modifier and head masks for every word in the dic- tionary D. The two types of masks to be learned can be formalized as two matrices W M , W H ∈ R n×|D| , where n is the size of the initial word representations. The masks of the word w i ∈ D are the i th rows in W M and W H . In Torch7 such representations can be learned using lookup table layers <ref type="bibr" target="#b4">(Collobert et al., 2011b</ref>), which map matrix indices to the corresponding row vector.</p><p>The masked representation of the modifier is obtained by first feeding the index of the word to LT W M , the modifier lookup table, to obtain the modifier mask, and then multiplying the modifier mask with the initial representation for the modi- fier. The masked representation of the head is ob- tained in a similar manner via a lookup operation in LT W H , the head lookup table. The Addmask and Wmask models differ only in the composi- tion method used after the masking process: the masked representations are directly added together in the case of Addmask and are passed through a composition matrix W ∈ R n×2n and a nonlin- earity g in the case of Wmask. The two matri- ces W M , W H are initialized with all ones and are modified via backpropagation during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Results</head><p>The twelve composition models presented in Sec- tion 3 were evaluated using word representations of increasing size (described in Section 2). All the models are trained on the train split and tested on the test split. We used the rank eval- uation method proposed by <ref type="bibr" target="#b0">(Baroni and Zamparelli, 2010</ref>) for a similar task: first, we generate a composite representation for each of the 6901 compounds in the test set; then, we use the co- sine similarity to rank each composite representa- tion with respect to the observed representations of the 41732 unique words in the dataset dictio- nary. If the observed representation is the nearest neighbour, the composition is assigned the rank 1. Similar to ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010)</ref>, we as- sign the rank 1000 (≥1K) when the observed rep- resentation is not one of the nearest 1000 neigh- bours of the composite representation. We then compute the first, second and third quartiles (Q1, Q2, Q3) across all the compounds in the test set. A Q1 value of 2 means that the first 25% of the data was only assigned ranks 1 and 2. Similarly, Q2 and Q3 refer to the ranks assigned to the first 50% and 75% of data, respectively. The results of our evaluation are displayed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>The observed representation of the head (model 1) was used as a strong baseline for the compound composition task. Two of the tested models, mul- tiplicative (model 3) and dilation (model 4) score worse than the head baseline, while the additive models (5 and 6) score only slightly above it. The fact that the worst performing model is the multi- plicative model is surprising considering its good performance in previous studies <ref type="bibr" target="#b12">(Mitchell and Lapata, 2010</ref>). This might be either a side-effect of the normalization procedure, or a genuine incom- patibility of this compositionality model with the vectorial representations produced by GloVe.</p><p>The new Addmask and Wmask models (intro- duced in Section 3.1) perform very well, with Wmask producing the best results on the test dataset across all dimensions. It is interesting to note that the linguistically motivated Lexfunc and Fulllex models, which build dedicated representa- tions for each individual constituent, are outper- formed by a simple model like Fulladd, that only learns two modification matrices, one for each po- sition. The explanation is, in our opinion, that the available training material is not enough for train- ing all the parameters of the complex Lexfunc and Fulllex models, but good enough for the more sim- ple Fulladd.</p><p>The mask models are computationally cheaper than models like Lexfunc and Fulllex, as they they only train 2n parameters for each word in the vo- cabulary, and not n 2 parameters like the aforemen- tioned models. They manage to strike a balance and learn a dedicated representation for each con- stituent with a small number of parameters, thus performing better than the more complex models.</p><p>We used non-parametric statistical tests to de- tect significant differences between the results ob- tained by the models. We focused our analysis on the best performing 4 models: model 9, which we will label the Matrix model, Fulladd (model 8), Addmask (model 11) and Wmask (model 12). The comparison takes into account two separate fac- tors: (i) differences between the models using rep- resentations of the same size; (ii) differences in the performance of the same model using representa- tions of different sizes.</p><p>A Friedman test on the ranks obtained by the 4 selected models on representations of size 300 showed that there is a significant difference be- tween the models (p &lt; 0.01). Pairwise compar- isons (using the Wilcoxon signed rank test and Bonferroni corrections) showed that there is a sig- nificant difference (p &lt; 0.01) between all but one pair of models, namely the Matrix and the Ad- dmask models (p = 0.9). The same test confirmed that there are significant differences in the perfor- mance of the best model Wmask when using repre- sentations of different sizes (p &lt; 0.01). Pairwise  <ref type="table" target="#tab_2">I  50d  100d  200d  300d  Q1 Q2 Q3 Q1 Q2  Q3 Q1 Q2  Q3 Q1</ref>    comparisons showed that Wmask model signifi- cantly improves its performance (p &lt; 0.01) when using word representations of increasing size (50, 100, 200 and 300 dimensions).</p><formula xml:id="formula_6">3 p = u v D ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K ≥1K 4 p = (u · u)v + (λ − 1)(u · v)u</formula><p>The twelve composition models were also com- pared in terms of the mean squared error (MSE) objective function, by computing the MSE be- tween the composite and the observed represen- tation of the compounds in the test set. The best scoring models in the rank evaluation were also the best in the MSE evaluation. However, the dif- ference in performance between the best and the worst models was considerably smaller: the MSE of the multiplicative model is only twice as large as the MSE of the best performing Wmask model. This is in contrast to the rank evaluation where the multiplicative model assigned the observed repre- sentations in the test set only ranks ≥ 1000, while Wmask assigned ranks ≤ 25 to 75% of the test data. Additional investigations are necessary to es- timate the impact of different objective functions on the performance of compositional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to related work</head><p>The experiments reported in this paper are, to the best of our knowledge, the first large scale experi- ments on the composition of German compounds. Other studies ( <ref type="bibr" target="#b9">Kisselew et al., 2015;</ref><ref type="bibr" target="#b10">Lazaridou et al., 2013</ref>) focused on morphologically complex words in German and English respectively. In terms of the size of the training and test material, our experiments are closest to the adjective-noun experiments in ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010)</ref> and ( <ref type="bibr" target="#b6">Dinu et al., 2013b)</ref> where the lexical function model performed the best, with lowest reported median ranks (Q2) above 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Twelve composition models were evaluated on the task of building compositional representations for German compounds. The best results (median rank 6) were obtained by the newly introduced Wmask model, p = g(W [u u ; v v ]). The re- sults show that it is possible to learn a composition function specific to compounds, an idea which we would like to further explore using existing com- pound datasets for English <ref type="bibr" target="#b13">( ´ O Séaghdha, 2008;</ref><ref type="bibr" target="#b21">Tratz and Hovy, 2010)</ref>. The implementation of the newly introduced composition methods can be downloaded from the author's website.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Quartiles for the 6901 composite representations in the test set, ranked with respect to the 
observed representations. Best possible rank is 1. D marks the models tested with DISSECT, R marks 
reimplementations of existing models and N marks new models. 

</table></figure>

			<note place="foot" n="1"> http://www.sfs.uni-tuebingen.de/lsd/compounds.shtml randomized and partitioned into train, test and dev splits according to the 70-20-10 rule. The dataset contains 8580 unique modifiers and heads, and a dictionary of 41732 unique words. 1345 compounds appear as the modifier or head of another compound.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to thank Emanuel Dima, Erhard Hinrichs, Daniël de Kok, Dörte de Kok and Jianqiang Ma, as well as the anonymous re-viewers for their insightful comments and sugges-tions. Financial support for the research reported in this paper was provided by the German Re-search Foundation (DFG) as part of the Collabo-rative Research Center "Emergence of Meaning" (SFB 833), project A3.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting the components of German nominal compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Matiasek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Trost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Artificial Intelligence (ECAI)</title>
		<editor>F. van Harmelen</editor>
		<meeting>the 15th European Conference on Artificial Intelligence (ECAI)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="470" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Compounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Bauer</surname></persName>
		</author>
		<editor>Martin Haspelmath, editor, Language Typology and Language Universals. Mouton de Gruyter</editor>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>The Hague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Torch7: A Matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop, number EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DISSECT-DIStributional SEmantics Composition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The</forename><surname>Pham Nghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">General estimation and evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The</forename><surname>Pham Nghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining Immediate Constituents of Compounds in GermaNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><forename type="middle">W</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing</title>
		<meeting>Recent Advances in Natural Language Processing<address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="420" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Obtaining a Better Understanding of Distributional Models of German Derivational Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kisselew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Snajder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015)</title>
		<meeting>the 11th International Conference on Computational Semantics (IWCS 2015)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1517" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning compound noun semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diarmuid´odiarmuid´ Diarmuid´o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Séaghdha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">735</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge. Published as University of Cambridge Computer Laboratory Technical Re</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Processing and querying large web corpora with the COW14 architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Challenges in the Management of Large Corpora (CMLC-3)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A taxonomy, dataset, and classifier for automatic noun compound interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating Linear Models for Compositional Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Massimo Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Korkontzelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Fallucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1263" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
