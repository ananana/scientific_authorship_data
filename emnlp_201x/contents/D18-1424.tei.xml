<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research Sunnyvale</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research Sunnyvale</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research Sunnyvale</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifa</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research Sunnyvale</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3901" to="3910"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3901</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Question generation, the task of automatically creating questions that can be answered by a certain span of text within a given passage, is important for question-answering and conversational systems in digital assistants such as Alexa, Cortana, Google Assistant and Siri. Recent sequence to sequence neural models have outperformed previous rule-based systems. Existing models mainly focused on using one or two sentences as the input. Long text has posed challenges for sequence to sequence neural models in question generation-worse performances were reported if using the whole paragraph (with multiple sentences) as the input. In reality, however, it often requires the whole paragraph as context in order to generate high quality questions. In this paper, we propose a maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation. With sentence-level inputs, our model outperforms previous approaches with either sentence-level or paragraph-level inputs. Furthermore, our model can effectively utilize paragraphs as inputs , pushing the state-of-the-art result from 13.9 to 16.3 (BLEU 4).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question generation (QG), aiming at creating questions from natural language text, e.g. a sen- tence or paragraph, is an important area in natural language processing (NLP). It is receiving increas- ing interests in recent years from both industrial and academic communities, due to the booming of Question-and-Answer (QnA) and conversation systems, including Alexa, Cortana, Google Assis- tant and Siri, the advancement of QnA or machine comprehension technologies together with the re- leases of datasets like SQuAD ( <ref type="bibr" target="#b21">Rajpurkar et al., 2016)</ref> and MS MARCO ( <ref type="bibr" target="#b17">Nguyen et al., 2016)</ref>, and the success of language generation technolo- gies for tasks like machine translation ( <ref type="bibr" target="#b33">Wu et al., 2016</ref>) and text summarization ( <ref type="bibr" target="#b23">See et al., 2017</ref>) in NLP. A conversational system can be proactive by asking the user questions <ref type="bibr" target="#b25">(Shum et al., 2018)</ref>, while a QnA system can benefit from a large scale question-answering corpus which can be created by an automated QG system ( . Education is another key application where QG can help with reading comprehension <ref type="bibr" target="#b8">(Heilman and Smith, 2010)</ref>.</p><p>In NLP, QG has been mainly tackled by two ap- proaches: 1) rule-based approach, e.g. <ref type="bibr" target="#b8">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b15">Mazidi and Nielsen, 2014;</ref><ref type="bibr" target="#b11">Labutov et al., 2015)</ref> 2) neural QG approach: end-to- end training a neural network using the sequence to sequence (also called encoder-decoder) frame- work, e.g. ( <ref type="bibr" target="#b3">Du et al., 2017;</ref><ref type="bibr" target="#b26">Song et al., 2017;</ref>. In this paper, we adopt the second approach.</p><p>More specifically, we focus on an answer-aware QG problem, which takes a passage and an answer as inputs, and generates a question that targets the given answer. It is also assumed the answer is comprised of certain spans of the text from the given passage. This is the exact setting of SQuAD, and similar problems have been addressed in, e.g. ( <ref type="bibr" target="#b31">Wang et al., 2017a)</ref>.</p><p>A paragraph often contains much richer con- text than a sentence, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. ( <ref type="bibr" target="#b3">Du et al., 2017)</ref> pointed out that about 20% questions in SQuAD require paragraph-level information to be asked and using the whole paragraph can im- prove QG performance on those questions. How- ever, a paragraph can contain irrelevant informa- tion w.r.t. the answer for generating the question. The challenge is thus how to effectively utilize rel- evant information at paragraph-level for QG. Ex- isting neural QG works conducted on SQuAD use 1 Paragraph: carolina suffered a major setback when thomas davis , an 11- year veteran who had already overcome three acl tears in his career , went down with a broken arm in the nfc championship game . despite this , he insisted he would still find a way to play in the super bowl . his prediction turned out to be accurate Human generated: what game did thomas davis say he would play in , despite breaking a bone earlier on ? Sentence-level QG: what sports game did spielberg decide to play in ? Paragraph-level QG: what competition did thomas davis think he would play in ?</p><p>2 Paragraph: walt disney and his brother roy contacted goldenson at the end of 1953 for abc to agree to finance part of the disneyland project in exchange for producing a television program for the network . walt wanted abc to invest $ 500,000 and accrued a guarantee of $ 4.5 million in addi- tional loans , a third of the budget intended for the park . around 1954 , abc agreed to finance disneyland in exchange for the right to broadcast a new sunday night program , disneyland , which debuted on the network on october <ref type="bibr">27 , 1954</ref> as the first of many anthology television programs that disney would broadcast over the course of the next 50 years Human generated: how much did walt disney want abc to invest in dis- neyland ? Sentence-level QG: how much money did walt wanted to invest ? Paragraph-level QG: how much money did walt wanted to invest in 1953 ? 4 Paragraph: the victoria and albert museum ( often abbreviated as the v &amp; a ) , london , is the world 's largest museum of decorative arts and design , housing a permanent collection of over 4.5 million objects . it was founded in 1852 and named after queen victoria and prince albert. the v &amp; a is located in the brompton district of the royal borough of kens- ington and chelsea , in an area that has become known as " albertopolis " because of its association with prince albert , the albert memorial and the major cultural institutions with which he was associated . these include the natural history museum , the science museum and the royal albert hall . the museum is a non-departmental public body sponsored by the department for culture , media and sport . like other national british museums , en- trance to the museum has been free since 2001</p><p>Human generated: when was the victoria and albert museum founded ? Sentence-level QG: when was prince albert and prince albert founded ? Paragraph-level QG: when was the victoria and albert museum founded ? In this paper, we extend previous sequence to sequence attention model with a maxout pointer mechanism and a gated self-attention encoder which outperforms existing neural QG approaches with either sentence or paragraph as inputs. Fur- thermore, with paragraph-level inputs, it outper- forms the results of previous approaches with sentence-level inputs, improving state-of-the-art result from 13.9 to 16.3 (BLEU 4). This is the first model that demonstrates large improvement with paragraph as input over sentence as input.</p><p>In addition, our model is more concise compared to most of existing ones, e.g. ( <ref type="bibr" target="#b26">Song et al., 2017)</ref>. Techniques like incorporating rich features (  and policy gradi- ent ( <ref type="bibr" target="#b26">Song et al., 2017;</ref> are or- thogonal to ours and can be leveraged for further performance improvement in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>We use P and A to represent input passage and an- swer respectively, and use Q to represent the gen- erated question. "Passage" in this section can rep- resent either a sentence or a paragraph. The task is to find ¯ Q that:</p><formula xml:id="formula_0">¯ Q = argmax Q P rob(Q|P, A)</formula><p>where passage is comprised of sequence of words:P = {x t } M t=1 , answer A must be sub spans of the passage. Words generated in Q = {y t } K t=1 are either from the input passage, {x t } M t=1 , or from a vocabulary V . <ref type="figure">Figure 2</ref> illustrates the end-to-end structure of our model proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage and Answer Encoding</head><p>Different types of encoders are designed for vari- ous domains <ref type="bibr" target="#b1">(Chung et al., 2014;</ref><ref type="bibr" target="#b10">Hochreiter and Schmidhuber, 1997</ref>). We are agnostic to the form of the encoder and simply use recurrent neural net- work (RNN) to present the encoding process:</p><formula xml:id="formula_1">u t = RN N E (u t−1 , [e t , m t ])<label>(1)</label></formula><p>Answer Tagging. In Eq. 1, u t represents the RNN hidden state at time step t, e t is the word embedding representation of word x t in passage P . m t is the meta-word representation of whether word x t is in or outside the answer. <ref type="bibr">[a, b]</ref> repre- sents the concatenation of vector a and b. We call this approach answer tagging which is similar to the techniques in ( . For applications, it is essential to be able to generate question that is coherent to an answer.  <ref type="figure">Figure 2</ref>: End-to-end diagram for the model with answer tagging, gated self-attention and maxout pointer mechanism.</p><p>If RN N E is bi-directional, u is the concate- nated representation of the forward and backward passes:</p><formula xml:id="formula_2">U = {[ − → u t , ← − u t ]} M t=1</formula><p>. Gated Self-attention. Our gated self-attention mechanism is designed to aggregate information from the whole passage and embed intra-passage dependency to refine the encoded passage-answer representation at every time step. It has two steps: 1) taking encoded passage-answer representation u as input and conducting matching against itself to compute self matching representation; (Wang et al., 2017b) 2) combining the input with self matching representation using a feature fusion gate ( <ref type="bibr" target="#b5">Gong and Bowman, 2017)</ref>.</p><formula xml:id="formula_3">a s t = sof tmax(U T W s u t ) (2) s t = U · a s t (3)</formula><p>Step 1. In Eq. 2, W s is a trainable weight ma- trix. In Eq. 3, s t is the weighted sum of all words' encoded representation in passage based on their corresponding matching strength to current word at t. s = {s t } M t=1 is the final self matching repre- sentation.</p><formula xml:id="formula_4">f t = tanh(W f [u t , s t ]) (4) g t = sigmoid(W g [u t , s t ]) (5) ˆ u t = g t f t + (1 − g t ) u t (6)</formula><p>Step 2. The self matching representation s t is combined with original passage-answer represen- tation u t as the new self matching enhanced rep- resentation f t , Eq. 4. A learnable gate vector g t , Eq. 5, chooses the information between the orig- inal passage-answer representation and the new self matching enhanced representation to form the final encoded passage-answer representationûrepresentationˆrepresentationû t , Eq. 6, where is the element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoding with Attention and Maxout Pointer</head><p>In the decoding stage, the decoder is another RNN that generates words sequentially conditioned on the encoded input representation and the previ- ously decoded words.</p><formula xml:id="formula_5">d t = RN N D (d t−1 , y t−1 ) (7) p(y t |{y &lt;t }) = sof tmax(W V d t )<label>(8)</label></formula><p>In Eq. 7, d t represents the hidden state of the RNN at time t where d 0 is passed from the final hidden state of the encoder. y t stands for the word gen- erated at time t. The bold font y t is used to rep- resent y t 's corresponding word embedding repre- sentation. In Eq. 8, first an affine layer projects d t to a space with vocabulary-size dimensions, then a sof tmax layer computes a probability distribu- tion over all words in a fixed vocabulary V . W V is a trainable weight matrix. Attention. Attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b14">Luong et al., 2015)</ref> has been used to improve sequence to sequence models' perfor- mance and has became a default setting for many applications.</p><p>We use Luong attention mechanism to compute raw attention scores r t , Eq. 9. An attention layer, Eq. 12 is applied above the concatenation of de- coder state d t and the attention context vector c t and its output is used as the new decoder state.</p><formula xml:id="formula_6">r t = ˆ U T W a d t<label>(9)</label></formula><formula xml:id="formula_7">a d t = sof tmax(r t )<label>(10)</label></formula><formula xml:id="formula_8">c t = ˆ U · a d t (11) ˆ d t = tanh(W b [d t , c t ])<label>(12)</label></formula><p>Copy/Pointer. Copy mechanism (Gu et al., 2016) or pointer network ( <ref type="bibr" target="#b23">See et al., 2017;</ref><ref type="bibr" target="#b30">Vinyals et al., 2015</ref>) was introduced to allow both copy- ing words from input via pointing, and generating words from a predefined vocabulary during decod- ing.</p><p>Similar to ( <ref type="bibr" target="#b7">Gu et al., 2016)</ref>, our pointer mecha- nism directly leverages raw attention scores r t = {r t,k } M k=1 over the input sequence which has a vo- cabulary of χ. Words at every time step (a pointer) are treated as unique copy targets and the final score on one word is calculated as the sum of all scores pointing to the same word, Eq. 13, where x k and y t stand for word vocabulary indices of the kth word in input and the tth word in decoded sequence respectively. The scores of the nonoc- curence words are set to negative infinity which will be masked out by the downstream sof tmax function.</p><formula xml:id="formula_9">sc copy (y t ) =      k,where x k =yt r t,k , y t ∈ χ − inf, otherwise<label>(13)</label></formula><p>We then concatenate sc copy t with the gener- ative scores (from Eq. 8 before sof tmax),</p><formula xml:id="formula_10">[sc gen t , sc copy t ]</formula><p>, which has dimension: |V | + |χ|. Then we perform sof tmax on the concatenated vectors and sum up the probabilities pointing to same words. Taking sof tmax on the concate- nated score vector enforces copy and generative modes to compete with each other due to the shared normalization denominator. Another pop- ular solution is to do sof tmax independently to the scores from each mode, and then combine their output probabilities with a dynamic weight which is generated by a trainable network ( <ref type="bibr" target="#b23">See et al., 2017)</ref>. We have tested both and didn't find sig- nificant difference in terms of accuracy on our QG task. We choose the former copy approach mainly because it doesn't add extra trainable parameters.</p><p>Maxout Pointer. Despite the outstanding per- formance of existing copy/pointer mechanisms, we observed that repeated occurrence of words in the input sequence tends to cause repetitions in output sequence, especially when the input se- quence is long, e.g. a paragraph. This issue exac- erbates the repetition problem which has already been commonly observed in sequence to sequence models ( <ref type="bibr" target="#b29">Tu et al., 2016;</ref><ref type="bibr" target="#b23">See et al., 2017)</ref>. In this paper, we propose a new maxout pointer mecha- nism to address this issue and improve the metrics for QG task. Related works ( <ref type="bibr" target="#b6">Goodfellow et al., 2013</ref>) have explored MLP maxout with dropout.</p><p>Instead of combining all the scores to calcu- late the probability, We limit the magnitude of scores of repeated words to their maximum value, as shown in Eq. 14. The rest remains the same as in the previous copy mechanism.</p><formula xml:id="formula_11">sc copy (y t ) = max k,where x k =yt r t,k , y t ∈ χ − inf, otherwise<label>(14)</label></formula><p>3 Experiments</p><p>In our experiments, we study the proposed model on the QG task on SQuAD ( <ref type="bibr" target="#b21">Rajpurkar et al., 2016)</ref> and MS MARCO ( <ref type="bibr" target="#b17">Nguyen et al., 2016</ref>) dataset, demonstrate the performance of proposed compo- nents on both sentence and paragraph inputs, and compare the model with existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD</head><p>The SQuAD dataset contains 536 Wikipedia arti- cles and more than 100K questions posed about the articles by crowd-workers. Answers are also  <ref type="table">Table 1</ref>: Performance of our models on Split1 with both sentence-level input and paragraph-level input.</p><note type="other">BLEU 1 BLEU 2 BLEU 3 BLEU 4 METEOR ROUGE-L Model Sen. Par. Sen. Par. Sen. Par. Sen. Par. Sen. Par. Sen</note><p>Sen. means sentence, while Par. means paragraph.</p><p>provided to the questions, which are spans of to- kens in the articles. Following ( <ref type="bibr" target="#b3">Du et al., 2017;</ref><ref type="bibr" target="#b26">Song et al., 2017)</ref>, our experiments are conducted using the accessible part of SQuAD: train and de- velopment (dev*) sets. To be able to directly com- pare with their works, we adopt two types of data split: 1) Split1: similar to ( <ref type="bibr" target="#b3">Du et al., 2017)</ref>, we use dev* set as test set, and split train set into train and dev sets randomly with ratio 90%-10%. The split is done at article level. However, we keep all sam- ples instead of only keeping the sentence-question pairs that have at least one non-stop-word in com- mon (with 6.7% pairs dropped) as in ( <ref type="bibr" target="#b3">Du et al., 2017)</ref>. This makes our dataset harder for training and evaluation. 2) Split2: similar to ( , we split dev* set into dev and test sets ran- domly with ratio 50%-50%. The split is done at sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS MARCO</head><p>MS MARCO datasets contains 100,000 queries with corresponding answers and passages. All questions are sampled from real anonymized user queries and context passages are extracted from real web documents. We picked a subset of MS MARCO data where answers are sub-spans within the passages, and use dev set as test set (7k), and split train set with ratio 90%-10% into train (51k) and dev (6k) sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We used 2 layers LSTM as the RNN cell for both encoding and decoding. For encoding, bi- directional LSTM was used. The cell hidden size was 600. Dropout with probability 0.3 was applied between vertical LSTM stacks. For word em- bedding, we used pre-trained GloVe word vectors with 300 dimensions ( <ref type="bibr" target="#b19">Pennington et al., 2014)</ref>, and froze them during training. Dimension of an- swer tagging meta-word embedding was 3. Both encoder and decoder shared the same vocabulary of the most frequent 45k GloVe words. For op- timization, we used SGD with momentum <ref type="bibr" target="#b20">(Qian, 1999;</ref><ref type="bibr" target="#b16">Nesterov, 1983)</ref>. Learning rate was initially set to 0.1 and halved since epoch 8 at every 2 epochs afterwards. Models were totally trained with 20 epochs. The mini-batch size for param- eter update was 64. After training, we looked at the 4 models with lowest perplexities and selected the one which used the most number of epochs as final model. During decoding for prediction, we used beam search with the beam size of 10, and stopped decoding when every beam in the stack generates the EOS token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We conduct automatic evaluation with metrics: BLEU 1, BLEU 2, BLEU 3, BLEU 4 ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>), METEOR <ref type="bibr" target="#b2">(Denkowski and Lavie, 2014</ref>) and ROUGE-L <ref type="bibr" target="#b13">(Lin, 2004)</ref>, and use eval- uation package released by <ref type="bibr" target="#b24">(Sharma et al., 2017)</ref> to compute them. <ref type="table">Table 1</ref> shows evaluation results for different models on SQuAD Split1. Results with both sentence-level and paragraph-level inputs are in- cluded. Similar results also have been observed on SQuAD Split2. The definitions of the models under comparison are: s2s: basic sequence to sequence model s2s-a: s2s + attention mechanism s2s-a-at: s2s-a + answer tagging s2s-a-at-cp: s2s-a-at + copy mechanism s2s-a-at-mp: s2s-a-at + maxout pointer mechanism s2s-a-at-mp-gsa: s2s-a-at-mp + gated self-attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism</head><p>s2s-a vs. s2s: we can see attention brings in large improvement on both sentence and paragraph in- puts. The lower performance on paragraph indi- cates the challenge of encoding paragraph-level information.</p><p>Answer Tagging s2s-a-at vs. s2s-a: answer tagging dramatically boosts the performance, which confirms the im- portance of answer-aware QG: to generate good question, we need to control/learn which part of the context the generated question is asking about. More importantly, answer tagging clearly reduces the gap between sentence and paragraph inputs, which could be explained with: by providing guid- ance on answer words, we can make the model learn to neglect noise when processing a long con- text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Copy Mechanism</head><p>s2s-a-at-cp vs. s2s-a-at: as expected, copy mecha- nism further improves the performance on the QG task. ( <ref type="bibr" target="#b3">Du et al., 2017</ref>) pointed out most of the sentence-question pairs in SQuAD have over 50% overlaps in non-stop-words. Our results prove that sequence to sequence models with copy mecha- nism can very well learn when to generate a word and when to copy one from input on such QG task. More interestingly, the performance is lower when paragraph is given as input than sentence as input. The gap, again, reveals the challenge of leveraging longer context. We found that, when paragraph is given, the model tends to generate more repetitive words, and those words (often entities/concepts) usually appear multiple times in the context, <ref type="figure">Fig- ure 3</ref>. The repetition issue can also be seen for sentence input, but it is more severe for paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maxout Pointer</head><p>s2s-a-at-mp vs. s2s-a-at-cp: Maxout pointer is de- signed to resolve the repetition issue brought by the basic copy mechanism, for example <ref type="figure">Figure 3</ref>. The maxout pointer mechanism outperforms the basic copy mechanism in all metrics. Moreover, the effectiveness of maxout pointer is more signif- icant when paragraph is given as the model input, as it reverses the performance gap between models Paragraph: a problem is regarded as inherently difficult if its solution requires significant resources , whatever the algorithm used . the theory formalizes this intuition , by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them , such as time and storage . other complexity measures are also used , such as the amount of communication ( used in communication complexity ) , the number of gates in a circuit ( used in circuit complexity ) and the number of processors ( used in parallel computing ) . one of the roles of computational complexity theory is to determine the practical limits on what computers can and can not do Human generated: what unit is measured to determine circuit complexity ? Basic copy QG: what is an example of a circuit complexity in complexity complexity ? Maxout Pointer QG: what is another name for circuit complexity ? trained with sentence and paragraph inputs, <ref type="table">Table  1</ref>.</p><p>To demonstrate that maxout pointer reduces repetitions in generated questions, we present the word duplication rates in generated questions for various models in <ref type="figure">Figure 4</ref>. Word duplication rate was computed by counting the number of words which appear more than once, and then taking a ratio of them over the total word counts. As shown in <ref type="figure">Figure 4</ref>, both the attention mechanism and the basic copy mechanism introduce more repetitions, although they improve overall accuracy according to <ref type="table">Table 1</ref>. For models trained with paragraph in- puts, where the duplication rates are much higher, maxout pointer reduces the duplication rates to half of their values in the basic copy and to the same level as model trained with sentence inputs.</p><p>Such repetition issue was also observed in other sequence to sequence applications, e.g. ( <ref type="bibr" target="#b23">See et al., 2017)</ref> who proposed a coverage model and cov- erage loss to resolve this issue. We implemented and tested their approach on our QG task. Even though the duplication ratio dropped as expected, we observed a slight decline in the accuracy when coverage loss was added.  <ref type="table">carolina  suffered  a  major  setback  when  thomas  davis  ,  an  11-year  veteran  who  had  already  overcome  three  acl  tears  in  his  career  ,  went  down  with  a  broken  arm  in  the  nfc  championship</ref> game</p><note type="other">. despite this , he insisted he would still find a way to play in the super bowl . his prediction turned out to be accurate Ground-truth: what game did thomas davis say he would play in , despite breaking a bone earlier on ?</note><p>QG: what competition did thomas davis think he would play in ? Gated Self-attention s2s-a-at-mp-gsa vs. s2s-a-at-mp: the results demonstrate the effectiveness of gated self- attention, in particular, when working with para- graph inputs. This is the first time, as we know, taking paragraph as input is better than sentence for neural QG tasks. The observation is consistent across all metrics. Gated self-attention helps re- fine encoded context by fusing important informa- tion with the context's self representation properly, especially when the context is long. To better understand how gated self-attention works, we visualize the self alignment vectors at each time step of the encoded sequence for one ex- ample, in <ref type="figure" target="#fig_2">Figure 5</ref>. This example corresponds to the example 1 in <ref type="figure" target="#fig_0">Figure 1</ref>. We can see the align- ments distribution concentrates near the answer sequence and the most relevant context: "thomas davis" in this example. Such alignments in turn would help to promote the most valuable informa- tion for decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beam Search</head><p>Beam search is commonly used for decoding for predictions. Existing neural QG works, e.g. ( <ref type="bibr" target="#b3">Du et al., 2017;</ref>, evaluated their models with beam search decoding. However, so far, none of them have reported the comparison be- tween beam search decoding with greedy decod- ing. In this paper, we give such comparison for our best model: s2s-a-at-mp-gsa with both sentence  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Existing Neural Question Generation Works</head><p>On SQuAD dataset, we compare the BLEU, ME- TEOR, ROUGE-L scores of our best model, s2s- a-at-mp-gsa, with the numbers in the existing works in  ) in <ref type="table" target="#tab_9">Table 4</ref>.</p><p>Our model with maxout pointer and gated self- attention achieves the state-of-the-art results in QG. Note that all those existing works in SQuAD encoded only sentence-level information, the re- sults from our model surpass them on the same sentence input while achieving much higher num- bers when working with paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we present some examples for which paragraph-level information is needed to ask good/correct questions. Generated questions from model s2s-a-ct-mp-gsa are also presented for both sentence and paragraph inputs. Those exam- ples demonstrate that generated questions contain richer information when paragraphs are provided   instead of sentences. In example 1 and 3, name "thomas davis" and "percy shelley" appear in the paragraphs not the sentences contain the answers.</p><p>In example 2, paragraph-level QG can gener- ate richer description "in 1953" from the para- graph, although human generated is "disneyland". Both generated questions lack one relative impor- tant piece of information "want abc to invest".</p><p>In example 4, paragraph-level QG correctly identifies "it" is referring to the museum which is out of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>QG has been mainly tackled with two types of ap- proaches. One is built on top of heuristic rules that creates questions with manually constructed tem- plate and ranks the generated results, e.g. <ref type="bibr" target="#b8">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b15">Mazidi and Nielsen, 2014;</ref><ref type="bibr" target="#b11">Labutov et al., 2015)</ref>. Those approaches heavily depend on human effort, which makes them hard to scale up to many domains. The other one, which is becoming increasingly popular, is to train an end-to-end neural network from scratch by using sequence to sequence or encoder-decoder frame- work, e.g. ( <ref type="bibr" target="#b3">Du et al., 2017;</ref><ref type="bibr" target="#b26">Song et al., 2017;</ref>). The second one is more related to us, so we will focus on describing those approaches.</p><p>( <ref type="bibr" target="#b3">Du et al., 2017</ref>) pioneered the work of auto- matic QG using an end-to-end trainable sequence to sequence neural model. Automatic and human evaluation results showed that their system outper- formed the previous rule-based systems <ref type="bibr" target="#b8">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b22">Rus et al., 2010)</ref>. However, in their study, there was no control about which part of the passage the generated question was asking about.</p><p>Answer-aware sequence to sequence neural QG systems (  encoded answer loca- tion information using an annotation vector cor- responding to the answer word positions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we proposed a new sequence to sequence network which contains a gated self-attention encoder and a maxout pointer decoder to address the answer-aware QG problem for long text input. We demonstrated the model can ef- fectively utilize paragraph-level context, and out- performed the results with sentence-level con- text. The new model exceeded state-of-the-art ap- proaches with either paragraph or sentence inputs.</p><p>We would like to discuss some potential chal- lenges when applying the QG model in practice. 1) Answer spans aren't provided as input. One straight-forward method is to extract entities or noun phrases and use them as potential answer spans. A neural entity selection model can also be leveraged to extract good answer candidates to im- prove the precision as proposed in ). 2) An input passage does not contain any eligible answers. In such case, we do not ex- pect the model to output valid questions. We could remove questions with low generation probability, while a better approach could be running entity se- lection or quality detection model before question generation step to eliminate ineligible passages. 3) An answer could be shared by different questions. We could output multiple questions using beam search. However, beam search does not guaran- tee to output diversified candidates. We would need to explicitly model diversity among candi- dates during generation, for example, leveraging the approach described in ( <ref type="bibr" target="#b12">Li and Jurafsky, 2016)</ref>.</p><p>Our future work lands in the following direc- tions: incorporate rich features, such as POS and entity, in input passages; directly optimize sequence-level metrics with policy gradient; re- lax the constraint on answer to accept abstractive answers; jointly model question generation and question answering; ask multiple questions simul- taneously with diverse perspectives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples where paragraph-level information is required to ask right questions. Sentences contain answers are in italic font, while answers are underscored. QG results are generated by model s2s-a-ct-mp-gsa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 Figure 4 :</head><label>34</label><figDesc>Figure 3: Example for maxout pointer vs. basic copy/pointer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Self-attention alignments map: each row represents an alignment vector of self-attention.</figDesc><graphic url="image-3.png" coords="7,95.79,76.26,193.46,193.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(Zhou et al., 2017) utilized rich features of the passage including answer positions. (Subramanian et al., 2017) deployed a two-stage neural model that de- tects key phrases and subsequently generates ques- tions conditioned on them. (Yuan et al., 2017) combined supervised and reinforcement learning in the training of their model using policy gradi- ent techniques to maximize several rewards that measure question quality. Instead of using an an- notation vector to tag the answer locations, (Song et al., 2017) proposed a unified framework for QG and question answering by encoding both the answer and the passage with a multi-perspective matching mechanism. (Tang et al., 2017; Wang et al., 2017a) proposed joint models to address QG and question answer- ing together. (Duan et al., 2017) conducted QG for improving question answering. Due to the mixed objectives including question answering, their ap- proaches' performance on QG were lower than the state-of-the-art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of beam search and greedy 
decodings for model s2s-a-ct-mp-gsa on Split1. 

and paragraph inputs in Table 2. We can clearly 
see beam search decoding boosts all metrics for 
both sentence and paragraph inputs. The effective-
ness of beam search has also been demonstrated 
for other tasks, like neural machine translation in 
(Wu et al., 2016). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Comparison with (Du et al., 
2017) and (Song et al., 2017) is conducted on 
SQuAD data Split1, while comparison with (Zhou 
et al., 2017) and (Song et al., 2017) is conducted 
on data SQuAD split2. Because (Song et al., 2017) 
had results on both splits, we compare with both of 
them. 
On MS MARCO dataset, we compare the 
BLEU 4 scores reported by </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of results on SQuAD dataset. 

Model 
BLEU 4 

MSMARCO 

(Du et al., 2017) 
10.46 
(Duan et al., 2017) 
11.46 
ours, sentence 
16.02 
ours, paragraph 
17.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of results on MSMARCO 
dataset. 

</table></figure>

			<note place="foot">t e t m t s t u ̂ t LSTM u t e t m t s t u ̂ t LSTM u t e t m t s t u ̂ t</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor universal: language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question generation for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ruminating reader: Reasoning with gated multi-hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Good question! statistical ranking for question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep questions without deep understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">889898</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mutual information and diverse decoding improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00372</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<editor>Stan Szpakowicz Marie-Francine Moens</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic considerations in automatic question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Mazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">321326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence o (1/kˆ2kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady AN USSR</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>30th Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The first question generation shared task evaluation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wyse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference</title>
		<meeting>the 6th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<idno>abs/1706.09799</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From eliza to xiaoice: challenges and opportunities with social chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A unified auery-based generative model for question generation and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01058</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural models for key phrase detection and question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Question answering and question generation as dual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02027</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for computation linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A joint model for question answering and question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Xingdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Eric) Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to generate natural language workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02012</idno>
		<title level="m">Machine comprehension by text-to-text neural question generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural question generation from text: A preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National CCF Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="662" to="671" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
