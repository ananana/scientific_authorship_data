<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-Training for Topic Classification of Scholarly Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
							<email>ccaragea@unt.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin</forename><surname>Bulgarov</surname></persName>
							<email>FlorinBulgarov@my.unt.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Co-Training for Topic Classification of Scholarly Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With the exponential growth of scholarly data during the past few years, effective methods for topic classification are greatly needed. Current approaches usually require large amounts of expensive labeled data in order to make accurate predictions. In this paper, we posit that, in addition to a research article&apos;s textual content, its citation network also contains valuable information. We describe a co-training approach that uses the text and citation information of a research article as two different views to predict the topic of an article. We show that this method improves significantly over the individual classifiers, while also bringing a substantial reduction in the amount of labeled data required for training accurate classifiers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As science advances, scientists around the world continue to produce a large number of research ar- ticles, which provide the technological basis for worldwide dissemination of scientific discoveries. Online digital libraries such as Google Scholar, CiteSeer x , and PubMed store and index millions of such research articles and their metadata, and make it easier for researchers to search for scien- tific information. These libraries require effective and efficient methods for topic classification of re- search articles in order to facilitate the retrieval of content that is tailored to the interests of specific individuals or groups. Supervised approaches for topic classification of research articles have been developed, which generally use either the content of the articles ), or take into account the citation relation between research ar- ticles ( <ref type="bibr" target="#b18">Lu and Getoor, 2003)</ref>.</p><p>To be successful, these supervised approaches assume the availability of large amounts of labeled data, which require intensive human labeling ef- fort. In this paper, we explore a semi-supervised approach that can exploit large amounts of un- labeled data together with small amounts of la- beled data for accurate topic classification of re- search articles, while minimizing the human ef- fort required for data labeling. In the scholarly do- main, research articles (or papers) are highly inter- connected in giant citation networks, in which pa- pers cite or are cited by other papers. We posit that, in addition to a document's textual content and its local neighborhood in the citation network, other information exists that has the potential to improve topic classification. For example, in a citation network, information flows from one pa- per to another via the citation relation ( <ref type="bibr" target="#b26">Shi et al., 2010)</ref>. This information flow and the topical influ- ence of one paper on another are specifically cap- tured by means of citation contexts, i.e., short text segments surrounding a citation's mention.</p><p>These contexts are not arbitrary, but they of- ten serve as brief summaries of a cited paper. We therefore hypothesize that these micro-summaries can be successfully used as an independent view of a research article in a co-training framework to reduce the amount of labeled data needed for the task of topic classification.</p><p>The idea of using terms from citation contexts stems from the analysis of hyperlinks and the graph structure of the Web, which are instrumen- tal in Web search ( <ref type="bibr" target="#b19">Manning et al., 2008</ref>). Many search engines follow the intuition that the an- chor text pointing to a page is a good descrip- tor of its content, and thus anchor text terms are used as additional index terms for a target web- page. The use of links and anchor text was thoroughly researched for information retrieval <ref type="bibr" target="#b14">(Koolen and Kamps, 2010</ref>), broadening a user's search ( <ref type="bibr" target="#b6">Chakrabarti et al., 1998)</ref>, query refinement ( <ref type="bibr" target="#b15">Kraft and Zien, 2004)</ref>, and enriching document representations ( <ref type="bibr" target="#b21">Metzler et al., 2009)</ref>. <ref type="bibr" target="#b3">Blum and Mitchell (1998)</ref> introduced the co-training algo- rithm using hyperlinks and anchor text as a sec- ond, independent view of the data for classifying webpages, in addition to a webpage content.</p><p>Contributions and Organization. We present a co-training approach to topic classification of re- search papers that effectively incorporates infor- mation from a citation network, in addition to the information contained in each paper. The result of this classification task will aid indexing of docu- ments in digital libraries, and hence, will lead to improved organization, search, retrieval, and rec- ommendation of scientific documents. Our contri- butions are as follows:</p><p>• We propose the use of citation contexts as an additional view in a co-training approach, which results in high accuracy classifiers. To our knowledge, this has not been addressed in the literature.</p><p>• We show experimentally that our co-training classifiers significantly outperform: (1) su- pervised classifiers trained using either con- tent or citation contexts independently, for the same fraction of labeled data; and (2) sev- eral other semi-supervised classifiers, trained on the same fractions of labeled and unla- beled data as co-training.</p><p>• We also show that using the citation context information available in citation networks, the human effort involved in data labeling for training accurate classifiers can be largely re- duced. Our co-training classifiers trained on a very small sample of labeled data and a large sample of unlabeled data yield accurate topic classification of research articles.</p><p>The rest of the paper is organized as follows. In Section 2, we discuss related work. Section 3 describes our data and its characteristics, followed by the presentation of our proposed co-training ap- proach in Section 4. We present experiments and results in Section 5, and conclude the paper and present future directions of our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We discuss here the most relevant works to our study. A large variety of methods have been pro- posed in the literature with regard to automatic text classification and topic prediction. Differ- ent classifiers have been applied on the Vector Space Model (VSM), in which a document is rep- resented as a vector of words or phrases asso- ciated with their TF-IDF score, i.e. term fre- quency -inverse document frequency <ref type="bibr" target="#b11">Kansheng et al., 2011</ref>). VSM is the most used method due to its simple, efficient and easy to understand implementation. Another widely used model is the Latent Semantic Indexing (LSI) where co-occurrences are analyzed to find seman- tic relationships between words or phrases ( <ref type="bibr" target="#b8">Ganiz et al., 2011</ref>). Moreover, a great range of classifiers were used for this task, includ- ing: Na¨ıveNa¨ıve Bayes ( <ref type="bibr" target="#b17">Lewis and Ringuette, 1994)</ref>, K- nearest neighbors <ref type="bibr" target="#b30">(Yang, 1999)</ref> and Support Vec- tor Machines <ref type="bibr" target="#b10">(Joachims, 1998)</ref>. These techniques, however, all require a large number of labeled doc- uments in order to build accurate classifiers. In contrast, we propose a co-training algorithm that only requires a small amount of labeled data in or- der to make accurate topic classification.</p><p>Semi-supervised methods essentially involve different means of transferring labels from labeled to unlabeled samples in the process of learning a classifier that can generalize well on new un- seen data. Co-training was originally introduced in <ref type="bibr" target="#b3">(Blum and Mitchell, 1998)</ref> where it was used to classify web pages into academic course home page or not. This approach has two views of the data as follows: the content of a web page, and the words found in the anchor text of the hyper- links that point to the web page. <ref type="bibr" target="#b29">Wan (2009)</ref> used co-training for cross-lingual sentiment clas- sification of product reviews, where English and Chinese features were considered as two indepen- dent views of the data. Furthermore, <ref type="bibr" target="#b9">Gollapalli et al. (2013)</ref> used co-training to identify authors' homepages from the current-day university web- sites. The paper presents novel features, extracted from the URL of a page, that were used in con- junction with content features, forming two com- plementary views of the data.</p><p>Citation networks have been used before in other problems. <ref type="bibr" target="#b5">Caragea et al. (2014)</ref> used ci- tation contexts to extract informative features for keyphrase extraction. <ref type="bibr" target="#b18">Lu and Getoor (2003)</ref> pro- posed an approach for document classification that used only citation links, without any textual data from the citation contexts. <ref type="bibr" target="#b24">Ritchie et al. (2006)</ref> used a combination of terms from citation contexts and existing index terms of a paper to improve in- dexing of cited papers. Citation contexts were also used to improve the performance of citation rec- ommendation systems ( <ref type="bibr" target="#b12">Kataria et al., 2010)</ref> and to study author influence in document networks ). Moreover, citation con- texts were used for scientific paper summariza- tion ( <ref type="bibr" target="#b0">Abu-Jbara and Radev, 2011;</ref><ref type="bibr" target="#b23">Qazvinian et al., 2010;</ref><ref type="bibr" target="#b22">Qazvinian and Radev, 2008;</ref><ref type="bibr" target="#b20">Mei and Zhai, 2008;</ref><ref type="bibr" target="#b16">Lehnert et al., 1990</ref>) For example, in <ref type="bibr" target="#b23">Qazvinian et al. (2010)</ref>, a set of important keyphrases is extracted first from the citation con- texts in which the paper to be summarized is cited by other papers and then the "best" subset of sen- tences that contain such keyphrases is returned as the summary. <ref type="bibr" target="#b20">Mei and Zhai (2008)</ref> used informa- tion from citation contexts to determine what sen- tences of a paper are of high impact (as measured by the influence of a target paper on further stud- ies of similar or related topics). These sentences constitute the impact-based summary of the paper.</p><p>Despite the use of citation contexts and anchor text in many information retrieval and natural lan- guage processing tasks, to our knowledge, we are the first to propose the incorporation of citation context information available in citation networks in a co-training framework for topic classification of research papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The dataset used in our experiments is a subset sampled from the CiteSeer x digital library 1 and la- beled by Dr. Lise Getoor's research group at the University of Maryland. This subset was previ- ously used in several studies including <ref type="bibr" target="#b18">(Lu and Getoor, 2003)</ref> and ( <ref type="bibr" target="#b12">Kataria et al., 2010</ref>). The dataset consists of 3186 labeled papers, with each paper being categorized into one of six classes: Agents, Artificial Intelligence (AI), Information Retrieval (IR), Machine Learning (ML), Human- Computer Interaction (HCI) and Databases (DB). For each paper, we acquire the citation contexts directly from CiteSeer x . A citation context is de- fined as a window of n words surrounding a cita- tion mention. We differentiate between cited and citing contexts for a paper as follows: let d be a target paper and C be a citation network such that As expected, we have a higher number of cited contexts than citing contexts. This is due to the page restrictions often imposed to research articles that can limit the number of papers each article can cite. On the other hand, a good research paper can accumulate hundreds of citations, and hence, cited contexts over the years.</p><formula xml:id="formula_0">d ∈ C. A cited context for d is a context in which d is cited by some paper d i in C. A citing context for d is a context in which d is citing some paper d j in C. If</formula><p>Context lengths. In CiteSeer x , citation con- texts have about 50 words on each side of a ci- tation mention. A previous study by <ref type="bibr" target="#b25">Ritchie et al. (2008)</ref> shows that a fixed window length of about 100 words around a citation mention is generally effective for information retrieval tasks. For this reason, we use the contexts provided by CiteSeer x directly. In future, it would be interesting to study more sophisticated approaches to identifying the text that is relevant to a target citation ( <ref type="bibr" target="#b1">Abu-Jbara and Radev, 2012;</ref><ref type="bibr" target="#b28">Teufel, 1999</ref>) and study the in- fluence of context lengths on our task.</p><p>For all experiments, our labeled dataset is split in train, validation and test sets. The validation and test sets have about 200 papers each. We sam- pled another set of papers from the labeled dataset in order to simulate the existence of unlabeled data, with a fixed size of around 2000 papers. The remaining 786 papers are used as labeled training data. Each experiment was repeated 10 times with 10 different random seeds and the results were av- eraged. <ref type="bibr" target="#b3">Blum and Mitchell (1998)</ref> proposed the co- training algorithm in the context of webpage clas- sification. In co-training, the idea is that two clas- sifiers trained on two different views of the data teach one another by re-training each classifier on the data enriched with predicted examples that the other classifier is most confident about. In <ref type="bibr" target="#b3">Blum and Mitchell (1998)</ref>, webpages are represented us- ing two different views: (1) using terms from web- pages' content and (2) using terms from the anchor text of hyperlinks pointing to these pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Co-Training for Topic Classification</head><formula xml:id="formula_1">Algorithm 1 Co-Training Input: L, U , 's' L 1 ← L, L 2 ← L while U = ∅ do Train classifier C 1 on L 1 Train classifier C 2 on L 2 S ← ∅ Move 's' examples from U to S U ← U \S S 1 , S 2 ← GetMostConfidentExamples(S, C 1 , C 2 ) L 1 ← L 1 ∪ S 1 , L 2 ← L 2 ∪ S 2 U ← U ∪ [S\(S 1 ∪ S 2 )] end while Ouput:</formula><p>The combined classifier C of C 1 and C 2</p><p>In this paper, we study the applicability and ex- tension of the co-training algorithm to the task of topic classification of research papers, which are embedded in large citation networks. Here, in ad- dition to the information contained in a paper it- self, citing and cited papers capture different as- pects (e.g., topicality, domain of study, algorithms used) about the target paper ( ), with citation contexts playing an instrumental role. We conjecture that citation contexts, which act as brief summaries about a cited paper, provide im- portant clues in predicting the topicality of a target paper. These clues give rise to the design of our co-training based model for topic classification of research papers. In our model, we use the content of a paper as one view and the citation contexts as another view of our data. In particular, for the con- tent of a paper, we use its title and abstract as it is commonly used in the literature ( <ref type="bibr" target="#b18">Lu and Getoor, 2003)</ref>; for the citation contexts, we use both the cited and citing contexts, as described in the pre- vious section.</p><p>Our co-training procedure is described in Algo- rithm 1. L and U represent the labeled and un- labeled datasets and contain instances from both views. The fractions of the training set are ob- tained from the 786 papers by selecting k% ran- dom examples from each class. For a round of co-training, we train classifiers C 1 and C 2 on the two views. Next, s examples are sampled from the unlabeled data into S, and C 1 , C 2 are used to obtain predictions for these s examples. The GetMostConfidentExamples method is a generic placeholder that stands for a function that deter- mines what examples from S are chosen to be added into training. Finally, at the end of an it- eration, the examples left into S are moved back to U , and the algorithm iterates until there are no more unlabeled examples in U . The final classi- fier C is obtained by combining C 1 and C 2 using the product of their class probability distributions. The class with the highest posterior probability (of the product of the two distributions) is chosen as the predicted class.</p><p>Unlike the original co-training algorithm de- scribed by <ref type="bibr" target="#b3">Blum and Mitchell (1998)</ref> <ref type="bibr">(1998)</ref>, the co-training algorithm moves p highest confidence positive examples and n highest confi- dence negative examples from S to L, where p : n represents the class distribution in the original la- beled training set (i.e., if there are 10 positive ex- amples and 90 negative examples in the labeled set L, then p = 1 positive and n = 9 negative examples are moved to the labeled set at each it- eration of co-training). Unlike, this approach that preserves the class distribution of the original la- beled training set, we move into L all examples that are classified with a confidence above a cer- tain threshold.</p><note type="other">, which tack- led a binary classification task (course vs. non- course page classification), we address a multi- class classification problem, where each example (i.e., research paper) is classified into one of six different classes. Moreover, in Blum and Mitchell</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>First, the proposed method is evaluated on the val- idation set. We first compare it against various supervised and semi-supervised baselines. Next, we report the performance of our co-training algo- rithm under different scenarios, where either cited or citing contexts are used. We also show the most informative words for each classifier. Finally, with the best parameters obtained on the validation set, we report the precision, recall and F1-score, ob- tained by each method, on the test set.</p><p>In experiments, the sample size 's' from Algo- rithm 1 is set to 300, i.e. the number of documents sampled from the unlabeled pool at each iteration; the confidence threshold is set to 0.95, i.e. if both classifiers agree on the class label and have a con- fidence ≥ 0.95, the instance is labeled and moved into the labeled training set. These parameters are estimated on the validation set, but the results are not shown due to space limitation. Evaluation Measures. We report results aver- aged over ten different runs with random splits. For each random split, we return the weighted average precision, recall and F1-score. In all the experiments, we use the Na¨ıveNa¨ıve Bayes Multi- nomial classifier and its Weka implementation 2 , with term-frequencies as feature values. We ex- perimented with both TF and TF-IDF scores, us- ing different classifiers (Support Vector Machine, Na¨ıveNa¨ıve Bayes Multinomial, and simple Na¨ıveNa¨ıve Bayes classifiers), but Naive Bayes Multinomial with TF performed best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Comparisons</head><p>How does co-training compare with supervised learning techniques? In this experiment, we com- pare our co-training method with two supervised baselines: (1) when only document content is used and (2) when only citation contexts are used. <ref type="figure" target="#fig_0">Figure 1</ref> shows the F1-scores achieved using different initial training sizes. We can see that overall, the citation contexts are better at predict- ing the topic of a document compared with the content, outperforming them in 9 out of 10 exper- imental settings. The only exception to this trend is when a small number (5%) of training instances is available, in which case the supervised con- tent view performs better, reaching an F1-score of 0.534. Regardless, the co-training method shows significant improvement over both baselines, in all experiments. Starting with an F1-score of 0.572, it continues to improve its performance as the train- ing percentage is increasing. The maximum F1- score, i.e. 0.742, is reached when 30% of the la- beled training set is used. Note that the difference in performance between co-training and the two supervised baselines is statistically significant for 2 http://www.cs.waikato.ac.nz/ml/weka/ a p value of 0.05.</p><p>A fully supervised baseline that uses 100% of the training set achieves an F1-score of 0.720 (us- ing content) and 0.738 (using citation contexts). In contrast, co-training requires only 15% of the labeled training set to outperform the fully super- vised content baseline and 30% of the training set to outperform the fully supervised citation con- texts baseline. Consequently, using a co-training approach that includes citation contexts as well as the document content can not only increase the performance, but will also significantly reduce the need of expensive labeled instances. <ref type="figure">Figure 2</ref> illustrates the confusion matrices of three experiments: (a) supervised content view, i.e. the title and abstract, (b) supervised citation contexts view, and (c) co-training that uses both views. These experiments use 10% of the training set. Each of the matrices are represented by a heat map, i.e. the redder the color, the higher the value assigned to that position. An accuracy of 1 will be represented by a matrix with red blocks on the main diagonal and white blocks everywhere else. This experiment was performed 10 times with 10 different seeds and the results have been averaged.</p><p>As can be seen, the matrix that uses only titles and abstracts, i.e. left side, is showing the high- est percentage of misclassified documents, classi- fying correctly about 58.8% instances, on average. Using only citation contexts in a supervised frame- work, i.e. center matrix, we reach a higher ac- curacy of 60.7%. The co-training method, which uses the content of the paper and citations as two independent views, significantly increases the av- erage accuracy to 67.3%. This experiment shows that citation contexts are better than titles and ab- stracts at predicting the topic of a document. Fur- thermore, our proposed approach, which uses the content of the paper as well as citation contexts, achieves higher results than each view used sepa- rately. The difference in accuracy is statistically significant across all three experiments for a p value of 0.05.</p><p>Overall, the Agents class seem to be the easiest to classify, reaching an accuracy value of 91.6% when using co-training. On the other hand, the AI class is the hardest to classify. One reason for this is that the AI class contains the lowest num- ber of instances in the dataset. Another can be that the AI class is the most general among all classes and therefore, classifying documents with this la- <ref type="figure">Figure 2</ref>: The accuracy of our method, against two supervised baselines. Left: using titles and abstracts; Center: using citation contexts; Right: using co-training. bel can be a difficult task even for a human. Other common misclassifications occur between classes like HCI and Agents, ML and IR or AI and ML, due to their similarity.</p><p>How does our co-training method compare with other supervised approaches? In this exper- iment, we compare the performance of co-training against two other methods: early and late fusion. In early fusion, the feature vectors of the two views are concatenated, creating a single represen- tation of the data. In contrast, late fusion trains two separate classifiers and then combines them by taking the label with the highest confidence. <ref type="figure" target="#fig_1">Figure 3</ref> shows this comparison over differ- ent training sizes. The results show that the co- training method is more accurate than all others, performing best in all 10 experimental settings. Late fusion has an overall lower performance com- pared with co-training, but is in a tight correlation with it. On the other hand, early fusion achieves the lowest F1-score across the experiments. The reported results are statistically significant at p value of 0.05, when the training percentage is be- tween 5 and 35. Therefore, we can say that train- ing two separate classifiers, one of each view, yields higher performance compared with train- ing a single classifier that incorporates both views. Moreover, using a co-training approach that incor- porates information from unlabeled data into the model, will help the two classifiers increase their confidences and minimize the error rate.</p><p>How does co-training compare with semi- supervised methods?</p><p>Here, we present re- sults comparing co-training with two other well- known semi-supervised techniques: self-training and Na¨ıveNa¨ıve Bayes with Expectation Maximization.</p><p>Self-Training. First, we show results of the com- parison of co-training with two variations of self- training: (1) self-training using only document content, and (2) self-training using only citation contexts. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results of this exper- iment. Self-training is similar to co-training, ex- cept that it uses only one view of the data ( <ref type="bibr" target="#b32">Zhu, 2005)</ref>. Self-training parameters, e.g., sample size 's' or number of iterations, are estimated as in co- training.</p><p>Although the document content version of self- training outperforms co-training when using 5% of the training instances, we can see that overall, there is a significant difference in terms of F1- score values in the favor of co-training. In 9 out of 10 experiments, our co-training approach is su- perior to both self-training methods. The results are statistically significant across all experimental setups for a p value of 0.05.</p><p>Expectation Maximization. <ref type="figure" target="#fig_3">Figure 5</ref> shows the F1-score values obtained after running NBM with EM with the same training, unlabeled and test sets. The EM algorithm uses the same classifier, i.e. NBM, and the weight for each unlabeled instance is set to 1, as this setting achieved the highest re- sults. Two different experiments were performed using EM: (1) using only document content, and (2) using only citation contexts. As can be seen in the figure, overall, the co-training approach signif- icantly outperforms both variations of EM. How- ever, the co-training method falls short when using 5% of the training instances, where EM Content and EM Citations methods are achieving higher F1-score values. Nonetheless, both EM variations tend to achieve an F1-score value below or equal to 0.710, whereas co-training reaches performance values of 0.74 or higher. Again, the comparison results between co-training and both variations of EM are statistically significant for training sizes between 10% and 50%, for a p value of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Using Different Citation Context Types</head><p>Which of the two types of citation contexts (cited or citing) help the task of topic classification more and how does co-training perform in the absence of either one? The answer to this ques- tion is important as there are cases in which cita- tion contexts are not readily available. One fre- quently encountered example includes newly pub- lished research papers that have no cited contexts. In this case, it is important to know how our method performs when we only have one type of citation contexts. <ref type="figure" target="#fig_4">Figure 6</ref> shows the difference in performance when using: (1) only cited contexts, (2) only citing contexts, and (3) both context types. Note that the content view remains the same across all three experiments.</p><p>The plot is showing that citing contexts are bringing in a significantly higher margin of knowl- edge compared with cited contexts. This is consis- tent over different training set sizes, as shown in the figure, with a more prominent impact when a small training size is used, i.e. 5-30%. The fact that the citing contexts achieve higher F1-score than cited contexts is consistent with the intuition that when citing a paper y, an author generally summarizes the main ideas from y using impor- tant words from a target paper x, making the citing contexts to have higher overlap with words from x. In turn, a paper z that cites x may use paraphras- ing to summarize ideas from x with words more similar to those from the content of z.</p><p>When the two types of contexts are used, co- training achieves higher results compared with cases when only one context type is used. This experiment shows that our method can be applied for both old and new research articles. Citing con- texts will be available in the text of the target paper and are independent of the existence of the cited contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Informative Features</head><p>What are the most informative words from each view: document content and citation contexts? <ref type="figure" target="#fig_5">Figure 7</ref> shows the words from each view that are most useful for our topic classification task. The larger the word, the more informative is for our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Labeled  <ref type="table">Table 2</ref>: A comparison of all methods on the test set.</p><p>task. To determine the informativeness of a word, we used its Information Gain score. For these ex- periments, we used training sets consisting of 30% of the instances, setting in which we achieved the best results on the validation and test sets using our proposed co-training approach. As can be seen, the two word clouds have a high word overlap. Words such as agent, database or query are almost equally important in the two views, dominating both clouds. However, differ- ences can be observed. For example, words like learning, multi-agent or interface are more impor- tant in the content view. On the other hand, words such as document or text achieve a higher informa- tion gain score for the citation contexts view. <ref type="table">Table 2</ref> summarizes the results obtained by all the baselines used so far, in comparison with our pro- posed co-training method. For this experiment, we show the training percentage used, the precision, recall and F1-score for each method, in the set- ting in which it returned the best results. All mea- sures were averaged after 10 runs with 10 different seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Co-Training vs. All Other Approaches</head><p>The results in <ref type="table">Table 2</ref> show that the pro- posed co-training method outperforms all com- pared models, reaching the highest F1-score of 0.742, while using the smallest amount of la- beled documents, i.e. 30%. Using only the cit- ing contexts, the performance is similar to that of co-training when both context types are used. However, using only the cited contexts, the per- formance decreases compared to that of the full model that uses both context types. We see that the citing contexts perform better, reaching an F1-score value of 0.740 compared against 0.714 when only cited contexts are used. Moreover, the method that uses only the citing contexts is using 10% less labeled data.</p><p>Self-training and EM show decreased perfor- mance compared with co-training. Late Fusion outperforms Early Fusion, i.e., 0.738 vs. 0.714, both obtaining lower results than co-training, while using significantly more labeled data.</p><p>The last two lines of the table show the results when all documents (except those in the valida- tion and test), are used for training, in a supervised framework. As can be seen, a supervised method that uses only citations will achieve a higher per- formance, compared against a method that uses titles and abstracts. Nonetheless, co-training ob- tains higher results than both fully supervised ap- proaches, while using only 30% of the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we studied the problem of using ci- tation contexts in order to predict more accurately the topic of a research article. We showed that a co-training technique, which uses the paper con- tent and its citation contexts as two conditionally independent and sufficient views of the data, can effectively incorporate cheap, unlabeled data to improve the classification performance and to re- duce the need of labeled examples to only a frac- tion. The results of the experiments showed that the proposed approach performs better than other semi-supervised and supervised methods.</p><p>This study also shows that citation contexts are rich sources of information that can be success- fully used in various IR and NLP tasks. We showed that document content and citation con- texts unified under the same algorithm can dra- matically decrease the annotation costs as well. In the future, we plan to extend co-training to in- clude active learning for more robust classifica- tion. Moreover, it would be interesting to extend the co-training approach to multi-views that could potentially handle more than two feature spaces, e.g., it could include topics by Latent Dirichlet Al- location ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> as an additional view.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Co-Training vs. Supervised Learning.</figDesc><graphic url="image-1.png" coords="5,80.73,62.81,200.81,138.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Co-Training vs. Early and Late Fusion.</figDesc><graphic url="image-5.png" coords="6,79.94,253.15,202.39,135.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Co-Training vs. Self-Training.</figDesc><graphic url="image-6.png" coords="6,316.13,253.15,200.55,137.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Co-Training vs. EM.</figDesc><graphic url="image-7.png" coords="7,80.60,62.81,201.08,137.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance when using only cited / only citing / or both citation contexts.</figDesc><graphic url="image-8.png" coords="7,315.87,62.81,201.08,136.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Most informative words from document content (left) and citation contexts (right).</figDesc><graphic url="image-9.png" coords="8,89.70,66.08,204.09,125.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>a paper is cited in multiple contexts within another paper, the contexts are aggregated into a single context. For each paper in the dataset, we have at least one cited or one citing context. A summary of the dataset is provided in Table 1.</figDesc><table>1 http://citeseerx.ist.psu.edu/ 

Number of papers in each class 
Agents AI 
IR ML HCI DB Total 
562 
239 641 569 490 685 3186 
cccAvg. Cited Contexts Avg. Citing Contexts 
45.59 
20.77 

Table 1: Dataset summary. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are thankful to Dr. Lise Getoor for making the Citeseer x labeled subset publicly available. We are also grateful to Dr. C. Lee Giles for the CiteSeer x data, which helped extract the citation contexts of the research papers in the collection. We very much thank our anonymous reviewers for their constructive feedback. This research is supported in part by the NSF award #1423337 to Cornelia Caragea. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coherent citation-based summarization of scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Abu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jbara</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT &apos;11</title>
		<meeting>of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reference scope identification in citing sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Abu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jbara</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="80" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98</title>
		<meeting>the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying scientific publications using abstract features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Silvescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Symposium on Abstraction, Reformulation, and Approximation (SARA)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Citationenhanced keyphrase extraction from research papers: A supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">Florin</forename><surname>Bulgarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Godea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujatha Das</forename><surname>Gollapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1435" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Automatic resource compilation by analyzing hyperlink structure and associated text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sridhar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comput. Netw. ISDN Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Higher order naive bayes: a novel non-iid approach to text classification. Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cibin</forename><surname>Murat Can Ganiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pottenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1022" to="1034" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Researcher homepage classification using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Sujatha Das Gollapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web, WWW &apos;13</title>
		<meeting>the 22Nd International Conference on World Wide Web, WWW &apos;13<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="482" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text categorization with suport vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Machine Learning, ECML &apos;98</title>
		<meeting>the 10th European Conference on Machine Learning, ECML &apos;98<address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient text classification method based on improved term reduction and term weighting. The Journal of China Universities of Posts and Telecommunications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi Kansheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>He Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nai-Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tao</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Utilizing context in generative bayesian models for linked corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context sensitive topic models for author influence in document networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Volume Three, IJCAI&apos;11</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Volume Three, IJCAI&apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2274" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The importance of anchor text for ad hoc search revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;10</title>
		<meeting>the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining anchor text for query refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiner</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on World Wide Web, WWW &apos;04</title>
		<meeting>the 13th International Conference on World Wide Web, WWW &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="666" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analyzing research papers using citation sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Lehnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Rilofl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 12th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of two learning algorithms for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ringuette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Annual Symposium on Document Analysis and Information Retrieval</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="81" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating impact-based summaries for scientific literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="816" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building enriched document representations using aggregated anchor text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srihari</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scientific paper summarization using citation summary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Intl. Conference on Computational Linguistics, COLING &apos;08</title>
		<meeting>of the 22nd Intl. Conference on Computational Linguistics, COLING &apos;08<address><addrLine>Manchester, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Citation summarization through keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan¨ozgür Arzucan¨</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arzucan¨ozgür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How to find better index terms through citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, CLIIR &apos;06</title>
		<meeting>of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, CLIIR &apos;06<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparing citation contexts for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Citing for high impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Mcfarland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL &apos;10</title>
		<meeting>the 10th Annual Joint Conference on Digital Libraries, JCDL &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic classification of citation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Argumentative Zoning: Information Extraction from Scientific Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
	<note>ACL &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An evaluation of statistical approaches to text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="88" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study of tf* idf, lsi and multi-words for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taketoshi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2758" to="2765" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-Supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
