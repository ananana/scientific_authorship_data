<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Text Recap Extraction for TV Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Text Recap Extraction for TV Series</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1797" to="1806"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sequences found at the beginning of TV shows help the audience absorb the essence of previous episodes, and grab their attention with upcoming plots. In this paper, we propose a novel task, text recap extraction. Compared with conventional summarization, text recap extraction captures the duality of sum-marization and plot contingency between adjacent episodes. We present a new dataset, TVRecap, for text recap extraction on TV shows. We propose an unsupervised model that identifies text recaps based on plot descriptions. We introduce two contingency factors , concept coverage and sparse reconstruction , that encourage recaps to prompt the up-coming story development. We also propose a multi-view extension of our model which can incorporate dialogues and synopses. We conduct extensive experiments on TVRecap, and conclude that our model outperforms summa-rization approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to a study by FX Networks, in U.S., the total number of ongoing scripted TV series hit a new high of 409 on broadcast, cable, and streaming in 2015 <ref type="bibr">1</ref> . Such a large number indicates there are more shows than anyone can realistically watch. To attract prospective audiences as well as help current view- ers recall the key plot when airing new episodes, some TV shows add a clip montage, which is called a recap sequence, at the beginning of new episodes or seasons. Recaps not only help the audience 1 http://tinyurl.com/jugyyu2 absorb the essence of previous episodes, but also grab people's attention with upcoming plots. How- ever, creating those recaps for every newly aired episode is labor-intensive and time-consuming. To our advantage, there are many textual scripts freely available online which describe the events and ac- tions happening during the TV show episodes 2 . These textual scripts contain plot descriptions of the events, dialogues of the actors, and sometimes also the synopsis summarizing the whole episode.</p><p>These abundant textual resources enable us to study a novel, yet challenging task: automatic text recap extraction, illustrated in <ref type="figure">Figure 1</ref>. The goal of text recap extraction is to identify seg- ments from scripts which both summarize the cur- rent episode and prompt the story development of the next episode. This unique task brings new technical challenges as it goes beyond summariz- ing prior TV episodes, by introducing a concept of plot contingency to the upcoming TV episode. It differs from conventional summarization techniques which do not consider the interconnectivity between neighboring episodes. Text recaps should capture the duality of summarization and plot contingency between neighboring episodes. To our knowledge, no dataset exists to study this research topic.</p><p>In this paper, we present an unsupervised model to automatically extrapolate text recaps of TV shows from plot descriptions. Since we assume recaps should cover the main plot of the current episode and also prompt the story development of the next episode, our model jointly optimizes these two ob-We see Shannon and Sayid working on the translation. Sayid finds the translation nonsense, slightly annoyed. Shannon walks off, upset and frustrated with Sayid and herself. Back to the robbery. Hutton opens the door as Jason is pointing gun at him.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kate shoots Joson in the leg. Kate opens the box which reveals an envelope inside.</head><p>On-Island -Jack is with the case asking Kate to tell him what is inside. Jack opens the box, and finds an envelope. Kate opens the envelope and pulls out a small airplane. After admitting it belongs to the man Kate loved and killed, Kate sits down and starts crying. Jack looks nonplussed, he closes up the case and walks away.</p><p>Shot of everyone moving up the beach. Rose sitting by a tree, Charlie approaches. Shot of Shannon walking up to Sayid on the beach. Boone stares at Sayid and Shannon from behind a tree with a weird look on his face. Kate just stares at her toy airplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next Episode Current Episode</head><p>Boone is watching Shannon read from far away. Sayid shows up, and hands a box to Shannon to thank for her help with the translation. Shannon opens the box which contains purple flowery shoes. They continue talking as the shot switches to Boone watching them.</p><p>Flashback -Shot of Boone with his arm around a girl, carrying tennis racket and ball, walking up steps from the tennis court to the pool area of a club. Sound of a cell phone ringing. Shannon is in a shaky voice. Shannon is yelling at someone on her end.</p><p>On-Island -Shot of Sayid limping along the beach. Boone confronts Sayid and tells him to stay away from his sister Shannon. Locke calls Boone away. Boone and Locke walk off into the jungle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… …</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Recap</head><p>We see Shannon and Sayid working on the translation. Kate opens the box which reveals an envelope inside.</p><p>After admitting it belongs to the man Kate loved and killed, Kate sits down and starts crying.</p><p>Boone stares at Sayid and Shannon from behind a tree with a weird look on his face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Recap Extraction</head><p>Figure 1: Illustration of text recap extraction. The system extracts sentences from the current episode. The text recap sentences in black summarize the current episode, while colored sentences motivate the next episode.</p><p>jectives. To summarize the current episode, our model exploits coverage-based summarization tech- niques. To connect to the next episode, we devise two types of plot contingency factors between adja- cent episodes. These factors implement the coverage and reconstruction assumptions to the next episode. We also show how our model can be extended to in- tegrate dialogues and synopses when available.</p><p>We introduce a new dataset 3 , named TVRecap for text recap extraction which consists of TV se- ries with textual scripts, including descriptions, di- alogues and synopses. The dataset enables us to study whether contingency-based methods which exploit relationships between adjacent episodes can improve summarization-based methods.</p><p>The rest of this paper is organized as follows. In Section 2, we discuss related work and the motiva- tion for our work. In Section 3, we introduce our new dataset for text recap extraction. Section 4 ex- plains our proposed model for text recap extraction, and Section 5 expands the model by incorporating synopses and dialogues. In Section 6 and 7, we present our experimental results and analyses, and finally conclude our work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we discuss three related research top- ics. Text summarization is an relevant task that aims to create a summary that retains the most important points of the original document. Then we discuss the Generic Text Summarization Alogrithms Text summarization is widely explored in the news do- main ( <ref type="bibr" target="#b8">Hong and Nenkova, 2014;</ref><ref type="bibr" target="#b14">McKeown, 2005</ref>). Generally, there are two approaches: extractive and abstractive summarization.</p><p>Extractive summarization forms a summary by choosing the most representative sentences from the original corpus. The early system LEAD ( <ref type="bibr" target="#b24">Wasson, 1998</ref>) was pioneering work. It selected lead- ing text of the document as the summary, and was applied in news searching to help online customers focus their queries on the beginning of news docu- ments. <ref type="bibr" target="#b6">He et al. (2012)</ref> assumed that summarization should consist of sentences that could best recon- struct the original document. They modeled rela- tionship among sentences by forming an optimiza- tion problem. Moreover, <ref type="bibr" target="#b23">Sipos et al. (2012)</ref> and Lin and Bilmes (2010) studied multi-document sum- marization using coverage-based methods. Among them, <ref type="bibr" target="#b11">Lin and Bilmes (2010)</ref> proposed to approxi- mate the optimal solution of a class of functions by exploiting submodularity.</p><p>Abstractive summarization automatically create new sentences. For example, compared with the sentence-level analysis in extractive summarization, <ref type="bibr" target="#b2">Bing et al. (2015)</ref> explored fine-grained syntactic units, i.e. noun/verb phrases, to represent concepts in input documents. The informative phrases were then used to generate sentences.</p><p>In this paper, we generalize the idea of text sum- marization to text recap extraction. Instead of sum- marizing a given document or collection, our model emphasizes plot contingency with the next episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization</head><p>Applications Summarization techniques are not restricted to informative re- sources (e.g. news), applications in broader areas are gaining attention <ref type="bibr" target="#b0">(Aparício et al., 2016)</ref>. As the prevailance of online forums, <ref type="bibr" target="#b16">Misra et al. (2015)</ref> developed tools to recognize arguments from opinionated conversations, and group them across discussions. In entertainment industry, Sang and Xu (2010) proposed a character-based movie summarization approach by incorporating scripts into movie analysis. Moreover, recent applications include multimedia artifact generation ( <ref type="bibr" target="#b4">Figueiredo et al., 2015)</ref>, music summarization ( <ref type="bibr" target="#b17">Raposo et al., 2015)</ref> and customer satisfaction analysis ( <ref type="bibr" target="#b20">Roy et al., 2016)</ref>.</p><p>Video Description Generating video descriptions is a task that studies automatic generation of natural language that describes events happening in video clips. Most work uses sequential learning for en- coding temporal information and language genera- tion ( <ref type="bibr" target="#b5">Guadarrama et al., 2013;</ref><ref type="bibr" target="#b19">Rohrbach et al., 2013</ref><ref type="bibr" target="#b3">Donahue et al., 2015)</ref>. Our work is com- plementary to video description: the large number of unlabeled videos can be utilized to train end-to- end recap extraction system when video description models can properly output textual descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of This Paper</head><p>In contrast with prior work, the main contributions of this paper are: (1) We propose a novel problem, text recap extrac- tion for TV series. Our task aims to identify seg- ments from scripts which both summarize the cur- rent episode and prompt the story development of the upcoming episode; (2) We propose an unsupervised model for text recap extraction from descriptions. It models the episode contingency through two factors, next episode sum- marization and sparse reconstruction; (3) We introduce a new dataset for TV show recap extraction, where descriptions, dialogues and syn- opses are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The TVRecap Dataset</head><p>We collected a new dataset, called TVRecap, for text recap extraction on TV series. We gathered and processed scripts, subtitles and synopses from web- sites 4 as components to build our model upon. We also established ground truth to help future research on this challenging topic. TVRecap includes all sea- sons from the widely-known show "Lost" with a total of 106 episodes. Statistics of our dataset are shown in <ref type="table">Table 1</ref> This section describes how textual scripts and synopses are processed, and how we automatically define the ground truth of text recap annotations.</p><p>Descriptions, Dialogues and Synopses A script for one TV series episode is a sequence of di- alogues interleaved with descriptions (marked by square brackets). We automatically split the script into descriptions and dialogues. For each episode, We also downloaded the synopsis, a human-written paragraph summarizing the main plot of the episode. <ref type="figure" target="#fig_0">Figure 2</ref> shows examples of a script and a synopsis from our TVRecap dataset. (a) Script: containing descriptions and dialogues.</p><p>Boone steals the decreasing water supply in a misguided attempt to help everyone, but the survivors turn on him. A sleep-deprived Jack chases after what appears to be his deceased father in the forests and eventually discovers caves with fresh water. Jack comes to terms with his role as leader. In flashbacks, Jack goes to Australia to retrieve his deceased father.</p><p>(b) Synopsis. All plot descriptions and dialogues are time- aligned automatically using the subtitle files <ref type="bibr">5</ref> . We first aligned the dialogue sentences from the script with the subtitle files which contain time-stamps (in milliseconds) of the spoken dialogues. Then we es- timated time-stamps of description sentences using surrounding dialogues.</p><p>Since descriptions sometimes contain words not relevant to the event, we manually post-processed all descriptions and recap sentences as follows: <ref type="formula" target="#formula_2">(1)</ref> remove trivial sentences such as "music on", (2) re- move introductory terms like "Shot of ", (3) com- plete missing grammatical components (like omitted subjects) of sentences when possible.</p><p>Text Recap Annotations The goal of our ground truth annotation is to identify the text descriptions associated with the TV show recap. We performed this annotation task in three steps.</p><p>First, we automatically extracted the recap se- quence, which is a montage of important scenes from previous episodes to inform viewers of what has happened in the show, from the TV show video. These recap sequences, if available, are always shown at the beginning of TV episodes. We auto- matically separated video recap sequences from full- length video files by detecting a lengthy appearance of black frames in the first several minutes of the episode. Second, we located the frames of the re- cap sequences in the videos of previous episodes, and recorded their time-stamps. Finally, the recap annotations are automatically identified by compar- ing the video time-stamps with the text description time-stamps. A description is annotated as part of the recap if at least 4 frames from the video recap are present during this description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Text Recap Extraction Model</head><p>In our Text Recap Extraction Model (TREM), we assume a good text recap should have two charac- teristics: (a) it covers the main plot of the current episode, and (b) it holds plot contingency with the next episode. Under the first assumption, the text recap can be seen as a summarization that retains the most important plots. Under assumption (b), the text recap should capture the connections between two consecutive episodes. Formally, the system is given E episodes from a specific TV show, where each episode contains tex- tual descriptions. We define these descriptions as</p><formula xml:id="formula_0">D = {D 1 , · · · , D E },</formula><p>where D i is the set of descrip- tions of episode i. D i is composed of descriptive sentences as</p><formula xml:id="formula_1">D i = {d i 1 , · · · , d i |D i |</formula><p>}, where d i j is the j-th sentence. The goal of our task is to find text re- caps R = {R 1 , · · · , R E−1 } where the components of R i are selected from D i with a length budget (constraint on the number of sentences) |R i | ≤ K.</p><p>In our TREM model, the text recap R i of the i-th episode is optimized by:</p><formula xml:id="formula_2">max R i ⊂D i F(R i ) = S(R i , D i ) + M(R i , D i+1 ) s.t |R i | ≤ K,<label>(1)</label></formula><p>where S(R i , D i ) measures how well R i summa- rizes D i , and M(R i , D i+1 ) quantifies the level of connectivity between the text recap of the current episode and the plot description of the next episode. By using M(·, ·), we expect to produce text re- caps with better plot contingency with the upcoming story.</p><p>In the following sections, we demonstrate in de- tails: (1) the definition of the summarization func- tion S(·, ·); (2) two factors that derive the contin- gency function M(·, ·) based on different hypothe- ses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Plot Summarization</head><p>In this section, we discuss the summarization com- ponent of our model's objective function. Our model is inspired by the coverage-based summarization ( <ref type="bibr" target="#b11">Lin and Bilmes, 2010)</ref>, whose key idea is to find a proxy that approximates the information overlap between the summary and the original document. In this work, any text is assumed to be represented by a set of "concepts" using weights to distinguish their importance. To be more specific, a concept is de- fined as a noun/verb/adjective or noun/verb phrase. In terms of concepts, we define the summarization term S(R i , D i ) as follows:</p><formula xml:id="formula_3">S(R i , D i ) = c∈C(D i ) z(c, D i ) max r∈R i w(c, r),<label>(2)</label></formula><p>where  <ref type="bibr" target="#b21">(Salton and Buckley, 1988</ref>) to calculate z(c, D i ). Finally, w(c, r) de- notes the relatedness of a concept c to a sentence r.</p><formula xml:id="formula_4">C(D i ) = {c |c ∈ d i j , ∀d i j ∈ D i } is</formula><p>We use Word2Vec ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) vectors as the semantic representation of concepts, and de- fine w(c, r) as:</p><formula xml:id="formula_5">w(c, r) = |c| · max c ∈r cos(c, c ),<label>(3)</label></formula><p>where bold notations are the Word2Vec representa- tions of c and c . Note that if c is a phrase, c is mean pooled by the embeddings of its component words. |c| is the number of words in c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Plot Contingency</head><p>We model plot contingency on the concept level as well as on the sentence level. Therefore, the compo- nent M(·, ·) is decomposed into two factors:</p><formula xml:id="formula_6">M(R i , D i+1 ) =λ s M s (R i , D i+1 )+ λ r M r (R i , D i+1 ).<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">M s (R i , D i+1 ) measures how well R i can summarize the next episode D i+1 and M r (R i , D i+1</formula><p>) is the factor that quantify the ability of R i to reconstruct D i+1 . λ s , λ r ≥ 0 are coefficients for M s (·, ·) and M r (·, ·) respectively.</p><p>In the following sections, we define and explain these two factors in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Concept Coverage</head><p>Following the coverage assumption of Section 4.1, we argue that the text recap should also cover important concepts from the next episode. There- fore, the first contingency factor can be defined in the same form as the summarization component where D i 's in Equation 2 are replaced by D i+1 's:</p><formula xml:id="formula_8">M s (R i , D i+1 ) = c∈C(D i+1 ) z(c, D i+1 ) max r∈R i w(c, r).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Sparse Reconstruction</head><p>As events happening in the current episode can have an impact on the next episode, there exist hid- den connections between the descriptive sentences in D i and D i+1 . To be more specific, assuming de- scriptive sentences from D i+1 are dependent on a few sentences in D i , we aim to infer such hidden contingency. Here we assume that sentence d i+1 j is related to a small number of sentences in D i .</p><p>Let α i+1 j ∈ R |D i | be the indicator that determines which sentences in D i prompt d i+1 j , and W be the matrix that transforms these contingent sentences to the embedding space of d i+1 j . Intuitively, our model learns W by assuming each sentence in D i+1 can be reconstructed by contingent sentences from D i :</p><formula xml:id="formula_9">d i+1 j ≈ WD i α i+1 j ,<label>(6)</label></formula><p>In the equation, we first convert every description sentence to its distributed representation using the pre-trained skip-thought model proposed by <ref type="bibr" target="#b9">Kiros et al. (2015)</ref>. The sentence embedding is denoted in bold (e.g.</p><formula xml:id="formula_10">d i j for sentence d i j ). D i = [d i 1 ; · · · ; d i |D i |</formula><p>] stacks the vector representations of all sentences in D i , and α i+1 j linearly combines the contingent sen- tences.</p><p>We propose to jointly optimize α i+1 j and W by:</p><formula xml:id="formula_11">min {α i+1 } E−1 i=1 ,W i,j WD i α i+1 j − d i+1 j 2 2 + γα i+1 j 1 + θW 2 F ,<label>(7)</label></formula><p>where we denote</p><formula xml:id="formula_12">α i+1 = [α i+1 1 ; · · · ; α i+1 |D i+1 |</formula><p>]. We impose sparsity constraint on α i+1 j with L 1 norm such that only a small fraction of sentences in D i will be linked to d i+1 j . γ and θ are coefficients of the regularization terms.</p><p>Given the optimal W * from Equation 7, our main objective is to identify the subset of descriptions in D i that best capture the contingency between D i and D i+1 . The reconstruction contingency factor can be defined as:</p><formula xml:id="formula_13">M r (R i , D i+1 ) = d∈D i+1 max r∈R i r W * d. (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>In this section, we describe our approach to optimize the main objective function expressed in Equations 1 and 7.</p><p>Finding an efficient algorithm to optimize a set function like Equation 1 is often challenging. However, it can be easily shown that the objec- tive function of Equation 1 is submodular, since all its components S(R i , D i ), M s (R i , D i+1 ) and M r (R i , D i+1 ) are submodular with respect to R i . According to <ref type="bibr" target="#b12">Lin and Bilmes (2011)</ref>, there exists a simple greedy algorithm for monotonic submodular function maximization where the solution is guar- anteed to be close to the real optimum. Specifi- cally, if we denote R i greedy as the approximation op- timized by greedy algorithm and R i * as the best pos- sible solution, then F(R i greedy )</p><formula xml:id="formula_14">≥ (1 − 1 e ) · F(R i * )</formula><p>, where F(·) is the objective function of Equation 1 and e ≈ 2.718 denotes the natural constant. The greedy approach is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Text Recap Extraction</head><p>Input: Vectorized sentence representations</p><formula xml:id="formula_15">{D i } E i=1 , parameters λ s , λ r , θ, γ, budget K, optimal W * for Equation 7. Output: Text recaps {R i } E i=1 . 1: for i = 1, · · · , E 2:</formula><p>Initialize R i ← ∅;</p><p>3:</p><formula xml:id="formula_16">REPEAT 4: r * ← arg max F(R i ∪ {r}); 5: R i ← R i ∪ {r * }; 6:</formula><p>UNTIL |R i | ≥ K 7: end Algorithm 1 requires the optimal W * learned from the adjacent episode pairs in Equation 7. We utilize the algorithm that iteratively updates W and α given the current solution. At each iteration, each variable (W or {α i+1 }) is updated by fixing the other. At t-th iteration, W (t) is computed as the solution of ridge regression <ref type="bibr" target="#b7">(Hoerl and Kennard, 1970)</ref>:</p><formula xml:id="formula_17">W (t) = DX (XX + θI) −1 ,<label>(9)</label></formula><p>where D and X stack all d i+1</p><formula xml:id="formula_18">j and x i+1 j D i α i+1 j , ∀i = 1, · · · , E − 1, j = 1, · · · , |D i |. Fix- ing W, each α i+1 j</formula><p>can be solved separately by gen- eral sparse coding algorithms as stated in <ref type="bibr" target="#b13">Mairal et al. (2009)</ref>. Algorithm 2 shows the optimization process of Equation 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multi-View Recap Extraction</head><p>In addition to plot descriptions, there are also dia- logues and plot synopses available for TV shows. Descriptions, dialogues and synopses can be seen as three different views of the same TV show episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Reconstruction Matrix Optimization</head><p>Input: Vectorized sentence representations</p><formula xml:id="formula_19">{D i } E i=1</formula><p>, θ and γ. Output: Contingency matrix W. </p><formula xml:id="formula_20">∀i, j, α i+1,(t) j ← sparse coding(W (t) ); 7: UNTIL W (t) − W (t−1) 2 F ≤</formula><p>Previously, we build TREM using plot descriptions.</p><p>In this section, we expand our TREM model to in- corporate plot synopses and dialogues. We define text synopses and dialogues as S = {S 1 , · · · , S E } and T = {T 1 , · · · , T E }, where S i and T i are the set of sentences from synopses and dialogues of the i-th episode.</p><p>Dialogues In TV shows, a lot of useful informa- tion is presented via actors' dialogues which moti- vates us to extend our TREM model to include di- alogues. Both views can be used to identify recap segments which are assumed to be summative and contingent. Denote the neighboring dialogues of R i as N (R i ) = {t ∈ T i ∃r ∈ R i , s.t. |time(t) − time(r)| &lt; δ}, we extend the optimization objective (Equation 1) into:</p><formula xml:id="formula_21">F(R i ) = S(R i , D i ) + S(N (R i ), T i ) + M(R i , D i+1 ) + M(N (R i ), T i+1 ) .<label>(10)</label></formula><p>Synopses Since a synopsis is a concise summary of each episode, we can treat plot summarization as text alignment where R i is assumed to match the content of S i . Therefore, the summarization term can be redefined by substituting D i with S i : <ref type="figure">, r)</ref>. <ref type="formula" target="#formula_2">(11)</ref> Similarly, the contingency component can be modified to include connections from synopses to detailed descriptions. For Equation 8, we substitute ROUGE-1 ROUGE-2 ROUGE-SU4 ILP-Ext ( <ref type="bibr" target="#b1">Banerjee et al., 2015)</ref> 0.308 0.112 0.091 ILP-Abs ( <ref type="bibr" target="#b1">Banerjee et al., 2015)</ref> 0  D i+1 to S i+1 where our model only focuses on high- level storyline:</p><formula xml:id="formula_22">S(R i , S i ) = c∈C(S i ) z(c, S i ) max r∈R i w(c</formula><formula xml:id="formula_23">M r (R i , S i+1 ) = s∈S i+1 max r∈R i r W * s. (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>We designed our experiments to evaluate whether our TREM model, by considering contingency be- tween adjacent episodes, can achieve better results than summarization techniques. Furthermore, we want to examine how each contingency factor as proposed in Section 4.2 contributes to the system performance. As our model can integrate multiple views, we want to dissect the effects of using differ- ent combinations of three views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison Models</head><p>To answer the research questions presented above, we compare the following methods in our experi- ments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Methodology</head><p>Using TVRecap, we measure the quality of gener- ated sentences following the standard metrics in the summarization community, ROUGE ( <ref type="bibr" target="#b10">Lin and Hovy, 2003)</ref>. For the purpose of evaluation, we defined a de- velopment and a test set, by randomly selecting 18 adjacent pairs of episodes from all seasons. These episodes were selected to have at least two recap description sentences. The remaining 70 episodes were only used during the learning process of W. After tuning hyper-parameters on development set, we report the comparison results on the test set. <ref type="table" target="#tab_4">Table 2</ref> shows our experimental results comparing TREM and baseline models using descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Overall Results</head><p>In general, contingency-based methods (TREM, TREM w/o SR and TREM w/o CC) outperform summarization-based methods. Our contingency assumptions are verified as adding CC and SC both improve TREM with summarization compo- nent only. Moreover, the best result is achieved by the complete TREM model with both contingency factors. It suggests that these two factors, modeling word-level summarization and sentence-level recon- struction, are complementary.</p><p>From the summarization-based methods, we can see that our TREM-Summ gets higher ROUGE scores than two ILP approaches. Additionally, we Target sentence from next episode Sentences with highest reconstruction value from current episode Kate is putting water bottles in a pack.</p><p>We see three bottles of water. They go into a room with a body bag on a gurney. Kate is going through clothes, as Claire approaches.</p><p>Locke is with his knife case, holding a pencil, sitting by a fire.</p><p>Boone is coming up to camp and sees Locke sitting by a fire. Locke throws a knife into the ground, just out of Boone's reach. Boone quickly cuts through the ropes and starts running. In another part of the temple grounds, Miles and Hurley are playing Tic-Tac-Toe by placing leaves in a grid of sticks on the ground.</p><p>John contemplates the fabric swatches he is holding. On the beach, Frank covers Locke's body with a tarp. Helen closes the door and brings the case inside to the kitchen.  and full TREM with contingency factors. "des", "syn", and "dial" are abbreviated for description, synopses and dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multi-view Comparison</head><p>As shown in <ref type="table" target="#tab_6">Table 4</ref>, The second study examines the effect of different views in both types of methods using the TREM model. In single-view summariza- tion, TREM-Summ with descriptions outperforms methods based on the other two views. In terms of hybrid of views, only ROUGE-1 is significantly im- proved, while ROUGE-2 and ROUGE-SU4, which focus more on semantic consistency, have little im- provement.</p><p>In contingency-based methods, we keep the cur- rent episode represented as descriptions which ob- tain the best performance in single-view summa- rization, and change the views of the next episode. Comparing the model using descriptions with the one fusing descriptions and synopses, we can see that simply adding views does not guarantee higher ROUGE scores. In both TREM-Summ and full TREM, dialogue is inferior to others. It might be be- cause dialogues contain too many trivial sentences. Synopses, however, are relatively short, but provide key plots to summarize the story, and hence achieve the best ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Qualitative Study on Sparse Reconstruction</head><p>In this section, we give some examples to illustrate the process of sparse reconstruction. Equation 7 assumes that each descriptive sentence can be re- constructed by a few sentences from the previous episode. <ref type="table" target="#tab_5">Table 3</ref> shows three examples of sentences with their top-3 reconstructive sentences, which are defined by values in the indicator vector α i+1 j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Limitations and Future Work</head><p>TREM restrains the contingency within adjacent episodes. However, storylines sometimes proceed through multiple episodes. In our model, with more connectivity terms M(R i , D j ) where i &lt; j, we can develop more general system with longer dependen- cies.</p><p>While our model and dataset are appropriate for text recap extraction and algorithm comparison, this task can be further applied to multimedia settings, where visual or acoustic information can be in- cluded. Therefore, in future work, we plan to expand our work to broader applications where intercon- nectivity between consecutive instances is crucial, such as educational lectures, news series and book chapters. Specifically, TREM can be integrated with video description results to get an end-to-end system that produces video recaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we explore a new problem of text recap extraction for TV shows. We propose an unsupervised model that identifies recap segments from multiple views of textual scripts. To facili- tate the study of this new research topic, we cre- ate a dataset called TVRecap, which we test our approach on. From the experimental results, we conclude that contingency-based methods improve summarization-based methods at ROUGE measure- ments by exploiting plot connection between adja- cent episodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a script (including descriptions and dialogues) and a synopsis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>−</head><label></label><figDesc>ILP-Ext and ILP-Abs (Banerjee et al., 2015): This summarizer generates sentences by optimizing the integer linear programming problem in which the information content and linguistic quality are de- fined. Both extractive and abstractive implementa- tions are used in our experiments. − TREM: Our TREM model proposed in Section 4 extracts sentences that can both summarize the cur- rent episode and prompt the next episode with two contingency factors. − TREM w/o SR: The TREM model without the sparse reconstruction factor proposed in Section 4.2.2. − TREM w/o CC: The TREM model without the concept coverage factor proposed in Section 4.2.1. − TREM w/o SR&amp;CC: The summarization-only TREM model without contingency factors. In the rest of the paper, we also call it as TREM-Summ. − Multi-view TREM: The augmented TREM model with descriptions, dialogues and synopses as proposed in Section 5. Different views and combi- nations will be tested in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table># sent. avg. # sent. # words avg. # w./s. 
description 14,686 
138.5 
140,684 
9.57 
dialogue 
37,714 
355.8 
284,514 
7.54 
synopsis 
453 
4.27 
7,868 
17.36 
recap 
619 
17.19 
5,892 
9.52 

Table 1: Statistics of TVRecap. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>LOCKE : Two players. Two sides.</head><label>LOCKE</label><figDesc></figDesc><table>One is light... one is dark. Walt, do 
you want to know a secret? 

[Claire writing in her diary. Jin approaches and offers her some 
urchin. She shakes her head, but then gives in and takes some.] 

CLAIRE: No. Thank you. No, it's okay. [Jin keeps insisting] No, 
really. Okay. Thanks. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on different methods using descriptions. Contingency-based methods generally outperforms 

summarization-based methods. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>A case study on sparse reconstruction as proposed in Section 4.2.2. Sentences in the first column are reconstructed by sentences in the second column. The first two examples successfully captures related sentences, while the third example fails. note that the performance of ILP-Ext is poor. This is because ILP-Ext tends to output short sentences, while ROUGE is a recall-oriented measurement.</figDesc><table>Model 
Current 
Next 
R-1 
R-2 R-SU4 

TREM-Summ 

des 
-
0.374 0.168 0.129 
syn 
-
0.369 0.163 0.121 
dial 
-
0.354 0.138 0.115 
des+syn 
-
0.384 0.172 0.132 
des+dial 
-
0.386 0.168 0.135 

TREM 

des 
des 
0.405 0.207 0.148 
des 
syn 
0.411 0.219 0.154 
des 
dial 
0.375 0.158 0.127 
des 
des+syn 0.409 0.210 0.154 
des 
des+dial 0.395 0.177 0.142 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Comparison of views in summarization-only TREM</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> http://www.simplyscripts.com/tv_all. html</note>

			<note place="foot" n="3"> http://multicomp.cs.cmu.edu evaluation metrics of text summarization. Finally, we discuss the video description which is complementary to our work.</note>

			<note place="foot" n="4"> http://lostpedia.wikia.com/ and https:// www.wikipedia.org/</note>

			<note place="foot" n="5"> http://www.tvsubtitles.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This material is based in part upon work partially supported by the National Science Foundation (IIS-1523162). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Summarization of films and documentaries based on subtitles and scripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Aparício</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martins De Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-document abstractive summarization using ilp based multi-sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting><address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Abstractive multi-document summarization via phrase selection and merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01597</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Aparício</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martins De Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03170</idno>
		<title level="m">Generation of multimedia artifacts: An extractive summarization-based approach</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niveda</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2712" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document summarization based on data reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert W</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving the estimation of word importance for news multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="712" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text summarization: News and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Workshop</title>
		<meeting>the Australasian Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using summarization to discover argument facets in online idealogical dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jef</forename><surname>Tree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the application of generic summarization algorithms to music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>Manfred Pinkal, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Qa rt: A system for real-time holistic quality assurance for contact center dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan</forename><surname>Mariappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainyam</forename><surname>Galhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Peddamuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing &amp; management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Characterbased movie summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitao</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="855" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal corpus summarization using submodular word coverage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Sipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pannaga</forename><surname>Shivaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information</title>
		<meeting>the 21st ACM international conference on Information</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using leading text for news summaries: Evaluation results and implications for commercial summarization applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Wasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Computational linguistics</title>
		<meeting>the 17th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1364" to="1368" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
