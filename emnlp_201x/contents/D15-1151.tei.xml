<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Improved Tag Dictionary for Faster Part-of-Speech Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
							<email>bobmoore@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Improved Tag Dictionary for Faster Part-of-Speech Tagging</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi&apos;s tag dictionary makes tagging faster but less accurate , an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi&apos;s tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratna-parkhi&apos;s method results in a much tighter tag dictionary than either Ratnaparkhi&apos;s or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. 1 Overview In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair. 2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi&apos;s method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented (Moore, 2014) a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi&apos;s, but with no decrease in tagging accuracy. In this paper , we show that a very simple semi-supervised variant of Ratnaparkhi&apos;s method results in a much tighter tag dictionary than either Ratnaparkhi&apos;s or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>In this paper, we present a new method of con- structing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are com- monly used to speed up POS-tag inference by re- stricting the tags considered for a particular word to those specified by the dictionary.</p><p>Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus ( <ref type="bibr" target="#b4">Leech et al., 1983;</ref><ref type="bibr" target="#b1">Church, 1988;</ref><ref type="bibr" target="#b3">Cutting et al., 1992)</ref>. <ref type="bibr" target="#b10">Merialdo (1994)</ref> relied only on a tag dictionary extracted from annotated data, but he used the annotated tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair. <ref type="bibr">2 Ratnaparkhi (1996)</ref> seems to have been the first to use a tag dictionary automatically extracted only from training data.</p><p>Ratnaparkhi's method of constructing a tag dic- tionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented <ref type="bibr" target="#b11">(Moore, 2014</ref>) a new method of constructing a tag dictionary that produces a tag- ging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this pa- per, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Tag Dictionaries and Tagging Speed</head><p>A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags t 1 , . . . , t n given a sequence of words w 1 , . . . , w n . The tag sequence assigned the highest score by the model for a given word sequence is selected as the tagging for the word sequence. If T is the set of possible tags, and there are no restrictions on the form of the model, then the time to find the highest scoring tag sequence is potentially O(n|T | n ) or worse, which would be intractable.</p><p>To make tagging practical, models are normally defined to be factorable in a way that reduces the time complexity to O(n|T | k ), for some small in- teger k. For models in which all tagging deci- sions are independent, or for higher-order mod-els pruned by fixed-width beam search, k = 1, so the time to find the highest scoring tag sequence is O(n|T |). But this linear dependence on the size of the tag set means that reducing the average number of tags considered per token should further speed up tagging, whatever the underlying model or tag- ger may be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Ratnaparkhi's Method</head><p>For each word observed in an annotated training set, Ratnaparkhi's tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratna- parkhi reported that using this tag dictionary im- proved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank ( <ref type="bibr" target="#b7">Marcus et al., 1993</ref>) Wall Street Journal (WSJ) development set, compared to considering all tags for all words.</p><p>With a more accurate model, however, we found <ref type="bibr" target="#b11">(Moore, 2014</ref>) that while Ratnaparkhi's tag dictio- nary decreased the average number of tags per to- ken from 45 to 3.7 on the current standard WSJ de- velopment set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the de- velopment set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accu- racy for these tokens is necessarily 0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our Previous Method</head><p>We previously presented <ref type="bibr" target="#b11">(Moore, 2014</ref>) a tag dic- tionary constructed by using the annotated train- ing set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probabil- ity greater than a fixed threshold T . In this ap- proach, the probability p(t|w) of tag t given word w is computed by interpolating a discounted rel- ative frequency estimate of p(t|w) with an esti- mate of p(t) based on "diversity counts", taking the count of a tag t to be the number of distinct words ever observed with that tag. The distribu- tion p(t) is also used to estimate tag probabilities for unknown words, so the set of possible tags for any word not explicitly listed is {t|p(t) &gt; T }.</p><p>If we think of w followed by t as a word bi- gram, this is exactly like a bigram language model estimated by the interpolated Kneser-Ney (KN) method described by <ref type="bibr" target="#b0">Chen and Goodman (1999)</ref>.</p><p>The way tag diversity counts are used has the de- sirable property that closed-class tags receive a very low estimated probability of being assigned to a rare or unknown word, even though they oc- cur very often with a small number of frequent words. A single value for discounting the count of all observed word/tag pairs is set to maximize the estimated probability of the reference tagging of the development set. When T was chosen to be the highest threshold that preserves our model's 97.31% per tag WSJ development set accuracy, we obtained an average of 3.5 tags per token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Our New Approach</head><p>We now present a new method that reduces the av- erage number of tags per token to about 1.5, with no loss of tagging accuracy. We apply a simple variant of Ratnaparkhi's method, with a training set more than 4,000 times larger than the Penn Treebank WSJ training set. Since no such hand- annotated corpus exists, we create the training set automatically by running a version of our tagger on the LDC English Gigaword corpus. We thus describe our approach as a semi-supervised vari- ant of Ratnaparkhi's method. Our method can be viewed as an instance of the well-known technique of self-training (e.g., <ref type="bibr" target="#b8">McClosky et al., 2006</ref>), but ours is the first use of self-training we know of for learning inference-time search-space pruning.</p><p>We introduce two additional modifications of Ratnaparki's approach. First, with such a large training corpus, we find it unnecessary to keep in the dictionary every tag observed with every word in the automatically-annotated data. So, we esti- mate a probability distribution over tags for each word in the dictionary according to unsmoothed relative tag frequencies, and include for each word in the dictionary only tags whose probability given the word is greater than a fixed threshold.</p><p>Second, since our tokenized version of the En- glish Gigaword corpus contains more than 6 mil- lion unique words, we reduce the vocabulary of the dictionary to the approximately 1 million words having 10 or more occurrences in the corpus. We treat all other tokens as instances of unknown words, and we use their combined unsmoothed relative tag frequencies to estimate a tag probabil- ity distribution for unknown words. We use the same threshold on this distribution as we do for words explicitly listed in the dictionary, to obtain a set of possible tags for unknown words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1304</head><p>In our experiments, we use the WSJ corpus from Penn Treebank-3, split into the standard training (sections 0-18), development (sections 19-21), and test (sections 22-24) sets for POS tagging.</p><p>The tagging model we use has the property that all digits are treated as indistinguishable for all features. We therefore also make all digits in- distinguishable in constructing tag dictionaries (by internally replacing all digits by "9"), since it does not seem sensible to give two different dictionary entries based on containing different digits, when the tagging model assigns them the same features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Tagging Model</head><p>The model structure, feature set, and learning method we use for POS tagging are essentially the same as those in our earlier work, treating POS tagging as a single-token independent multiclass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence fea- tures frequently used for POS tagging, and ad- ditional word-class features obtained by unsuper- vised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper <ref type="bibr" target="#b11">(Moore, 2014)</ref>.</p><p>The model is trained by optimizing the mul- ticlass SVM hinge loss objective <ref type="bibr" target="#b2">(Crammer and Singer, 2001</ref>), using stochastic subgradient de- scent as described by <ref type="bibr" target="#b14">Zhang (2004)</ref>, with early stopping and averaging. The only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we pre- viously used tag dictionaries only at test time.</p><p>Our training procedure makes multiple passes through the training data considering each train- ing example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring incorrect tag and updating the model if the score of the correct tag does not ex- ceed the score of the highest scoring incorrect tag by a specified margin. In our new version of this procedure, we use the KN-smoothed tag dictio- nary described in Section 1.3. to speed up finding the highest scoring incorrect tag.</p><p>Recall that the KN-smoothed tag dictionary es- timates a non-zero probability p(t|w) for every possible word/tag pair, and that the possible tags for a given word are determinted by setting a threshold T on this probability. In each pass through the training set, we use the same probabil- ity distribution p(t|w) determined from the statis- tics of the annotated training data, but we employ an adaptive method to determine what threshold T to use in each pass.</p><p>For the first pass through the training set, we set an initial threshold T 0 to the highest value such that for every token in the development set, p(t|w) ≥ T 0 , where t is the correct tag for the to- ken and w is the word for the token. At the end of each training pass i, while evaluating the cur- rent model on the development set for early stop- ping using threshold T i−1 , we also find the high- est probability threshold T i such that choosing a lower threshold would not enable any additional correct taggings on the development set using the current model. This threshold will normally be higher than T 0 , because we disregard tokens in the development set for which the correct tag would not be selected by the model resulting from the previous pass at any threshold. T i is then used as the threshold for training pass i + 1. Whenever the selected threshold leaves only one tag remain- ing for a particular training example, we skip that example in training.</p><p>On the first pass through the training set, use of this method resulted in consideration of an aver- age of 31.36 tags per token, compared to 45 to- tal possible tags. On the second and all subse- quent passes, an average of 10.48 tags were con- sidered per token. This sped up training by a fac- tor of 3.7 compared to considering all tags for all tokens, with no loss of tagging accuracy when a development-set-optimized KN-smoothed tag dic- tionary is also used at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tagging the Gigaword Corpus</head><p>To construct our new tag dictionary, we need an automatically-tagged corpus several orders of magnitude larger than the hand-tagged WSJ train- ing set. To obtain this corpus we ran a POS tagger on the LDC English Gigaword Fifth Edi- tion 3 corpus, which consists of more than 4 bil- lion words of English text from seven newswire sources. We first removed all SGML mark-up, and performed sentence-breaking and tokenization us- ing the Stanford CoreNLP toolkit <ref type="figure">(Manning et al,  2014</ref>). This produced 4,471,025,373 tokens of  <ref type="table">Table 1</ref>: WSJ development set token accuracy and tagging speed for different tag dictionaries 6,616,812 unique words. We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Sec- tion 1.3, with a threshold T = 0.0005. The tagger we used is based on the fastest of the methods de- scribed in our previous work (Moore, 2014, Sec- tion 3.1). Tagging took about 26 hours using a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors.</p><note type="other">Tag Dictionary Accuracy Tags/Token Unambig Tokens/Sec Pruned KN-</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extracting the Tag Dictionary</head><p>We extracted a Ratnaparkhi-like tag dictionary for the 957,819 words with 10 or more occurrences in our corpus. Tokens of all other words in the corpus were treated as unknown word tokens and used to define a set of 24 tags 4 to be used for words not explicitly listed in the dictionary. To allow pruning the dictionary as described in Section 1.4, for each word (including the unknown word), we computed a probability distribution p(t|w) using unsmoothed relative frequencies. As noted above, we treated all digits as indistinguishable in con- structing and applying the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Tagging the WSJ development set with an un- pruned semi-supervised tag dictionary obtained from the automatic tagging of the English Gi- gaword corpus produced the same tagging accu- racy as allowing all tags for all tokens or using the pruned KN-smoothed tag dictionary used in tagging the Gigaword corpus. Additional exper- iments showed that we could prune this dictionary with a threshold on p(t|w) as high as T = 0.0024 without decreasing development set accuracy. In addition to applying this threshold to the tag prob- abilities for all listed words, we also applied it to the tag probabilities for unknown words, leaving 13 possible tags 5 for those.</p><p>Tagging the WSJ development set with these two dictionaries is compared in <ref type="table">Table 1</ref> to tag- ging with our previous pruned KN-smoothed dic- tionary. The second column shows the accuracy per tag, which is 97.31% for all three dictionaries. The third column shows the mean number of tags per token allowed by each dictionary. The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tag- ger need not apply the model for such tokens-it can simply output the single possible tag.</p><p>The last column shows the tagging speed in tokens per second for each of the three tag dic- tionaries, using the fast tagging method we pre- viously described <ref type="bibr" target="#b11">(Moore, 2014)</ref>, in a single- threaded implementation in Perl on a Linux work- station equipped with Intel Xeon X5550 2.67 GHz processors. Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred. For the pruned KN-smoothed dictionary, we pre- viously reported a speed of 49,000 tokens per sec- ond under similar conditions. Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2).</p><p>The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain eval- uation. For comparison, we tested our previous tagger and the fast version (english-left3words- distsim) of the Stanford tagger ( <ref type="bibr" target="#b13">Toutanova et al., 2003;</ref><ref type="bibr" target="#b5">Manning, 2011</ref>) recommended for prac- tical use on the Stanford tagger website, which we found to be by far the fastest of the six pub- licly available taggers tested in our previous work <ref type="bibr" target="#b11">(Moore, 2014)</ref>. The results of these tests are shown in  <ref type="table" target="#tab_1">Table 2</ref>: WSJ test set and Brown corpus tagging speeds and token accuracies</p><p>For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a dupli- cate of the earlier experiment using the faster ver- sion of Perl that we use here, and a third measure- ment including both the faster version of Perl and our improved low-level tagger implementation.</p><p>With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accu- rate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported. The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of un- ambiguous tokens by by 47%, compared to the previous state of the art <ref type="bibr" target="#b11">(Moore, 2014</ref>) for a tag dictionary that does not degrade tagging accuracy. When combined with our previous work on fast high-accuracy POS tagging, this tag dictionary produces by far the fastest POS tagger reported with anything close to comparable accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 . 6</head><label>2</label><figDesc></figDesc><table>WSJ 

All WSJ 
OOV WSJ 
Brown 
All Brown OOV Brown 
Tagger 
Tokens/Sec 
Accuracy 
Accuracy 
Tokens/Sec 
Accuracy 
Accuracy 
This work 
102k 
97.36% 
91.09% 
96k 
96.55% 
89.25% 
Our previous 51k/54k/69k 
97.34% 
90.98% 
40k/43k/56k 
96.54% 
89.36% 
Stanford fast 
80k 
96.87% 
89.69% 
50k 
95.53% 
87.38% 

</table></figure>

			<note place="foot" n="1"> According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word.</note>

			<note place="foot" n="2"> Merialdo (1994, p. 161) acknowledged this: &quot;In some sense this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags it actually had within the text.&quot;</note>

			<note place="foot" n="3"> https://catalog.ldc.upenn.edu/ LDC2011T07</note>

			<note place="foot" n="4"> CC, CD, DT, FW, IN, JJ, JJR, JJS, MD, NN, NNP, NNPS, NNS, PRP, RB, RBR, RP, UH, VB, VBD, VBG, VBN, VBP, and VBZ 5 CD, FW, JJ, NN, NNP, NNPS, NNS, RB, VB, VBD, VBG, VBN, and VBZ</note>

			<note place="foot" n="6"> In Table 2, &quot;OOV&quot; has the standard meaning of a token</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">of a word not occurring in the annotated training set. Many of these tokens do match words in our large semi-supervised tag dictionary, however. of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Applied Natural Language Processing</title>
		<meeting>the Second Conference on Applied Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-02-09" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
	<note>A stochastic parts program and noun phrase parser for unrestricted text</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A practical part-ofspeech tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penelope</forename><surname>Siburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Applied Natural Language Processing of the Association for Computational Linguistics</title>
		<meeting>the Third Conference on Applied Natural Language Processing of the Association for Computational Linguistics<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-03-31" />
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The automatic tagging of the LOB corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Atwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICAME Journal: International Computer Archive of Modern and Medieval English Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="13" to="33" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: Is it time for some linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing, 12th International Conference</title>
		<editor>Alexander Gelbukh</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6608</biblScope>
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
	<note>CICLing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Effective Self-Training for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parsing</surname></persName>
		</author>
		<title level="m">Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-04" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tagging English text with a probabalistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast high-accuracy partof-speech tagging by independent classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="1165" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-05-17" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Featurerich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-27" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-04" />
			<biblScope unit="page" from="919" to="926" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
