<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>2 BITS Pilani</addrLine>
									<settlement>Goa Campus</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>2 BITS Pilani</addrLine>
									<settlement>Goa Campus</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>2 BITS Pilani</addrLine>
									<settlement>Goa Campus</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>2 BITS Pilani</addrLine>
									<settlement>Goa Campus</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Steven</forename><surname>White</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>2 BITS Pilani</addrLine>
									<settlement>Goa Campus</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="67" to="81"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>67</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a large scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at http://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A plethora of new natural language inference (NLI) 1 datasets has been created in recent years ( <ref type="bibr">Bowman et al., 2015;</ref><ref type="bibr" target="#b34">Williams et al., 2017;</ref><ref type="bibr">Lai et al., 2017;</ref><ref type="bibr">Khot et al., 2018)</ref>. However, these datasets do not provide clear insight into what type of reasoning or inference a model may be perform- ing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differ- entiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models cap- ture distinct semantic phenomena necessary for general natural language understanding (NLU).</p><p>To answer these questions, we introduce the Diverse NLI Collection (DNC), a large-scale NLI dataset that tests a model's ability to perform di- verse types of reasoning. DNC is a collection of NLI problems, each requiring a model to perform <ref type="bibr">1</ref> The task of determining if a hypothesis would likely be inferred from a context, or premise; also known as Recogniz- ing Textual Entailment (RTE) ( <ref type="bibr">Dagan et al., 2006</ref><ref type="bibr">Dagan et al., , 2013</ref> a unique type of reasoning. Each NLI dataset con- tains labeled context-hypothesis pairs that we re- cast from semantic annotations for specific struc- tured prediction tasks. We extend various prior works on challenge NLI datasets ( <ref type="bibr" target="#b38">Zhang et al., 2017)</ref>, and define recasting as leveraging existing datasets to create NLI examples <ref type="bibr">(Glickman, 2006;</ref><ref type="bibr" target="#b30">White et al., 2017</ref>). We recast annotations from a total of 13 datasets across 7 NLP tasks into labeled NLI examples. The tasks include event factual- ity, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extrac- tion, pun detection, and lexicosyntactic inference. Currently, the DNC contains over half a million labeled examples. <ref type="table">Table 1</ref> includes NLI pairs that test specific types of reasoning.</p><p>Using a hypothesis-only NLI model, with ac- cess to just hypothesis sentences, as a strong base- line <ref type="bibr" target="#b25">(Tsuchiya, 2018;</ref><ref type="bibr">Gururangan et al., 2018;</ref><ref type="bibr" target="#b9">Poliak et al., 2018b</ref>), our experiments demonstrate how DNC can be used to probe a model's ability to capture different types of semantic reasoning necessary for general NLU. In short, this work answers a recent plea to the community to test "more kinds of inference" than in previous chal- lenge sets <ref type="bibr">(Chatzikyriakidis et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation &amp; Background</head><p>Compared to eliciting NLI datasets directly, i.e. asking humans to author contexts and/or hypoth- esis sentences, recasting can 1) help determine whether an NLU model performs distinct types of reasoning; 2) limit types of biases observed in pre- vious NLI data; and 3) generate examples cheaply, potentially at large scales.</p><p>NLU Insights Popular NLI datasets, e.g. Stan- ford Natural Language Inference (SNLI) <ref type="bibr">(Bowman et al., 2015</ref>) and its successor Multi- NLI ( <ref type="bibr" target="#b34">Williams et al., 2017)</ref>, were created by elic- iting hypotheses from humans. Crowd-source workers were tasked with writing one sentence each that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k cor- pus ( <ref type="bibr" target="#b37">Young et al., 2014)</ref>. Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothe- sis for each context and label. Such datasets can- not be used to determine how well an NLI model captures many desired capabilities of language un- derstanding systems, e.g. paraphrastic inference, complex anaphora resolution ( <ref type="bibr" target="#b30">White et al., 2017)</ref>, or compositionality <ref type="bibr" target="#b3">(Pavlick and Callison-Burch, 2016;</ref><ref type="bibr">Dasgupta et al., 2018)</ref>. By converting prior annotation of a specific phenomenon into NLI ex- amples, recasting allows us to create a diverse NLI benchmark that tests a model's ability to perform distinct types of reasoning.</p><p>Limit Biases Studies indicate that many NLI datasets contain significant biases. Examples in the early Pascal RTE datasets could be correctly predicted based on syntax alone <ref type="bibr" target="#b27">(Vanderwende and Dolan, 2006;</ref>). Statistical irregularities, and annotation artifacts, within class labels allow a hypothesis-only model to significantly outperform the majority baseline on at least six recent NLI datasets <ref type="bibr" target="#b9">(Poliak et al., 2018b</ref>). Class label biases may be attributed to the human-elicited protocol. Moreover, examples in such NLI datasets may contain racial and gen- dered stereotypes ). We limit some biases by not relying on hu- mans to generate hypotheses. Recast NLI datasets may still contain some biases, e.g. non-uniform distributions over NLI labels caused by the dis- tribution of labels in the original dataset that we recast. <ref type="bibr">2</ref> Experimental results using <ref type="bibr" target="#b9">Poliak et al. (2018b)</ref>'s hypothesis-only model indicate to what degree the recast datasets retain some biases that may be present in the original semantic datasets.</p><p>NLI Examples at Large-scale Generating NLI datasets from scratch is costly. Humans must be paid to generate or label natural language text. This linearly scales costs as the amount of gen- erated NLI-pairs increases. Existing annotations for a wide array of semantic NLP tasks are freely available. By leveraging existing semantic annota- tions already invested in by the community we can generate and label NLI pairs at little cost and cre- ate large NLI datasets to train data hungry models.</p><p>Why These Semantic Phenomena? A long term goal is to develop NLU systems that can achieve human levels of understanding and rea- soning. Investigating how different architectures and training corpora can help a system perform human-level general NLU is an important step in this direction. DNC contains recast NLI pairs that are easily understandable by humans and can be used to evaluate different sentence encoders and NLU systems. These semantic phenomena cover distinct types of reasoning that an NLU system may often encounter in the wild. While higher per- formance on these benchmarks might not be con- clusive proof of a system achieving human-level reasoning, a system that does poorly should not be viewed as performing human-level NLU. We ar- gue that these semantic phenomena play integral roles in NLU. There exist more semantic phenom- ena integral to NLU <ref type="bibr">(Allen, 1995)</ref> and we plan to include them in future versions of the DNC.</p><p>Previous Recast NLI Example sentences in RTE1 ( <ref type="bibr">Dagan et al., 2006</ref>) were extracted from MT, IE, and QA datasets, with the process re- ferred to as 'recasting' in the thesis by <ref type="bibr">Glickman (2006)</ref>. NLU problems were reframed under the NLI framework and candidate sentence pairs were extracted from existing NLP datasets and then la- beled under NLI ( <ref type="bibr">Dagan et al., 2006</ref>). Years later, this term was independently used by <ref type="bibr" target="#b30">White et al. (2017)</ref>, who proposed to "leverage existing large- scale semantic annotation collections as a source of targeted textual inference examples." The term 'recasting' was limited to automatically convert- ing existing semantic annotations into labeled NLI examples without manual intervention. We adopt the broader definition of 'recasting' since our NLI examples were automatically or manually gener- ated from prior NLU datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applied Framework versus Inference Probing</head><p>Traditionally, NLI has not been viewed as a down- stream, applied NLP task. <ref type="bibr">3</ref> Instead, the com- munity has often used it as "a generic evaluation framework" to compare models for distinct down- stream tasks <ref type="bibr">(Dagan et al., 2006</ref>) or to determine whether a model performs distinct types of rea- soning ( <ref type="bibr">Cooper et al., 1996</ref>). These two different evaluation goals may affect which datasets are re- cast. We target both goals as we recast applied tasks and linguistically focused phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recasting Semantic Phenomena</head><p>We describe efforts to recast 7 semantic phenom- ena from a total of 13 datasets into labeled NLI examples. Many of the recasting methods rely on simple templates that do not include nuances and variances typical of natural language. This allows us to specifically test how sentence representations capture distinct types of reasoning. When recast- ing, we preserve each dataset's train/dev/test split. If a dataset does not contain such a split, we cre- ate a random split with roughly a 80:10:10 ratio. <ref type="table">Table 2</ref> reports statistics about each recast dataset.</p><p>Event Factuality (EF) Event factuality pre- diction is the task of determining whether an event described in text occurred. Determining whether an event occurred enables accurate infer- ences, e.g. monotonic inferences, based on the event ( ). <ref type="bibr">4</ref> Incorporating factuality has been shown to improve NLI ( <ref type="bibr" target="#b19">Sauri and Pustejovsky, 2007)</ref>.</p><p>We recast event factuality annotations from UW ( <ref type="bibr">Lee et al., 2015</ref>), MEANTIME ( <ref type="bibr">Minard et al., 2016)</ref>, and Decomp ( ). We use sentences from original datasets as con- texts and templates (1a) and (1b) as hypotheses. <ref type="bibr">5</ref> (1) a. The Event happened b. The Event did not happen If the predicate denoting the Event was annotated as having happened in the factuality dataset, the context paired with (1a) is labeled as ENTAILED and the same context paired with (1b) is labeled as NOT-ENTAILED. Otherwise, we swap the labels.</p><p>Named Entity Recognition (NER) Distinct types of entities have different properties and re- lational objects <ref type="bibr" target="#b10">(Prince, 1978</ref>) that can help infer facts from a given context. For example, if a sys- tem can detect that an entity is a name of a nation, then that entity likely has a leader, a language, and a culture <ref type="bibr" target="#b10">(Prince, 1978;</ref><ref type="bibr" target="#b26">Van Durme, 2010</ref>). When classifying NLI pairs, a model can determine if an object mentioned in the hypothesis can be a re- lational object typically associated with the type of entity described in the context. NER tags can also be directly used to determine if a hypothesis is likely to not be entailed by a context, such as when entities in contexts and hypotheses do not share NER tags <ref type="bibr">(Castillo and Alemany, 2008;</ref><ref type="bibr" target="#b18">Sammons et al., 2009;</ref><ref type="bibr" target="#b1">Pakray et al., 2010)</ref>. Given a sentence annotated with NER tags, we recast the annotations by preserving the original sentences as contexts and creating hypotheses us- ing the template "NP is a Label." 6 For ENTAILED hypotheses we replace Label with the correct NER label of the NP; for NOT-ENTAILED hypotheses, we choose an incorrect label from the prior dis- tribution of NER tags for the given phrase. This prevents us from adding additional biases besides any class-label statistical irregularities present in the original data. We apply this procedure on the Gronigen Meaning Bank ( <ref type="bibr">Bos et al., 2017</ref>) and the <ref type="bibr">ConLL-2003 Shared Task (Tjong Kim Sang and</ref><ref type="bibr" target="#b24">De Meulder, 2003)</ref>.</p><p>Gendered Anaphora Resolution (GAR) The ability to perform pronoun resolution is essen- tial to language understanding, in many cases requiring common-sense reasoning about the world ( <ref type="bibr">Levesque et al., 2012</ref>). <ref type="bibr" target="#b30">White et al. (2017)</ref> show that this task can be directly recast as an NLI problem by transforming Winograd schemas into NLI sentence pairs.</p><p>Using  <ref type="table">Table 2</ref>: Statistics summarizing the recast datasets. The first column refers to the original annotation that was recast, the 'Combined' row refers to the combination of our recast datasets. The second column indicates the datasets that were recast, and the 3rd column reports how many labeled NLI pairs were extracted from the corresponding dataset. The last column indicates whether the recasting method was fully-automatic without human involvement (), manual (), or used a semi-automatic method that included human intervention (). The Multi-NLI and SNLI numbers contextualize the scale of our dataset.</p><p>adapted pronoun resolution task, they demonstrate the presence of systematic gender bias in corefer- ence resolution systems. We recast Winogender schemas as an NLI task, introducing a potential method of detecting gender bias in NLI systems or sentence embeddings. In recasting, the context is the original, unmodified Winogender sentence; the hypothesis is a short, manually constructed sen- tence having a correct (ENTAILED) or incorrect (NOT-ENTAILED) pronoun resolution.</p><p>Lexicosyntactic Inference (Lex) While many inferences in natural language are triggered by lex- ical items alone, there exist pervasive inferences that arise from interactions between lexical items and their syntactic contexts. This is particularly apparent among propositional attitude verbs -e.g. think, want, know -which display complex distri- butional profiles <ref type="bibr" target="#b31">(White and Rawlins, 2016)</ref>. For instance, the verb remember can take both finite clausal complements and infinitival clausal com- plements.</p><p>(2) a. Jo didn't remember that she ate b. Jo didn't remember to eat</p><p>This small change in the syntactic structure gives rise to large changes in the inferences that are li- censed: (2a) presupposes that Jo ate while (2b) entails that Jo didn't eat. We recast data from three datasets that are relevant to these sorts of lex- icosyntactic interactions. To recast these annotations, we assign the con- text sentences like (3a) to the majority class -yes, maybe or maybe not, no -across 10 different an- notators, after applying an ordinal model-based normalization to their responses. We then pair each context sentence with three hypotheses.</p><p>(4) a. That thing happened b. That thing may or may not have happened c. That thing didn't happen If annotated yes, maybe or maybe not, or no, the pair (3a)-(4a), (3a)-(4b), or (3a)-(4c) is respec- tively assigned ENTAILED and the other pairings are assigned NOT-ENTAILED; train/dev/test split labels are randomly assigned to every pair that context sentence appears in.</p><p>Lex #2: Recasting VerbNet (VN) We create ad- ditional lexicosyntactic NLI examples from Verb- Net ( <ref type="bibr" target="#b21">Schuler, 2005)</ref>. VerbNet contains classes of verbs that each can have multiple frames. Each frame contains a mapping from syntactic argu- ments to thematic roles, which are used as argu- ments in Neo-Davidsonian first-order logical pred- icates (5b) that describe the frame's semantics. Each frame additionally contains an example sen- tence (5a) that we use as our NLI context and we create templates (5c) from the most frequent se- mantic predicates to generate hypotheses (5d). <ref type="formula">(5)</ref>  application of force, change of physical or men- tal state, and valence, all of which "may be central organizing principles for a human's . . . conceptu- alization of the world." <ref type="bibr">(Hartshorne et al., 2013)</ref>. Each sentence in VC is judged based on the de- composed semantic properties. We convert each semantic property into declarative statements <ref type="bibr">10</ref> to create hypotheses and pair them with the original sentences which we preserve as contexts. The NLI pair is ENTAILED or NOT-ENTAILED depending on the given sentence's semantic judgment. Puns are prime examples of figurative language that may perplex general NLU systems as they are one of the more regular uses of linguistic ambi- guity <ref type="bibr">(Binsted, 1996)</ref> and rely on a wide-range of phonetic, morphological, syntactic, and semantic ambiguity ( <ref type="bibr" target="#b6">Pepicello and Green, 1984;</ref><ref type="bibr">Binsted, 1996;</ref><ref type="bibr">Bekinschtein et al., 2011</ref>).</p><p>We recast puns from <ref type="bibr" target="#b36">Yang et al. (2015)</ref> and <ref type="bibr">Miller et al. (2017)</ref> using templates to generate contexts (6a) and hypotheses (6b), (6c). We re- place Name with names sampled from a distribu- tion based on US census data, <ref type="bibr">11</ref> and Pun with the original sentence. If the original sentence was la- beled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT-ENTAILED, otherwise we swap the labels. (6) a. Name heard that Pun b. Name heard a pun c. Name did not hear a pun</p><p>Relation Extraction (RE) The goal of the rela- tion extraction (RE) task is to infer the real-world relationships between pairs of entities from natu- ral language text. The task is "grounded" in the sense that the input is natural language text and the output is entity1, relation, entity2 tuples defined in the schema of some knowledge base. RE requires a system to understand the many different surface forms which may entail the same underlying relation, and to distinguish those from surface forms which involve the same entities but do not entail the relation of interest. For example, (7a) is entailed by (7b) and (7c) but not by (7d).</p><p>(7) a. Name was born in Place b. Name is from Place c. Name, a Place native, . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Name visited Place</head><p>Natural language surface forms are often used in RE in a weak-supervision setting ( <ref type="bibr">Mintz et al., 2009;</ref><ref type="bibr">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b13">Riedel et al., 2013)</ref>. That is, if entity1 and entity2 are known to be related by relation, it is assumed that every sentence observed which mentions both entity1 and entity2 is assumed to be a real- ization of relation: i.e. (7d) would (falsely) be taken as evidence of the birthPlace relation.</p><p>Here we first generate hypotheses and then corresponding contexts.</p><p>To generate hypotheses, we begin with entity-relation triples extracted from DBPedia infoboxes: e.g. Barack Obama, birthPlace, Hawaii. These relation predicates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations are redundant with one another (birthPlace, hometown) and some relations do not correspond to obvious natural language glosses based on the name alone (demographics1Info). Thus, we construct a template for each predicate p by manually in- specting 1) a sample of entities which are related by p 2) a sample of sentences in which those entities co-occur and 3) the most frequent natural language strings which join entities related by p according to a OpenIE triple database ( <ref type="bibr" target="#b20">Schmitz et al., 2012;</ref><ref type="bibr">Fader et al., 2011</ref>) extracted from a large text corpus. We then manually write a simple template (e.g. Mention1 was born in Mention2) for p, ignoring any unclear relations. In total, we end up with 574 unique relations, expressed by 354 unique templates.</p><p>For each such hypothesis generated, we cre- ate a number of contexts.</p><p>We begin with the FACC1 corpus ( <ref type="bibr">Gabrilovich et al., 2013</ref>) which contains natural language sentences from ClueWeb in which entities have been auto- matically linked to disambiguated Freebase en- tities, when possible.</p><p>Then, given a tuple entity1, relation, entity2, we find ev- ery sentence which contains both entity1 and entity2. Since many of these sentences are false positives (7d), we have human annotators vet each context/hypothesis pair, using the ordinal en- tailment scale described in <ref type="bibr" target="#b38">Zhang et al. (2017)</ref>. We include optional binary labels by converting pairs labeled as 1 − 4 and 5 to ENTAILED and NOT-ENTAILED respectively. <ref type="bibr">12</ref> We apply pruning methods (described in <ref type="bibr">Appendix B.4</ref>) to combat issues related to noisy, ungrammatical hypotheses and disagreement between multiple annotators. Subjectivity (Sentiment) Some of the previ- ously discussed semantic phenomena deal with objective information -did an event occur or what type of entities does a specific name represent. Subjective information is often expressed differ- ently ( <ref type="bibr" target="#b33">Wiebe et al., 2005)</ref>, making it important to use other tests to probe whether an NLU sys- tem understands language that expresses subjec- tive information. We are interested in determining whether general NLU models capture 'subjective clues' that can help identify and understand emo- tions, opinions, and sentiment within a subjective text ( <ref type="bibr" target="#b35">Wilson et al., 2006</ref>).</p><p>We recast a sentiment analysis dataset since the task is the "expression of subjectivity as either a positive or negative opinion" <ref type="bibr" target="#b23">(Taboada, 2016)</ref>. We extract sentences from product, movie, and restau- rant reviews labeled as containing positive or neg- ative sentiment ( <ref type="bibr">Kotzias et al., 2015</ref>). Contexts (8a) and hypotheses (8b), (8c) are generated using the following templates: Item is replaced with either "product", "movie", or "restaurant", and the Name is sampled as previ- ously discussed. If the original sentence contained positive (negative) sentiment, the (8a)-(8b) pair is labeled as ENTAILED (NOT-ENTAILED) and (8a)- (8c) is labeled as NOT-ENTAILED (ENTAILED).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Noise in Recast Data</head><p>Recasting  In the DNC, most of the noisy examples are in the recast VerbNet and Relation Extraction por- tions. In recast VerbNet, some examples are noisy because of incorrect subject-verb agreement. <ref type="bibr">13</ref> Since more noisy examples appeared in the Rela- tion Extraction set, we relied on Amazon Mechan- ical Turk workers to flag ungrammatical hypothe- ses in the recast dataset, and we remove NLI pairs with ungrammatical hypotheses. <ref type="bibr">14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments demonstrate how these recast datasets may be used to evaluate how well mod- els capture different types of semantic reasoning necessary for general language understanding. We also include results from a hypothesis-only model as a strong baseline. This may reveal whether the recast datasets retain statistical irregularities from the original, task-specific annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>For demonstrating how well an NLI model performs these fine-grained types of reasoning, we use InferSent ( <ref type="bibr">Conneau et al., 2017)</ref>. InferSent independently encodes a context and hypothesis with a bi-directional LSTM and combines the sentence representations by con- catenating the individual sentence representations, <ref type="bibr">13</ref> "Her teeth was cared for" or "Floss were used". 14 See Appendix B.4 for details. their element-wise subtraction and product. The combined representation is then fed into a MLP with a single hidden layer. The hypothesis-only model is a modified version of InferSent that only accesses hypotheses ( <ref type="bibr" target="#b9">Poliak et al., 2018b</ref>). We report experimental details in Appendix C. <ref type="table" target="#tab_4">Table 3</ref> reports the models' accuracies across the recast NLI datasets. Even though we catego- rize VerbNet, MegaVeridicality, and VerbCorner as lexicosyntatic inference, we train and evaluate models separately on these three datasets because we use different strategies to individually recast them. When evaluating NLI models, our base- line is the maximum between the accuracies of the hypothesis-only model and the majority class label (MAJ). In six of the eight recast datasets that we use to train our models the hypothesis- only model outperforms MAJ. The two datasets where the hypothesis-only model does not outper- form MAJ are Sentiment and VN, each of which contain less than 10K examples. <ref type="bibr">15</ref> We do not train on GAR because of its small size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Our results suggest that InferSent, when not pre-trained on any other data, might capture spe- cific semantic phenomena better than other seman-tic phenomena. InferSent seems to learn the most about determining if an event occurred, since the difference between its accuracy and that of the hypothesis-only baseline (+13.93) is largest on the recast EF dataset compared to the other recast an- notations. The model seems to similarly learn to perform (or detect) the type of lexicosyntactic in- ference present in VC and MV. Interestingly, the hypothesis-only model outperforms InferSent on the recast RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis Only Baseline</head><p>The hypothesis-only model can demonstrate how likely it is that an NLI label applies to a hypothesis, regardless of its context and indicates how well each recast dataset tests a model's ability to perform each spe- cific type of reasoning when performing NLI. The high hypothesis-only accuracy on the recast NER dataset may demonstrate that the hypothesis-only model is able to detect that the distribution of class labels for a given word may be peaky. For ex- ample, Hong Kong appears 130 times in the train- ing set and is always labeled as a location. Based on this, in future work we may consider different methods to recast NER annotations into labeled NLI examples, or limit the dataset's training size.</p><p>Pre-training models on DNC We would like to know whether initializing models with pre-trained parameters improves scores. We notice that when we pre-train our models on DNC, for the larger datasets, a pre-trained model does not seem to significantly outperform randomly initializing the parameters. For the smaller datasets, specifically Puns, Sentiment and VN, a pre-trained model sig- nificantly outperforms random initialization. <ref type="bibr">16</ref> We are also interested to know whether fine- tuning these pre-trained models on each cate- gory (update) improves a model's ability to per- form well on the category compared to keeping the pre-trained models' parameters static (fixed). Across all of the recast datasets, updating the pre- trained model's parameters during training im- proves InferSent's accuracies more than keep- ing the model's parameters fixed. When updating a model pre-trained on the entire DNC, we see the largest improvements on VN (+9.15).</p><p>Models trained on Multi-NLI <ref type="bibr" target="#b34">Williams et al. (2017)</ref> argue that Multi-NLI " <ref type="bibr">[makes]</ref> it possible to evaluate systems on nearly the full complexity <ref type="bibr">16</ref> By 32.81, 31.00, and 30.83 points respectively. of the language." However, how well does Multi- NLI test a model's capability to understand the di- verse semantic phenomena captured in DNC? We posit that if a model, trained on and performing well on Multi-NLI, does not perform well on our recast datasets, then Multi-NLI might not evaluate a model's ability to understand the "full complex- ity" of language as argued. <ref type="bibr">17</ref> When trained on Multi-NLI, our InferSent model achieves an accuracy of 70.22% on (matched) Multi-NLI. <ref type="bibr">18</ref> When we test the model on the recast datasets (without updating the param- eters), we see significant drops. <ref type="bibr">19</ref> On the datasets testing a model's lexicosyntactic inference capa- bilities, the model performs below the majority class baseline. On the NER, EF, and Puns datasets its performs below the hypothesis-only baseline. We also notice that on three of the datasets (EF, Puns, and VN), the fixed hypothesis-only model outperforms the fixed InferSent model.</p><p>These results might suggest that Multi-NLI does not evaluate whether sentence representa- tions capture these distinct semantic phenomena. This is a bit surprising for some of the recast phe- nomena. We would expect Multi-NLI's fiction section (especially its humor subset) in the training set to contain some figurative language that might be similar to puns, and the travel guides (and pos- sibly telephone conversations) to contain text re- lated to sentiment.</p><p>Pre-training on DNC or Multi-NLI? Initializ- ing a model with parameters pre-trained on DNC or Multi-NLI often outperforms random initial- ization. <ref type="bibr">20</ref> Is it better to pre-train on DNC or Multi-NLI? On five of the recast datasets, using a model pre-trained on DNC outperforms a model pre-trained on Multi-NLI. The results are flipped on the two datasets focused on downstream tasks (Sentiment and RE) and MV. However, the differ- ences between pre-training on the DNC or Multi- NLI are small. From this, it is unclear whether pre-training on DNC is better than Multi-NLI.</p><p>Size of Pre-trained DNC Data We randomly sample 10K and 20K examples from each datasets' training set to investigate what happens if we train our models on a subsample of each train- ing set instead of the entire DNC. Although we no- ticed a slight decrease across each recast test set, the decrease was not significant. We leave this in- vestigating for a future thorough study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different lin- guistic phenomena. <ref type="bibr">Linzen et al. (2016)</ref> use "num- ber agreement in English subject-verb dependen- cies" to show that LSTMs learn about syntax- sensitive dependencies. In addition to syntax <ref type="bibr" target="#b22">(Shi et al., 2016)</ref>, researchers have used other label- ing tasks to investigate whether neural machine translation (NMT) models learn different linguis- tic phenomena <ref type="bibr">(Belinkov et al., 2017a,b;</ref><ref type="bibr">Dalvi et al., 2017;</ref><ref type="bibr">Marvin and Koehn, 2018)</ref>. Recently, <ref type="bibr" target="#b8">Poliak et al. (2018a)</ref> used recast NLI datasets to investigate semantics captured by NMT encoders.</p><p>Targeted Tests for Natural Language Under- standing We follow a long line of work focused on building datasets to test how well NLU sys- tems perform distinct types of semantic reason- ing. FraCaS uses a limited number of sentence- pairs to test whether systems understand seman- tic phenomena, e.g. generalized quantifiers, tem- poral references, and (nominal) anaphora <ref type="bibr">(Cooper et al., 1996</ref>). FraCas cannot be used to train neu- ral models -it includes just roughly 300 high- quality instances manually created by linguists. <ref type="bibr">MacCartney (2009)</ref> created the FraCaS textual in- ference test suite by automatically "convert <ref type="bibr">[ing]</ref> each FraCaS question into a declarative hypoth- esis." <ref type="bibr">Levesque et al. (2012)</ref>'s Winograd Schema Challenge forces a model to choose between two possible answers for a question based on a sen- tence describing an event.</p><p>Recent benchmarks test whether NLI models handle adjective-noun composition <ref type="bibr" target="#b3">(Pavlick and Callison-Burch, 2016</ref>), other types of composi- tion ( <ref type="bibr">Dasgupta et al., 2018)</ref>, paraphrastic infer- ence, anaphora resolution, and semantic proto- roles ( <ref type="bibr" target="#b30">White et al., 2017)</ref>. Concurrently, Con- neau et al. (2018)'s benchmark can be used to probe whether sentence representations capture many linguistic properties. It includes syntactic and surface form tests but does not focus on as a wide range of semantic phenomena as in the DNC. <ref type="bibr">Glockner et al. (2018)</ref> introduce a modified ver- sion of SNLI to test how well NLI models perform when requiring lexical and world knowledge. <ref type="bibr" target="#b29">Wang et al. (2018)</ref>'s GLUE dataset is intended to evaluate and potentially train a sentence rep- resentation to perform well across different NLP tasks. This continues an aspect of the initial RTE collection, designed to be representative of down- stream tasks like QA, MT, and IR ( <ref type="bibr">Dagan et al., 2010)</ref>. While GLUE is therefore concerned with applied tasks, DNC, as well as <ref type="bibr" target="#b0">Naik et al. (2018)</ref>'s NLI stress tests, is concerned with probing the ca- pabilities of NLU models to capture explicitly dis- tinguished aspects of meaning. While one may conjecture that the latter is needed to be "solved" to eventually "solve" the former, it may be that these goals only partially overlap. Some NLP researchers might focus on probing for semantic phenomena in sentence representations while oth- ers may be more interested in developing single sentence representations that can help models per- form well on a wide array of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described how we recast a wide range of se- mantic phenomena from many NLP datasets into labeled NLI sentence pairs. These examples serve as a diverse NLI framework that may help di- agnose whether NLU models capture and per- form distinct types of reasoning. Our experiments demonstrate how to use this framework as an NLU benchmark. The DNC is actively growing as we continue recasting more datasets into labeled NLI examples. We encourage dataset creators to re- cast their datasets in NLI and invite them to add their recast datasets into the DNC. The collection, along with baselines and trained models are avail- able online at http://www.decomp.net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.</p><p>Generating entailment rules from framenet. In Pro- ceedings of the ACL 2010 Conference Short Papers, pages 241-246. Association for Computational Lin- guistics.</p><p>James Allen. 1995. Natural language understanding.</p><p>Pearson.</p><p>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceed- ings of the 17th international conference on Compu- tational linguistics-Volume 1, pages 86-90. Associ- ation for Computational Linguistics. Evgeniy Gabrilovich, Michael Ringgaard, and Amar- nag Subramanya. 2013. Facc1: Freebase an- notation of clueweb corpora, version 1 (re- lease date 2013-06-26, format version 1, cor- rection level 0).</p><p>Note: http://lemurproject. org/clueweb09/FACC1/Cited by, 5.  <ref type="table">Table 4</ref> includes examples from all of the recast NLI datasets. We include one ENTAILED and one NOT-ENTAILED example from each dataset that tests a distinct type of reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Recasting Semantic Phenomena</head><p>Here we add secondary information about the original datasets and our recasting efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Event Factuality</head><p>We demonstrate how determining whether an event occurred can enable accurate inferences based on the event. Consider the following sen- tences:</p><p>(9) a. She walked a beagle b. She walked a dog c. She walked a brown beagle</p><p>If the walking occurred, (9a) entails (9b) but not (9c). If we negate the action in sentences (9a), (9b), and (9c) to respectively become:</p><p>(10) a. She did not walk a beagle b. She did not walk a dog c. She did not walk a brown beagle</p><p>The new hypothesis (10c) is now entailed by the context (10a) while (10b) is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Lexicosyntactic Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 VerbCorner</head><p>When recasting VerbCorner, we use the following templates for hypotheses, assigning them as EN- TAILED and NOT-ENTAILED based on the positive or negative answers to the annotation task ques- tions about the context sentence.</p><p>(11) a. Someone {moved/did not move} from their location b. Something touched another thing / Noth- ing touched anything else c. Someone or something {applied/did not apply} force onto something d. Someone or something {changed/did not change} physically e. Someone {changed/did not change} their thoughts, feelings, or beliefs f. Something {good/neutral/bad} happened</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Figurative Language</head><p>Puns in <ref type="bibr" target="#b36">Yang et al. (2015)</ref> were originally ex- tracted from punsoftheday.com, and sen- tences without puns came from newswire and proverbs. The sentences are labeled as contain- ing a pun or not. Puns in <ref type="bibr">Miller et al. (2017)</ref> were sampled from prior pun detection datasets <ref type="bibr">(Miller and Gurevych, 2015;</ref><ref type="bibr">Miller and Turkovi´cTurkovi´c, 2016)</ref> and includes new examples generated from scratch for the shared task; the original labels denote whether the sentences contain homographic, het- erographic, or no pun at all. Here, we are only interested in whether a sentence contains a pun or not instead of discriminating between homo- graphic and heterographic puns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Relation Extraction</head><p>Since hypotheses were automatically generated from Wikipedia infoboxes, many examples are noisy and ungrammatical. We presented hypothe- ses (independent of their corresponding contexts) to Mechanical Turk workers and asked them to la- bel each sentence as containing no grammatical error, minor grammatical issues, or major gram- matical issues. We removed the 2, 056 NLI exam- ples with hypothesis containing major grammati- cal issues, resulting in 28, 041 labeled pairs. In- terestingly, almost 70% of those examples where labeled between 1 − 4, which we view as NOT- ENTAILED. We release the ungrammatical NLI examples as supplementary data.</p><p>A second source of noise in the recast relation extraction dataset can be caused by disagreement amongst multiple annotators. Examples in our training and development sets are annotated by a single annotator while we use 3-to 5-way redun- dancy to annotate the test examples. To guaran- tee high-quality test examples, we only include examples with 100% inner-annotator agreement. Additionally, we remove the 16 examples labeled with 4 from our NOT-ENTAILED examples in this pruned test set since some of these examples are arguably entailments. Consequently, the test set contains 761 examples, out of the original 3, 670 test examples. Nevertheless, we separately release all 3, 670 test examples and include the original annotations as well, enabling others to consider other methods to collapse the multi-way annota- tions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Lex #1: MegaVeridicality (MV) White and Rawlins (2018) build the MegaVeridicality dataset by selecting verbs from the MegaAttitude dataset (White and Rawlins, 2016) based on their gram- matical acceptability in the [NP _ that S] and [NP was _ed that S] frames. 7 They then asked anno- tators to answer questions of the form in (3) us- ing three possible responses: yes, maybe or maybe not, and no (Karttunen et al., 2014). (3) a. Someone {knew, didn't know} that a par- ticular thing happened. b. Did that thing happen? We use the same procedure to annotate sentences containing verbs that take various types of infini- tival complement: [NP _ for NP to VP], [NP _ to VP], [NP _ NP to VP], and [NP was _ed to VP]. 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figurative</head><label></label><figDesc>Figurative Language (Puns) Figurative language demonstrates natural language's expressiveness and wide variations. Understanding and recognizing figurative language "entail[s] cognitive capabilities to abstract and meta-represent meanings beyond physical words" (Reyes et al., 2012). Puns are prime examples of figurative language that may perplex general NLU systems as they are one of the more regular uses of linguistic ambiguity (Binsted, 1996) and rely on a wide-range of phonetic, morphological, syntactic, and semantic ambiguity (Pepicello and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data, 11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT-ENTAILED, otherwise we swap the labels. (6) a. Name heard that Pun b. Name heard a pun c. Name did not hear a pun</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 8 )</head><label>8</label><figDesc>a. When asked about Item, Name said Review b. Name liked the Item c. Name did not like the Item</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Find him before he finds the dog food 
Event 
The finding did not happen 


Factuality I'll need to ponder 
The pondering happened 


Ward joined Tom in their native Perth 
Relation 
Ward was born in Perth 


Extraction Stefan had visited his son in Bulgaria 
Stefan was born in Bulgaria 


Kim heard masks have no face value 
Kim heard a pun 


Tod heard that thrift is better than annuity 
Puns 

Tod heard a pun 


Table 1: Example sentence pairs for different semantic phe-

nomena. indicates the line is a context and the following 
line is its corresponding hypothesis. and respectively in-
dicate that the context entails, or does not entail the hypothe-
sis. Appendix A includes more recast examples. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>NLI accuracies on test data. Columns correspond to each semantic phenomena and rows correspond to the model used. Columns are ordered from larger to smaller in size, but the last three (VC, MV, VN) are separated since they fall under lexicosyntactic inference. (update) refers to a model that was initialized with pre-trained parameters and then re-trained on the corresponding recast data. (fixed) refers to a model that was trained and then evaluated on these data sets. Bold numbers in each column indicate which settings were responsible for the highest accuracy on the specific recast dataset.</figDesc><table></table></figure>

			<note place="foot" n="2"> In a corpus with part-of-speech tags, the distribution of labels for the word &quot;the&quot; will likely peak at the Det tag.</note>

			<note place="foot" n="3"> This changed as large NLI datasets have recently been used to train, or pre-train, models to perform NLI, or other tasks (Conneau et al., 2017; Pasunuru and Bansal, 2017). 4 Appendix B.1 provides an example. 5 We replace Event with the event described in the context.</note>

			<note place="foot" n="6"> We ensure grammatical hypotheses by appropriately conjugating &quot;is a&quot; when needed.</note>

			<note place="foot" n="7"> NP is always instantiated by someone; and S is always instantiated by a particular thing happened. 8 NP is always instantiated by either someone, a particular person, or a particular thing; and VP is always instantiated by happen, do a particular thing, or have a particular thing.</note>

			<note place="foot" n="9"> This is similar to Aharon et al. (2010)&apos;s template matching to generate entailment rules from FrameNet (Baker et al., 1998).</note>

			<note place="foot" n="10"> We list the declarative statements in Appendix B.2.1. 11 http://www.ssa.gov/oact/babynames/ names.zip</note>

			<note place="foot" n="12"> Following the label set in SNLI, Zhang et al. (2017) converted pairs labeled with 1 as CONTRADICTION, 2 − 4 as NEUTRAL and 5 to ENTAILMENT. Since here we are generally interested in binary classification, we merge the CONTRADICTION and NEUTRAL examples as NOT-ENTAILED.</note>

			<note place="foot" n="15"> This is similar to Poliak et al. (2018b)&apos;s results where a hypothesis-only model did not outperform MAJ on datasets with ≤ 10K examples.</note>

			<note place="foot" n="17"> We treat Multi-NLI&apos;s NEUTRAL and CONTRADICTION labels as equivalent to the DNC&apos;s NOT-ENTAILED label. 18 Although this is about 10 points below SoTA, we believe that the pre-trained model performs well enough to evaluate whether Multi-NLI tests a model&apos;s capability to understand the diverse semantic phenomena in the DNC. 19 InferSent (pre-trained, fixed) in Table 3. 20 Pre-training does not improve accuracies on NER or MV.</note>

			<note place="foot" n="21"> http://www.yelp.com/dataset_challenge divided by 5. As described in Poliak et al. (2018b), our hypothesis-only model feeds the hypotheses&apos; encoded representation directly into the MLP.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Diyi Yang for help with the Pun-sOfTheDay dataset, the JSALT "Sentence Rep-resentation" team for insightful discussions, and three anonymous reviewers for feedback. This work was supported by the JHU HLT-COE, DARPA LORELEI and AIDA, NSF-BCS (1748969/1749025), and NSF-GRFP (1232825). The views and conclusions contained in this pub-lication are those of the authors and should not be interpreted as representing official policies or en-dorsements of DARPA or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head><p>In all our experiments, we use pre-computed GloVe embeddings ( <ref type="bibr" target="#b5">Pennington et al., 2014</ref>) and use the OOV vector for words that do not have a defined embedding. We follow <ref type="bibr">Conneau et al. (2017)</ref>'s procedure to train our models. During training, our models are optimized with SGD. Our initial learning rate is 0.1 with a decay rate of 0.99. Our models train for at most 20 epochs and can optionally terminate early when the learning rate is less than 10 −5 . If the accuracy deceases on the development set in any epoch, the learning rate is</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stress test evaluation for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2340" to="2353" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ju_cse_tac: Textual entailment recognition system at tac rte-6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santanu</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander F</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TAC Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask video captioning with video and entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Most &quot;babies&quot; are &quot;little&quot; and most &quot;problems&quot; are &quot;huge&quot;: Compositional entailment in adjectivenouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2164" to="2173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Framenet+: Fast paraphrastic tripling of framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="408" to="413" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language of riddles: new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pepicello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas A Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
		<respStmt>
			<orgName>The Ohio State University Press</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the evaluation of semantic phenomena in neural machine translation using natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypothesis only baselines in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the function of existential presupposition in discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellen F Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Regional Meeting. Chicago Ling. Soc. Chicago, Ill</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="362" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From humor recognition to irony detection: The figurative language of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Social bias in elicited natural language inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural models of factuality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North</title>
		<meeting>the 2018 Conference of the North</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">American</forename><surname>Chapter</surname></persName>
		</author>
		<title level="m">the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="731" to="744" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation alignment for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Vg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vydiswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Johri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TAC Workshop</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Determining modality and factuality for text entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Sauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Computing, 2007. ICSC 2007. International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Verbnet: A broadcoverage, comprehensive verb lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentiment analysis: an overview from linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="325" to="347" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CONLL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Performance impact caused by hidden bias of training data for recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Tsuchiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Language Resources and Evaluation (LREC2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Extracting Implicit Knowledge from Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Rochester, NY 14627</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Rochester</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What syntax can contribute in the entailment task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second PASCAL Challenges Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Arul Menezes, and Rion Snow</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amapreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inference is everything: Recasting semantic resources into a unified evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A computational model of s-selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Rawlins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantics and linguistic theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="641" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of veridicality and factivity in clause selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Rawlins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the North East Linguistic Society</title>
		<meeting>the 48th Annual Meeting of the North East Linguistic Society<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<publisher>GLSA Publications</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing strong and weak opinion clauses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Humor recognition and humor anchor extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ordinal common-sense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="379" to="395" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
