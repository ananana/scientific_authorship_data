<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam Science</orgName>
								<address>
									<addrLine>Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam Science</orgName>
								<address>
									<addrLine>Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">436</biblScope>
							<biblScope unit="page" from="436" to="446"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 BLEU points over back-translation using random sampling for GermanÑEnglish and EnglishÑGerman, respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) using a sequence-to-sequence model has achieved state- of-the-art performance for several language pairs ( <ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b22">Sutskever et al., 2014;</ref><ref type="bibr" target="#b5">Cho et al., 2014</ref>). The availability of large-scale train- ing data for these sequence-to-sequence models is essential for achieving good translation quality.</p><p>Previous approaches have focused on leverag- ing monolingual data which is available in much larger quantities than parallel data <ref type="bibr" target="#b12">(Lambert et al., 2011</ref>). <ref type="bibr" target="#b8">Gulcehre et al. (2017)</ref> proposed two methods, shallow and deep fusion, for integrat- ing a neural language model into the NMT sys- tem. They observe improvements by combining the scores of a neural language model trained on target monolingual data with the NMT system. <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref> proposed back- translation of monolingual target sentences to the source language and adding the synthetic sentences to the parallel data. In this approach a reverse model trained on parallel data is used to translate sentences from target-side monolingual data into the source language. This synthetic parallel data is then used in combination with the actual parallel data to re-train the model. This approach yields state-of-the-art results even when large parallel data are available and has become common practice in NMT ( <ref type="bibr">Sennrich et al., 2017;</ref><ref type="bibr" target="#b7">García-Martínez et al., 2017;</ref>.</p><p>While back-translation has been shown to be very effective to improve translation quality, it is not exactly clear why it helps. Generally speak- ing, it mitigates the problem of overfitting and fluency by exploiting additional data in the tar- get language. An important question in this con- text is how to select the monolingual data in the target language that is to be back-translated into the source language to optimally benefit transla- tion quality. <ref type="bibr" target="#b16">Pham et al. (2017)</ref> experimented with using domain adaptation methods to select mono- lingual data based on the cross-entropy between the monolingual data and in-domain corpus <ref type="bibr" target="#b1">(Axelrod et al., 2015</ref>) but did not find any improve- ments over random sampling as originally pro- posed by <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref>. Earlier work has explored to what extent data selection of paral- lel corpora can benefit translation quality <ref type="bibr" target="#b0">(Axelrod et al., 2011;</ref><ref type="bibr" target="#b23">van der Wees et al., 2017</ref>), but such selection techniques have not been investigated in the context of back-translation.</p><p>In this work, we explore different aspects of the back-translation method to gain a better under- standing of its performance. Our analyses show that the quality of the synthetic data acquired with a reasonably good model has a small impact on the effectiveness of back-translation, but the ratio of synthetic to real training data plays a more im- portant role. With a higher ratio, the model gets bi- ased towards noises in synthetic data and unlearns the parameters completely. Our findings show that it is mostly words that are difficult to predict in the target language that benefit from additional back- translated data. These are the words with high prediction loss during training when the transla- tion model converges. We further investigate these difficult words and explore alternatives to random sampling of sentences with a focus on increasing occurrences of such words.</p><p>Our proposed approach is twofold: identifying difficult words and sampling with the objective of increasing occurrences of these words, and iden- tifying contexts where these words are difficult to predict and sample sentences similar to the diffi- cult contexts. With targeted sampling of sentences for back-translation we achieve improvements of up to 1.7 BLEU points over back-translation using random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Back-Translation for NMT</head><p>In this section, we briefly review a sequence-to- sequence NMT system and describe our experi- mental settings. We then investigate different as- pects and modeling challenges of integrating the back-translation method into the NMT pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>The NMT system used for our experiments is an encoder-decoder network with recurrent archi- tecture ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref>. For training the NMT system, two sequences of tokens, X " " x 1 , . . . , x n ‰ and Y " " y 1 , . . . , y m ‰ , are given in the source and target language, respectively.</p><p>The source sequence is the input to the encoder which is a bidirectional long short-term memory network generating a representation s n . Using an attention mechanism ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, the attentional hidden state is: r h t " tanhpW c rc t ; h t sq where h t is the target hidden state at time step t and c t is the context vector which is a weighted average of s n .</p><p>The decoder predicts each target token y t by computing the probability:</p><formula xml:id="formula_0">ppy t |y ăt , s n q " softmaxpW o r h t q</formula><p>For the token y t , the conditional probability ppy t |y ăt , s n q during training quantifies the diffi- culty of predicting that token in the context y ăt . The prediction loss of token y t is the negative log- likelihood of this probability.</p><p>During training on a parallel corpus D, the cross-entropy objective function is defined as:</p><formula xml:id="formula_1">L " ÿ pX,Y qPD m ÿ i"1</formula><p>´ log ppy i |y ăi , s n q</p><p>The objective of this function is to improve the model's estimation of predicting target words given the source sentence and the target context.</p><p>The model is trained end-to-end by minimizing the negative log likelihood of the target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Setup</head><p>For the translation experiments, we use EnglishØGerman WMT17 training data and report results on newstest <ref type="bibr">, 2016</ref><ref type="bibr" target="#b3">(Bojar et al., 2017</ref>).</p><p>As NMT system, we use a 2-layer attention- based encoder-decoder model implemented in OpenNMT ( <ref type="bibr" target="#b11">Klein et al., 2017</ref>) trained with em- bedding size 512, hidden dimension size 1024, and batch size 64. We pre-process the training data with Byte-Pair Encoding (BPE) using 32K merge operations ( <ref type="bibr" target="#b21">Sennrich et al., 2016b)</ref>.</p><p>We compare the results to <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref> by back-translating random sentences from the monolingual data and combine them with the parallel training data. We perform the random selection and re-training 3 times and report the av- eraged outcomes for the 3 models. In all experi- ments the sentence pairs are shuffled before each epoch.</p><p>We measure translation quality by single- reference case-sensitive BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) computed with the multi-bleu.perl script from Moses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Size of the Synthetic Data in Back-Translation</head><p>One selection criterion for using back-translation is the ratio of real to synthetic data. <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref> showed that higher ratios of synthetic data leads to decreases in translation performance.</p><p>In order to investigate whether the improve- ments in translation performance increases with higher ratios of synthetic data, we perform three experiments with different sizes of synthetic data.   GermanÑEnglish translation quality (BLEU) of systems with different ratios of real:syn data. <ref type="figure" target="#fig_2">Figure 1</ref> shows the perplexity as a function of training time for different sizes of synthetic data. One can see that all systems perform similarly in the beginning and converge after observing in- creasingly more training instances. However, the model with the ratio of (1:10) synthetic data gets increasingly biased towards the noisy data after 1M instances. Decreases in performance with more synthetic than real data is also inline with findings of <ref type="bibr" target="#b17">Poncelas et al. (2018)</ref>.</p><p>Comparing the systems using ratios of (1:1) and (1:4), we see that the latter achieves lower perplex- ity during training. <ref type="table">Table 1</ref> presents the perfor- mance of these systems on the GermanÑEnglish translation task. The BLEU scores show that the translation quality does not improve linearly with the size of the synthetic data. The model trained on (1:4) real to synthetic ratio of training data achieves the best results, however, the perfor- mance is close to the model trained on (1:1) train- ing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Direction of Back-Translation</head><p>Adding monolingual data in the target language to the training data has the benefit of introducing new context and improving the fluency of the transla- tion model. The automatically generated trans- lations in the source language while being erro- neous, introduce new context for the source words and will not affect the translation model signifi- cantly.</p><p>Monolingual data is available in large quantities for many languages. The decision of the direction of back-translation is subsequently not based on the monolingual data available, but on the advan- tage of having more fluent source or target sen- tences. <ref type="bibr" target="#b12">Lambert et al. (2011)</ref> show that adding syn- thetic source and real target data achieves im- provements in traditional phrase-based machine translation (PBMT). Similarly in previous works  <ref type="table">Table 2</ref>:</p><p>EnglishÑGerman translation quality (BLEU) of systems using forward and reverse models for generating synthetic data.</p><p>in NMT, back-translation is performed on mono- lingual data in the target language.</p><p>We perform a small experiment to measure whether back-translating from source to target is also beneficial for improving translation quality. <ref type="table">Table 2</ref> shows that in both directions the perfor- mance of the translation system improves over the baseline. This is in contrast to the findings of Lam- bert et al. <ref type="formula">(2011)</ref> for PBMT systems where they show that using synthetic target data does not lead to improvements in translation quality.</p><p>Still, when adding monolingual data in the tar- get language the BLEU scores are slightly higher than when using monolingual data in the source language. This indicates the moderate importance of having fluent sentences in the target language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Quality of the Synthetic Data in Back-Translation</head><p>One selection criterion for back-translation is the quality of the synthetic data. <ref type="bibr" target="#b10">Khayrallah and Koehn (2018)</ref> studied the effects of noise in the training data on a translation model and discov-ered that NMT models are less robust to many types of noise than PBMT models. In order for the NMT model to learn from the parallel data, the data should be fluent and close to the man- ually generated translations. However, automati- cally generating sentences using back-translation is not as accurate as manual translations.  GermanÑEnglish translation quality (BLEU).</p><p>To investigate the oracle gap between the performance of manually generated and back- translated sentences, we perform a simple exper- iment using the existing parallel training data. In this experiment, we divide the parallel data into two parts, train the reverse model on the first half of the data and use this model to back-translate the second half. The manually translated sentences of the second half are considered as ground truth for the synthetic data. <ref type="table" target="#tab_3">Table 3</ref> shows the BLEU scores of the experi- ments. As to be expected, re-training with addi- tional parallel data yields higher performance than re-training with additional synthetic data. How- ever, the differences between the BLEU scores of these two models are surprisingly small. This indi- cates that performing back-translation with a rea- sonably good reverse model already achieves re- sults that are close to a system that uses additional manually translated data. This is inline with find- ings of <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref> who observed that the same monolingual data translated with three translation systems of different quality and used in re-training the translation model yields similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Back-Translation and Token Prediction</head><p>In the previous section, we observed that the re- verse model used to back-translate achieves re- sults comparable to manually translated sentences. Also, there is a limit in learning from synthetic data, and with more synthetic data the model un- learns its parameters completely. In this section, we investigate the influence of the sampled sentences on the learning model. <ref type="bibr" target="#b6">Fadaee et al. (2017)</ref> showed that targeting specific words during data augmentation improves the gen- eration of these words in the right context. Specifi- cally, adding synthetic data to the training data has an impact on the prediction probabilities of indi- vidual words. In this section, we further examine the effects of the back-translated synthetic data on the prediction of target tokens. As mentioned in Section 2.1, the objec- tive function of training an NMT system is to minimize L by minimizing the prediction lossíog lossíog ppy t |y ăt , s n q for each target token in the training data. The addition of monolingual data in the target language improves the estimation of the probability ppY q and consequently the model generates more fluent sentences. <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref> show that by using back- translation, the system with target-side monolin- gual data reaches a lower perplexity on the de- velopment set. This is expected since the do- main of the monolingual data is similar to the do- main of the development set. To investigate the model's accuracy independent from the domains of the data, we collect statistics of the target token prediction loss during training. <ref type="figure" target="#fig_3">Figure 2</ref> shows changes of token prediction loss when training converges and the weights are verg- ing on being stable. The values are sorted by mean token prediction loss of the system trained on real parallel data.</p><p>We observe an effect similar to distributional smoothing <ref type="bibr" target="#b4">(Chen and Goodman, 1996)</ref>: While prediction loss increases slightly for most tokens, the largest decrease in loss occurs for tokens with high prediction loss values. This indicates that by randomly sampling sentences for back-translation, the model improves its estimation of tokens that were originally more difficult to predict, i.e., to- kens that had a high prediction loss. Note that we compute the token prediction loss in just one pass over the training corpus with the final model and as a result it is not biased towards the order of the data.</p><p>This finding motivates us to further explore sampling criteria for back-translation that con- tribute considerably to the parameter estimation of the translation model. We propose that by over- sampling sentences containing difficult-to-predict tokens we can maximize the impact of using the monolingual data. After translating sentences con- taining such tokens and including them in the training data, the model becomes more robust in predicting these tokens.</p><p>In the next two sections, we propose several methods of using the target token prediction loss to identify the most rewarding sentences for back- translating and re-training the translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Targeted Sampling for Difficult Words</head><p>One of the main outcomes of using synthetic data is better estimation of words that were originally difficult to predict as measured by their high pre- diction losses during training. In this section, we propose three variations of how to identify these words and perform sampling to target these words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sampling for difficult words</head><p>Input: Difficult tokens D " tyiu D i"1 , monolingual corpus M, number of required samples N Output: Sampled sentences S " tSiu N i"1 where each sentence Si is sampled from M 1: procedure DIFFSAMPLING (D, M, N ): 2:</p><p>Initialize S " tu 3: repeat 4:</p><p>Sample Sc from M 5:</p><p>for all tokens y in Sc do 6:</p><p>if y P D then 7:</p><p>Add Sc to S 8:</p><p>until |S| " N 9:</p><p>return S <ref type="figure" target="#fig_3">Figure 2</ref> shows that the majority of tokens with high mean prediction losses have low frequencies in the training data. Additionally, the majority of decreases in prediction loss after adding synthetic sentence pairs to the training data occurs with less frequent tokens. Note that these tokens are not necessarily rare in the traditional sense; in <ref type="figure" target="#fig_3">Figure 2</ref> the 10k less frequent tokens in the target vocabulary benefit most from back-translated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Token Frequency as a Feature of Difficulty</head><p>Sampling new contexts from monolingual data provides context diversity proportional to the to- ken frequencies and less frequent tokens benefit most from new contexts. Algorithm 1 presents this approach where the list of difficult tokens is defined as:</p><p>D " t@y i P V t : f reqpy i q ă ηu V t is the target vocabulary and η is the frequency threshold for deciding on the difficulty of the to- ken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Difficult Words with High Mean Prediction Losses</head><p>In this approach, we use the mean losses to iden- tify difficult-to-predict tokens. The mean predic- tion lossˆpyqlossˆ lossˆpyq of token y during training is defined as follows:</p><formula xml:id="formula_2">ˆ pyq " 1 n y N ÿ n"1 |Y n | ÿ t"1</formula><p>´ log ppy n t |y n ăt , s n qδpy n t , yq where n y is the number of times token y is ob- served during training, i.e., the token frequency of y, N is the number of sentences in the training data, |Y n | is the length of target sentence n, and δpy n t , yq is the Kronecker delta function, which is 1 if y n t " y and 0 otherwise. By specifically providing more sentences for difficult words, we improve the model's estimation and decrease the model's uncertainty in prediction. During sampling from the monolingual data, we select sentences that contain difficult words.</p><p>Algorithm 1 presents this approach where the list of difficult tokens is defined as:</p><p>D " t@y i P V t : ˆ py i q ą µu V t is the vocabulary of the target language and µ is the threshold on the difficulty of the token.   <ref type="table">Table 4</ref>: GermanØEnglish translation quality (BLEU). Experiments marked : are averaged over 3 runs. MEANPREDLOSS and FREQ are difficulty criteria based on mean token prediction loss and token fre- quency respectively. MEANPREDLOSS + STDPREDLOSS is experiments favoring tokens with skewed prediction losses. PRESERVE PREDLOSS RATIO preserves the ratio of the distribution of difficult con- texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Difficult Words with Skewed Prediction Losses</head><p>By using the mean loss for target tokens as defined above, we do not discriminate between differences in prediction loss for occurrences in different con- texts. This lack of discrimination can be problem- atic for tokens with high loss variations. For in- stance, there can be a token with ten occurrences, out of which two have high and eight have low prediction loss values. We hypothesize that if a particular token is eas- ier to predict in some contexts and harder in oth- ers, the sampling strategy should be context sensi- tive, allowing to target specific contexts in which a token has a high prediction loss. In order to dis- tinguish between tokens with a skewed and tokens with a more uniform prediction loss distribution, we use both mean and standard deviation of token prediction losses to identify difficult tokens.</p><p>Algorithm 1 formalizes this approach where the list of the difficult tokens is defined as:</p><p>D " t@y i P V t : ˆ py i q ą µ ^ σppy i qq ą ρû ρû py i q is the mean and σppy i qq is the standard deviation of prediction loss of token y i , V t is the vocabulary list of the target language, and µ and ρ are the thresholds for deciding on the difficulty of the token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Preserving Sampling Ratio of Difficult Occurrences</head><p>Above we examined the mean of prediction loss for each token over all occurrences, in order to identify difficult-to-predict tokens. However, the uncertainty of the model in predicting a difficult</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Sampling with ratio preservation</head><p>Input: Difficult tokens and the corresponding sentences in the bitext D " tyt, Yy t " ry1, . . . , yt, . . . , ymsu, monolingual corpus M, number of required samples N Output: Sampled sentences S " tSiu N i"1 where each sentence Si is sampled from M 1: procedure PREDLOSSRATIOSAMPLING(D, M, N ): 2:</p><p>Initialize S " tu 3:</p><p>Hpytq " N ˆ|py t ,¨qPD| |py¨,¨qPD| 4:</p><formula xml:id="formula_3">repeat 5:</formula><p>Sample Sc from M 6:</p><p>for all tokens y in Sc do 7:</p><p>if |y P S| ă Hpyq then 8:</p><p>Add Sc to S 9:</p><p>until |S| " N 10: return S token varies for different occurrences of the to- ken: one token can be easy to predict in one con- text, and hard in another. While the sampling step in the previous approaches targets these tokens, it does not ensure that the distribution of sampled sentences is similar to the distribution of problem- atic tokens in difficult contexts.</p><p>To address this issue, we propose an approach where we target the number of times a token oc- curs in difficult-to-predict contexts and sample sentences accordingly, thereby ensuring the same ratio as the distribution of difficult contexts. If to- ken y 1 is difficult to predict in two contexts and token y 2 is difficult to predict in four contexts, the number of sampled sentences containing y 2 is double the number of sampled sentences contain- ing y 1 . Algorithm 2 formalizes this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>We measure the translation quality of var- ious models for GermanÑEnglish and EnglishÑGerman translation tasks.</p><p>The re-sults of the translation experiments are presented in <ref type="table">Table 4</ref>. As baseline we compare our approach to <ref type="bibr" target="#b20">Sennrich et al. (2016a)</ref>. For all experiments we sample and back-translate sentences from the monolingual data, keeping a one-to-one ratio of back-translated versus original data (1:1). We set the hyperparameters µ, ρ, and η to 5, 10, and 5000 respectively. The values of the hy- perparameters are chosen on a small sample of the parallel data based on the token loss distribution.</p><p>As expected using random sampling for back- translation improves the translation quality over the baseline.</p><p>However, all targeted sam- pling variants in turn outperform random sam- pling. Specifically, the best performing model for GermanÑEnglish, MEANPREDLOSS, uses the mean of prediction loss for the target vocabulary to oversample sentences including these tokens.</p><p>For the EnglishÑGerman experiments we ob- tain the best translation performance when we pre- serve the prediction loss ratio during sampling.</p><p>We also observe that even though the model tar- geting tokens with skewed prediction loss distribu- tions (MEANPREDLOSS + STDPREDLOSS) im- proves over random selection of sentences, it does not outperform the model using only mean predic- tion losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Context-Aware Targeted Sampling</head><p>In the previous section, we proposed methods for identifying difficult-to-predict tokens and per- formed targeted sampling from monolingual data. While the objective was to increase the occur- rences of difficult tokens, we ignored the context of these tokens in the sampled sentences.</p><p>Arguably, if a word is difficult to predict in a given context, providing more examples of the same or similar context can aid the learning pro- cess. In this section, we focus on the context of difficult-to-predict words and aim to sample sen- tences that are similar to the corresponding diffi- cult context. The general algorithm is described in Algo- rithm 3. In the following sections, we discuss dif- ferent definitions of the local context (line 7 and line 9) and similarity measures (line 10) in this al- gorithm, and report the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Definition of Local Context</head><p>Prediction loss is a function of the source sentence and the target context. One reason that a token has high prediction loss is that the occurrence of the word is a deviation from what occurs more fre- quently in other occurrences of the context in the training data. This indicates an infrequent event, in particular a rare sense of the word, a domain that is different from other occurrences of the word, or an idiomatic expression.</p><p>source wer glaube, dass das ende, sobald sie in Deutschland ank|ä|men, ir|re, erzählt B|ahr. reference if you think that this stops as soon as they arrive in Germany, you'd be wrong, says B|ahr. NMT output who believe that the end, as soon as they go to Germany, tells B|risk. We identify pairs of tokens and sentences from parallel data where in each pair the NMT model assigns high prediction loss to the token in the given context. Note that a token can occur sev- eral times in this list, since it can be considered as difficult-to-predict in different sentences.</p><p>We propose two approaches to define the local context of the difficult token:</p><p>Neighboring tokens A straightforward way is to use positional context: tokens that precede and follow the target token, typically in a window of w tokens to each side. For sentence S containing a difficult token at index i the context function in Algorithm 3 is: contextpS, iq " rS i´w , . . . , S i´1 , S i`1 , . . . , S i`w s where S j is the token at index j in sentence S.</p><p>Sentence from bitext containing difficult token:</p><p>He attended Stan|ford University, where he double maj|ored in Spanish and History.</p><p>Sampled sentences from monolingual data:</p><p>´ The group is headed by Aar|on K|ush|ner, a Stan|ford University gradu|ate who formerly headed a gre|eting card company. ´ Ford just opened a new R&amp;D center near Stan|ford Uni- versity, a hot|bed of such technological research. ´ Joe Grund|fest, a professor and a colleague at Stan|ford Law School, outlines four reasons why the path to the IP|O has become so steep for asp|iring companies. <ref type="table">Table 6</ref>: Results of targeted sampling for the diffi- cult subword unit 'Stan'.</p><p>Neighboring subword units In our analysis of prediction loss during training, we observe that several tokens that are difficult to predict are in- deed subword units. Current state-of-the-art NMT systems apply BPE to the training data to ad- dress large vocabulary challenges ( <ref type="bibr" target="#b21">Sennrich et al., 2016b)</ref>.</p><p>By using BPE the model generalizes common subword units towards what is more frequent in the training data. This is inherently useful since it allows for better learning of less frequent words. However, a side effect of this approach is that at times the model generates subword units that are not linked to any words in the source sentence.</p><p>As an example, in <ref type="table" target="#tab_5">Table 5</ref> German source and English reference translation show this problem. The word B|ahr consisting of two subword units is incorrectly translated into B|risk.</p><p>We address the insufficiency of the context for subword units with high prediction losses by tar- geting these tokens in sentence sampling.</p><p>Algorithm 3 formalizes this approach in sam- pling sentences from the monolingual data. For a sentence S containing a difficult subword at index i, the context function is defined as:</p><formula xml:id="formula_4">contextpS, iq " r. . . , S i´1 , S i`1 , . . .s</formula><p>where every token S j in the local context is a sub- word unit and part of the same word as S i . Ta- ble 6 presents examples of sampled sentences for the difficult subword unit 'Stan'.</p><p>Sentence from bitext containing difficult word:</p><p>Bud|dy Hol|ly was part of the first group induc|ted into the Rock and R|oll Hall of F|ame on its formation in 1986.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Similarity of the Local Contexts</head><p>In context-aware targeted sampling, we compare the context of a sampled sentence and the difficult context in the parallel data and select the sentence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Sampling with context</head><p>Input: Difficult tokens and the corresponding sentences in the bitext D " tyt, Yy t " ry1, . . . , yt, . . . , ymsu, monolingual corpus M, context function context, number of required samples N, similarity threshold s Output: Sampled sentences S " tSiu N i"1 where each sentence Si is sampled from M 1: procedure CNTXTSAMPLING(D, M, context, N, s): 2:</p><p>Initialize S " tu 3: repeat 4:</p><p>Sample Sc from M 5:</p><p>for all tokens yt in Sc do 6:</p><p>if yt P D then 7:</p><p>Cm Ð contextpSc, indxofpSc, ytqq 8:</p><p>for all Yy t do 9:</p><p>Cp Ð contextpYy t , indxofpYy t , ytqq</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>if SimpCm, Cpq ą s then 11:</p><p>Add Sc to S 12:</p><p>until |S| " N 13:</p><p>return S if they are similar. In the following, we propose two approaches for quantifying the similarities.</p><p>Matching the local context In this approach we aim to sample sentences containing the difficult to- ken, matching the exact context to the problematic context. By sampling sentences that match in a lo- cal window with the problematic context and dif- fer in the rest of the sentence, we have more in- stances of the difficult token for training. Algorithm 3 formalizes this approach where the similarity function is defined as:</p><formula xml:id="formula_5">SimpC m , C p q " 1 c c ÿ i"1 δpC i m , C i p q</formula><p>C m and C p are the contexts of the sentences from monolingual and parallel data respectively, and c is the number of tokens in the contexts.</p><p>Word representations Another approach to sampling sentences that are similar to the problem- atic context is to weaken the matching assumption. Acquiring sentences that are similar in subject and not match the exact context words allows for lex- ical diversity in the training data. We use em- beddings obtained by training the Skipgram model ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) on monolingual data to cal- culate the similarity of the two contexts.</p><p>For this approach we define the similarity func- tion in Algorithm 3 as:</p><p>SimpC m , C p q " cospvpC m q, vpC p qq where vpC m q and vpC p q are the averaged embed- dings of the tokens in the contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>De-En</head><p>En-De System test2014 test2015 test2016 test2017 test2014 test2015 test2016 test2017   <ref type="table" target="#tab_6">Table 7</ref> presents examples of sampled sentences for the difficult word rock. In this example, the context where the word 'Rock' has high predic- tion loss is about the music genre and not the most prominent sense of the word, stone. Sampling sen- tences that contain this word in this particular con- text provides an additional signal for the transla- tion model to improve parameter estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The results of the translation experiments are given in <ref type="table" target="#tab_8">Table 8</ref>. In these experiments, we set the hyperparameters s and w to 0.75 and 4, respec- tively. Comparing the experiments with different similarity measures, MATCH and EMB, we observe that in all test sets we achieve the best results when using word embeddings. This indicate that for tar- geted sampling, it is more beneficial to have diver- sity in the context of difficult words as opposed to having the exact ngrams.</p><p>When using embeddings as the similarity mea- sure, it is worth noting that with a context of size 4 the model performs very well but fails when we increase the window size to include the whole sen- tence.</p><p>The experiments focusing on subword units (SWORD) achieve improvements over the base- lines, however they perform slightly worse than the experiments targeting tokens (TOKEN).</p><p>The best BLEU scores are obtained with the mean of prediction loss as difficulty criterion (MEANPREDLOSS) and using word representa- tions to identify the most similar contexts. We observe that summarizing the distribution of the prediction losses by its mean is more beneficial than using individual losses. Our results motivate further explorations of using context for targeted sampling sentences for back-translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we investigated the effective method of back-translation for NMT and explored alter- natives to select the monolingual data in the tar- get language that is to be back-translated into the source language to improve translation quality.</p><p>Our findings showed that words with high pre- diction losses in the target language benefit most from additional back-translated data.</p><p>As an alternative to random sampling, we pro- posed targeted sampling and specifically targeted words that are difficult to predict. Moreover, we used the contexts of the difficult words by incor- porating context similarities as a feature to sample sentences for back-translation. We discovered that using the prediction loss to identify weaknesses of the translation model and providing additional synthetic data targeting these shortcomings im- proved the translation quality in GermanØEnglish by up to 1.7 BLEU points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Baseline</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training plots for systems with different ratios of (real : syn) training data, showing perplexity on development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: Changes in mean token prediction loss after re-training with synthetic data sorted by mean prediction loss of the baseline system. Decreases and increases in values are marked blue and red, respectively. Bottom: Frequencies (log) of target tokens in the baseline training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>De</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Sampled sentences from monolingual data: ´ A 2008 Rock and R|oll Hall of F|ame induc|t|ee, Mad|onna is ran|ked by the Gu|inn|ess Book of World Rec|ords as the top-selling recording artist of all time. ´ The Rock and R|oll Hall of Fam|ers gave birth to the California rock sound. ´ The winners were chosen by 500 voters, mostly musi- cians and other music industry veter|ans, who belong to the Rock and R|oll Hall of F|ame Foundation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>An example from the synthetic data where 
the word B|ahr is incorrectly translated to B|risk. 
Subword unit boundaries are marked with '|'. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results of context-aware targeted sam-
pling for the difficult token 'Rock' 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>GermanØEnglish translation quality (BLEU). Experiments marked : are averaged over 3 runs. 
PREDLOSS is the contextual prediction loss and MEANPREDLOSS is the average loss. TOKEN and 
SWORD are context selection definitions from neighboring tokens and subword units respectively. Note 
that token includes both subword units and full words. EMB is computing context similarities with token 
embeddings and MATCH is comparing the context tokens. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 612.001.218. We also thank NVIDIA for their hardware support and the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classbased n-gram language difference models for data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Martindale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johns</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT (International Workshop on Spoken Language Translation)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (wmt17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
	<note>Shared Task Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST8)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lium machine translation systems for wmt17 news translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="288" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On integrating a language model into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Effective Strategies in Zero-Shot Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thanh-Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the impact of various types of noise on neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigations on translation model adaptation using monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaf</forename><surname>Abdul-Rauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT &apos;11</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation, WMT &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The karlsruhe institute of technology systems for the news translation task in wmt 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Investigating Backtranslation in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitar</forename><surname>Shterionov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Gideon Maillette de Buy Wenniger, and Peyman Passban. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<editor>Valerio Miceli Barone, and Philip Williams</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The university of edinburgh&apos;s neural mt systems for wmt17</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="389" to="399" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic data selection for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Marlies Van Der Wees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1400" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
