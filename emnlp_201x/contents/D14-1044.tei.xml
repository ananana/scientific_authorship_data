<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
							<email>jayantk@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="397" to="406"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much work in recent years has gone into the construction of large knowledge bases, either by collecting contributions from many users, as with Freebase ( <ref type="bibr" target="#b0">Bollacker et al., 2008</ref>) and <ref type="bibr">DBPedia (Mendes et al., 2012)</ref>, or automat- ically from web text or other resources, as done by NELL ( <ref type="bibr" target="#b1">Carlson et al., 2010)</ref> and <ref type="bibr">YAGO (Suchanek et al., 2007)</ref>. These knowl- edge bases contain millions of real-world enti- ties and relationships between them. However, even though they are very large, they are still very incomplete, missing large fractions of possi- ble relationships between common entities <ref type="bibr" target="#b18">(West et al., 2014)</ref>. Thus the task of inference over these knowledge bases, predicting new relation- ships simply by examining the knowledge base it- self, has become increasingly important.</p><p>A promising technique for inferring new re- lation instances in a knowledge base is random walk inference, first proposed by <ref type="bibr" target="#b4">Lao and Cohen (2010)</ref>. In this method, called the Path Ranking Algorithm (PRA), the knowledge base is encoded as a graph, and random walks are used to find paths that connect the source and target nodes of relation instances. These paths are used as features in a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base re- lations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical in- ference.</p><p>One major deficiency of random walk inference is the connectivity of the knowledge base graph- if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA ( <ref type="bibr" target="#b5">Lao et al., 2012;</ref><ref type="bibr" target="#b2">Gardner et al., 2013)</ref>. This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and dif- ferent relations have distinct meanings, this is not true of surface text. For example, "The Nile flows through Cairo" and "The Nile runs through Cairo" have very similar if not identical meaning. Adding a text corpus to the inference graph increases con- nectivity, but it also dramatically increases feature sparsity.</p><p>We introduce two new techniques for making better use of a text corpus for knowledge base in- ference. First, we describe a new way of incor- porating the text corpus into the knowledge base graph that enables much more efficient process- ing than prior techniques, allowing us to approach problems that prior work could not feasibly solve. Second, we introduce the use of vector space sim- ilarity in random walk inference in order to reduce the sparsity of surface forms. That is, when fol- lowing a sequence of edge types in a random walk on a graph, we allow the walk to follow edges that are semantically similar to the given edge types, as defined by some vector space embedding of the edge types. If a path calls for an edge of type "flows through", for example, we accept other edge types (such as "runs through") with probabil- ity proportional to the vector space similarity be- tween the two edge types. This lets us combine notions of distributional similarity with symbolic logical inference, with the result of decreasing the sparsity of the feature space considered by PRA. We show with experiments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph Construction</head><p>Our method for knowledge base inference, de- scribed in Section 3, performs random walks over a graph to obtain features for a logistic regression classifier. Prior to detailing that technique, we first describe how we produce a graph G = (N , E, R) from a set of knowledge base (KB) relation in- stances and a set of surface relation instances ex- tracted from a corpus. Producing a graph from a knowledge base is straightforward: the set of nodes N is made up of the entities in the KB; the set of edge types R is the set of relation types in the KB, and the typed edges E correspond to re- lation instances from the KB, with one edge of type r connecting entity nodes for each (n 1 , r, n 2 ) triple in the KB. Less straightforward is how to construct a graph from a corpus, and how to con- nect that graph to the KB graph. We describe our methods for each of those below.</p><p>To create a graph from a corpus, we first prepro- cess the corpus to obtain a collection of surface relations, such as those extracted by open infor- mation extraction systems like OLLIE <ref type="bibr" target="#b8">(Mausam et al., 2012</ref>). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by <ref type="bibr" target="#b16">Talukdar et al. (2012)</ref>, a dependency path, as done by <ref type="bibr" target="#b12">Riedel et al. (2013)</ref>, or OpenIE relations ( <ref type="bibr" target="#b8">Mausam et al., 2012)</ref>). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already repre- sented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes cor- responding to noun phrase types and edges corre- sponding to surface relation triples.</p><p>So far these two subgraphs we have created are entirely disconnected, with the KB graph con- taining nodes representing entities, and the sur- face relation graph containing nodes representing noun phrases, with no edges between these noun phrases and entities. We connect these two graphs by making use of the ALIAS relation in the KB, which links entities to potential noun phrase ref- erents. Each noun phrase in the surface relation graph is connected to those entity nodes which the noun phrase can possibly refer to according to the KB. These edges are not the output of an entity linking system, as done by <ref type="bibr" target="#b5">Lao et al. (2012)</ref>, but express instead the notion that the noun phrase can refer to the KB entity. The use of an entity linking system would certainly allow a stronger connec- tion between noun phrase nodes and entity nodes, but it would require much more preprocessing and a much larger graph representation, as each men- tion of each noun phrase would need its own node, as opposed to letting every mention of the same noun phrase share the same node. This graph rep- resentation allows us to add tens of millions of sur- face relations to a graph of tens of millions of KB relations, and perform all of the processing on a single machine.</p><p>As will be discussed in more detail in Section 4, we also allow edge types to optionally have an as- sociated vector that ideally captures something of the semantics of the edge type. <ref type="figure" target="#fig_0">Figure 1</ref> shows the graph constructions used in our experiments on a subset of KB and surface re-KB Relations: (Monongahela, RIVERFLOWSTHROUGHCITY, Pittsburgh) (Pittsburgh, ALIAS, "Pittsburgh") (Pittsburgh, ALIAS, "Steel City") (Monongahela, ALIAS, "Monongahela River") (Monongahela, ALIAS, "The Mon") Surface Relations: ("The Mon", "flows through", "Steel City") ("Monongahela River", "runs through", "Pittsburgh")</p><p>Embeddings: "flows through": [.2, -.1, .9] "runs through": [.1, -.3, .8] (a) An example data set.</p><p>(c) An example graph that replaces surface relations with a cluster label, as done by <ref type="bibr" target="#b2">Gardner et al. (2013)</ref>. Note, how- ever, that the graph structure differs from that prior work; see Section 5.</p><p>(b) An example graph that combines a KB and surface rela- tions.</p><p>(d) An example graph that uses vector space representations of surface edges, as introduced in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Path Ranking Algorithm</head><p>We perform knowledge base inference using the Path Ranking Algorithm (PRA) ( <ref type="bibr" target="#b4">Lao and Cohen, 2010)</ref>. We begin this section with a brief overview of PRA, then we present our modification to the PRA algorithm that allows us to incorporate vector space similarity into random walk inference. PRA can be thought of as a method for exploit- ing local graph structure to generate non-linear feature combinations for a prediction model. PRA generates a feature matrix over pairs of nodes in a graph, then uses logistic regression to classify those node pairs as belonging to a particular rela- tion.</p><p>More formally, given a graph G with nodes N , edges E, and edge labels R, and a set of node pairs (s i , t i ) ∈ D, one can create a connectivity matrix where rows correspond to node pairs and columns correspond to edge lables. PRA augments this matrix with additional columns corresponding to sequences of edge labels, called path types, and changes the cell values from representing the pres- ence of an edge to representing the specificity of the connection that the path type makes between the node pair.</p><p>Because the feature space considered by PRA is so large (the set of all possible edge label se- quences, with cardinality l i=1 |R| i , assuming a bound l on the maximum path length), the first step PRA must perform is feature selection, which is done using random walks over the graph. The second step of PRA is feature computation, where each cell in the feature matrix is computed using a constrained random walk that follows the path type corresponding to the feature. We now explain each of these steps in more detail.</p><p>Feature selection finds path types π that are likely to be useful in predicting new instances of the relation represented by the input node pairs . These path types are found by performing random walks on the graph G starting at the source and target nodes in D, recording which paths connect some source node with its target. The edge se- quences are ranked by frequency of connecting a source node to a corresponding target node, and the top k are kept.</p><p>Feature computation. Once a set of path types is selected as features, the next step of the PRA algorithm is to compute a value for each cell in the feature matrix, corresponding to a node pair and a path type. The value computed is the probability of arriving at the target node of a node pair, given that a random walk began at the source node and was constrained to follow the path type: p(t|s, π). Once these steps have been completed, the re- sulting feature matrix can be used with whatever model or learning algorithm is desired; in this and prior work, simple logistic regression has been used as the prediction algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Vector space random walks</head><p>Our modifications to PRA are confined entirely to the feature computation step described above; fea- ture selection (finding potentially useful sequences of edge types) proceeds as normal, using the sym- bolic edge types. When computing feature val- ues, however, we allow a walk to follow an edge that is semantically similar to the edge type in the path, as defined by Euclidean distance in the vec- tor space.</p><p>More formally, consider a path type π. Re- call that π is a sequence of edge types &lt; e 1 , e 2 , . . . , e l &gt;, where l is the length of the path; we will use π i to denote the i th edge type in the sequence. To compute feature values, PRA begins at some node and follows edges of type π i until the sequence is finished and a target node has been reached. Specifically, if a random walk is at a node n with m outgoing edge types {e 1 , e 2 , . . . , e m }, PRA selects the edge type from that set which matches π i , then selects uniformally at random from all outgoing edges of that type. If there is no match in the set, the random walk restarts from the original start node.</p><p>We modify the selection of which edge type to follow. When a random walk is at a node n with m outgoing edge types {e 1 , e 2 , . . . , e m }, instead of selecting only the edge type that matches π i , we allow the walk to select instead an edge that is close to π i in vector space. For each edge type at node n, we select the edge with the following probability:</p><formula xml:id="formula_0">p(e j |π i ) ∝ exp(β × v(e j ) · v(π i )), ∀j, 1 ≤ j ≤ m</formula><p>where v(·) is a function that returns the vector representation of an edge type, and β is a spiki- ness parameter that determines how much weight to give to the vector space similarity. As β ap- proaches infinity, the normalized exponential ap- proximates a delta function on the closest edge type to π i , in {e 1 , e 2 , . . . , e m }. If π i is in the set of outgoing edges, this algorithm converges to the original PRA.</p><p>However, if π i is not in the set of outgoing edge types at a node and all of the edge types are very dissimilar to π i , this algorithm (with β not close to infinity) will lead to a largely uniform distribution over edge types at that node, and no way for the random walk to restart. To recover the restart be- havior of the original PRA, we introduce an addi- tional restart parameter α, and add another value to the categorical distribution before normalization:</p><formula xml:id="formula_1">p(restart|π i ) ∝ exp(β * α)</formula><p>When this restart type is selected, the random walk begins again, following π 1 starting at the source node. With α set to a value greater than the maximum similarity between (non-identical) edge type vectors, and β set to infinity, this algorithm exactly replicates the original PRA.</p><p>Not all edge types have vector space representa- tions: the ALIAS relation cannot have a meaning- ful vector representation, and we do not use vec- tors to represent KB relations, finding that doing so was not useful in practice (which makes intu- itive sense: KB relations are already latent repre- sentations themselves). While performing random walks, if π i has no vector representation, we fall back to the original PRA algorithm for selecting the next edge.</p><p>We note here that when working with vector spaces it is natural to try clustering the vectors to reduce the parameter space. Each path type π is a feature in our model, and if two path types dif- fer only in one edge type, and the differing edge types have very similar vectors, the resultant fea- ture values will be essentially identical for both path types. It seems reasonable that running a simple clustering algorithm over these path types, to reduce the number of near-duplicate features, would improve performance. We did not find this to be the case, however; all attempts we made to use clustering over these vectors gave performance indistinguishable from not using clustering. From this we conclude that the main issue hindering per- formance when using PRA over these kinds of graphs is one of limited connectivity, not one of too many parameters in the model. Though the feature space considered by PRA is very large, the number of attested features in a real graph is much smaller, and it is this sparsity which our vector space methods address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Knowledge base inference. Random walk infer- ence over knowledge bases was first introduced by <ref type="bibr" target="#b4">Lao and Cohen (2010)</ref>. This work was improved upon shortly afterward to also make use of a large corpus, by representing the corpus as a graph and connecting it to the knowledge base ( <ref type="bibr" target="#b5">Lao et al., 2012</ref>). <ref type="bibr" target="#b2">Gardner et al. (2013)</ref> </p><note type="other">further showed that replacing surface relation labels with a represen- tation of a latent embedding of the relation led to improved prediction performance. This result is intuitive: the feature space considered by PRA is exponentially large, and surface relations are sparse. The relations "[river] flows through [city]" and "[river] runs through [</note><p>city]" have near iden- tical meaning, and both should be very predic- tive for the knowledge base relation RIVERFLOW- STHROUGHCITY. However, if one of these rela- tions only appears in the training data and the other only appears in the test data, neither will be useful for prediction. <ref type="bibr" target="#b2">Gardner et al. (2013)</ref> attempted to solve this issue by finding a latent symbolic repre- sentation of the surface relations (such as a cluster- ing) and replacing the edge labels in the graph with these latent representations. This makes it more likely for surface relations seen in training data to also be seen at test time, and naturally improved performance.</p><p>This representation, however, is still brittle, as it is still a symbolic representation that is prone to mismatches between training and test data. If the clustering algorithm used is too coarse, the fea- tures will not be useful, and if it is too fine, there will be more mismatches. Also, verbs that are on the boundaries of several clusters are problematic to represent in this manner. We solve these prob- lems by modifying the PRA algorithm to directly use vector representations of edge types during the random walk inference.</p><p>These two prior techniques are the most directly related work to what we present in this paper, and we compare our work to theirs.</p><p>Graph construction. In addition to the incor- poration of vector space similarity into the PRA algorithm, the major difference between our work and the prior approaches mentioned above is in the construction of the graph used by PRA. We con- trast our method of graph construction with these prior approaches in more detail below. <ref type="bibr" target="#b5">Lao et al. (2012)</ref> represent every word of ev- ery sentence in the corpus as a node in the graph, with edges between the nodes representing depen- dency relationships between the words. They then connect this graph to the KB graph using a simple entity linking system (combined with coreference resolution). The resultant graph is enormous, such that they needed to do complex indexing on the graph and use a cluster of 500 machines to per- form the PRA computations. Also, as the edges represent dependency labels, not words, with this graph representation the PRA algorithm does not have access to the verbs or other predicative words that appear in the corpus, which frequently express relations. PRA only uses edge types as feature components, not node types, and so the rich infor- mation contained in the words is lost. This graph construction also would not allow the incorpora- tion of vector space similarity that we introduced, as dependency labels do not lend themselves well to vector space representations. <ref type="bibr" target="#b2">Gardner et al. (2013)</ref> take an approach very sim- ilar to the one presented in Section 2, preprocess- ing the corpus to obtain surface relations. How- ever, instead of creating a graph with nodes rep- resenting noun phrases, they added edges from the surface relations directly to the entity nodes in the graph. Using the ALIAS relation, as we do, they added an edge between every possible con- cept pair that could be represented by the noun phrases in a surface relation instance. This leads to some nonsensical edges added to the graph, and if the ALIAS relation has high degree (as it does for many common noun phrases in Freebase), it quickly becomes unscalable-this method of graph construction runs out of disk space when attempting to run on the Freebase experiments in Section 6. Also, in conflating entity nodes in the graph with noun phrases, they lose an important distinction that turns out to be useful for predic- tion, as we discuss in Section 6.4. 1</p><p>Other related work. Also related to the present work is recent research on programming lan- guages for probabilistic logic ( <ref type="bibr" target="#b17">Wang et al., 2013)</ref>. This work, called ProPPR, uses random walks to locally ground a query in a small graph before per- forming propositional inference over the grounded representation. In some sense this technique is like a recursive version of PRA, allowing for more complex inferences than a single iteration of PRA can make. However, this technique has not yet been extended to work with large text corpora, and it does not yet appear to be scalable enough to han- dle the large graphs that we use in this work. How best to incorporate the work presented in this pa- per with ProPPR is an open, and very interesting, question.</p><p>Examples of other systems aimed at reason- ing over common-sense knowledge are the CYC project <ref type="bibr" target="#b6">(Lenat, 1995)</ref> and ConceptNet ( <ref type="bibr" target="#b7">Liu and Singh, 2004</ref>). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference.</p><p>Lines of research that seek to incorporate dis- tributional semantics into traditional natural lan- guage processing tasks, such as parsing ( <ref type="bibr" target="#b13">Socher et al., 2013a</ref>), named entity recognition ( <ref type="bibr" target="#b11">Passos et al., 2014)</ref>, and sentiment analysis <ref type="bibr" target="#b14">(Socher et al., 2013b)</ref>, are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take in- spiration from these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We perform both the feature selection step and the feature computation step of PRA using GraphChi, an efficient single-machine graph processing li- brary ( <ref type="bibr" target="#b3">Kyrola et al., 2012</ref>). We use MAL- LET's implementation of logistic regression, with both L1 and L2 regularization <ref type="bibr" target="#b9">(McCallum, 2002</ref>).</p><p>To obtain negative evidence, we used a closed world assumption, treating any (source, target) pair found during the feature computation step as a negative example if it was not given as a positive example. We tuned the parameters to our methods using a coarse, manual grid search with cross vali- dation on the training data described below. The parameters we tuned were the L1 and L2 regu- larization parameters, how many random walks to perform in the feature selection and computation NELL <ref type="table" target="#tab_0">Freebase  Entities  1.2M  20M  Relation instances  3.4M  67M  Total relation types  520  4215  Relation types tested  10  24  Avg. instances/relation  810  200  SVO triples used  404k  28M   Table 1</ref>: Statistics of the data used in our experi- ments.</p><p>steps of PRA, and spikiness and restart parameters for vector space walks. The results presented were not very sensitive to changes in these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data</head><p>We ran experiments on both the NELL and Free- base knowledge bases. The characteristics of these knowledge bases are shown in <ref type="table">Table 1</ref>. The Free- base KB is very large; to make it slightly more manageable we filtered out relations that did not seem applicable to relation extraction, as well as a few of the largest relations. <ref type="bibr">2</ref> This still left a very large, mostly intact KB, as can be seen in the ta- ble. For our text corpus, we make use of a set of subject-verb-object triples extracted from depen- dency parses of ClueWeb documents <ref type="bibr" target="#b16">(Talukdar et al., 2012</ref>). There are 670M such triples in the data set, most of which are completely irrelevant to the knowledge base relations we are trying to pre- dict. For each KB, we filter the SVO triples, keep- ing only those which can possibly connect training and test instances of the relations we used in our experiments. The number of SVO triples kept for each KB is also shown in <ref type="table">Table 1</ref>. We obtained vector space representations of these surface rela- tions by running PCA on the SVO matrix. We selected 10 NELL relations and 24 Free- base relations for testing our methods. The NELL relations were hand-selected as the relations with the largest number of known instances that had a reasonable precision (the NELL KB is automati- cally created, and some relations have low preci- sion). We split the known instances of these rela- tions into 75% training and 25% testing, giving on average about 650 training instances and 160 test instances for each relation.</p><p>The 24 Freebase relations were semi-randomly selected. We first filtered the 4215 relations based on two criteria: the number of relation instances must be between 1000 and 10000, and there must be no mediator in the relation. <ref type="bibr">3</ref> Once we selected the relations, we kept all instances of each rela- tion that had some possible connection in the SVO data. <ref type="bibr">4</ref> This left on average 200 instances per rela- tion, which we again split 75%-25% into training and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Methods</head><p>The methods we compare correspond to the graphs shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The KB method uses the orig- inal PRA algorithm on just the KB relations, as presented by <ref type="bibr" target="#b4">Lao and Cohen (2010)</ref>. KB + SVO adds surface relations to the graph <ref type="figure" target="#fig_0">(Figure 1b</ref>). We present this as roughly analogous to the methods introduced by <ref type="bibr" target="#b5">Lao et al. (2012)</ref>, though with some significant differences in graph representation, as described in Section 5. KB + Clustered SVO fol- lows the methods of <ref type="bibr" target="#b2">Gardner et al. (2013)</ref>, but us- ing the graph construction introduced in this pa- per <ref type="figure" target="#fig_0">(Figure 1c</ref>; their graph construction techniques would have made graphs too large to be feasible for the Freebase experiments). KB + Vector SVO is our method <ref type="figure" target="#fig_0">(Figure 1d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation</head><p>As evaluation metrics, we use mean average pre- cision (MAP) and mean reciprocal rank (MRR), following recent work evaluating relation extrac- tion performance <ref type="bibr" target="#b18">(West et al., 2014</ref>). We test sig- nificance using a paired permutation test.</p><p>The results of these experiments are shown in <ref type="table" target="#tab_0">Table 2 and Table 3</ref>. In <ref type="table" target="#tab_3">Table 4</ref> we show average precision for every relation tested on the NELL KB, and we show the same for Freebase in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion</head><p>We can see from the tables that KB + Vector SVO (the method presented in this paper) significantly outperforms prior approaches in both MAP and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MAP MRR KB 0.193 0.635 KB + SVO 0.218 0.763 KB + Clustered SVO 0.276 0.900 KB + Vector SVO 0.301 0.900  MRR. We believe that this is due to the reduction in feature sparsity enabled by using vector space instead of symbolic representations (as that is the only real difference between KB + Clustered SVO and KB + Vector SVO), allowing PRA to make better use of path types found in the training data. When looking at the results for individual relations in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table">Table 5</ref>, we see that KB + Vector SVO outperforms other methods on the majority of relations, and it is a close second when it does not. We can also see from the results that mean av- erage precision seems a little low for all meth- ods tested. This is because MAP is computed as the precision of all possible correct predictions in a ranked list, where precision is counted as 0 if the correct prediction is not included in the list. In other words, there are many relation instances in our randomly selected test set that are not in- ferrable from the knowledge base, and the low re- call hurts the MAP metric. MRR, which judges the precision of the top prediction for each relation, gives us some confidence that the main issue here is one of recall, as MRR is reasonably high, es- pecially on the NELL KB. As further evidence, if we compute average precision for each query node (instead of for each relation), excluding queries for which the system did not return any predictions, MAP ranges from . <ref type="bibr">29</ref>    <ref type="table">Table 5</ref>: Average precision for each relation tested on the Freebase KB. The best performing method on each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector SVO" is shortened to "V-SVO" in the table header.</p><p>Vector SVO) on Freebase, (where 21% of queries gave no prediction). Our methods thus also im- prove MAP when calculated in this manner, but it is not an entirely fair metric, 5 so we use standard MAP to present our main results. One interesting phenomenon to note is a novel use of the ALIAS relation in some of the relation models.</p><p>The best exam- ple of this was found with the relation /people/ethnicity/languages spoken.</p><p>A high-weighted feature when adding surface relations was the edge sequence &lt;ALIAS, ALIAS INVERSE&gt;. This edge sequence reflects the fact that languages frequently share a name with the group of people that speaks them (e.g., Maori, French). And because PRA can gen- erate compositional features, we also find the following edge sequence for the same relation: &lt;/people/ethnicity/included in group, ALIAS, ALIAS INVERSE&gt;. This feature captures the same notion that languages get their names from groups of people, but applies it to subgroups within an ethnicity. These features would be very difficult, perhaps impossible, to include in systems that do not distinguish between noun phrases and knowledge base entities, such as the graphs constructed by <ref type="bibr" target="#b2">Gardner et al. (2013)</ref>, or typical relation extraction systems, which generally only work with noun phrases after performing a heuristic entity linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have offered two main contributions to the task of knowledge base inference. First, we have pre- sented a new technique for combining knowledge base relations and surface text into a single graph representation that is much more compact than graphs used in prior work. This allowed us to ap- ply methods introduced previously to much larger problems, running inference on a single machine over the entire Freebase KB combined with tens of millions of surface relations. Second, we have de- scribed how to incorporate vector space similarity into random walk inference over knowledge bases, reducing the feature sparsity inherent in using sur- face text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate knowledge bases, we have shown that our methods significantly outper- form prior work on knowledge base inference.</p><p>The code and data used in the ex- periments in this paper are available at http://rtw.ml.cmu.edu/emnlp2014 vector space pra/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example graph construction as used in the experiments in this paper. A graph using only KB edges is simply a subset of these graphs containing only the RIVERFLOWSTHROUGHCITY edge, and is not shown.</figDesc><graphic url="image-2.png" coords="3,72.00,226.31,213.16,83.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on the NELL knowledge base. 
The bolded line is significantly better than all other 
results with p &lt; 0.025. 

Method 
MAP MRR 
KB 
0.278 0.614 
KB + SVO 
0.294 0.639 
KB + Clustered SVO 0.326 0.651 
KB + Vector SVO 
0.350 0.670 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the Freebase knowledge base. 
The bolded line is significantly better than all other 
results with p &lt; 0.0002. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Average precision for each relation tested on the NELL KB. The best performing method on 
each relation is bolded. 

Relation 
KB KB + SVO KB + C-SVO KB + V-SVO 
/amusement parks/park/rides 
0.000 
0.009 
0.004 
0.013 
/architecture/architect/structures designed 
0.072 
0.199 
0.257 
0.376 
/astronomy/constellation/contains 
0.004 
0.017 
0.000 
0.008 
/automotive/automotive class/examples 
0.003 
0.001 
0.002 
0.006 
/automotive/model/automotive class 
0.737 
0.727 
0.742 
0.768 
/aviation/airline/hubs 
0.322 
0.286 
0.298 
0.336 
/book/literary series/author s 
0.798 
0.812 
0.818 
0.830 
/computer/software genre/software in genre 
0.000 
0.001 
0.001 
0.001 
/education/field of study/journals in this discipline 
0.001 
0.003 
0.003 
0.001 
/film/film/rating 
0.914 
0.905 
0.914 
0.905 
/geography/island/body of water 
0.569 
0.556 
0.580 
0.602 
/geography/lake/basin countries 
0.420 
0.361 
0.409 
0.437 
/geography/lake/cities 
0.111 
0.134 
0.177 
0.175 
/geography/river/cities 
0.030 
0.038 
0.045 
0.066 
/ice hockey/hockey player/hockey position 
0.307 
0.243 
0.222 
0.364 
/location/administrative division/country 
0.989 
0.988 
0.991 
0.989 
/medicine/disease/symptoms 
0.061 
0.078 
0.068 
0.067 
/medicine/drug/drug class 
0.169 
0.164 
0.135 
0.157 
/people/ethnicity/languages spoken 
0.134 
0.226 
0.188 
0.223 
/spaceflight/astronaut/missions 
0.010 
0.186 
0.796 
0.848 
/transportation/bridge/body of water spanned 
0.534 
0.615 
0.681 
0.727 
/tv/tv program creator/programs created 
0.164 
0.179 
0.163 
0.181 
/visual art/art period movement/associated artists 
0.044 
0.040 
0.046 
0.037 
/visual art/visual artist/associated periods or movements 0.276 
0.295 
0.282 
0.290 

</table></figure>

			<note place="foot" n="1"> Recent notions of &quot;universal schema&quot; (Riedel et al., 2013) also put KB entities and noun phrases into the same conceptual space, though they opt for using noun phrases instead of the KB entities used by Gardner et al. In general this is problematic, as it relies on some kind of entity linking system as preprocessing, and cannot handle common noun references of proper entities without losing information. Our method, and that of Lao et al., skirts this issue entirely by not trying to merge KB entities with noun phrases.</note>

			<note place="foot" n="2"> We removed anything under /user, /common, /type (except for the relation /type/object/type), /base, and /freebase, as not applicable to our task. We also removed relations dealing with individual music tracks, book editions, and TV epidsodes, as they are very large, very specific, and unlikely to be useful for predicting the relations in our test set.</note>

			<note place="foot" n="3"> A mediator in Freebase is a reified relation instance meant to handle n-ary relations, for instance /film/performance. PRA in general, and our implementation of it in particular, needs some modification to be well-suited to predicting relations with mediators. 4 We first tried randomly selecting instances from these relations, but found that the probability of selecting an instance that benefited from an SVO connection was negligible. In order to make use of the methods we present, we thus restricted ourselves to only those that had a possible SVO connection.</note>

			<note place="foot" n="5"> MAP is intended to include some sense of recall, but excluding queries with no predictions removes that and opens the metric to opportunistic behavior.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported in part by DARPA under contract number FA8750-13-2-0005, by NSF under grant 31043,18,1121946, and by generous support from Yahoo! and Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Association for Computational Linguistics</title>
		<meeting>EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graphchi: Large-scale graph computation on just a pc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cyc: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas B Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conceptnet: a practical commonsense reasoning tool-kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT Technology Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dbpedia for nlp: A multilingual cross-domain knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5367</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acquiring temporal constraints between relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Programming with personalized pagerank: A locally groundable first-order probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM International Conference on Conference on Information &amp;#38; Knowledge Management, CIKM &apos;13</title>
		<meeting>the 22Nd ACM International Conference on Conference on Information &amp;#38; Knowledge Management, CIKM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
