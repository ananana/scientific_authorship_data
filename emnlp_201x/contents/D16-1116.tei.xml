<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Parsing with Semi-Supervised Sequential Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Parsing with Semi-Supervised Sequential Autoencoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1078" to="1087"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural approaches, in particular attention-based sequence-to-sequence models, have shown great promise and obtained state-of-the-art performance for sequence transduction tasks including machine translation ( <ref type="bibr" target="#b5">Bahdanau et al., 2015</ref>), syntactic con- stituency parsing <ref type="bibr" target="#b27">(Vinyals et al., 2015)</ref>, and seman- tic role labelling ( <ref type="bibr" target="#b33">Zhou and Xu, 2015)</ref>. A key re- quirement for effectively training such models is an abundance of supervised data.</p><p>In this paper we focus on learning mappings from input sequences x to output sequences y in domains where the latter are easily obtained, but annotation in the form of (x, y) pairs is sparse or expensive to produce, and propose a novel architecture that ac- commodates semi-supervised training on sequence transduction tasks. To this end, we augment the transduction objective (x → y) with an autoencod- ing objective where the input sequence is treated as a latent variable (y → x → y), enabling training from both labelled pairs and unpaired output sequences. This is common in situations where we encode nat- ural language into a logical form governed by some grammar or database.</p><p>While such an autoencoder could in principle be constructed by stacking two sequence transduc- ers, modelling the latent variable as a series of dis- crete symbols drawn from multinomial distributions creates serious computational challenges, as it re- quires marginalising over the space of latent se- quences Σ * x . To avoid this intractable marginalisa- tion, we introduce a novel differentiable alternative for draws from a softmax which can be used with the reparametrisation trick of <ref type="bibr" target="#b17">Kingma and Welling (2014)</ref>. Rather than drawing a discrete symbol in Σ x from a softmax, we draw a distribution over sym- bols from a logistic-normal distribution at each time step. These serve as continuous relaxations of dis- crete samples, providing a differentiable estimator of the expected reconstruction log likelihood.</p><p>We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the GEO- QUERY benchmark <ref type="bibr" target="#b29">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b28">Wong and Mooney, 2006</ref>), the SAIL maze navigation task ( <ref type="bibr" target="#b22">MacMahon et al., 2006</ref>) and the Natural Language Querying corpus <ref type="bibr" target="#b12">(Haas and Riezler, 2016</ref>) on Open- StreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks.</p><p>In most settings, the semi-supervised model out- performs the supervised model, both when trained on additional generated data as well as on subsets of the existing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Example</head><p>GEO what are the high points of states surrounding mississippi answer(high point 1(state(next to 2(stateid('mississippi')))))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLMAPS</head><p>Where are kindergartens in Hamburg? query(area(keyval('name','Hamburg')),nwr(keyval('amenity','kindergarten')),qtype(latlong)) SAIL turn right at the bench into the yellow tiled hall (1, 6, 90) FORWARD -FORWARD -RIGHT -STOP (3, 6, 180) <ref type="table">Table 1</ref>: Examples of natural language x and logical form y from the three corpora and tasks used in this paper. Note that the SAIL corpus requires additional information in order to map from the instruction to the action sequence.</p><formula xml:id="formula_0">y1 y2 y3 y4 &lt; s &gt; &lt; s &gt; ˜ x1˜x2˜x3 x1˜ x1˜x2 x1˜x2˜ x1˜x2˜x3 h y 1 h y 2 h y 3 h y 4 h ˜ x 1 h ˜ x 2 h ˜ x 3 h x 1 h x 2 h x 3 h ˆ y 1 h ˆ y 2 h ˆ y 3 h ˆ y 4 ˆ y1ˆy2ˆy3ˆy4ˆy1ˆy2ˆy3 y1ˆ y1ˆy2 y1ˆy2ˆ y1ˆy2ˆy3 y1ˆy2ˆy3ˆ y1ˆy2ˆy3ˆy4 y1ˆy2ˆy3ˆy4ˆ y1ˆy2ˆy3ˆy4ˆy1 y1ˆy2ˆy3ˆy4ˆy1ˆ y1ˆy2ˆy3ˆy4ˆy1ˆy2 y1ˆy2ˆy3ˆy4ˆy1ˆy2ˆ y1ˆy2ˆy3ˆy4ˆy1ˆy2ˆy3</formula><p>Figure 1: SEQ4 model with attention-sequence-to-sequence encoder and decoder. Circle nodes represent random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our sequential autoencoder is shown in <ref type="figure">Figure 1</ref>. At a high level, it can be seen as two sequence- to-sequence models with attention ( <ref type="bibr" target="#b5">Bahdanau et al., 2015</ref>) chained together. More precisely, the model consists of four LSTMs <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>), hence the name SEQ4. The first, a bidirectional LSTM, encodes the sequence y; next, an LSTM with stochastic output, described below, draws a sequence of distributions˜xdistributions˜ distributions˜x over words in vocabulary Σ x . The third LSTM encodes these dis- tributions for the last one to attend over and recon- struct y asˆyasˆ asˆy. We now give the details of these parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoding y</head><p>The first LSTM of the encoder half of the model reads the sequence y, represented as a sequence of one-hot vectors over the vocabulary Σ y , using a bidirectional RNN into a sequence of vectors h y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1:Ly</head><p>where L y is the sequence length of y,</p><formula xml:id="formula_1">h y t = f → y (y t , h y,→ t−1 ); f ← y (y t , h y,← t+1 ) ,<label>(1)</label></formula><p>where f → y , f ← y are non-linear functions applied at each time step to the current token y t and their re- current states h y,→ t−1 , h y,← t+1 , respectively. Both the forward and backward functions project the one-hot vector into a dense vector via an embed- ding matrix, which serves as input to an LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Predicting a Latent Sequence˜xSequence˜ Sequence˜x</head><p>Subsequently, we wish to predict x. Predicting a discrete sequence of symbols through draws from multinomial distributions over a vocabulary is not an option, as we would not be able to backpropa- gate through this discrete choice. Marginalising over the possible latent strings or estimating the gradient through na¨ıvena¨ıve Monte Carlo methods would be a pro- hibitively high variance process because the num- ber of strings is exponential in the maximum length (which we would have to manually specify) with the vocabulary size as base. To allow backpropaga- tion, we instead predict a sequence of distributions˜xdistributions˜ distributions˜x over the symbols of Σ x with an RNN attending over h y = h y 1:Ly , which will later serve to reconstruct y:</p><formula xml:id="formula_2">y3 y4 &lt; s &gt; ˜ x1˜x2 x1˜ x1˜x2 µ2, log( 2 )2 µ1, log( 2 )1 ˜ x3 µ3, log( 2 )3 ✏1 ✏2 ✏3 h x 1 h x 2 h x 3 h ˜ x 1 h ˜ x 2 h ˜ x 3 h y 3 h y 4 h ˆ y 1 h ˆ y 2 h ˆ y 3 h ˆ y 4 &lt; s &gt; ˆ y1ˆy2ˆy3ˆy4ˆy1ˆy2ˆy3 y1ˆ y1ˆy2 y1ˆy2ˆ y1ˆy2ˆy3 y1ˆy2ˆy3ˆ y1ˆy2ˆy3ˆy4 y1ˆy2ˆy3ˆy4ˆ y1ˆy2ˆy3ˆy4ˆy1 y1ˆy2ˆy3ˆy4ˆy1ˆ y1ˆy2ˆy3ˆy4ˆy1ˆy2 y1ˆy2ˆy3ˆy4ˆy1ˆy2ˆ y1ˆy2ˆy3ˆy4ˆy1ˆy2ˆy3</formula><formula xml:id="formula_3">˜ x = q(x|y) = Lx t=1 q(˜ x t |{˜x|{˜x 1 , · · · , ˜ x t−1 }, h y ) (2)</formula><p>where q(x|y) models the mapping y → x. We define</p><formula xml:id="formula_4">q(˜ x t |{˜x|{˜x 1 , · · · , ˜ x t−1 }, h y )</formula><p>in the following way: Let the vector˜xvector˜ vector˜x t be a distribution over the vocabu- lary Σ x drawn from a logistic-normal distribution 1 , the parameters of which, µ t , log(σ 2 ) t ∈ R |Σx| , are predicted by attending by an LSTM attending over the outputs of the encoder (Equation 2), where |Σ x | is the size of the vocabulary Σ x . The use of a logis- tic normal distribution serves to regularise the model in the semi-supervised learning regime, which is de- scribed at the end of this section. Formally, this pro- cess, depicted in <ref type="figure" target="#fig_0">Figure 2</ref>, is as follows:</p><formula xml:id="formula_5">h ˜ x t = f ˜ x (˜ x t−1 , h ˜ x t−1 , h y ) (3) µ t , log(σ 2 t ) = l(h ˜ x t )<label>(4)</label></formula><formula xml:id="formula_6">∼ N (0, I)<label>(5)</label></formula><formula xml:id="formula_7">γ t = µ t + σ t<label>(6)</label></formula><formula xml:id="formula_8">˜ x t = softmax(γ t )<label>(7)</label></formula><p>where the f ˜ x function is an LSTM and l a linear transformation to R 2|Σx| . We use the reparametrisa- tion trick from <ref type="bibr" target="#b17">Kingma and Welling (2014)</ref> to draw from the logistic normal, allowing us to backpropa- gate through the sampling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Encoding x</head><p>Moving on to the decoder part of our model, in the third LSTM, we embed 2 and encode˜xencode˜ encode˜x:</p><formula xml:id="formula_9">h x t = f → x (˜ x t , h x,→ t−1 ); f ← x (˜ x t , h x,← t+1 ) (8)</formula><p>When x is observed, during supervised training and also when making predictions, instead of the distri- butioñ x we feed the one-hot encoded x to this part of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reconstructing y</head><p>In the final LSTM, we decode into y:</p><formula xml:id="formula_10">p(ˆ y|˜xy|˜x) = Ly t=1 p(ˆ y t |{ˆy|{ˆy 1 , · · · , ˆ y t−1 }, h ˜ x ) (9)</formula><p>Equation 9 is implemented as an LSTM attending over h ˜ x producing a sequence of symbolsˆysymbolsˆ symbolsˆy based on recurrent states h ˆ y , aiming to reproduce input y:</p><formula xml:id="formula_11">h ˆ y t = f ˆ y (ˆ y t−1 , h ˆ y t−1 , h ˜ x ) (10) ˆ y t ∼ softmax(l (h ˆ y t ))<label>(11)</label></formula><p>where f ˆ y is the non-linear function, and the actual probabilities are given by a softmax function after a linear transformation l of h ˆ y . At training time, rather thanˆythanˆ thanˆy t−1 we feed the ground truth y t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Loss function</head><p>The complete model described in this section gives a reconstruction function y → ˆ y. We define a loss on this reconstruction which accommodates the unsu- pervised case, where x is not observed in the train- ing data, and the supervised case, where (x, y) pairs are available. Together, these allow us to train the SEQ4 model in a semi-supervised setting, which ex- periments will show provides some benefits over a purely supervised training regime.</p><p>Unsupervised case When x isn't observed, the loss we minimise during training is the recon- struction loss on y, expressed as the negative log- likelihood N LL(ˆ y, y) of the true labels y relative to the predictionsˆypredictionsˆ predictionsˆy. To this, we add as a regularising term the KL divergence KL[q(γ|y)p(γ)] which ef- fectively penalises the mean and variance of q(γ|y) from diverging from those of a prior p(γ), which we model as a diagonal Gaussian N (0, I). This has the effect of smoothing the logistic normal distribu- tion from which we draw the distributions over sym- bols of x, guarding against overfitting of the latent distributions over x to symbols seen in the super- vised case discussed below. The unsupervised loss is therefore formalised as <ref type="formula" target="#formula_1">(12)</ref> with regularising factor α is tuned on validation, and</p><formula xml:id="formula_12">L unsup = N LL(ˆ y, y) + αKL[q(γ|y)p(γ)]</formula><formula xml:id="formula_13">KL[q(γ|y)p(γ)] = Lx i=1 KL[q(γ i |y)p(γ)] (13)</formula><p>We use a closed form of these individual KL diver- gences, described by <ref type="bibr" target="#b17">Kingma and Welling (2014)</ref>.</p><p>Supervised case When x is observed, we addi- tionally minimise the prediction loss on x, expressed as the negative log-likelihood N LL(˜ x, x) of the true labels x relative to the predictions˜xpredictions˜ predictions˜x, and do not im- pose the KL loss. The supervised loss is thus</p><formula xml:id="formula_14">L sup = N LL(˜ x, x) + N LL(ˆ y, y)<label>(14)</label></formula><p>In both the supervised and unsupervised case, be- cause of the continuous relaxation on generating˜xgenerating˜ generating˜x and the reparameterisation trick, the gradient of the losses with regard to the model parameters is well defined throughout SEQ4.</p><p>Semi-supervised training and inference We train with a weighted combination of the supervised and unsupervised losses described above. Once trained, we simply use the x → y decoder segment of the model to predict y from sequences of sym- bols x represented as one-hot vectors. When the de- coder is trained without the encoder in a fully super- vised manner, it serves as our supervised sequence- to-sequence baseline model under the name S2S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks and Data Generation</head><p>We apply our model to three tasks outlined in this section. Moreover, we explain how we generated ad- ditional unsupervised training data for two of these tasks. Examples from all datasets are in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GeoQuery</head><p>The first task we consider is the prediction of a query on the GEO corpus which is a frequently used bench- mark for semantic parsing. The corpus contains 880 questions about US geography together with exe- cutable queries representing those questions. We follow the approach established by <ref type="bibr" target="#b30">Zettlemoyer and Collins (2005)</ref> and split the corpus into 600 training and 280 test cases. Following common practice, we augment the dataset by referring to the database dur- ing training and test time. In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in <ref type="bibr" target="#b9">Dong and Lapata (2016)</ref>.</p><p>Most prior work on the GEO corpus relies on stan- dard semantic parsing methods together with custom heuristics or pipelines for this corpus. The recent pa- per by <ref type="bibr" target="#b9">Dong and Lapata (2016)</ref> is of note, as it uses a sequence-to-sequence model for training which is the unidirectional equivalent to S2S, and also to the decoder part of our SEQ4 network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Open Street Maps</head><p>The second task we tackle with our model is the NLMAPS dataset by <ref type="bibr" target="#b12">Haas and Riezler (2016)</ref>. The dataset contains 1,500 training and 880 testing in- stances of natural language questions with corre- sponding machine readable queries over the geo- graphical OpenStreetMap database. The dataset contains natural language question in both English and German but we focus only on single language semantic parsing, similar to the first task in <ref type="bibr" target="#b12">Haas and Riezler (2016)</ref>. We use the data as it is, with the only pre-processing step being the tokenization of both natural language and query form 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Navigational Instructions to Actions</head><p>The SAIL corpus and task were developed to train agents to follow free-form navigational route in- structions in a maze environment ( <ref type="bibr" target="#b22">MacMahon et al., 2006;</ref><ref type="bibr" target="#b8">Chen and Mooney, 2011)</ref>. It consists of a small number of mazes containing features such as objects, wall and floor types. These mazes come to- gether with a large number of human instructions paired with the required actions 4 to reach the goal state described in those instructions.</p><p>We use the sentence-aligned version of the SAIL route instruction dataset containing 3,236 sentences ( <ref type="bibr" target="#b8">Chen and Mooney, 2011)</ref>. Following previous work, we accept an action sequence as correct if and only if the final position and orientation exactly match those of the gold data. We do not perform any pre-processing on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Generation</head><p>As argued earlier, we are focusing on tasks where aligned data is sparse and expensive to obtain, while it should be cheap to get unsupervised, monomodal data. Albeit that is a reasonable assumption for real world data, the datasets considered have no such component, thus the approach taken here is to gen- erate random database queries or maze paths, i.e. the machine readable side of the data, and train a semi-supervised model. The alternative not ex- plored here would be to generate natural language questions or instructions instead, but that is more difficult to achieve without human intervention. For this reason, we generate the machine readable side of the data for GEOQUERY and SAIL tasks <ref type="bibr">5</ref> .</p><p>For GEOQUERY, we fit a 3-gram Kneser-Ney ( <ref type="bibr" target="#b7">Chen and Goodman, 1999</ref>) model to the queries in the training set and sample about 7 million queries from it. We ensure that the sampled queries are dif- ferent from the training queries, but do not enforce validity. This intentionally simplistic approach is to demonstrate the applicability of our model.</p><p>The SAIL dataset has only three mazes. We added a fourth one and over 150k random paths, in- cluding duplicates. The new maze is larger (21 × 21 grid) than the existing ones, and seeks to approxi- mately replicate the key statistics of the other three mazes (maximum corridor length, distribution of ob- jects, etc). Paths within that maze are created by randomly sampling start and end positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on the three tasks in multiple settings. First, we establish a supervised baseline to compare the S2S model with prior work. Next, we <ref type="bibr">5</ref> Our randomly generated unsupervised datasets can be downloaded from http://deepmind.com/ publications</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy</head><p>Zettlemoyer and <ref type="bibr" target="#b30">Collins (2005)</ref> 79.3 Zettlemoyer and <ref type="bibr" target="#b31">Collins (2007)</ref> 86.1 <ref type="bibr" target="#b21">Liang et al. (2013)</ref> 87.9 <ref type="bibr" target="#b18">Kwiatkowski et al. (2011)</ref> 88.6 <ref type="bibr" target="#b32">Zhao and Huang (2014)</ref> 88.9 <ref type="bibr" target="#b19">Kwiatkowski et al. (2013)</ref> 89.0</p><p>Dong and Lapata (2016) 84.6 Jia and Liang (2016) <ref type="bibr">6</ref> 89.3 S2S 86.5 SEQ4 87.3 train our SEQ4 model in a semi-supervised setting on the entire dataset with the additional monomodal training data described in the previous section. Finally, we perform an "ablation" study where we discard some of the training data and compare S2S to SEQ4. S2S is trained solely on the reduced data in a supervised manner, while SEQ4 is once again trained semi-supervised on the same reduced data plus the machine readable part of the discarded data (SEQ4-) or on the extra generated data (SEQ4+).</p><p>Training We train the model using standard gra- dient descent methods. As none of the datasets used here contain development sets, we tune hyperparam- eters by cross-validating on the training data. In the case of the SAIL corpus we train on three folds (two mazes for training and validation, one for test each) and report weighted results across the folds follow- ing prior work ( <ref type="bibr" target="#b24">Mei et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GeoQuery</head><p>The evaluation metric for GEOQUERY is the ac- curacy of exactly predicting the machine readable query. As results in <ref type="table" target="#tab_0">Table 2</ref> show, our supervised S2S baseline model performs slightly better than the comparable model by <ref type="bibr" target="#b9">Dong and Lapata (2016)</ref>. The semi-supervised SEQ4 model with the addi- tional generated queries improves on it further.</p><p>The ablation study in <ref type="table" target="#tab_2">Table 3</ref> demonstrates a widening gap between supervised and semi-Sup. data S2S SEQ4-SEQ4+ 5%</p><p>21   supervised as the amount of labelled training data gets smaller. This suggests that our model can lever- age unlabelled data even when only small amount of labelled data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Open Street Maps</head><p>We report results for the NLMAPS corpus in <ref type="table" target="#tab_3">Table 4</ref>, comparing the supervised S2S model to the results posted by <ref type="bibr" target="#b12">Haas and Riezler (2016)</ref>. While their model used a semantic parsing pipeline including alignment, stemming, language modelling and CFG inference, the strong performance of the S2S model demonstrates the strength of fairly vanilla attention- based sequence-to-sequence models. It should be pointed out that the previous work reports the num- ber of correct answers when queries were executed against the dataset, while we evaluate on the strict accuracy of the generated queries. While we expect these numbers to be nearly equivalent, our evalua- tion is strictly harder as it does not allow for reorder- ing of query arguments and similar relaxations.</p><p>We investigate the SEQ4 model only via the abla- tion study in <ref type="table" target="#tab_5">Table 5</ref> and find little gain through the semi-supervised objective. Our attempt at cheaply generating unsupervised data for this task was not successful, likely due to the complexity of the un- derlying database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Navigational Instructions to Actions</head><p>Model extension The experiments for the SAIL task differ slightly from the other two tasks in that the language input does not suffice for choosing an Sup. data S2S SEQ4-   action. While a simple instruction such as 'turn left' can easily be translated into the action sequence LEFT-STOP, more complex instructions such as 'Walk forward until you see a lamp' require knowl- edge of the agent's position in the maze.</p><p>To accomplish this we modify the model as fol- lows. First, when encoding action sequences, we concatenate each action with a representation of the maze at the given position, representing the maze- state akin to <ref type="bibr" target="#b24">Mei et al. (2016)</ref> with a bag-of-features vector. Second, when decoding action sequences, the RNN outputs an action which is used to update the agent's position and the representation of that new position is fed into the RNN as its next input.</p><p>Training regime We cross-validate over the three mazes in the dataset and report overall results weighted by test size (cf. <ref type="bibr" target="#b24">Mei et al. (2016)</ref>). Both our supervised and semi-supervised model perform worse than the state-of-the-art (see <ref type="table" target="#tab_6">Table 6</ref>), but the latter enjoys a comfortable margin over the former. As the S2S model broadly reimplements the work of <ref type="bibr" target="#b24">Mei et al. (2016)</ref>, we put the discrepancy in per- formance down to the particular design choices that we did not follow in order to keep the model here as general as possible and comparable across tasks.</p><p>The ablation studies <ref type="table" target="#tab_7">(Table 7)</ref> show little gain for the semi-supervised approach when only using data from the original training set, but substantial im- provement with the additional unsupervised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Supervised training The prediction accuracies of our supervised baseline S2S model are mixed with respect to prior results on their respective tasks. For GEOQUERY, S2S performs significantly better than the most similar model from the literature <ref type="bibr" target="#b9">(Dong and Lapata, 2016)</ref>, mostly due to the fact that y and x are Input from unsupervised data (y) Generated latent representation <ref type="bibr">(x)</ref> answer smallest city loc 2 state stateid STATE what is the smallest city in the state of STATE &lt;/S&gt; answer city loc 2 state next to 2 stateid STATE what are the cities in states which border STATE &lt;/S&gt; answer mountain loc 2 countryid COUNTRY what is the lakes in COUNTRY &lt;/S&gt; answer state next to 2 state all which states longer states show peak states to &lt;/S&gt; <ref type="table">Table 8</ref>: Positive and negative examples of latent language together with the randomly generated logical form from the unsupervised part of the GEOQUERY training. Note that the natural language (x) does not occur anywhere in the training data in this form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy</head><p>Chen and Mooney (2011) 54.40 <ref type="bibr" target="#b15">Kim and Mooney (2012)</ref> 57.22 <ref type="bibr" target="#b1">Andreas and Klein (2015)</ref> 59.60 <ref type="bibr" target="#b16">Kim and Mooney (2013)</ref> 62.81 <ref type="bibr" target="#b4">Artzi et al. (2014)</ref> 64.36  65.28</p><p>Mei et al. <ref type="formula" target="#formula_1">(2016)</ref> 69.98</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2S</head><p>58.60 SEQ4</p><p>63.25  the discrepancy between the 100% result and S2S in <ref type="table" target="#tab_6">Table 6.</ref> encoded with bidirectional LSTMs. With a unidirec- tional LSTM we get similar results to theirs. On the SAIL corpus, S2S performs worse than the state of the art. As the models are broadly equiv- alent we attribute this difference to a number of task- specific choices and optimisations 7 made in <ref type="bibr" target="#b24">Mei et al. (2016)</ref> which we did not reimplement for the sake of using a common model across all three tasks.</p><p>For NLMAPS, S2S performs much better than the state-of-the-art, exceeding the previous best result by 11% despite a very simple tokenization method <ref type="bibr">7</ref> In particular we don't use beam search and ensembling. and a lack of any form of entity anonymisation.</p><p>Semi-supervised training In both the case of GEOQUERY and the SAIL task we found the semi- supervised model to convincingly outperform the fully supervised model. The effect was particu- larly notable in the case of the SAIL corpus, where performance increased from 58.60% accuracy to 63.25% (see <ref type="table" target="#tab_6">Table 6</ref>). It is worth remembering that the supervised training regime consists of three folds of tuning on two maps with subsequent testing on the third map, which carries a risk of overfitting to the training maps. The introduction of the fourth unsupervised map clearly mitigates this effect. Ta- ble 8 shows some examples of unsupervised logi- cal forms being transformed into natural language, which demonstrate how the model can learn to sen- sibly ground unsupervised data.</p><p>Ablation performance The experiments with ad- ditional unsupervised data prove the feasibility of our approach and clearly demonstrate the useful- ness of the SEQ4 model for the general class of sequence-to-sequence tasks where supervised data is hard to come by. To analyse the model fur- ther, we also look at the performance of both S2S and SEQ4 when reducing the amount of supervised training data available to the model. We compare three settings: the supervised S2S model with re- duced training data, SEQ4-which uses the removed training data in an unsupervised fashion (throwing away the natural language) and SEQ4+ which uses the randomly generated unsupervised data described in Section 3. The S2S model behaves as expected on all three tasks, its performance dropping with the size of the training data. The performance of SEQ4- and SEQ4+ requires more analysis.</p><p>In the case of GEOQUERY, having unlabelled data from the true distribution (SEQ4-) is a good thing when there is enough of it, as clearly seen when only 5% of the original dataset is used for supervised training and the remaining 95% is used for unsuper- vised training. The gap shrinks as the amount of supervised data is increased, which is as expected. On the other hand, using a large amount of extra, generated data from an approximating distribution (SEQ4+) does not help as much initially when com- pared with the unsupervised data from the true dis- tribution. However, as the size of the unsupervised dataset in SEQ4-becomes the bottleneck this gap closes and eventually the model trained on the ex- tra data achieves higher accuracy.</p><p>For the SAIL task the semi-supervised models do better than the supervised results throughout, with the model trained on randomly generated additional data consistently outperforming the model trained only on the original data. This gives further credence to the risk of overfitting to the training mazes already mentioned above.</p><p>Finally, in the case of the NLMAPS corpus, the semi-supervised approach does not appear to help much at any point during the ablation. These indis- tinguishable results are likely due to the task's com- plexity, causing the ablation experiments to either have to little supervised data to sufficiently ground the latent space to make use of the unsupervised data, or in the higher percentages then too little un- supervised data to meaningfully improve the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural lan- guage to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment.</p><p>Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming ( <ref type="bibr" target="#b29">Zelle and Mooney, 1996)</ref>, string- to-tree ( <ref type="bibr" target="#b10">Galley et al., 2004</ref>) and string-to-graph ( <ref type="bibr" target="#b14">Jones et al., 2012</ref>) transducers, grammar induction ( <ref type="bibr" target="#b18">Kwiatkowski et al., 2011;</ref><ref type="bibr" target="#b25">Reddy et al., 2014</ref>) or machine translation ( <ref type="bibr" target="#b28">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b2">Andreas et al., 2013)</ref>.</p><p>While a large number of relevant literature fo- cuses on defining the grammar of the logical forms ( <ref type="bibr" target="#b30">Zettlemoyer and Collins, 2005</ref>), other models learn purely from aligned pairs of text and logical form <ref type="bibr" target="#b6">(Berant and Liang, 2014)</ref>, or from more weakly su- pervised signals such as question-answer pairs to- gether with a database ( <ref type="bibr" target="#b20">Liang et al., 2011</ref>). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional train- ing examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup pro- posed here offers an alternative solution to this issue.</p><p>Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing <ref type="bibr" target="#b26">(Suster et al., 2016;</ref><ref type="bibr">Marcheggiani and Titov, 2016, i.a.</ref>) This work presents a first approach to using effectively dis- cretised sequential information as the latent rep- resentation without resorting to draconian assump- tions ( <ref type="bibr" target="#b0">Ammar et al., 2014</ref>) to make marginalisation tractable. While our model is not exactly marginalis- able either, the continuous relaxation makes training far more tractable. A related idea was recently pre- sented in <ref type="bibr">Gülçehre et al. (2015)</ref>, who use monolin- gual data to improve machine translation by fusing a sequence-to-sequence model and a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We described a method for augmenting a supervised sequence transduction objective with an autoen- coding objective, thereby enabling semi-supervised training where previously a scarcity of aligned data might have held back model performance. Across multiple semantic parsing tasks we demonstrated the effectiveness of this approach, improving model per- formance by training on randomly generated unsu- pervised data in addition to the original data. Going forward it would be interesting to fur- ther analyse the effects of sampling from a logistic- normal distribution as opposed to a softmax in or- der to better understand how this impacts the dis- tribution in the latent space. While we focused on tasks with little supervised data and additional un- supervised data in y, it would be straightforward to reverse the model to train it with additional labelled data in x, i.e. on the natural language side. A natural extension would also be a formulation where semi- supervised training was performed in both x and y.</p><p>For instance, machine translation lends itself to such a formulation where for many language pairs paral- lel data may be scarce while there is an abundance of monolingual data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Unsupervised case of the SEQ4 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5%</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Non-neural and neural model results on GEOQUERY 

using the train/test split from (Zettlemoyer and Collins, 2005). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Results of the GEOQUERY ablation study.</head><label>3</label><figDesc></figDesc><table>Model 
Accuracy 

Haas and Riezler (2016) 
68.30 
S2S 
78.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Results on the NLMAPS corpus.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results of the NLMAPS ablation study. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Results on the SAIL corpus. 

Sup. data 
S2S SEQ4-SEQ4+ 

5% 
37.79 41.48 
43.44 
10% 
40.77 41.26 
48.67 
25% 
43.76 43.95 
51.19 
50% 
48.01 49.42 
55.97 
75% 
48.99 49.20 
57.40 
100% 
49.49 49.49 
58.28 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results of the SAIL ablation study. Results are from 

models trained on L and Jelly maps, tested on Grid only, hence 

</table></figure>

			<note place="foot" n="1"> The logistic-normal distribution is the exponentiated and normalised (i.e. taking softmax) normal distribution.</note>

			<note place="foot" n="2"> Multiplying the distribution over words and an embedding matrix averages the word embedding of the entire vocabulary weighted by their probabilities.</note>

			<note place="foot" n="3"> We removed quotes, added spaces around (), and separated the question mark from the last word in each question. 4 There are four actions: LEFT, RIGHT, GO, STOP.</note>

			<note place="foot" n="6"> Jia and Liang (2016) used hand crafted grammars to generate additional supervised training data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional Random Field Autoencoders for Unsupervised Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alignment-based Compositional Semantics for Instruction Following</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic Parsing as Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Compact Lexicons for CCG Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic Parsing via Paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to Interpret Natural Language Navigation Instructions from Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01280</idno>
		<title level="m">Language to Logical Form with Neural Attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On Using Monolingual Corpora in Neural Machine Translation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A corpus and semantic parser for multilingual natural language querying of openstreetmap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolin</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL, June. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory</title>
		<meeting>NAACL, June. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory</meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemanticsBased Machine Translation with Hyperedge Replacement Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adapting Discriminative Reranking to Grounded Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AutoEncoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lexical Generalization in CCG Grammar Induction for Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Citeseer</title>
		<meeting>EMNLP. Citeseer</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Dependency-based Compositional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-HLT</title>
		<meeting>the ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discrete-state variational autoencoders for joint discovery and factorization of relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale Semantic Parsing without QuestionAnswer Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<idno>abs/1603.09128</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grammar as a Foreign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning for Semantic Parsing with Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to Parse Database Queries using Inductive Logic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI/IAAI</title>
		<meeting>AAAI/IAAI</meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online Learning of Relaxed CCG Grammars for Parsing to Logical Form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5379</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
