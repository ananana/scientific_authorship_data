<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Millions of Personalized Dialogue Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><forename type="middle">Bordes</forename><surname>Facebook</surname></persName>
						</author>
						<title level="a" type="main">Training Millions of Personalized Dialogue Agents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2775" to="2779"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2775</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current dialogue systems are not very engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some person-alized back-story to the model. However, the dataset used in (Zhang et al., 2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from (Zhang et al., 2018) and achieving state-of-the-art results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end dialogue systems, based on neural ar- chitectures like bidirectional LSTMs or Memory Networks ( <ref type="bibr" target="#b7">Sukhbaatar et al., 2015</ref>) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple con- texts ( <ref type="bibr" target="#b9">Wen et al., 2016;</ref><ref type="bibr" target="#b0">Bordes et al., 2016)</ref>. One of their main advantages is that they can rely on large data sources of existing di- alogues to learn to cover various domains without requiring any expert knowledge. However, the flip side is that they also exhibit limited engagement, especially in chit-chat settings: they lack consis- tency and do not leverage proactive engagement strategies as (even partially) scripted chatbots do. <ref type="bibr" target="#b12">Zhang et al. (2018)</ref> introduced the PERSONA- CHAT dataset as a solution to cope with this issue. This dataset consists of dialogues between pairs of agents with text profiles, or personas, attached to each of them. As shown in their paper, condition- ing an end-to-end system on a given persona im- proves the engagement of a dialogue agent. This paves the way to potentially end-to-end personal- ized chatbots because the personas of the bots, by being short texts, could be easily edited by most users. However, the PERSONA-CHAT dataset was created using an artificial data collection mecha- nism based on Mechanical Turk. As a result, nei- ther dialogs nor personas can be fully represen- tative of real user-bot interactions and the dataset coverage remains limited, containing a bit more than 1k different personas.</p><p>In this paper, we build a very large-scale persona-based dialogue dataset using conversa- tions previously extracted from REDDIT <ref type="bibr">1</ref> . With simple heuristics, we create a corpus of over 5 million personas spanning more than 700 million conversations. We train persona-based end-to-end dialogue models on this dataset. These models outperform their counterparts that do not have ac- cess to personas, confirming results of <ref type="bibr" target="#b12">Zhang et al. (2018)</ref>. In addition, the coverage of our dataset seems very good since pre-training on it also leads to state-of-the-art results on the PERSONA-CHAT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>With the rise of end-to-end dialogue systems, per- sonalized trained systems have started to appear. <ref type="bibr" target="#b4">Li et al. (2016)</ref> proposed to learn latent vari- ables representing each speaker's bias/personality in a dialogue model. Other classic strategies in- clude extracting explicit variables from structured knowledge bases or other symbolic sources as in ( <ref type="bibr">Ghazvininejad et al., 2017;</ref><ref type="bibr" target="#b3">Joshi et al., 2017;</ref><ref type="bibr" target="#b11">Young et al., 2017)</ref>. Still, in the context of per-sonal chatbots, it might be more desirable to con- dition on data that can be generated and inter- preted by the user itself such as text rather than relying on some knowledge base facts that might not exist for everyone or a great variety of situ- ations. PERSONA-CHAT ( <ref type="bibr" target="#b12">Zhang et al., 2018)</ref> re- cently introduced a dataset of conversations re- volving around human habits and preferences. In their experiments, they showed that conditioning on a text description of each speaker's habits, their persona, improved dialogue modeling.</p><p>In this paper, we use a pre-existing REDDIT data dump as data source. REDDIT is a massive on- line message board. <ref type="bibr" target="#b1">Dodge et al. (2015)</ref> used it to assess chit-chat qualities of generic dialogue mod- els. <ref type="bibr" target="#b10">Yang et al. (2018)</ref> used response prediction on REDDIT as an auxiliary task in order to improve prediction performance on natural language infer- ence problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Building a dataset of millions of persona-based dialogues</head><p>Our goal is to learn to predict responses based on a persona for a large variety of personas. To that end, we build a dataset of examples of the follow- ing form using data from REDDIT:</p><p>• Persona: ["I like sport", "I work a lot"]</p><p>• Context: "I love running."</p><p>• Response: "Me too! But only on weekends."</p><p>The persona is a set of sentences representing the personality of the responding agent, the con- text is the utterance that it responds to, and the re- sponse is the answer to be predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>As in ( <ref type="bibr" target="#b1">Dodge et al., 2015)</ref>, we use a preexist- ing dump of REDDIT that consists of 1.7 billion comments. We tokenize sentences by padding all special characters with a space and splitting on whitespace characters. We create a dictionary con- taining the 250k most frequent tokens. We trun- cate comments that are longer than 100 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Persona extraction</head><p>We construct the persona of a user by gathering all the comments they wrote, splitting them into sentences, and selecting the sentences that satisfy the following rules: (i) each sentence must contain between 4 and 20 words or punctuation marks, (ii) it contains either the word I or my, (iii) at least one verb, and (iv) at least one noun, pronoun or adjective.</p><p>To handle the quantity of data involved, we limit the size of a persona to N sentences for each user. We compare four different setups for persona cre- ation. In the rules setup, we select up to N random sentences that satisfy the rules above. In the rules + classifier setup, we filter with the rules then score the resulting sentences using a bag-of-words classifier that is trained to discriminate PERSONA- CHAT persona sentences from random comments. We manually tune a threshold on the score in order to select sentences. If there are more than N eli- gible persona sentences for a given user, we keep the highest-scored ones. In the random from user setup, we randomly select sentences uttered by the user while keeping the sentence length require- ment above (we ignore the other rules). The ran- dom from dataset baseline refers to random sen- tences from the dataset. They do not necessarily come from the same user. This last setup serves as a control mechanism to verify that the gains in prediction accuracy are due to the user-specific in- formation contained in personas.</p><p>In the example at the beginning of this section, the response is clearly consistent with the persona. There may not always be such an obvious relation- ship between the two: the discussion topic may not be covered by the persona, a single user may write contradictory statements, and due to errors in the extraction process, some persona sentences may not represent a general trait of the user (e.g. I am feeling happy today).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset creation</head><p>We take each pair of successive comments in a thread to form the context and response of an ex- ample. The persona corresponding to the response is extracted using one of the methods of Sec- tion 3.2. We split the dataset randomly between training, validation and test. Validation and test sets contain 50k examples each. We extract per- sonas using training data only: test set responses cannot be contained explicitly in the persona.</p><p>In total, we select personas covering 4.6m users in the rule-based setups and 7.2m users in the ran- dom setups. This is a sizable fraction of the total 13.2m users of the dataset; depending on the per- sona selection setup, between 97 and 99.4 % of the training set examples are linked to a persona.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">End-to-end dialogue models</head><p>We model dialogue by next utterance retrieval ( <ref type="bibr" target="#b5">Lowe et al., 2016)</ref>, where a response is picked among a set of candidates and not generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>The overall architecture is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. We encode the persona and the context using separate modules. As in <ref type="bibr" target="#b12">Zhang et al. (2018)</ref>, we combine the encoded context and persona using a 1-hop memory network with a residual connection, us- ing the context as query and the set of persona sentences as memory. We also encode all candi- date responses and compute the dot-product be- tween all those candidate representations and the joint representation of the context and the persona. The predicted response is the candidate that maxi- mizes the dot product.</p><p>We train by passing all the dot products through a softmax and maximizing the log-likelihood of the correct responses. We use mini-batches of training examples and, for each example therein, all the responses of the other examples of the same batch are used as negative responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context and response encoders</head><p>Both context and response encoders share the same architecture and word embeddings but have different weights in the subsequent layers. We train three different encoder architectures.</p><p>Bag-of-words applies two linear projections separated by a tanh non-linearity to the word em- beddings. We then sum the resulting sentence rep- resentation across all positions in the sentence and divide the result by √ n where n is the length of the sequence.</p><p>LSTM applies a 2-layer bidirectional LSTM. We use the last hidden state as encoded sentence.</p><p>Transformer is a variation of an End-to-end Memory Network (Sukhbaatar et al., 2015) intro- duced by <ref type="bibr">Vaswani et al. (2017)</ref>. Based solely on attention mechanisms, it exhibited state-of-the-art performance on next utterance retrieval tasks in di- alogues ( <ref type="bibr" target="#b10">Yang et al., 2018</ref>  <ref type="table">Table 1</ref>: Test results when classifying the correct an- swer among a total of 100 possible answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Persona encoder</head><p>The persona encoder encodes each persona sen- tence separately. It relies on the same word em- beddings as the context encoder and applies a lin- ear layer on top of them. We then sum the repre- sentations across the sentence. We deliberately choose a simpler architecture than the other encoders for performance reasons as the number of personas encoded for each batch is an order of magnitude greater than the number of training examples. Most personas are short sen- tences; we therefore expect a bag-of-words repre- sentation to encode them well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We train models on the persona-based dialogue dataset described in Section 3.3 and we evaluate its accuracy both on the original task and when transferring onto PERSONA-CHAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental details</head><p>We optimize network parameters using Adamax with a learning rate of 8e−4 on mini-batches of size 512. We initialize embeddings with FastText word vectors and optimize them during learning.</p><p>REDDIT LSTMs use a hidden size of 150; we concatenate the last hidden states for both direc- tions and layers, resulting in a final representation of size 600. Transformer architectures on reddit use 4 layers with a hidden size of 300 and 6 at- tention heads, resulting in a final representation of size 300. We use Spacy for part-of-speech tag- ging in order to verify the persona extraction rules. We distribute the training by splitting each batch across 8 GPUs; we stop training after 1 full epoch, which takes about 3 days.  The answer is constrained to be at most 10 tokens and is retrieved among 1M candidates sampled randomly from the training set.</p><p>PERSONA-CHAT We used the revised version of the dataset where the personas have been rephrased, making it a harder task. The dataset being only a few thousands samples, we had to re- duce the architecture to avoid overfitting for the models trained purely on PERSONA-CHAT. 2 lay- ers, 2 attention heads, a dropout of 0.2 and keeping the size of the word embeddings to 300 units yield the highest accuracy on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IR Baseline</head><p>As basic baseline, we use an infor- mation retrieval (IR) system that ranks candidate responses according to a TF-IDF weighted exact- match similarity with the context alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Impact of personas We report the accuracy of the different architectures on the reddit task in Ta- ble 1. Conditioning on personas improves the pre- diction performance regardless of the encoder ar- chitecture. <ref type="table" target="#tab_1">Table 2</ref> gives some examples of how the persona affects the predicted answer.</p><p>Influence of the persona extraction In <ref type="table" target="#tab_3">Table 3</ref>, we report precision results for several persona ex- traction setups. The rules setup improves the re- sults somewhat, however adding the persona clas- sifier actually degrades the results. A possible in- terpretation is that the persona classifier is trained only on the PERSONA-CHAT revised personas, and that this selection might be too narrow and lack di-  versity. Increasing the maximum persona size also improves the prediction performance.</p><p>Transfer learning We compare the perfor- mance of transformer models trained on REDDIT and on PERSONA-CHAT on both datasets. We re- port results in 35.4 - between the style of personas of the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper shows how to create a very large dataset for persona-based dialogue. We show that training models to align answers both with the persona of their author and the context improves the predicting performance. The trained models show promising coverage as exhibited by the state- of-the-art transfer results on the PERSONA-CHAT dataset. As pretraining leads to a considerable im- provement in performance, future work could be done fine-tuning this model for various dialog sys- tems. Future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the pre- diction performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Persona-based network architecture. Context (Persona) Predicted Answer Where do you come from? (I was born in London.) I'm from London, studying in Scotland. (I was born in New York.) I'm from New York. What do you do? (I am a doctor.) I am a sleep and respiratory therapist. (I am an engineer.) I am a software developer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). Here we use only its encoding module. We subsequently average the resulting representation across all positions in the sentence, yielding a fixed-size representation.</figDesc><table>hits@k 
Persona k=1 k=3 k=10 
IR Baseline 
No 
5.6 
9.9 
19.5 
BOW 
No 
51.7 64.7 77.9 
BOW 
Yes 
53.9 67.9 81.9 
LSTM 
No 
63.1 75.6 87.3 
LSTM 
Yes 
66.3 79.5 90.6 
Transformer 
No 
69.1 80.7 90.7 
Transformer 
Yes 
74.4 85.6 94.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Sample predictions from the best model. In all selected cases the persona consists of a single sentence.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Retrieval precision on the REDDIT test set us-
ing a Transformer and different persona selection sys-
tems. N : maximum number of sentences per persona. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>This architecture provides 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>hits@1 results for the best found Transformer 
architecture on different test sets. FT-PC: REDDIT-
trained model fine-tuned on the PERSONA-CHAT train-
ing set. To be comparable to the state of the art on each 
dataset, results on PERSONA-CHAT are computed using 
20 candidates, while results on REDDIT use 100. 

</table></figure>

			<note place="foot" n="1"> https://www.reddit.com/r/datasets/ comments/3bxlg7/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evaluating prerequisite qualities for learning end-to-end dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06931</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1702.01932</idno>
		<title level="m">Wen-tau Yih, and Michel Galley. 2017. A knowledge-grounded neural conversation model. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chaitanya K Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boi</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faltings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07503</idno>
		<title level="m">Personalization in goal-oriented dialog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno>abs/1603.06155</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the evaluation of dialogue systems with next utterance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05414</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning semantic textual similarity from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Pilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>abs/1804.07754</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Augmenting end-to-end dialog systems with commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subham</forename><surname>Biswas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05453</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>do you have pets too? CoRR, abs/1801.07243</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
