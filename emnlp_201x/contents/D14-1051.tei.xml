<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An I-vector Based Approach to Compact Multi-Granularity Topic Spaces Representation of Textual Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouallegue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dufour</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linar√®s</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Driss</forename><surname>Matrouf</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>De Mori</surname></persName>
							<email>rdemori@cs.mcgill.ca</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">University of Avignon</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An I-vector Based Approach to Compact Multi-Granularity Topic Spaces Representation of Textual Documents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="443" to="454"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents. Nonetheless, these documents may contain more than one main theme, and their automatic transcription inevitably contains errors. In this study, we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity. Firstly, various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation. Then, this multiple topic space representation is compacted into an elementary segment , called c-vector, originally developed in the context of speaker recognition. Experiments are conducted on the DECODA corpus of conversations. Results show the effectiveness of the proposed multi-view compact representation paradigm. Our identification system reaches an accuracy of 85%, with a significant gain of 9 points compared to the baseline (best single topic space configuration).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic Speech Recognition (ASR) systems frequently fail on noisy conditions and high Word Error Rates (WER) make the analysis of the au- tomatic transcriptions difficult. Speech analyt- ics suffer from these transcription issues that may be overcome by improving the ASR robustness and/or the tolerance of speech analytic systems to ASR errors. This paper proposes a new method to improve the robustness of speech analytics by combining a semantic multi-model approach and a noise reduction technique based on the i-vector paradigm.</p><p>This method is evaluated in the application framework of the RATP call centre (Paris Public Transportation Authority), focusing on the theme identification task <ref type="bibr" target="#b1">(Bechet et al., 2012)</ref>.</p><p>Telephone conversations are a particular case of human-human interaction whose automatic processing raises problems, especially due to the speech recognition step required to obtain the transcription of the speech contents. First, the speaker's behavior may be unexpected and the training/test mismatch may be very large. Second, the speech signal may be strongly impacted by various sources of variability: environment and channel noises, acquisition devices, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Telephone conversation issues</head><p>Topics are related to the reason why the customer called. Various classes corresponding to the main customer's requests are considered (lost and founds, traffic state, timelines, etc). In addition to classical issues in such adverse conditions, the topic-identification system should deal with problems related to class proximity. For example, a lost &amp; found request is related to itinerary (where was the object lost?) or timeline (when?), that could appear in most of the classes. In fact, these conversations involve a relatively small set of basic concepts related to transportation issues. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of a dialogue which is manually labeled by the agent as an issue related to an infraction. However, words in bold suggest that this conversation could be related to a transportation card. Thus, we assume that a dialogue representation should be seen as a multi-view problem to substantiate the claims regarding the multi-theme representation of a given dialogue.</p><p>On the other hand, multi-view approaches in- troduce additional variability due to the diversity of the views. This variability is also due to the vocabulary used by both agent and customer during a telephone conversation. Indeed, an agent have to follow an predefined scenario of conversation. Thus, the agent can find the main reason for the call which corresponds to the theme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed solutions</head><p>An efficient way to tackle both ASR robustness and class ambiguity could be to map dialogues into a topic space abstracting the ASR outputs. Then, dialogue categorization is achieved in this topic space. Numerous unsupervised methods for topic-space estimation were proposed in the past. Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) has been largely used for speech analytics; one of its main drawbacks is the tuning of the model, that involves various meta-parameters such as the number of classes (that determines the model granularity), word distribution meth- ods, temporal spans. . . If the decision process is highly dependent on these features, the system's performance could be quite unstable.</p><p>Classically, this abstract representation involves selecting the right number of classes composing the topic space. This decision is crucial since topic model perplexity, which expresses its qual- ity, is highly dependent on this feature. Further- more, the multi-theme context of the study (see <ref type="figure" target="#fig_0">Figure 1</ref>) involves a more complex dialogue rep- resentation. In this paper, we propose to deal with these two drawbacks by using a compact represen- tation from multiple topic spaces. This model is based on a robust multi-view representation of the textual documents.</p><p>A multi-view representation of a dialogue intro- duces both a relevant variability needed to repre- sent different contexts of the dialogue, and a noisy variability related to topic space processing. Thus, a topic-based representation of a dialogue is built from the dialogue content itself. For this reason, the mapping process of a dialogue into several topic spaces generates a noisy variability related to the difference between the dialogue and the con- tent of each class. In the same way, the relevant variability comes from the common content be- tween the dialogue and the classes composing the topic space.</p><p>We propose to reduce the noisy variability by using a factor analysis technique, which was ini- tially developed in the domain of speaker identifi- cation. In this field, the factor analysis paradigm is used as a decomposition model that enables to separate the representation space into two sub- spaces containing respectively useful and useless information. The general Joint Factor Analysis (JFA) paradigm ( <ref type="bibr" target="#b18">Kenny et al., 2008</ref>) considers multiple variabilities that may be cross-dependent. Therefore, JFA representation allows us to com- pensate the variability within sessions of a same speaker. This representation is an extension of the GMM-UBM (Gaussian Mixture Model-Universal Background Model) models <ref type="bibr" target="#b27">(Reynolds and Rose, 1995)</ref>. <ref type="bibr" target="#b9">(Dehak et al., 2011</ref>) extract a compact super-vector (called an i-vector) from the GMM super-vector. The aim of the compression pro- cess (i-vector extraction) is to represent the super- vector variability in a low dimensional space. Al- though this compact representation is widely used in speaker recognition systems, this method has not been used yet in the field of text classification.</p><p>In this paper, we propose to apply factor anal- ysis to compensate noisy variabilities due to the multiplication of LDA models. Furthermore, a normalization approach to condition dialogue rep- resentations (multi-model and i-vector) is pre- sented. The two methods showed improvements for speaker verification: within Class Covariance Normalization (WCCN) <ref type="bibr" target="#b9">(Dehak et al., 2011</ref>) and Eigen Factor Radial (EFR) <ref type="bibr" target="#b6">(Bousquet et al., 2011</ref>). The latter includes length normalization <ref type="bibr">(GarciaRomero and Espy-Wilson, 2011</ref>). Both methods dilate the total variability space as a means of re- ducing the within-class variability. In our multi- model representation, the within class variability is redefined according to both dialogue content (vocabulary) and topic space characteristics (word distributions among the topics). Thus, the speaker is represented by a theme, and the speaker session is a set of topic-based representations (frames) of a dialogue (session).</p><p>The paper is organized as follows. Section 2 presents previous related works. The dialogue rep- resentation is described in Section 3. Section 4 in- troduces the i-vector compact representation and presents its application to text documents. Sec- tions 5 and 6 report experiments and results. The last section concludes and proposes some perspec- tives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In the past, several approaches considered a text document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) <ref type="bibr" target="#b8">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b2">Bellegarda, 1997)</ref>, Probabilistic LSA (PLSA) <ref type="bibr" target="#b15">(Hofmann, 1999)</ref> or Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>, build a higher-level representation of the document in a topic space. ¬ø Document is then considered as a bag-of-words <ref type="bibr" target="#b28">(Salton, 1989)</ref> where the word order is not taken into account. These methods have demonstrated their perfor- mance on various tasks, such as sentence <ref type="bibr" target="#b3">(Bellegarda, 2000</ref>) or keyword ( <ref type="bibr" target="#b29">Suzuki et al., 1998</ref>) ex- traction.</p><p>In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic to the complete doc- ument. Therefore, a document can change topics from a word to another one. However, word oc- currences are connected by a latent variable which controls the global match of the distribution of the topics in the document. These latent topics are characterized by a distribution of associated word probabilities. PLSA and LDA models have been shown to generally outperform LSA on IR tasks <ref type="bibr" target="#b16">(Hofmann, 2001</ref>). Moreover, LDA provides a direct estimate of the relevance of a topic given a word set. In this paper, probabilities of hidden topic features, estimated with LDA, are considered for possibly capturing word dependencies express- ing the semantic contents of a given conversation.</p><p>Topic-based approaches involve defining a number of topics composing the topic space. The choice of the "right" number of topics is a crucial step, especially when the documents may contain multiple themes. Many studies have tried to find a relevant method to deal with this issue. ( <ref type="bibr" target="#b0">Arun et al., 2010)</ref> proposed to use a Singular Value De- composition (SVD) to represent the separability between the words contained in the vocabulary. Then, if the singular values of the topic-word ma- trix M equal the norm of the rows of M, this means that the vocabulary is well separated among the topics. This method has to be evaluated with the Kullback-Liebler divergence metric for each topic space. However, this process would be time con- suming for thousands of representations of a dia- logue.</p><p>( <ref type="bibr" target="#b30">Teh et al., 2004</ref>) proposed the Hierarchical Dirichlet Process (HDP) method to find the "right" number of topics by assuming that the data has a hierarchical structure. The HDP models were then compared to the LDA ones on the same dataset. ( <ref type="bibr" target="#b32">Zavitsanos et al., 2008</ref>) presented a method to learn the right depth of an ontology de- pending of the number of topics of LDA models. The study presented by <ref type="bibr" target="#b7">(Cao et al., 2009</ref>) is quite similar to ( <ref type="bibr" target="#b30">Teh et al., 2004</ref>). The authors consider the average correlation between pairs of topics at each stage as the right number of topics.</p><p>All these methods assume that a document can have only one representation since they consider that finding the optimal topic model is the best so- lution. Another solution would be to consider a set of topic models to represent a document. Nonethe- less, a multi-topic-based representation of a dia- logue can involve a noisy variability due to the mapping of a dialogue in each topic space. Indeed, a dialogue does not share its content (i.e. words) with each class composing the topic space. Thus, a variability is added during the mapping pro- cess. Another weakness of the multi-view repre- sentation is the relation between classes in a topic space. ( <ref type="bibr" target="#b4">Blei and Lafferty, 2006)</ref> show that classes into a LDA topic space are correlated. More- over, ( <ref type="bibr" target="#b19">Li and McCallum, 2006</ref>) consider a class as a node of an acyclic graph and as a distribu- tion over other classes contained in the same topic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-view representation of automatic dialogue transcriptions in a homogeneous space</head><p>The purpose of the considered application is the identification of the major theme of a human- human telephone conversation in the customer care service (CCS) of the RATP Paris transporta- tion system. The approach considered in this pa- per focuses on modeling the variability between different dialogues expressing the same theme t. For this purpose, it is important to select relevant features that represent semantic contents for the theme of a dialogue. An attractive set of features for capturing possible semantically relevant word dependencies is obtained with Latent Dirichlet Al- location (LDA) ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>, as described in section 2.</p><p>Given a training set of conversations D, a hid- den topic space is derived and a conversation d is represented by its probability in each topic of the hidden space. Estimation of these probabili- ties is affected by a variability inherent to the es- timation of the model parameters. If many hidden spaces are considered and features are computed for each hidden space, it is possible to model the estimation variability together with the variability of the linguistic expression of a theme by different speakers in different real-life situations. Even if the purpose of the application is theme identifica- tion and a training corpus annotated with themes is available, supervised LDA ( <ref type="bibr" target="#b13">Griffiths and Steyvers, 2004</ref>) is not suitable for the proposed approach. LDA is used only for producing different feature sets used involved in statistical variability models.</p><p>In order to estimate the parameters of differ- ent hidden spaces, a set of discriminative words V is constructed as described in <ref type="bibr" target="#b25">(Morchid et al., 2014a</ref>). Each theme t contains a set of specific words. Note that the same word may appear in several discriminative word sets. All the selected words are then merged without repetition to form V .</p><p>Several techniques, such as Variational Meth- ods ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>, Expectation-propagation ( <ref type="bibr" target="#b23">Minka and Lafferty, 2002</ref>) or Gibbs Sam- pling ( <ref type="bibr" target="#b13">Griffiths and Steyvers, 2004</ref>), have been proposed for estimating the parameters describ- ing a LDA hidden space. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) <ref type="bibr" target="#b12">(Geman and Geman, 1984)</ref> and gives a simple algorithm for approximate inference in high-dimensional models such as LDA <ref type="bibr" target="#b14">(Heinrich, 2005)</ref>. This overcomes the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as:</p><formula xml:id="formula_0">p(W | ‚àí ‚Üí Œ± , ‚àí ‚Üí Œ≤ ) = w‚ààW p( ‚àí ‚Üí w | ‚àí ‚Üí Œ± , ‚àí ‚Üí Œ≤ )</formula><p>for the whole data collection W knowing the Dirichlet parame- ters ‚àí ‚Üí Œ± and ‚àí ‚Üí Œ≤ . Gibbs Sampling allows us both to estimate the LDA parameters in order to represent a new dia- logue d with the r th topic space of size n, and to obtain a feature vector V z r d of the topic representa- tion of d. The j th feature V z r j d = P (z r j |d) (where 1 ‚â§ j ‚â§ n) is the probability of topic z r j to be generated by the unseen dialogue d in the r th topic space of size n (see <ref type="figure">Figure 2</ref>) and V w z r j = P (w|z r j ) is the vector representation of a word into r. ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent: Hello</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">n</head><p>Figure 2: Example of a dialogue d mapped into a topic space of size n.</p><p>In the LDA technique, topic z j , j is drawn from a multinomial over Œ∏ which is drawn from a Dirichlet distribution over ‚àí ‚Üí Œ± . Thus, a set of p topic spaces are learned using LDA by varying the number of topics n to obtain p topic spaces of size n. The number of topics n varies from 10 to 3, 010. Thus, a set of 3, 000 topic spaces is esti- mated. This is high enough to generate, for each dialogue, many feature sets for estimating the pa- rameters of a variability model. The next process allows us to obtain a homo- geneous representation of transcription d for the r th topic space r. </p><note type="other">of size |V | for the r th topic space r of size n</note><p>where the i th (0 ‚â§ i ‚â§ |V |) feature is:</p><formula xml:id="formula_1">V w i d,r = P (w i |d) = n j=1 P (w i |z r j )P (z r j |d) = n j=1 V w i z r j √ó V z r j d = ‚àí ‚àí ‚Üí V w i z r , ‚àí ‚àí ‚Üí V z r d</formula><p>where ¬∑, ¬∑¬∑ is the inner product, Œ¥ being the fre- quency of the term w</p><note type="other">i in d, V w i z r j = P (w i |z j ) and V z r j d = P (z j |d) evaluated using Gibbs Sampling in the topic space r.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Compact multi-view representation</head><p>In this section, an i-vector-based method to represent automatic transcriptions is presented. Initially introduced for speaker recognition, i- vectors ( <ref type="bibr" target="#b18">Kenny et al., 2008</ref>) have become very popular in the field of speech processing and re- cent publications show that they are also reli- able for language recognition <ref type="bibr" target="#b21">(Martƒ±nez et al., 2011</ref>) and speaker diarization <ref type="bibr" target="#b10">(Franco-Pedroso et al., 2010)</ref>. I-vectors are an elegant way of re- ducing the imput space dimensionality while re- taining most of the relevant information. The technique was originally inspired by the Joint Factor Analysis framework ( <ref type="bibr" target="#b17">Kenny et al., 2007)</ref>. Hence, i-vectors convey the speaker characteris- tics among other information such as transmission channel, acoustic environment or phonetic content of speech segments. The next sections describe the i-vector extraction process, the application of this compact representation to textual documents (called c-vector), and the vector transformation with the EFR method and the Mahalanobis met- ric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Total variability space definition</head><p>I-vector extraction could be seen as a probabilistic compression process that reduces the dimension- ality of speech super-vectors according to a linear- Gaussian model. The speech (of a given speech recording) super-vector m s of concatenated GMM means is projected in a low dimensionality space, named Total Variability space, with:</p><formula xml:id="formula_2">m (h,s) = m + Tx (h,s) ,<label>(1)</label></formula><p>where m is the mean super-vector of the UBM 1 .</p><p>T is a low rank matrix (M D √ó R), where M is the number of Gaussians in the UBM and D is the cepstral feature size, which represents a basis of the reduced total variability space. T is named To- tal Variability matrix; the components of x (h,s) are the total factors which represent the coordinates of the speech recording in the reduced total variabil- ity space called i-vector (i for identification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">From i-vector speaker identification to c-vector textual document classification</head><p>The proposed approach uses i-vectors to model transcription representation through each topic space in a homogeneous vocabulary space. These short segments are considered as basic semantic- based representation units. Indeed, vector V w d rep- resents a segment or a session of a transcription d. In the following, (d, r) will indicate the dialogue representation d in the topic space r. In our model, the segment super-vector m (d,r) of a transcription d knowing a topic space r is modeled:</p><formula xml:id="formula_3">m (d,r) = m + Tx (d,r)<label>(2)</label></formula><p>where x (d,r) contains the coordinates of the topic- based representation of the dialogue in the re- duced total variability space called c-vector (c for classification). Let N (d,r) and X (d,r) be two vectors containing the zero order and first order dialogue statistics re- spectively. The statistics are estimated against the UBM:</p><formula xml:id="formula_4">N r [g] = t‚ààr Œ≥ g (t); {X (d,r) } [g] = t‚àà(d,r) Œ≥ g (t) ¬∑ t (3)</formula><p>where Œ≥ g (t) is the a posteriori probability of Gaus- sian g for the observation t. In the equation, t‚àà(d,r) represents the sum over all the frames be- longing to the dialogue d.</p><p>Let X (d,r) be the state dependent statistics de- fined as follows:</p><formula xml:id="formula_5">{X (d,r) } [g] ={X (d,r) } [g] ‚àí m [g] ¬∑ (d,r) N (d,r) [g]<label>(4)</label></formula><p>Let L (d,r) be a R √ó R matrix, and B (d,r) a vector Algorithm 1: Estimation algorithm of T and latent variable x.</p><p>For each dialogue d mapped into the topic space r: x (d,r) ‚Üê 0, T ‚Üê random ; Estimate statistics: N (d,r) , X (d,r) (eq.3); for i = 1 to nb iterations do for all d and r do Center statistics: X (d,r) (eq.4); Estimate L (d,r) and B (d,r) (eq.5); Estimate x (d,r) (eq.6); end Estimate matrix T (eq <ref type="figure">. 7 and 8)</ref> ; end of dimension R, both defined as:</p><formula xml:id="formula_6">L (d,r) = I + g‚ààUBM N (d,r) [g] ¬∑ {T} t [g] ¬∑ Œ£ ‚àí1 [g] ¬∑ {T} [g] B (d,r) = g‚ààUBM {T} t [g] ¬∑ Œ£ ‚àí1 g ¬∑ {X (d,r) } [g] ,<label>(5)</label></formula><p>By using L (d,r) and B (d,r) , x (d,r) can be obtained using the following equation:</p><formula xml:id="formula_7">x (d,r) = L ‚àí1 (d,r) ¬∑ B (d,r)<label>(6)</label></formula><p>The matrix T can be estimated line by line, with {T} i</p><p>[g] being the i th line of {T} <ref type="bibr">[g]</ref> then:</p><formula xml:id="formula_8">T i [g] = LU ‚àí1 g ¬∑ RU i g ,<label>(7)</label></formula><p>where RU i g and LU g are given by:</p><formula xml:id="formula_9">LU g = (d,r) L ‚àí1 (d,r) + x (d,r) x t (d,r) ¬∑ N (d,r) [g] RU i g = (d,r) {X (d,r) } [i] [g] ¬∑ x (d,r)<label>(8)</label></formula><p>Algorithm 1 presents the method adopted to es- timate the multi-view variability dialogue matrix with the above developments where the standard likelihood function can be used to assess the con- vergence. One can refer to (  to find out more about the implementation of the factor analysis.</p><p>C-vector representation suffers from 3 raised c- vector issues: (i) the c-vectors x of equation 2 have to be theoretically distributed among the nor- mal distribution N (0, I), (ii) the "radial" effect should be removed, and (iii) the full rank total factor space should be used to apply discriminant transformations. The next section presents a solu- tion to these 3 problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">C-vector standardization</head><p>A solution to standardize c-vectors has been <ref type="bibr">developed in (Bousquet et al., 2011</ref>). The authors proposed to apply transformations for training and test transcription representations. The first step is to evaluate the empirical mean x and covariance matrix V of the training c-vector. Covariance ma- trix V is decomposed by diagonalization into:</p><formula xml:id="formula_10">PDP T (9)</formula><p>where P is the eigenvector matrix of V and D is the diagonal version of V. A training i-vector x (d,r) is transformed in x (d,r) as follows:</p><formula xml:id="formula_11">x (d,r) = D ‚àí 1 2 P T (x (d,r) ‚àí x) (x (d,r) ‚àí x) T V ‚àí1 (x (d,r) ‚àí x)<label>(10)</label></formula><p>The numerator is equivalent by rotation to</p><formula xml:id="formula_12">V ‚àí 1 2 (x (d,r) ‚àí x)</formula><p>and the Euclidean norm of x <ref type="bibr">(d,r)</ref> is equal to 1. The same transformation is applied to the test c-vectors, using the training set parame- ters x and mean covariance Vas estimations of the test set of parameters. <ref type="figure" target="#fig_1">Figure 3</ref> shows the transformation steps: <ref type="figure" target="#fig_1">Fig- ure 3-(a)</ref> is the original training set; <ref type="figure" target="#fig_1">Figure 3</ref>- (b) shows the rotation applied to the initial train- ing set around the principal axes of the total vari- ability when P T is applied; <ref type="figure" target="#fig_1">Figure 3-(c)</ref> shows the standardization of c-vectors when D ‚àí 1 2 is applied; and finally, <ref type="figure" target="#fig_1">Figure 3-(d)</ref> shows the c- vector x <ref type="bibr">(d,r)</ref> on the surface area of the unit hyper- sphere after a length normalization by a division</p><formula xml:id="formula_13">of (x (d,r) ‚àí x) T V ‚àí1 (x (d,r) ‚àí x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Protocol</head><p>The proposed c-vector representation of automatic transcriptions is evaluated in the context of the theme identification of a human-human telephone conversation in the customer care service (CCS) of the RATP Paris transportation system. The met- ric used to identify of the best theme is the Maha- lanobis metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Theme identification task</head><p>The DECODA project corpus <ref type="bibr" target="#b1">(Bechet et al., 2012</ref>) was designed to perform experiments on the iden- tification of conversation themes. It is composed of 1,514 telephone conversations, corresponding to about 74 hours of signal, split into a training set (740 dialogues), a development set (175 dia- logues) and a test set (327 dialogues), and manu- ally annotated with 8 conversation themes: prob- lems of itinerary, lost and found, time schedules, transportation cards, state of the traffic, fares, in- fractions and special offers. An LDA model allowed us to elaborate 3,000 topics spaces by varying the number of topics from 10 to 3,010. A topic space having less than 10 topics is not suitable for a corpus of more than 700 dialogues (training set). For each theme {C i } 8 i=1 , a set of 50 specific words is identified. All the selected words are then merged without repetition to compose V , which is made of 166 words. The topic spaces are made with the LDA Mallet Java implementation 2 .</p><p>The LIA-Speeral ASR system ( <ref type="bibr" target="#b20">Linar√®s et al., 2007</ref>) is used for the experiments. Acoustic model parameters were estimated from 150 hours of speech in telephone conditions. The vocabulary contains 5,782 words. A 3-gram language model (LM) was obtained by adapting a basic LM with the training set transcriptions. A "stop list" of 126 words 3 was used to remove unnecessary words (mainly function words), which results in a Word Error Rate (WER) of 33.8% on the training, 45.2% on the development, and 49.5% on the test. These 2 http://mallet.cs.umass.edu/ 3 http://code.google.com/p/stop-words/ high WER are mainly due to speech disfluencies and to adverse acoustic environments (for exam- ple, calls from noisy streets with mobile phones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mahalanobis metric</head><p>Given a new observation x, the goal of the task is to identify the theme belonging to x. Probabilistic approaches ignore the process by which c-vectors were extracted and they pretend instead they were generated by a prescribed generative model. Once a c-vector is obtained from a dialogue, its repre- sentation mechanism is ignored and it is regarded as an observation from a probabilistic generative model. The Mahalanobis scoring metric assigns a dialogue d with the most likely theme C. Given a training dataset of dialogues, let W denote the within dialogue covariance matrix defined by:</p><formula xml:id="formula_14">W = K k=1 n t n W k = 1 n K k=1 nt i=0 x k i ‚àí x k x k i ‚àí x k t (11)</formula><p>where W k is the covariance matrix of the k th theme C k , n t is the number of utterances for the theme C k , n is the total number of dialogues, and x k is the centroid (mean) of all dialogues x k i of C k .</p><p>Each dialogue does not contribute to the co- variance in an equivalent way. For this reason, the term nt n is introduced in equation 11. If ho- moscedasticity (equality of the class covariances) and Gaussian conditional density models are as- sumed, a new observation x from the test dataset can be assigned to the most likely theme C k Bayes us- ing the classifier based on the Bayes decision rule:</p><formula xml:id="formula_15">C k Bayes = arg max k {N (x | x k , W)} = arg max k ‚àí 1 2 (x ‚àí x k ) t W ‚àí1 (x ‚àí x k ) + a k</formula><p>where W is the within theme covariance ma- trix defined in eq. 11; N denotes the normal dis- tribution and a k = log (P (C k )). It is noted that, with these assumptions, the Bayesian approach is similar to Fisher's geometric approach: x is as- signed to the class of the nearest centroid, accord- ing to the Mahalanobis metric ( <ref type="bibr" target="#b31">Xing et al., 2002</ref>) of W ‚àí1 :</p><formula xml:id="formula_16">C k Bayes = arg max k ‚àí 1 2 ||x ‚àí x k || 2 W ‚àí1 + a k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and results</head><p>The proposed c-vector approach is applied to the same classification task and corpus proposed in <ref type="bibr" target="#b25">(Morchid et al., 2014a;</ref><ref type="bibr" target="#b26">Morchid et al., 2014b;</ref><ref type="bibr" target="#b24">Morchid et al., 2013</ref>) (state-of-the-art in text clas- sification in <ref type="bibr" target="#b25">(Morchid et al., 2014a)</ref>). Experiments are conducted using the multiple topic spaces esti- mated with an LDA approach. From these mul- tiple topic spaces, a classical way is to find the one that reaches the best performance. <ref type="figure" target="#fig_2">Figure 4</ref> presents the theme classification performance ob- tained on the development and test sets using vari- ous topic-based representation configurations with the EFR normalization algorithm (baseline).</p><p>For sake of comparison, experiments are con- ducted using the automatic transcriptions only (ASR) only. The conditions indicated by the ab- breviations between parentheses are considered for the development (Dev) and the test (Test) sets.</p><p>Only homogenous conditions (ASR for both training and validations sets) are considered in this study. Authors in <ref type="bibr" target="#b25">(Morchid et al., 2014a</ref>) notice that results collapse dramatically when heteroge- nous conditions are employed (TRS or TRS+ASR for training set and ASR for validation set).</p><p>First of all, we can see that this baseline ap- proach reached a classification accuracy of 83% and 76%, respectively on the development and the test sets. However, we note that the classifica- tion performance is rather unstable, and may com- pletely change from a topic space configuration to another. The gap between the lower and the higher classification results is also important, with a dif- ference of 25 points on the development set (the same trend is observed on the test set). As a result, finding the best topic space size seems crucial for this classification task, particularly in the context of highly imperfect automatic dialogue transcrip- tions containing more than one theme.</p><p>The topic space that yields the best accuracy with the baseline method (n = 15 topics) is pre- sented in <ref type="figure">Figure 5</ref>. This figure presents each of the 15 topics and their 10 most representative words (highest P (w|z)). Several topics contain more or less the same representative words, such as topics 3, 6 and 9. This figure points out some interesting topics that allow us to distinguish a theme from the others. For example:</p><p>‚Ä¢ topics 2, 10 and 15 represent some words re- lated to itinerary problems,</p><p>‚Ä¢ the transportation cards theme is mostly rep- resented in topic 4 and 15 (Imagine and Nav- igo are names of transportation cards),</p><p>‚Ä¢ the words which represent the time schedules theme are contained in topic 5,6,7 and less in topic 9,</p><p>‚Ä¢ state of the traffic could be discussed with words such as: departure, line, service, day. These words and others are contained in topic 13,</p><p>‚Ä¢ topics 4 and 12 are related to the infractions theme with to words fine, pass, zone or ticket,</p><p>‚Ä¢ but topic 12 could be related to theme fares or special offers as well . <ref type="table" target="#tab_2">Table 1</ref> presents results obtained with the pro- posed c-vector approach coupled with the EFR al- gorithm. We can firstly note that this compact rep- resentation allows it to outperform the best topic space configuration (baseline), with a gain of 9.4 points on the development data and of 9 points on the test data. Moreover, if we consider the differ-   and test sets, the gap between accuracies is much smaller: classification accuracy does not go be- low 82.6%, while it reached 56% for the worst topic-based configuration. Indeed, as shown in Ta- ble 2, the difference between the maximum and the minimum theme classification accuracies is of 20% using the baseline approach while it is only of 2.4% using the c-vector method.</p><p>We can conclude that this original c-vector ap- proach allows one to better handle the variabilities  Figure 5: Topic space (15 topics) that obtains the best accuracy with the baseline system (see <ref type="figure" target="#fig_2">Fig. 4</ref>).</p><p>contained in dialogue conversations: in a classi- fication context, better accuracy can be obtained and the results can be more consistent when vary- ing the c-vector size and the number of Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper presents an original multi-view repre- sentation of automatic speech dialogue transcrip- tions, and a fusion process with the use of a factor analysis method called i-vector. The first step of the proposed method is to represent a dialogue in multiple topic spaces of different sizes (i.e. num- ber of topics). Then, a compact representation of the dialogue from the multiple views is pro- cessed to compensate the vocabulary and the vari- ability of the topic-based representations. The ef- fectiveness of the proposed approach is evaluated in a classification task of theme dialogue identifi- cation. Thus, the architecture of the system iden- tifies conversation themes using the i-vector ap- proach. This compact representation was initially developed for speaker recognition and we showed that it can be successfully applied to a text clas- sification task. Indeed, this solution allowed the system to obtain better classification accuracy than with the use of the classical best topic space con- figuration. In fact, we highlighted that this original compact version of all topic-based representations of dialogues, called c-vector in this work, coupled with the EFR normalization algorithm, is a better solution to deal with dialogue variabilities (high word error rates, bad acoustic conditions, unusual word vocabulary, etc). This promising compact representation allows us to effectively solve both the difficult choice of the right number of topics and the multi-theme representation issue of partic- ular textual documents. Finally, the classification accuracy reached 85% with a gain of 9 points com- pared to usual baseline (best topic space configu- ration). In a future work, we plan to evaluate this new representation of textual documents in other information retrieval tasks, such as keyword ex- traction or automatic summarization systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a dialogue from the DECODA corpus labeled by the agent as an infraction issue which contains more than one theme (infraction + transportation cards).</figDesc><graphic url="image-1.png" coords="2,199.24,96.48,82.01,78.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of the standardization with the EFR algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Theme classification accuracies using various topic-based representations with EFR normalization (baseline) on the development and test sets (X-coordinates start at 10 indeed, but to show the best configuration point (15), the origine (10) has been removed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Theme classification accuracy (%) with different c-vectors and GMM-UBM sizes. 

DEV 
TEST 
c-vector 
Number of Gaussians in GMM-UBM 
size 
32 
64 
128 256 
32 
64 
128 256 
60 
88.8 86.5 91.2 90.6 85.0 82.6 83.5 84.7 
100 
91.2 92.4 92.4 87.7 86.0 85.0 83.5 84.7 
120 
89.5 92.2 89.5 87.7 85.0 83.5 85.4 84.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Maximum (M ax), minimum (M in) and Difference (M ax ‚àí M in) theme classification accu-
racies (%) using the baseline and the proposed c-vector approaches. 

Max 
Min 
Difference 
Method DEV TEST 
DEV TEST 
DEV TEST 
baseline 
83.3 
76.0 
58.6 
56.8 
14.7 
20.8 
c-vector 
92.4 
85.0 
86.5 
82.6 
5.9 
2.4 

</table></figure>

			<note place="foot" n="1"> The UBM is a GMM that represents all the possible observations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their help-ful comments. This work was funded by the SUMACC and ContNomina projects supported by the French National Research Agency (ANR) un-der contracts ANR-10-CORD-007 and ANR-12-BS02-0009.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On finding the natural number of topics with latent dirichlet allocation: Some observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatasubramaniyan</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Veni Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Musti Narasimha</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="391" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Decoda: a callcentre human-human spoken conversation corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Bechet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Maza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bigouroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Bazillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>El-Beze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arbillot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A latent semantic analysis framework for large-span language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting latent semantic information in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1279" to="1296" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Correlated topic models. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">147</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Intersession compensation and scoring methods in the i-vectors space for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Michel</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Driss</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanfran√ßois</forename><surname>Bonastre</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="485" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A density-based method for adaptive lda model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1775" to="1781" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis. Journal of the American society for information science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©da</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Atvs-uam system description for the audio segmentation and speaker diarization albayzin 2010 evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Franco-Pedroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doroteo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FALA VI Jornadas en Tecnologa del Habla and II Iberian SLTech Workshop</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="415" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis of i-vector length normalization in speaker recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">Y</forename><surname>Espy-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National academy of Sciences of the United States of America</title>
		<meeting>the National academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Parameter estimation for text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Heinrich</surname></persName>
		</author>
		<ptr target="http://www.arbylon.net/publications/text-est.pdf" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Uncertainty in Artificial Intelligence, UAI &apos; 99</title>
		<meeting>of Uncertainty in Artificial Intelligence, UAI &apos; 99</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="177" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint factor analysis versus eigenchannels in speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1435" to="1447" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study of interspeaker variability in speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwa</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="980" to="988" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pachinko allocation: Dag-structured mixture models of topic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The lia speech recognition system: from 10xrt to 1xrt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linar√®s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Noc√©ra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Massonie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Driss</forename><surname>Matrouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language recognition in ivectors space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martƒ±nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oldrich</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk√°s</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="861" to="864" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Ondrej Glembek, and Pavel Matejka</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A straightforward and efficient implementation of the factor analysis model for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Driss</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Fauve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bonastre</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1242" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Expectationpropagation for the generative aspect model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eighteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="352" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Theme identification in telephone service conversations using quaternions of speech features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linar√®s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>El-Beze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<editor>Interspeech. ISCA</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving dialogue classification using a topic space representation and a gaussian classifier based on the decision rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Michel</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouallegue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linar√®s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A LDA-Based Topic Classification Approach from Highly Imperfect Automatic Transcriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linar√®s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identification using gaussian mixture speaker models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic text processing: the transformation. Analysis and Retrieval of Information by Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Keyword extraction using termdomain interdependence for dictation of radio news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Sekiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th international conference on Computational linguistics</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1272" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sharing clusters among related groups: Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Determining automatically the size of learned ontologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Zavitsanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Vouros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="775" to="776" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
