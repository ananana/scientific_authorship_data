<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simpler and More Generalizable Story Detector using Verb and Character Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Eisenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">ECS Building</orgName>
								<address>
									<addrLine>11200 S.W. 8th Street</addrLine>
									<postCode>33141</postCode>
									<settlement>Miami</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Finlayson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Florida International University Miami</orgName>
								<address>
									<settlement>Florida</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simpler and More Generalizable Story Detector using Verb and Character Features</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2708" to="2715"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Story detection is the task of determining whether or not a unit of text contains a story. Prior approaches achieved a maximum performance of 0.66 F 1 , and did not generalize well across different corpora. We present a new state-of-the-art detector that achieves a maximum performance of 0.75 F 1 (a 14% improvement), with significantly greater general-izability than previous work. In particular , our detector achieves performance above 0.70 F 1 across a variety of combinations of lexically different corpora for training and testing, as well as dramatic improvements (up to 4,000%) in performance when trained on a small, disfluent data set. The new detector uses two basic types of features-ones related to events, and ones related to characters-totaling 283 specific features overall; previous detectors used tens of thousands of features, and so this detector represents a significant simplification along with increased performance .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>Understanding stories is a long-held goal of both artificial intelligence and natural language pro- cessing. Stories can be used for many interest- ing natural language processing tasks, and much can be learned from them, including concrete facts about specific events, people, and things; com- monsense knowledge about the world; and cul- tural knowledge about the societies in which we live. Applying NLP directly to the large and growing number of stories available electronically, however, has been limited by our inability to effi- ciently separate story from non-story text. For the most part, studies of stories per se has relied on manual curation of story data sets <ref type="bibr" target="#b19">(Mostafazadeh et al., 2016)</ref>, which is, naturally, time-consuming, expensive, and doesn't scale. These human-driven methods pay no attention to the large number of stories generated daily in news, entertainment, and social media.</p><p>The goal of this work is to build and evaluate a high performing story detector that is both sim- ple in design and generalizable across lexically different story corpora. Our definition of story can be found in §1.2, and is based on definitions used in prior work on story detection. Previous approaches to story detection have relied on tens of thousands of features ( <ref type="bibr" target="#b4">Ceran et al., 2012;</ref><ref type="bibr" target="#b15">Gordon and Swanson, 2009)</ref>, and have used compli- cated pre-processing pipelines <ref type="bibr" target="#b4">(Ceran et al., 2012)</ref>. Moreover these prior systems, while clearly im- portant advances, did not, arguably, include fea- tures that captured the "essence" of stories. Fur- thermore, these prior efforts had poor generaliz- ability, i.e. when trained on one corpus, the detec- tors perform poorly when tested on a different cor- pus. Building on this prior work, we begin to ad- dress these shortcomings, presenting a new detec- tor that has many orders of magnitude fewer fea- tures than used previously, significantly improved cross corpus performance, and higher F 1 on all training and testing combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Task</head><p>Our goal is to design a system that can automat- ically decide whether or not a paragraph of text contains a story. We say a paragraph contains a story if any portion of it expresses a significant part of a story, including the characters and events involved in major plot points. Corpora used in prior work included Islamic Extremist texts <ref type="bibr" target="#b4">(Ceran et al., 2012)</ref>, and personal weblog posts <ref type="bibr" target="#b15">(Gordon and Swanson, 2009)</ref>, which were both annotated at this level of granularity. In this paper we test com- binations of new features on both of these corpora. Once we determined the best-performing feature set, we ran experiments using those features to evaluate its generalizability across corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">What is a Story?</head><p>Author E.M. Forster said "A story is a narrative of events arranged in their time sequence" <ref type="bibr" target="#b14">(Forster, 2010)</ref>. A more precise definition, of our own coinage, is that a narrative is a discourse pre- senting a coherent sequence of events which are causally related and purposely related, concern specific characters and times, and overall displays a level of organization beyond the commonsense coherence of the events themselves. In sum, a story is a series of events effected by animate ac- tors. This reflects a general consensus among nar- ratologists that there are at least two key elements to stories, namely, the plot (fabula) and the char- acters (dramatis personae) who move the plot for- ward <ref type="bibr" target="#b0">(Abbott, 2008)</ref>. While a story is more than just a plot carried out by characters-indeed, criti- cal to 'storyness' is the connective tissue between these elements that can transport an audience to a different time and place-here we focus on these two core elements to effect better story detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Outline of the Paper</head><p>We begin by discussing prior work on story de- tection ( §2). Then we introduce our new detector ( §3), which relies on simple verb ( §3.1) and char- acter ( §3.2) features. We test our detector on two corpora ( §3.3)-one of blog posts and one of Is- lamist Extremist texts-using an SVM model to classify each paragraph as to whether or not it con- tains a story ( §3.4). We conduct an array of exper- iments evaluating different combinations and vari- ants of our features ( §4). We also detail our use of undersampling for the majority class ( §4.1), as well as our cross validation procedure ( §4.2). We present both the results of the single corpus exper- iments ( §4.3) and the cross-corpus and generaliz- ability experiments ( §4.4). We conclude with a list of contributions and discussion of future directions ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been three major contributions to the study of automatic story detection. In 2009, Gor- dan and Swanson developed a bag-of-words-based detector using blog data <ref type="bibr" target="#b15">(Gordon and Swanson, 2009)</ref>. They annotated a subset of paragraph-sized posts in the Spinn3r Blog corpus for the presence of stories, and used this data to train a confidence weighted linear classifier using all unigrams, bi- grams, and trigrams from the data. Their best F 1 was 0.55. This was an important first step in story detection, and the annotated corpus of blog stories is an invaluable resource.</p><p>In 2012, Corman et al. developed a semantic- triplet-based detector using Islamist Extremist texts ( <ref type="bibr" target="#b4">Ceran et al., 2012</ref>). They annotated para- graphs of the CSC Islamic Extremist corpus for the presence of stories, and used this data to train an SVM with a variety of features includ- ing the top 20,000 tf-idf tokens, use of stative verbs, and agent-verb-patient triplets ("seman- tic triplets," discussed in more detail below in §3.1). Their best performing detector in that study achieved 0.63 F 1 . The intent of the semantic triplet features was to encode the plot and the char- acters. These features were intended to capture the action of stories, but the specifics of the implemen- tation was problematic: each unique agent-verb- patient triplet has its own element in the feature vector, and so this detector was sensitive primarily to the words that appeared in stories, not general- ized actions or events.</p><p>Although Corman's detector has a higher F 1 than Gordon's, it was not clear which one was ac- tually better; they were tested on different corpora. We compared the two detectors by reimplement- ing both, confirmed the correctness of the reimple- mentations, and running experiments where each detector was trained and tested on the corpora ( <ref type="bibr" target="#b7">Eisenberg et al., 2016)</ref>. After these experiments, we showed that Corman's detector had better per- formance on the majority of experiments. Some of the results of these experiments are shown in Ta- ble 2. We also slightly improved the performance of Corman's detector to 0.66 F 1 . In addition we reported results investigating the generalizability of the detectors; these results showed that neither the Gordon nor the Corman detectors generalized across corpora. We ascribed this problem to the fact that the features of each detector were closely tied to the literal words used, and did not attempt to generalize beyond those specific lexical items.</p><p>In terms of domain independence, we surveyed other discourse related tasks to see how general- ization across domains has been achieved. For example, Braud et al. achieved domain indepen- dence in the identification of implicit relations be- tween discourse units by training their system on both natural and synthetic data, weighting the in- fluence of the two types <ref type="bibr" target="#b2">(Braud and Denis, 2014</ref>). Jansen et al., as another example, demonstrated domain independence on the task of non-factoid question answering by using both shallow and deep discourse structure, along with lexical fea- tures, to train their classifiers <ref type="bibr" target="#b16">(Jansen et al., 2014</ref>). Thus, domain independence is certainly possible for discourse related tasks, but there does not yet seem to be a one-size-fits-all solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Developing the Detector</head><p>In contrast to focusing on specific lexical items, our implementation focuses on features which we believe capture more precisely the essence of sto- ries, namely, features focusing on (a) events in- volving characters, and (b) the characters them- selves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Verb Features</head><p>Verbs are often used to express events. We use this fact to approximate event detection in a computa- tionally efficient but still relatively accurate man- ner. The first part of each feature vector for a para- graph comprises 278 dimensions, where each ele- ment of this portion of the vector represents one of the 278 verb classes in VerbNet ( <ref type="bibr" target="#b20">Schuler, 2005</ref>). The value of each element depends on whether a verb from the associated verb class is used in the paragraph. Each element of the vector can have three values: the first value represents when a verb from the element's corresponding verb class is used in the paragraph and also involves a char- acter as an argument of the verb. The second value represents when a verb from the verb class is used, but there are no characters involved. The third value represents the situation where no verbs from the verb class are used in the paragraph.</p><p>For clarity, we list the general steps of the verb feature extraction pipeline:</p><p>1. Split each paragraph into tokens, assign part of speech tags, and split the text into sen- tences, all using Stanford CoreNLP (Man- ning et al., 2014). 2. Parse each sentence with OpenNLP (Apache Foundation, 2017). 3. Label each predicate with its semantic roles using the SRL from the Story Workbench <ref type="bibr" target="#b10">(Finlayson, 2008</ref><ref type="bibr" target="#b11">(Finlayson, , 2011</ref>). 4. Disambiguate the Wordnet sense <ref type="bibr" target="#b8">(Fellbaum, 1998)</ref> for each open-class word using the It Makes Sense WSD system <ref type="bibr" target="#b21">(Zhong and Ng, 2010)</ref>, using the Java WordNet Inter- face (JWI) to load and interact with WordNet <ref type="bibr" target="#b13">(Finlayson, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Assign one of 278 VerbNet verb classes to</head><p>each predicate, based on the assigned Word- net sense, and using the jVerbnet library to interact with VerbNet. (Finlayson, 2012). 6. Determine whether the arguments of each predicate contains characters by using the Stanford Named Entity Recognizer ( <ref type="bibr" target="#b9">Finkel et al., 2005</ref>) and a gendered pronoun list.</p><p>We considered an argument to involve a char- acter if it contained either (1) a gendered pro- noun or (2) a named entity of type Person or Or- ganization. We treated organizations as charac- ters because they often fulfill that role in stories: for example, in the Extremist stories, organiza- tions or groups like the Islamic Emirate, Hamas, or the Jews are agents or patients of important plot events. The verb features were encoded as a vector with length 278, each entry representing a differ- ent VerbNet verb class with three possible values: the verb class does not appear in the paragraph; the verb class appears but does not involve characters; or the verb class appears and a character is either an agent, patient, or both.</p><p>The verb features represent the types of events that occur in a paragraph, and whether or not char- acters are involved in those events. This is a gener- alized version of the semantic triplets that Corman et al. used for their story detector ( <ref type="bibr" target="#b4">Ceran et al., 2012</ref>), where they paired verbs with the specific tokens in the agent and patient arguments. The dis- advantage of Corman's approach was that it led to phrases with similar meaning being mapped to dif- ferent features: for example, the sentences "Bob played a solo" and "Mike improvised a melody" are mapped to different features by the semantic triplet based detector, even though the meaning of the sentences are almost the same: a character is performing music. On the other hand, in our ap- proach, when we extract verb feature vectors from these sentences, both result in the same feature value, because the verbs played and improvised belong to the performance VerbNet class, and both verbs have a character in one of their arguments. This allow a generalized encoding of the types of action that occurs in a text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Character Features</head><p>Our second focus is on character coreference chains. Characters, as discussed previously, are a key element of stories. A character must be present to drive the action of the story forward. We hypothesize that stories will contain longer coref- erence chains than non-stories. To encode this as a feature, we calculated the normalized length of the five longest coreference chains, and used those numbers as the character features. We computed these values as follows:</p><p>1. Extract coreference chains from each para- graph using Stanford CoreNLP coreference facility (Clark and Manning, 2016). 2. Filter out coreference chains that do not con- tain a character reference as defined in the Verb section above (a named entity of type Person or Organization, or a gendered pro- noun). 3. Sort the chains within each paragraph with respect to the number of references in the chain. 4. Normalize the chain lengths by dividing the number of referring expression in each chain by the number of sentences in the paragraph.</p><p>These normalized chain lengths were used to construct a five-element feature vector for use by the SVM. We experimented with different num- bers of longest chains, anywhere from the single longest to the ten longest chains. Testing on a development set of 200 Extremist paragraphs re- vealed using the five longest chains produced the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Corpora</head><p>As noted, we used two corpora that were annotated by other researchers for the presence of stories at the paragraph level. The CSC Islamic Extremist Corpus comprises 24,009 paragraphs (Ceran et al., 2012), of which 3,300 were labeled as containing a story. These texts recount Afghani and Jihadi activities in the mid-2000's in a variety of loca- tion around the world. This corpus was originally used to train and test Corman's semantic-triplet- based story detector. The web blog texts come from the ICWSM 2009 Spinn3r Dataset ( <ref type="bibr" target="#b3">Burton et al., 2009</ref>). The full data set contains 44 mil- lion texts in many languages. Gordon and Swan- son (2009) annotated a sample of 4,143 English texts from the full data set, 201 of which were identified as containing stories. This corpus was originally used to train and test Gordon's bag-of- words-based detector. Most of the texts in the blog corpus are no more than 250 characters, roughly a paragraph. The distribution of texts can be seen in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Story Non-Story Extremist 3,300 20,709 Blog 201 3,942 <ref type="table">Table 1</ref>: Distribution of story paragraphs across the Extremist and blog corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SVM Machine Learning</head><p>We used the Java implementation of LibSVM (Chang and Lin, 2011) to train an SVM classifier with our features. The hyper-parameters for the linear kernel were γ = 0.5, ν = 0.5, and c = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments &amp; Results</head><p>The results of our new experiments are shown in   , and the combination of these two feature sets (Verb+Char). Undersampling is utilized in each of these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Undersampling</head><p>In each of the new experiments, we undersam- pled the non-story class before training <ref type="bibr" target="#b17">(Japkowicz, 2000</ref>). Undersampling is a technique used to help supervised machine learning classifiers learn more about a class that has a significantly smaller number of examples relative to an alternative. In our case, non-story labels outnumbered story la- bels by a factor of 7 overall. Extremist story para- graphs are only 15.9% of the total annotated para- graphs in that set, and in the blog corpus stories were only 4.9% of the paragraphs. To prevent the detector from being over trained on non-story paragraphs, we thus reduced the size of the non- story training data to that of the story data, by ran- domly selecting a number of non-story texts equal to the number of story texts for training and test- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross Validation</head><p>We used three versions of cross validation for the new experiments, one for each experimental con- dition: training and testing on a single corpus; training on a single corpus and testing on the com- bined corpus; or training on the combined corpus and testing on a single corpus. These procedures are the same as in our previous work <ref type="bibr" target="#b7">(Eisenberg et al., 2016</ref>). We performed undersampling before cross validation, so when we are explaining how to divide up the story and non-story texts into cross validation folds, this refers to the full set of story texts and the set of non-story texts that was ran- domly selected to equal the number of story texts.</p><p>For all experiments with cross validation, we use ten folds.</p><p>Train and Test on a Single Corpus: If the training and testing corpus is the same, divide up the stories into ten subsets of equal size, and the undersampled non-stories into ten subsets of equal size. For each fold of cross validation a different story set and non-story set (of the same index) are used as the testing set and the remaining nine are used for training.</p><p>Train on Combined, Test on Single: If the training is done on the combined corpus, and the test corpus is either the weblog or Extremist cor- pus, which we will refer to as the single test cor- pus, first divide the stories from the single test cor- pus into ten equal sized sets, and then divide up that corpus's non-stories into ten equal sets. For each fold of cross validation a different story set and non-story set (of the same index) from the sin- gle test corpus are used as the testing set and the remaining nine are used for training. The texts from the other corpus (the corpus that is not the single test corpus), are undersampled and added to all ten folds of training.</p><p>Train on Single, Test on Combined: If train- ing is done on a single corpus, and the test cor- pus is the combined corpus, first divide the sto- ries from the single training corpus into ten equal sized sets, and the undersampled non-stories from the single training corpus into ten equal sized sets. For each fold of cross validation a different story set and non-story set (of the same index) from the single training corpus are used as the testing set and the remaining nine are used for training. The texts from the other corpus (the corpus that is not the single training corpus), are undersampled and added to all ten folds of testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single Corpus Experiments</head><p>For every experiment that used only a single cor- pus, the best feature set included both the verb and character features, achieving up to 0.74 F 1 when trained and tested on the Extremist cor- pus. This represents a new state-of-the-art, about 12.6% greater than the performance of Corman's detector when trained and tested on the same cor- pus (0.66 F 1 ).</p><p>When the detector uses only verb features it achieves an F 1 of 0.74 on the Extremist corpus, only 0.002 lower than the detector using all the features. Interestingly, the detector achieves 0.55 F 1 using only the five character features, which is respectful given such a small feature set. To put this in perspective, the Corman detector ( <ref type="bibr" target="#b4">Ceran et al., 2012</ref>) uses more than 20,000 features, and achieves an F 1 of 0.66. Thus we were able to achieve 83% of the performance of the Corman detector with 4,000 times fewer features.</p><p>When training and testing on the blog corpus, the detector using all the features achieved 0.70 F 1 , a 74% increase from the Corman detector's 0.425 F 1 . This is the best performing model on the blog corpus, from any experiment to date. The detector using only verb features achieves 0.74 F 1 , which is only slightly worse than when both sets of features are used. When we trained using only the character features, the system achieves 0.65 F 1 , which is still 54% higher than when the Corman detector is trained and tested on the blog corpus.</p><p>In the single corpus experiments, the detectors that we trained and tested on the Extremist para- graphs have higher performance than those trained on the web blogs, except for when we use only the five character features. A possible reason for this is the Stanford NER may not be recogniz- ing the correct named entities in the Extremist texts, which contain many non-Western names, e.g., Mujahidin, Okba ibn Nafi, or Wahid. How- ever, when we include the verb features, the detec- tors trained on the Extremist texts achieve better performance. We believe this is partially due to the greater number of stories in the Extremist corpus, and their increased grammatical fluency. The Ex- tremist corpus is actually well written compared to the blog corpus, the latter of which contains nu- merous fragmentary and disjointed posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross Corpus Experiments</head><p>We show the generalizability of our best- performing detector (that including both verb and character features) by training it on one corpus and testing it on another.</p><p>When we trained the detector on the Extremist texts and tested on the blog texts, it scores a 0.68 F 1 . This is 142% improvement over Corman's de- tector in the same setup (0.28 F 1 ), and is a higher F 1 than the previous state-of-the-art on any sin- gle corpus test. When we trained the detector on the Extremist corpus and tested on the combined corpus, it achieved 0.71 F 1 , which is an 121% in- crease from Corman's detector in the equivalent setup.</p><p>For the detector trained on the blog corpus and tested on the Extremist corpus, the detector that uses both verbs and character features achieves an 0.27 F 1 , which is a 2,600% increase over the Cor- man detector's 0.01 F 1 in this same setup. While 0.27 F 1 can by no means be called good per- formance, it is significantly better than the Cor- man detector's performance on this task, and so demonstrates significantly better generalizability. As seen in our experiments, detectors trained on only the blog corpus do not perform as well as de- tectors trained on the Extremist corpus. We sus- pect that this is partially due to the disfluent nature of the blog corpus, which includes many fragmen- tary sentences, grammatical errors, and slang, all of which are difficult for the NLP pipeline to han- dle.</p><p>Note that we performed no cross validation in the above experiments where we trained the de- tector on the Extremist corpus and tested on the blog corpus, or vice versa, because in these cases the training and testing sets have no intersection.</p><p>The cross corpus experiment with the largest percent increase is for the verb and character de- tector trained on the blog corpus and tested on the combined corpus. The new detector's F 1 is 0.41, a 4,000% increase from the Corman detector's 0.01 F 1 on this task. Although a 0.41 F 1 is also not good, this is a massive improvement over previ- ous performance. This is further evidence that our verb and character feature based detector is sig- nificantly more generalizable than Corman's ap- proach.</p><p>The remaining five cross corpus experiments in- volved the combined corpus. In this case, our detector out-performed Corman's detector. Of special note is the detector trained on the com- bined corpus and tested on the Extremist corpus. It achieved 0.75 F 1 , which is 0.01 points of F 1 higher than our best single corpus detector, which was trained and tested on the Extremist corpus. This isn't a substantial increase in performance, but it suggests that information gleaned from the blog corpus does potentially-albeit marginally- help classification of the Extremist texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced a new story detection ap- proach which uses simple verb and character fea- tures. This new detector outperforms the prior state-of-the-art in all tasks, sometimes by orders of magnitude. Further, we showed that our de- tector generalizes significantly better across lexi- cally different corpora. We propose that this in- crease in performance and generalizability is due to the more general nature of our features, espe- cially those related to verb classes. This approach has additional advantages, for example, the fea- ture vector is fixed in size and does not grow in an unbounded fashion as new texts (with new verbs, agents, and patents) are added to the training data.</p><p>In future work we plan to develop richer character-based features. The current approach uses only normalized lengths of the five longest coreference chains, which leaves out important information about characters that could be use- ful to story detection. Indeed, our experiments showed that these character features only add a small amount of information above and beyond the verb features. However, when used alone, the character features still yield reasonable per- formance, which suggests that more meaningful character-based features could lead to story detec- tors with even better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>We report precision, recall, and F 1 rel-
ative to the story and non-story classes. We per-
formed experiments on three feature sets: the verb 
features alone (indicated by Verb in the table), 
character features alone (indicated by Char), and 
all features together (Verb+Char). We conducted 
experiments ranging over three corpora: the Ex-
tremist corpus (Ext), the blog corpus (Web), and 
the union of the two (Comb). These results may 
be compared with the previously best performing 
detector, namely, Corman's semantic triplet based 
detector (Ceran et al., 2012), as tested by us in 
prior work (Eisenberg et al., 2016), and shown in 
Table 2. 

Training Testing Prec. Recall F1 
Ext 
Ext 
0.77 
0.57 
0.66 
Ext 
Web 
0.23 
0.37 
0.28 
Ext 
Comb 
0.43 
0.41 
0.32 
Web 
Web 
0.66 
0.31 
0.43 
Web 
Ext 
0.59 
0.003 
0.01 
Web 
Comb 
0.59 
0.01 
0.01 
Comb 
Ext 
0.62 
0.51 
0.43 
Comb 
Web 
0.36 
0.49 
0.30 
Comb 
Comb 
0.64 
0.47 
0.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for the Corman semantic triplet 
based detector as reported in (Eisenberg et al., 
2016). These results are with respect to the story 
class. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of the new detectors as trained and tested on the Extremist (Ext), weblog (Web), or 
combined (Comb) corpora. The feature sets tested include the 278 verb class features (Verb), the nor-
malized length of the five longest coreference chains (Char)</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Cambridge Introduction to Narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Porter</forename><surname>Abbott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Apache Foundation</title>
		<ptr target="https://opennlp.apache.org" />
		<imprint>
			<date type="published" when="2017-07-20" />
		</imprint>
	</monogr>
	<note>OpenNLP v1.6.0</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining Natural and Artificial Examples to Improve Implicit Discourse Relation Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1694" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ICWSM 2009 Spinn3r Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Annual Conference on Weblogs and Social Media (ICWSM 2009)</title>
		<meeting>the 3rd Annual Conference on Weblogs and Social Media (ICWSM 2009)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Hybrid Model and Memory Based Story Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Betul</forename><surname>Ceran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Karad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Corman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Davulcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Computational Models of Narrative (CMN&apos;12)</title>
		<meeting>the 3rd International Workshop on Computational Models of Narrative (CMN&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="60" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBSVM: A Library for Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 21st Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2256" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing extant story classifiers: Results &amp; new directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshua D Eisenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark A</forename><surname>Victor H Yarlott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Computational Models of Narrative (CMN&apos;16)</title>
		<meeting>the 7th International Workshop on Computational Models of Narrative (CMN&apos;16)<address><addrLine>Krakow, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Paper 6</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005)</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005)<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collecting Semantics in the Wild: The Story Workbench</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fall Symposium on Naturally Inspired Artificial Intelligence</title>
		<meeting>the AAAI Fall Symposium on Naturally Inspired Artificial Intelligence<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Story Workbench: An Extensible Semi-Automatic Text Annotation Tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Intelligent Narrative Technologies (INT4)</title>
		<meeting>the 4th Workshop on Intelligent Narrative Technologies (INT4)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finlayson</surname></persName>
		</author>
		<ptr target="http://projects.csail.mit.edu/jverbnet" />
	</analytic>
	<monogr>
		<title level="j">JVerbnet</title>
		<imprint>
			<date type="published" when="2012-07-17" />
		</imprint>
	</monogr>
	<note>v1.2.0</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Global Wordnet Conference (GWC 2014)</title>
		<meeting>the 7th International Global Wordnet Conference (GWC 2014)<address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aspects of the Novel. RosettaBooks</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying Personal Stories in Millions of Weblog Entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Weblogs and Social Media (ICWSM 2009), Data Challenge Workshop</title>
		<meeting>the 3rd International Conference on Weblogs and Social Media (ICWSM 2009), Data Challenge Workshop<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discourse Complements Lexical Semantics for Non-factoid Answer Reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)<address><addrLine>Long Papers; Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from Imbalanced Data Sets: a Comparison of Various Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Learning from Imbalanced Data Sets</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP Natural Language Processing Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), System Demonstrations</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), System Demonstrations<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">VerbNet: A BroadCoverage, Comprehensive Verb Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Assocation for Computational Linguistics (ACL 2010), System Demonstrations</title>
		<meeting>the 48th Annual Meeting of the Assocation for Computational Linguistics (ACL 2010), System Demonstrations<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
