<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP) and Research Training Group AIPHES</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP) and Research Training Group AIPHES</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="338" to="348"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
					<note>Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research www.ukp.tu-darmstadt.de 1 The implementation of our network is publicly available. 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p &lt; 10 −4) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F 1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters. The full experimental results are published in (Reimers and Gurevych, 2017).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large efforts are spent in our community on devel- oping new state-of-the-art approaches. To docu- ment that those approaches are better, they are ap- plied to unseen data and the obtained performance score is compared to previous approaches. In or- der to make results comparable, a provided split between train, development and test data is often used, for example from a former shared task.</p><p>In recent years, deep neural networks were shown to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tag- ging tasks <ref type="bibr" target="#b24">(Ma and Hovy, 2016)</ref>, dependency pars- ing ( <ref type="bibr" target="#b0">Andor et al., 2016)</ref>, and machine translation ( <ref type="bibr">Wu et al., 2016</ref>). The training process for neural networks is highly non-deterministic as it usually depends on a random weight initialization, a ran- dom shuffling of the training data for each epoch, and repeatedly applying random dropout masks. The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima ( <ref type="bibr" target="#b20">LeCun et al., 1998;</ref><ref type="bibr" target="#b5">Erhan et al., 2010)</ref>. Depending on the seed value for the pseudo-random number genera- tor, the network will converge to a different local minimum.</p><p>Our experiments show that these different local minima have vastly different characteristics on un- seen data. For the recent NER system by <ref type="bibr" target="#b24">Ma and Hovy (2016)</ref> we observed that, depending on the random seed value, the performance on unseen data varies between 89.99% and 91.00% F 1 -score. The difference between the best and worst performance is statistically significant (p &lt; 10 −4 ) using a ran- domization test <ref type="bibr">3</ref> . In conclusion, whether this newly developed approach is perceived as state-of-the-art or as mediocre, largely depends on which random seed value is selected. This issue is not limited to this specific approach, but potentially applies to all approaches with non-deterministic training processes.</p><p>This large dependence on the random seed value creates several challenges when evaluating new approaches:</p><p>• Observing a (statistically significant) improve- ment through a new non-deterministic ap- proach might not be the result of a superior approach, but the result of having a more fa- vorable sequence of random numbers.</p><p>• Promising approaches might be rejected too early, as they fail to deliver an outperformance simply due to a less favorable sequence of random numbers.</p><p>• Reproducing results is difficult.</p><p>To study the impact of the random seed value on the performance we will focus on five linguis- tic sequence tagging tasks: POS-tagging, Chunk- ing, Named Entity Recognition, Entity Recogni- tion 4 , and Event Detection. Further we will fo- cus on Long-Short-Term-Memory (LSTM) Net- works <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997b</ref>), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks ( <ref type="bibr" target="#b24">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b36">Søgaard and Goldberg, 2016)</ref>.</p><p>Fixing the random seed value would solve the issue with the reproducibility, however, there is no justi- fication for choosing one seed value over another seed value. Hence, instead of reporting and compar- ing a single performance, we show that comparing score distributions can lead to new insights into the functioning of algorithms.</p><p>Our main contributions are:</p><p>1. Showing the implications of non-deterministic approaches on the evaluation of approaches and the requirement to compare score distri- butions instead of single performance scores.</p><p>2. Comparison of two recent, state-of-the-art sys- tems for NER and showing that reporting a single performance score can be misleading.</p><p>3. In-depth analysis of different LSTM- architectures for five sequence tagging tasks with respect to: superior performance, stability of results, and importance of tuning parameters. <ref type="bibr">4</ref> Entity Recognition labels all tokens that refer to an entity in a sentence, also generic phrases like U.S. president.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Validating and reproducing results is an important activity in science to manifest the correctness of previous conclusions and to gain new insights into the presented approaches. <ref type="bibr" target="#b6">Fokkens et al. (2013)</ref> show that reproducing results is not always straight- forward, as factors like preprocessing (e.g. tok- enization), experimental setup (e.g. splitting data), the version of components, the exact implementa- tion of features, and the treatment of ties can have a major impact on the achieved performance and sometimes on the drawn conclusions.</p><p>For approaches with non-deterministic training pro- cedures, like neural networks, reproducing exact results becomes even more difficult, as randomness can play a major role in the outcome of experiments. The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima ( <ref type="bibr" target="#b20">LeCun et al., 1998;</ref><ref type="bibr" target="#b5">Erhan et al., 2010)</ref>. The sequence of random numbers plays a major role to which min- ima the network converges during the training pro- cess. However, not all minima generalize equally well to unseen data. <ref type="bibr" target="#b5">Erhan et al. (2010)</ref> showed for the MNIST handwritten digit recognition task that different random seeds result in largely varying performances. They noted further that with increas- ing depth of the neural network, the probability of finding poor local minima increases. As (informally) defined by Hochreiter and Schmid- huber (1997a), a minimum can be flat, where the error function remains approximately constant for a large connected region in weight-space, or it can be sharp, where the error function increases rapidly in a small neighborhood of the minimum. A concep- tual sketch is given in <ref type="figure" target="#fig_0">Figure 1</ref>. The error functions for training and testing are typically not perfectly synced, i.e. the local minima on the train or devel- opment set are not the local minima for the held-out test set. A sharp minimum usually depicts poorer generalization capabilities, as a slight variation re- sults in a rapid increase of the error function. On the other hand, flat minima generalize better on new data ( <ref type="bibr" target="#b15">Keskar et al., 2016</ref>). Keskar et al. ob- serve for the MNIST, TIMIT, and CIFAR dataset, that the generalization gap is not due to over-fitting or over-training, but due to different generaliza- tion capabilities of the local minima the networks converge to.</p><p>A priori it is unknown to which type of local mini- mum a neural network will converge. Some meth- ods like the weight initialization ( <ref type="bibr" target="#b5">Erhan et al., 2010;</ref><ref type="bibr" target="#b8">Glorot and Bengio, 2010)</ref> or small-batch training ( <ref type="bibr" target="#b15">Keskar et al., 2016</ref>) help to avoid bad (e.g. sharp) minima. Nonetheless, the non-deterministic behav- ior of approaches must be considered when they are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Impact of Randomness in the Evaluation of Neural Networks</head><p>Two recent, state-of-the-art systems for NER are proposed by <ref type="bibr" target="#b24">Ma and Hovy (2016)</ref>  <ref type="bibr">5</ref> and by <ref type="bibr" target="#b18">Lample et al. (2016)</ref>  <ref type="bibr">6</ref> . Lample et al. report an F 1 -score of 90.94% and Ma and Hovy report an F 1 -score of 91.21%. Ma and Hovy draw the conclusion that their system achieves a significant improvement over the system by Lample et al.</p><p>We re-ran both implementations multiple times, each time only changing the seed value of the ran- dom number generator. We ran the Ma and Hovy system 86 times and the Lample et al. system, due to its high computational requirement, for 41 times. The score distribution is depicted as a violin plot in <ref type="figure" target="#fig_1">Figure 2</ref>. Using a Kolmogorov-Smirnov significance test <ref type="bibr" target="#b27">(Massey, 1951)</ref>, we observe a statistically significant difference between these two distributions (p &lt; 0.01). The plot reveals that the quartiles for the Lample et al. system are above those of the Ma and Hovy system. Further it reveals a smaller standard deviation σ of the F 1 - 5 https://github.com/XuezheMax/ LasagneNLP 6 https://github.com/glample/tagger scores for the Lample et al. system. Using a Brown- Forsythe test, the standard deviations are different with p &lt; 0.05. <ref type="table" target="#tab_0">Table 1</ref> shows the minimum, the maximum, and the median performance for the test performances. Based on this observation, we draw the conclusion that the system by Lample et al. outperforms the system by Ma and Hovy, as their implementation achieves a higher score distribution and shows a lower standard deviation.</p><p>In a usual setup, approaches would be compared on a development set and the run with the highest development score would be used for unseen data, i.e. be used to report the test performance. For the Lample et al. system we observe a Spearman's rank correlation between the development and the test score of ρ = 0.229. This indicates a weak correla- tion and that the performance on the development set is not a reliable indicator. Using the run with the best development score (94.44%) would yield a test performance of mere 90.31%. Using the second best run on development set (94.28%), would yield state-of-the-art performance with 91.00%. This dif- ference is statistically significant (p &lt; 0.002). In conclusion, a development set will not necessarily solve the issue with bad local minima.</p><p>The main difference between these two approaches is in the generation of character-based represen- tations: Ma and Hovy uses a Convolutional Neu- ral Network (CNN) ( <ref type="bibr" target="#b19">LeCun et al., 1989</ref>  In the next step, we evaluated the impact of the random seed value for the five sequence tagging tasks described in section 4. We sampled randomly 1830 different configurations, for example different numbers of recurrent units, and ran the network twice, each time with a different seed value. The results are depicted in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The largest difference was observed for the ACE 2005 Entities dataset: Using one seed value, the net- work achieved an F 1 performance of 82.5% while using another seed value, the network achieved a performance of only 74.3%. Even though this is a rare extreme case, the median difference between different weight initializations is still large. For example for the CoNLL 2003 NER dataset, the me- dian difference is at 0.38% and the 95th percentile is at 1.08%.</p><p>In conclusion, if the fact of different local minima is not taken care of and single performance scores are compared, there is a high chance of drawing false conclusions and either rejecting promising approaches or selecting weaker approaches due to a more or less favorable sequence of random numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In order to find LSTM-network architectures that perform robustly on different tasks, we selected five classical NLP tasks as benchmark tasks: Part- of-Speech tagging (POS), Chunking, Named Entity Recognition (NER), Entity Recognition (Entities) and Event Detection (Events).</p><p>For Part-of-Speech tagging, we use the benchmark setup described by <ref type="bibr" target="#b38">Toutanova et al. (2003)</ref>. Using the full training set for POS tagging would hin- der our ability to detect design choices that are consistently better than others. The error rate for this dataset is approximately 3% ( <ref type="bibr" target="#b26">Marcus et al., 1993)</ref>, making all improvements above 97% accu- racy likely the result of chance. A 97.24% accuracy was achieved by <ref type="bibr" target="#b38">Toutanova et al. (2003</ref> For the POS-task, we report accuracy and for the other tasks we report the F 1 -score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>We use a BiLSTM-network for sequence tagging as described in <ref type="bibr" target="#b12">(Huang et al., 2015;</ref><ref type="bibr" target="#b24">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016)</ref>. To be able to evaluate a large number of different network configurations, we optimized our implementation for efficiency, reducing by a factor of 6 the time required per epoch compared to Ma and Hovy (2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluated Parameters</head><p>We evaluate the following design choices and hy- perparameters:  <ref type="bibr" target="#b28">Mikolov, 2012)</ref> and gradient normalization ( <ref type="bibr">Pascanu et al., 2013)</ref>. Gradient clipping involves clipping the gradient's components element-wise if it exceeds a defined threshold. Gradient normalization has a better theo- retical justification and rescales the gradient when- ever the norm goes over a threshold.</p><note type="other">ent problem are gradient clipping (</note><p>Tagging schemes. We evaluate the BIO and IOBES schemes for tagging segments.</p><p>Dropout. We compare no dropout, naive dropout, and variational dropout ( <ref type="bibr" target="#b7">Gal and Ghahramani, 2016)</ref>. Naive dropout applies a new dropout mask at every time step of the LSTM-layers. Variational dropout applies the same dropout mask for all time steps in the same sentence. Further, it applies dropout to the recurrent units. We evaluate the dropout rates {0.05, 0.1, 0.25, 0.5}.</p><p>Classifier. We evaluate a Softmax classifier as well as a CRF classifier as the last layer of the network.</p><p>Number of LSTM-layers. We evaluated 1, 2, and 3 stacked BiLSTM-layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of recurrent units.</head><p>For each LSTM-layer, we selected independently a number of recurrent units from the set {25, 50, 75, 100, 125}.</p><p>Mini-batch sizes. We evaluate the mini-batch sizes 1, 8, 16, 32, and 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robust Model Evaluation</head><p>We have shown in section 3 that re-running non- deterministic approaches multiple times and com- paring score distributions is essential to draw cor- rect conclusions. However, to truly understand the capabilities of an approach, it is interesting to test the approach with different sets of hyperparameters for the complete network.</p><p>Training and tuning a neural network can be time consuming, sometimes taking multiple days to train a single instance of a network. A priori it is hard to know which hyperparameters will yield the best performance and the selection of the parameters often makes the difference between mediocre and state-of-the-art performance ( <ref type="bibr">Hutter et al., 2014)</ref>. If an approach yields good performance only for a narrow set of parameters, it might be difficult to adapt the approach to new tasks, new domains or new languages, as a large range of possible pa- rameters must be evaluated, each time requiring a significant amount of training time. Hence it is desirable, that the approach yields stable results for a wide range of parameters.</p><p>In order to find approaches that result in high per- formance and are robust against the remaining pa- rameters, we decided to randomly sample several hundred network configurations from the set de- scribed in section 4.2. For each sampled configu- ration, we compare different options, e.g. different options for the last layer of the network. For ex- ample, we sampled in total 975 configurations and each configuration was trained with a Softmax clas- sifier as well as with a CRF classifier, totaling to 1950 trained networks.  Our results are presented in <ref type="table" target="#tab_5">Table 3</ref>. The table shows that for the NER task 232 configurations were sampled randomly and for 210 of the 232 configurations (90.5%), the CRF setup achieved a better test performance than the setup with a Soft- max classifier. To measure the difference between these two options, we compute the median of the absolute differences: Let S i be the test performance (F 1 -measure) for the Softmax setup for configura- tion i and C i the test performance for the CRF setup. We then compute ∆F 1 = median(S 1 − C 1 , S 2 − C 2 , . . . , S 232 − C 232 ). For the NER task, the median difference was ∆F 1 = −0.66%, i.e. the setup with a Softmax classifier achieved on av- erage an F 1 -score of 0.66 percentage points below that of the CRF setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset # Configs Softmax</head><p>We also evaluated the standard deviation of the F 1 - scores to detect approaches that are less dependent on the remaining hyperparameters and the random number generator. The standard deviation σ for the CRF-classifier is with 0.0060 significantly lower (p &lt; 10 −3 using Brown-Forsythe test) than for the Softmax classifier with σ = 0.0082.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>This section highlights our main insights in the evaluation of different design choices for BiL- STM architectures. We limit the number of results we present for reasons of brevity. Detailed infor- mation can be found in ( <ref type="bibr" target="#b35">Reimers and Gurevych, 2017</ref>). 11 <ref type="table" target="#tab_5">Table 3</ref> shows a comparison between using a Soft- max classifier as a last layer and using a CRF classi- fier. The BiLSTM-CRF architecture by <ref type="bibr" target="#b12">Huang et al. (2015)</ref> achieves a better performance on 4 out of 5 tasks. For the NER task it further achieves a 27% lower standard deviation (statistically significant with p &lt; 10 −3 ), indicating that it is less sensitive to the remaining configuration of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Classifier</head><p>The CRF classifier only fails for the Event Detec- tion task. This task has nearly no dependency be- tween tags, as often only a single token is annotated as an event trigger in a sentence.</p><p>We studied the differences between these two clas- sifiers in terms of number of LSTM-layers. As <ref type="figure">Figure 3</ref> shows, a Softmax classifier profits from a deep LSTM-network with multiple stacked lay- ers. On the other hand, if a CRF classifier is used, the effect of additional LSTM-layers is much smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Optimizer</head><p>We evaluated six optimizers with the suggested default configuration from their respective papers. We observed that SGD is quite sensitive towards the selection of the learning rate and it failed in many instances to converge. For the optimizers SGD, Adagrad and Adadelta we observed a large standard deviation in terms of test performance, <ref type="figure">Figure 3</ref>: Difference between Softmax and CRF classifier for different number of BiLSTM-layers for the CoNLL 2003 NER dataset.</p><p>which was for the NER task at 0.1328 for SGD, 0.0139 for Adagrad, and 0.0138 for Adadelta. The optimizers RMSProp, Adam, and Nadam on the other hand produced much more stable results. Not only were the medians for these three optimizers higher, but also the standard deviation was with 0.0096, 0.0091, and 0.0092 roughly 35% smaller in comparison to Adagrad. A large standard devia- tion indicates that the optimizer is sensitive to the hyperparameters as well as to the random initializa- tion and bears the risk that the optimizer produces subpar results.</p><p>The best result was achieved by Nadam. For 453 out of 882 configurations (51.4%), it yielded the highest performance out of the six tested optimiz- ers. For the NER task, it produced on average a 0.82 percentage points better performance than Adagrad.</p><p>Besides test performance, the convergence speed is important in order to reduce training time. Here, Nadam had the best convergence speed. For the NER dataset, Nadam converged on average after 9 epochs, whereas SGD required 42 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Word Embeddings</head><p>The pre-trained word embeddings had a large im- pact on the performance as shown in <ref type="table">Table 4</ref>. The embeddings by <ref type="bibr" target="#b17">Komninos and Manandhar (2016)</ref> resulted in the best performance for the POS, the Entities and the Events task. For the Chunking task, the dependency-based embeddings of <ref type="bibr" target="#b21">Levy and Goldberg (2014)</ref> are slightly ahead of the Komninos embeddings, the significance level is at p = 0.025. For NER, the GloVe embeddings trained on common crawl perform on par with the Komninos embeddings (p = 0.391).</p><p>We observe that the underlying word embeddings have a large impact on the performance for all tasks. Well suited word embeddings are especially critical for datasets with small training sets. For the POS task we observe a median difference of 4.97% be- tween the Komninos embeddings and the GloVe2 embeddings.</p><p>Note we only evaluated the pre-trained embeddings provided by different authors, but not the underly- ing algorithms to generate these embeddings. The quality of word embeddings depends on many fac- tors, including the size, the quality, and the prepro- cessing of the data corpus. As the corpora are not comparable, our results do not allow concluding that one approach is superior for generating word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Character Representation</head><p>We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of <ref type="bibr" target="#b18">Lample et al. (2016)</ref> using LSTM-networks to derive character-based repre- sentations. <ref type="table" target="#tab_7">Table 5</ref> shows that character-based representations yield a statistically significant difference only for the POS, the Chunking, and the Events task. For NER and Entity Recognition, the difference to not using a character-based representation is not signif- icant (p &gt; 0.01).</p><p>Dataset Le. Dep. Le. BoW GloVe1 GloVe2 GloVe3 Komn. G. News FastText POS 6.5% 0.0% 0.0% 0.0% 0.0% 93.5% 0.0% 0.0% ∆Acc.</p><p>-0.39% -2.52% -4.14% -4.97% -2.60% -1.95% -2.28% Chunking 60.8% 0.0% 0.0% 0.0% 0.0% 37.1% 2.1% 0.0% ∆F 1 -0.52% -1.09% -1.50% -0.93% -0.10% -0.48% -0.75% NER 4.5% 0.0% 22.7% 0.0% 43.6% 27.3% 1.8% 0.0% ∆F 1 -0.85% -1.17% -0.15% -0.73% -0.08% -0.75% -0.89% Entities 4.2% 7.6% 0.8% 0.0% 6.7% 57.1% 21.8% 1.7% ∆F 1 -0.92% -0.89% -1.50% -2.24% -0.80% -0.33% -1.13% Events 12.9% 4.8% 0.0% 0.0% 0.0% 71.8% 9.7% 0.8% ∆F 1 -0.55% -0.78% -2.77% -3.55% -2.55% -0.67% -1.36% Average 17.8% 2.5% 4.7% 0.0% 10.1% 57.4% 7.1% 0.5% <ref type="table">Table 4</ref>: Randomly sampled configurations were evaluated with 8 possible word embeddings. 108 configurations were sampled for POS, 97 for Chunking, 110 for NER, 119 for Entities, and 124 for Events.</p><p>The difference between the CNN approach by <ref type="bibr" target="#b24">Ma and Hovy (2016)</ref> and the LSTM approach by <ref type="bibr" target="#b18">Lample et al. (2016)</ref> to derive a character-based repre- sentations is statistically insignificant for all tasks. This is quite surprising, as both approaches have fundamentally different properties: The CNN ap- proach from Ma and Hovy (2016) takes only tri- grams into account. It is also position independent, i.e. the network will not be able to distinguish be- tween trigrams at the beginning, in the middle, or at the end of a word, which can be crucial information for some tasks. The BiLSTM approach from <ref type="bibr" target="#b18">Lample et al. (2016)</ref> takes all characters of the word into account. Further, it is position aware, i.e. it can distinguish between characters at the start and at the end of the word. Intuitively, one would think that the LSTM approach by Lample et al. would be superior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Gradient Clipping and Normalization</head><p>For gradient clipping (Mikolov, 2012) we couldn't observe any improvement for the thresholds of 1, 3, 5, and 10 for any of the five tasks.</p><p>Gradient normalization has a better theoretical jus- tification ( <ref type="bibr">Pascanu et al., 2013</ref>) and we can confirm with our experiments that it performs better. Not normalizing the gradient was the best option only for 5.6% of the 492 evaluated configurations (un- der null-hypothesis we would expect 20%). Which threshold to choose, as long as it is not too small or too large, is of lower importance. In most cases, a threshold of 1 was the best option (30.5% of the  We observed a large performance increase com- pared to not normalizing the gradient. The median increase was between 0.29 percentage points F 1 - score for the Chunking task and 0.82 percentage points for the POS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Dropout</head><p>Dropout is a popular method to deal with overfit- ting for neural networks ( <ref type="bibr" target="#b37">Srivastava et al., 2014</ref>). We could observe that variational dropout ( <ref type="bibr" target="#b7">Gal and Ghahramani, 2016)</ref> clearly outperforms naive dropout and not using dropout. It was the best op-tion in 83.5% of the 479 evaluated configurations. The median performance increase in comparison to not using dropout was between 0.31 percentage points for the POS-task and 1.98 for the Entities task. We also observed a large improvement in comparison to naive dropout between 0.19 percent- age points for the POS task and 1.32 percentage points for the Entities task. Variational dropout showed the smallest standard deviation, indicating that it is less dependent on the remaining hyperpa- rameters and the random number sequence.</p><p>We further evaluated whether variational dropout should be applied to the output units of the LSTM- network, to the recurrent units, or to both. We observed that applying dropout to both dimensions gave in most cases (62.6%) the best results. The me- dian performance increase was between 0.05 per- centage points and 0.82 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Further Evaluated Parameters</head><p>The tagging schemes BIO and IOBES performed on par for 4 out of 5 tasks. For the Entities task, the BIO scheme significantly outperformed the IOBES scheme for 88.7% of the tested con- figurations. The median difference was ∆F 1 = −1.01%.</p><p>For the evaluated tasks, 2 stacked LSTM-layers achieved the best performance. For the POS- tagging task, 1 and 2 layers performed on par. For flat networks with a single LSTM-layer, around 150 recurrent units yielded the best performance. For networks with 2 or 3 layers, around 100 recurrent units per network yielded the best performance. However, the impact of the number of recurrent units was extremely small.</p><p>For tasks with small training sets, smaller mini- batch sizes of 1 up to 16 appears to be a good choice. For larger training sets sizes of 8 -32 appears to be a good choice. Mini-batch sizes of 64 usually performed worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we demonstrated that the sequence of random numbers has a statistically significant im- pact on the test performance and that wrong conclu- sions can be made if performance scores based on single runs are compared. We demonstrated this for the two recent state-of-the-art NER systems by <ref type="bibr" target="#b24">Ma and Hovy (2016)</ref> and <ref type="bibr" target="#b18">Lample et al. (2016)</ref>. Based on the published performance scores, Ma and Hovy draw the conclusion of a significant improvement over the approach of Lample et al. Re-executing the provided implementations with different seed values however showed that the implementation of Lample et al. results in a superior score distribution generalizing better to unseen data.</p><p>Comparing score distributions reduces the risk of rejecting promising approaches or falsely accepting weaker approaches. Further it can lead to new in- sights on the properties of an approach. We demon- strated this for ten design choices and hyperparam- eters of LSTM-networks for five tasks.</p><p>By studying the standard deviation of scores, we estimated the dependence on hyperparameters and on the random seed value for different approaches. We showed that SGD, Adagrad and Adadelta have a higher dependence than RMSProp, Adam or Nadam. We have shown that variational dropout also reduces the dependence on the hyperparame- ters and on the random seed value. As future work, we will investigate if those methods are either less dependent on the hyperparameters or are less de- pendent on the random seed value, e.g. if they avoid converging to bad local minima.</p><p>By testing a large number of configurations, we showed that some choices consistently lead to su- perior performance and are less dependent on the remaining configuration of the network. Thus, there is a good chance that these configurations require less tuning when applied to new tasks or domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A conceptual sketch of flat and sharp minima from Keskar et al. (2016). The Y-axis indicates values of the error function and the Xaxis the weight-space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of scores for re-running the system by Ma and Hovy (left) and Lample et al. (right) multiple times with different seed values. Dashed lines indicate quartiles.</figDesc><graphic url="image-1.png" coords="3,337.04,140.82,158.75,143.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : The system by Ma and Hovy (2016) and Lample et al. (2016) were run multiple times with different seed values.</head><label>1</label><figDesc></figDesc><table>), while 
Lample et al. uses an LSTM-network. As our ex-
periments in section 6.4 show, both approaches 
perform comparably if all other parameters were 
kept the same. Further, we could only observe a </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The table depicts the median, the 95th percentile and the maximum difference between networks 
with the same hyperparameters but different random seed values. 

statistically significant improvement for the tasks 
POS, Chunking and Event Detection. For NER 
and Entity Recognition, the difference was statis-
tically not significant given the number of tested 
hyperparameters. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Percentages of configurations where Soft-
max or CRF classifiers demonstrated a higher test 
performance. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of not using character-based 
representations and using CNNs (Ma and Hovy, 
2016) or LSTMs (Lample et al., 2016) to derive 
character-based representations. 225 configura-
tions were sampled for POS, 241 for Chunking, 
217 for NER, 228 for Entities, and 219 for Events. 

cases). 

</table></figure>

			<note place="foot" n="1"> https://arxiv.org/abs/1707.06799 2 https://github.com/UKPLab/ emnlp2017-bilstm-cnn-crf</note>

			<note place="foot" n="3"> 1 Million iterations. p-value adapted using the Bonferroni correction to take the 86 tested seed values into account.</note>

			<note place="foot" n="11"> https://public.ukp.informatik. tu-darmstadt.de/reimers/Optimal_ Hyperparameters_for_Deep_LSTM-Networks. pdf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the German Re-search Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1. Calculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<idno>abs/1603.06042</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<pubPlace>Slav Petrov, and Michael Collins</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching Word Vectors with Subword Information</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Incorporating Nesterov Momentum into Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why Does Unsupervised Pre-training Help Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offspring from Reproduction Problems: What Replication Failure Teaches Us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antske</forename><surname>Fokkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Marieke Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Postma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1691" to="1701" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
	<note>Ted Pedersen, Piek Vossen, and Nuno Freire</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural Networks for Machine Learning-Lecture 6a-Overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flat Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF Models for Sequence Tagging. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Efficient Approach for Assessing Hyperparameter Importance</title>
		<idno>I- 754-I-762. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mudigere</surname></persName>
		</author>
		<idno>abs/1609.04836</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dependency based embeddings for sentence classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Komninos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1490" to="1500" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno>abs/1603.01360</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation Applied to Handwritten Zip Code Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient BackProp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop</title>
		<meeting><address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint Event Extraction via Structured Prediction with Global Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1603.01354</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The kolmogorov-smirnov test for goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">253</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/sqr(k))</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the Difficulty of Training Recurrent Neural Networks</title>
		<idno>III-1310-III- 1318. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06799</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multitask learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
