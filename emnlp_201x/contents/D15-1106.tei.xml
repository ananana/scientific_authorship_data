<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Recurrent Neural Network for Document Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Recurrent Neural Network for Document Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a novel hierarchical recurrent neural network language model (HRNNLM) for document modeling. After establishing a RNN to capture the coherence between sentences in a document , HRNNLM integrates it as the sentence history information into the word level RNN to predict the word sequence with cross-sentence contextual information. A two-step training approach is designed, in which sentence-level and word-level language models are approximated for the convergence in a pipeline style. Examined by the standard sentence reordering scenario , HRNNLM is proved for its better accuracy in modeling the sentence coherence. And at the word level, experimental results also indicate a significant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Network (DNN), a neural network with multiple layers, has been proven powerful in many different domains, such as visual recogni- tion ( <ref type="bibr">Kavukcuoglu et al., 2010</ref>) and speech recog- nition ( <ref type="bibr" target="#b5">Dahl et al., 2012</ref>), ever since <ref type="bibr" target="#b6">Hinton et al. (2006)</ref> formulated an efficient training method for it.</p><p>In addition to the applications mentioned above, many neural network based methods have also been applied to natural language processing (NLP) tasks with great success. For example, <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> propose a generalized DNN framework for a variety of fundamental NLP tasks, including part-of-speech tagging (postag), chunking, named * Contribution during internship at Microsoft Research. entity recognition (NER), and semantic role label- ing.</p><p>DNN is successfully introduced to do word- level language modeling, aka., to predict the next word given the history words. <ref type="bibr" target="#b1">Bengio et al. (2003)</ref> propose a feedforward neural network to train a word-level language model with a limited n-gram history. To leverage as much history as possible, <ref type="bibr">Mikolov et al. (2010)</ref> apply recurrent neural net- work to word-level language modeling. The mod- el absorbs one word each time, keeps the informa- tion in a history vector, and predicts the next word with all the word history in the vector.</p><p>Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of co- herence of sentences, <ref type="bibr">Le and Mikolov (2014)</ref> ex- tend word embedding learning network ( <ref type="bibr">Mikolov et al., 2013)</ref> to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. <ref type="bibr">Li and Hovy (2014)</ref> propose a neu- ral network coherence model which employs dis- tributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not.</p><p>In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierar- chical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a combination of a word- level language model and a sentence-level lan- guage model, both of which are recurrent neu- ral networks. The word-level recurrent neural network follows ( <ref type="bibr">Mikolov et al., 2010)</ref>. The sentence-level language model is another recur- rent neural network that takes sentence represen-tation as input, and predicts the words in the next sentence. Similar to ( <ref type="bibr">Mikolov et al., 2010)</ref>, the hidden layer in the sentence-level recurrent neural network contains the sentence history information. The hidden layer containing the history informa- tion of previous sentences is then linked as an in- put to the word-level recurrent neural network to predict the next word together with the word-level history vector. This allows the language model to predict the next word probability distribution be- yond the words in the current sentence.</p><p>We propose a two-step training approach to op- timize the parameters of HRNNLM. In the first step, we train the sentence-level language model- s independently . And then, we connect the hid- den layer of the sentence-level language model to the input of word-level RNNLM and train the two models jointly until converged. At sentence level, we evaluate our model with a sentence ordering task and the result shows our method can outper- form a maximum entropy based and another state- of-the-art solution. At word level, we compare our method with the conventional recurrent neural net- work based language model, finding the perplexity is reduced significantly. We also apply our method to rank machine translation output and conduct ex- periments on a Chinese-English document transla- tion task, yielding a better translation results com- pared with a state-of-the-art baseline system.</p><p>The rest of this paper is organized as follows: Section 2 introduces work related to applying neu- ral network to document modeling and SMT. Sec- tion 3 introduces the general framework for doc- ument modeling. Our sentence-level language model and its training is described in Section 4, and the overall HRNNLM and its training is pre- sented in Section 5. Section 6 presents our exper- iments and their results. Finally, we conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section, we introduce previous efforts on applying neural network to model words coher- ence across sentence boundaries as well as works on improving machine translation performance at discourse level.</p><p>Mikolov and Zweig (2012) propose a RNN- LDA model to implement a context dependent lan- guage model. They augment the contextual infor- mation into the conventional RNNLM via a real- valued input vector, which is the probability distri- bution computed by LDA topics for using a block of preceding text. They train a Latent Dirichlet Al- location (LDA) model using documents consisting of about 10 sentences long text from Penn Tree- bank (PTB) training data. Their approach outper- forms RNNLM in perplexity on PTB data with a limited context history over topics instead of com- plete information of preceding sentences. <ref type="bibr">Le and Mikolov (2014)</ref> extend the Continu- ous Bag-of-Words Model (CBOW) and Continu- ous Skip-gram Model (Skip-gram) ( <ref type="bibr">Mikolov et al., 2013</ref>) by introducing a paragraph vector. In their method, the paragraph vector is learnt in a simi- lar way of word vector model, and there will be N × P parameters, if there are N paragraphs and each paragraph is mapped to P dimensions. Dif- ferent from them, the sentence vectors of our mod- el are learnt with nearly unlimited sentence his- tory based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sen- tence id, but only based on the words in the sen- tence. And our sentence vector also integrates n- early all the history information of previous sen- tences, while their model cannot.</p><p>Li and Hovy (2014) implement a neural net- work model to predict discourse coherence qual- ity in essays. In their work, recurrent (Sutskever et al., 2011) and recursive <ref type="bibr" target="#b11">(Socher et al., 2013</ref>) neural networks are both examined to learn dis- tributed sentence representation given pre-trained word embedding. The distributed sentence repre- sentation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural net- work classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the co- herence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence.</p><p>An attempt of introducing RNN into convolu- tional neural network (CNN) is investigated by (X- u and Sarikaya, 2014) for spoken language un- derstanding (SLU). To alleviate more contextual information, they apply a CNN with Jordan-type (Jordan, 1997) recurrent connections. The recur- rent connections send the distribution of the last softmax layer's output to the current input layer as additional features. Aimed to improve SLU domain classification, their model is essentially a kind of document representation with certain text information, neglecting the coherence information between sentences.</p><p>Following the thread modeling the word se- quence relationship within and across sentences, we propose a hierarchical recurrent neural net- work language model consist of a sentence-level language model and a word-level language model. This overall network is trained to capture the co- herence between sentences and predict words se- quence with preceding sentence contexts.</p><p>For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. <ref type="bibr" target="#b16">Yang et al. (2013)</ref> adapt and extend the CD-DNN-HMM ( <ref type="bibr" target="#b5">Dahl et al., 2012</ref>) model to the HMM-based word alignment model. In their method, they use bilingual word embed- ding to capture the lexical translation information and modeling the context with surrounding word- s. <ref type="bibr">Liu et al. (2014)</ref> propose a recursive recurrent neural network (R 2 NN) for end-to-end decoding to help improve translation quality. And Cho et al. <ref type="formula" target="#formula_4">(2014)</ref> propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little re- port on applying DNN to boost the translation re- sult of a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Language Modeling</head><p>Statistical language model assigns a probability to a natural language sequence. Conventional lan- guage models only focus on the word sequence within a sentence. For sentences in one documen- t talking about one or several specific topics, the adjacent sentences should be in a coherent order. Therefore, the words in the next sentence are also dependent on the preceding sentences. To mod- el the coherence of sentences in the document D, which contains N sentences S 1 , S 2 , S 3 , ..., S N , we need to maximize the objective as follow:</p><formula xml:id="formula_0">p(D) =p(S 1 , S 2 , ..., S N ) =p(S 1 ) · p(S 2 |S 1 ) · p(S 3 |S 1 , S 2 ) ...p(S N |S 1 , S 2 , ..., S N −1 ) (1)</formula><p>For the sentence S k containing words w 1 , w 2 , w 3 , ..., w T , p(S k |S 1 , S 2 , ..., S k−1 ) is defined as:</p><formula xml:id="formula_1">p(S k |S 1 , S 2 , ..., S k−1 ) = p(w 1 , w 2 , ..., w T |S 1 , ..., S k−1 ) = p(w 1 |S 1 , ..., S k−1 ) · p(w 2 |w 1 , S 1 , ..., S k−1 ) ...p(w T |w 1 , w 2 , ..., w T −1 , S 1 , ..., S k−1 )</formula><p>(2) As a special case of approximation to this, clas- sical n-gram language model keep only sever- al words as history, discarding any information across the sentence boundaries. Recurrent neural network language model ( <ref type="bibr">Mikolov et al., 2010</ref>) us- es a hidden layer which employs a real-valued vec- tor recurrently as network's input to keep as many history as possible. This makes RNNLM be able to extend for capturing history beyond a sentence.</p><p>To prevent the potential exponential decay of the history, the history length in RNN can not be too long. Here we approximate the history information of previous sentences, p(S k |S 1 , S 2 , ..., S k−1 ), by the following:</p><formula xml:id="formula_2">p(S k |S 1 , S 2 , ..., S k−1 ) = p(BoW S k |BoW S 1 , ..., BoW S k−1 ) · p(S k |BoW S k )<label>(3)</label></formula><p>where BoW S k denotes the bag of words for the sentence S k . The document is thus generated in two steps.</p><p>• Given the previous sentences BoW S 1 , ..., BoW S k−1 (treating them as bag of words here), first generate the words which will show in the next sentence without considering their order with p(BoW S k |BoW S 1 , ..., BoW S k−1 )</p><p>• Generate the words one by one with</p><formula xml:id="formula_3">p(S k |BoW S k ).</formula><p>The first phase actually completes sentence-level language modeling, and the second addresses the word-level language modeling. Because recurrent neural network has a natural advantage in process- ing sequential data, we investigate how to model the whole process under a unified framework of recurrent neural network.</p><p>for word ordering. It overcomes the limitations of classical language model in capturing only a fixed- length history, yielding a significant performance improvements in terms of perplexity reduction and speech recognition accuracy. Here we adept this framework for a RNN based sentence-level lan- guage modeling, i.e. RNNSLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>A conventional language model reads a word each time, keeps several words as history and then pre- dict the probability distribution of the next word. Similar to this, our sentence-level language model reads a sentence which is a bag of words repre- sentation. And then it stores the sentence history which captures coherence of sentences in a real- valued history vector. With the history vector, our model can predict which words are most likely to appear in the next sentence. All these will be mod- eled by a recurrent neural network. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, similar to the convention- al recurrent neural network, for the sentence j, our network has two input layers xs j and hs j−1 . xs j is the current sentence representation, and hs j−1 is the history information vector before sentence j. The model has a hidden layer hs j , which will combine the history information of hs j−1 and the current sentence input xs j , and an output layer ys j+1 , which generates the probabilities of the words in the sentence j + 1. The layers are com- puted as follows:</p><formula xml:id="formula_4">hs j = f (U s · hs j−1 + W s · xs j )<label>(4)</label></formula><formula xml:id="formula_5">ys j+1 = g(V s · hs j )<label>(5)</label></formula><p>where W s , U s and V s denote the weight matrix.</p><p>f (z) is a HT anh function:</p><formula xml:id="formula_6">f (z j ) =      −1 z j &lt; −1 z −1 &lt; z j &lt; 1 1 z j &gt; 1<label>(6)</label></formula><p>and g(z) is a softmax function:</p><formula xml:id="formula_7">g(z j ) = e z j ∑ k e z k<label>(7)</label></formula><p>The output layer ys j+1 is a 1×V vector that repre- sents probability distribution of words in the next sentence given the current sentence xs j and pre- vious history hs j−1 , where V denotes vocabulary size.</p><p>To emphasize coherence between the adjacen- t sentences, we further add some bigram-like bag of words feature to the output layer. As mentioned in <ref type="bibr" target="#b8">(Mikolov, 2012)</ref>, this is kind of maximum en- tropy feature which can be derived by a two-layer neural network. Some experiments show that per- plexity significantly decreases after adding these features. Following <ref type="bibr" target="#b8">(Mikolov, 2012)</ref>, where, the maximum entropy bigram features are added to our RNNSLM by a direct connection between the feature input array and output layer ys j+1 . Fol- lowing <ref type="bibr">(Mahoney, 2000</ref>), we map bigram maxi- mum entropy features to a fixed-length array to re- duce the memory complexity of direct connections with feature hashing. Then the output layer can be computed as follow:</p><formula xml:id="formula_8">ys j+1 (t) = g(V s (t) · hs j + ∑ w∈xs j D hash(w,t) )<label>(8)</label></formula><p>where (t) denotes the t-th row of a vector or a ma- trix. D denotes that the hash array contains feature weights and hash(w i , w j ) denotes the hash func- tion for mapping bigram features to a fixed-length array. For a output ys j+1 , multiple connections may be activated according to the words in sen- tence xs j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>The training objective of our RNNSLM is to find the best parameters for predicting the words of next sentence. Formally, given the next sentence S k containing words w 1 , w 2 , w 3 , ..., w T . The training objective according to <ref type="bibr">(Mikolov et al., 2013)</ref> can be denoted by:</p><formula xml:id="formula_9">log(p(BoW S k |BoW S 1 , ..., BoW S k−1 )) = 1 T T ∑ t=1 logp(w t |BoW S 1 , ..., BoW S k−1 )<label>(9)</label></formula><p>For weight matrix W s , U s , V s and hash feature weight D, the parameter are trained similar to the conventional recurrent neural network. The learn- ing rate α is set to 0.1 at the start of the training as suggested in ( <ref type="bibr">Mikolov et al., 2010)</ref>. After each epoch, it can be determined by the training loss of network. If the loss decreases significantly, train- ing continues with the same learning rate. Other- wise, if the loss increases, the training will be ex- ecuted with a new learning rate α/2. The training process will be terminated after about 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Initialization</head><p>All elements in weight matrix W s and U s are ini- tialized by randomly sampling from a uniform dis- tribution [− 1 For the initialization of hs 0 , it can be set to a vector of the same values, which is 0.1.</p><formula xml:id="formula_10">K 1 , 1 K 1 ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hierarchical Recurrent Neural Network</head><p>In the previous section, we propose a RNNSLM which models the coherence between sentences but ignores the word sequence within a sentence. Ideally, a perfect document model should not only capture the information between sentences but al- so the information with sentence. So we propose a hierarchical recurrent neural network language model (HRNNLM) to fulfill this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model</head><p>A hierarchical recurrent neural network consists of two independent recurrent neural network. For a conventional word-level language model, it pre- dict the next word only using the word history within the sentence. To capture the longer his- tory, we integrate the sentence history into the word-level language model from sentence-level language model, which forms a hierarchical recur- rent neural network. As illustrated <ref type="figure" target="#fig_1">Figure 2</ref>, the upper part is the un- folded illustration of conventional recurrent neural network based language model. It takes one word w i each time with the previous history informa- tion hw i−1 together and predicts the probability of the next word p(w i+1 ) with the information kep- t in the history vector hw i . The lower part is our RNNSLM, which takes the bag of words represen- tation of a sentence xs j each time with the history information of previous sentences hs j−1 together and predicts the bag of words in the next sentence p(s j+1 ) with the information kept in hs j .</p><p>We integrate these two recurrent neural net- works together by adding connections between the sentence-level history vector hs j−1 and word level history vector hw i . So while predicting the nex- t word w i+1 of the current sentence, our model will consider the current word w i , history of previ- ous sentences hs j−1 and history of previous words hw i−1 . The new word-level history vector hw i is computed as:</p><formula xml:id="formula_11">hw i = f (U w · hw i−1 + W w · xw i + U sw · hs j−1 )<label>(10)</label></formula><p>where f (z) is a HT anh function. For HRNNLM, we also add a bigram hash feature, similar as we do for RNNSLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training</head><p>The HRNNLM can be trained from scratch fol- lowing <ref type="bibr">Mikolov et al. (2010)</ref> with a dual objec- tive. But this is not without problem. Beginning of training phase, the sentence history is unstable since the parameters of sentence-level language model are kept updating. Consequently, the train- ing of HRNNLM will be also unstable and hard to converge with unstable sentence history. In this paper, we approximate the whole training of HRNNLM by a two-step training method. We first train a RNNSLM until it converges. Then we connect the hidden layer of RNNSLM to the hid- den layer of RNNWLM. To increase the training speed, all the parameters of RNNSLM are fixed while training HRNNLM. We only update the ran- dom initialized parameters in HRNNLM, though ideally the gradient of the sentence history vector could change and the RNNSLM could be updated again. The learning rate α is set to 0.1 and the up- dating of learning rate is the same as suggested in Section 4.2. All the parameters can be initialize as suggested in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the sentence-level performance of HRNNLM by the common coherence evaluation of sentence ordering task, its word-level perfor- mance by perplexity measure. We also apply our HRNNLM to SMT reranking task in an open Chinese-English translation dataset. The trans- lation performance index is the IBM version of BLEU-4 ( <ref type="bibr" target="#b10">Papineni et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sentence Ordering</head><p>We follow ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008</ref>) to evaluate our sentence-level language model via a sentence ordering task with test set 2010 (tst2010), 2011 (tst2011) and 2012 (tst2012) from IWSLT 2014, totaling 37 English documents. 20 random permu- tations of sentences for each document are gener- ated. Each permutation and its original document are combined as an article pair. Our goal is to find the original one among all the article pairs.</p><p>The training data for sentence-level language model is the 1,414 English documents from the parallel corpus also provided by the IWSLT 2014 spoken language translation task. 90% of the doc- uments are for training and the rest are reserved for validation. The size of the hidden layer is set to 30 and hash array size is 10 7 .</p><p>We define the log probability of a given docu- ment as its coherence score. The document with the higher score is regarded as the original docu- ment.</p><p>We provide two baselines for sentence order- ing. One is the state-of-the-art recursive neural network based method proposed by <ref type="bibr">(Li and Hovy, 2014</ref>). We implement their model trained and tested with our data. The other is a maximum en- tropy classifier trained with bag of words features of adjacent sentences which can generate a coher- ent probability of adjacent sentences. The docu- ment with the higher sum of log probability for each adjacency sentences is regarded as the origi- nal document. <ref type="table">Table 1</ref> shows the accuracy of our system and baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>Accuracy Recursive 91.39% ME system 91.89% Our system 95.68% <ref type="table">Table 1</ref>: Accuracy of the sentence ordering task for each system</p><p>From <ref type="table">Table 1</ref> we can see that the maximum entropy model and the recursive neural network model has almost the same performance. Com- pared with the baseline systems, the proposed HRNNLM achieves significant improvement with nearly 4.3% improvement in term of accuracy. The experimental result shows that the HRNNLM can model document coherence and capture cross- sentence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Word-level Model Perplexity</head><p>We compare the word level performance of HRNNLM with the most popular RNNLM in terms of model perplexity. For a fair compari- son, we follow ( <ref type="bibr">Mikolov et al., 2010</ref>) and train the model also on 90% of the 1.414 English docu- ments form IWSLT 2014, totaling about 3M word- s. Then we train our model with the same hidden layer size and hash array size as the baseline sys- tem. The perplexity of these two models is eval- uated on held-out documents, about 370K words. The results are shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>Perplexity <ref type="table" target="#tab_2">RNNLM-30  183  HRNNLM-30  174   Table 2</ref>: Perplexity of the different language mod- el</p><p>According to <ref type="table">Table 2</ref>, it is reasonable to claim that, by integrating history information of previous sentences, the model perplexity decreased signif- icantly. Empirically, this confirms the hypothesis that the words selection for the next sentence is dependent on its preceding sentences in the same document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Spoken Language Translation</head><p>The conventional SMT systems translate sen- tences independently, without considering the co- herence of the sentences in the same document. In order to learn translation coherence between sen- tences, we apply the HRNNLM to machine trans- lation reranking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Data Setting and Baselines</head><p>The data comes from the IWSLT 2014 spoken lan- guage translation task. The training data consists of 1,414 documents on TED talks, and contain- s 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The devel- opment set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012.</p><p>The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by <ref type="bibr" target="#b2">(Cettolo et al., 2012</ref>). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) ( <ref type="bibr" target="#b13">Wu, 1997</ref>) in a CKY-style decoder with a lexical reordering model trained with maximum entropy ( <ref type="bibr" target="#b14">Xiong et al., 2006</ref>). The decoder uses commonly used features, such as translation prob- abilities, lexical weights, a language model, word penalty, and distortion probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Rerank System</head><p>Our reranking system is a linear model with sev- eral features, including the SMT system final s- cores, sentence-level language model scores, and HRNNLM scores. It should be noted all these fea- tures are actually employed by the SMT model ex- cept for the HRNNLM score. Since Minimum Er- ror Rate Training (MERT) <ref type="bibr" target="#b9">(Och, 2003)</ref> is the most general method adopted in SMT systems for tun- ing, the feature weights are fixed by MERT.</p><p>For our reranking system, to score the transla- tion of one sentence we need the translation re- sults of all the previous sentences in the documen- t. Our SMT decoder generates 10-best results of all the sentences of the documents and the rerank- ing system select the best translation result for the first sentence at first. With the translation of first sentence, we score all the translation candidates of the second sentence and select the best one as the result. Following this procedure, we can get the translation results for all the sentences in the doc- ument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Results</head><p>The HRNNLM focus on exploiting longer context, esp. cross-sentence word dependencies. Therefor the translation data for IWSLT 2014 is organized as documents instead of sentences for our rerank system. We hope HRNNLM will enable a context- sensitive reranking process, capturing the syntac- tic and logic relationships between the sentences in the same document.  The translation performance comparison is shown in <ref type="table" target="#tab_2">Table 3</ref>. From <ref type="table" target="#tab_2">Table 3</ref>, we can find that the rerank system improves SMT performance consistently. For a single sentence without the context information, there are several appropriate translations and it is hard to tell which one is bet- ter. When considering the context of a document (previous sentences for our model), some transla- tion candidates may not be coherent with the oth- ers which should not be selected. Our model can generate the most coherent translation results by considering previous sentence history.</p><p>For example, we have the following two Chi- nese sentence in one document together with their correct translation:</p><p>我 拍摄 过 的 冰山, 有些 冰 是 非常 年 轻 --几千 年 年龄 Some of the ice in the icebergs that I pho- tograph is very young --a couple thou- sand years old. 有些 冰 超过 十万 年 And some of the ice is over 100,000 years old.</p><p>Chinese word " 有 些" means "some" in En- glish. But when it is used in parallelism sentences, it means "some of" instead of "some". The tradi- tional SMT system translates the italics part with- out considering the context. The translation result for this kind of system is:</p><p>Some ice more than 100,000 years. For our system, the HRNNLM can take previ- ous sentence as context and learn the parallelism between the two sentences. It can select the best translation "some of" for 有些, and the output of our system is:</p><p>Some of the ice more than 100,000 years. We also calculate the BLEU increase ratio of our system on document level. The ratio is de- fined as 1 N #(BleuD rerank &gt; BleuD baseline ), where N denotes the number of documents, and #(BleuD rerank &gt; BleuD baseline ) denotes the number of documents for which document level BLEU score of reranking system is higher than the baselines. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. tst2010 tst2011 tst2012 72.73% 71.43% 75%  <ref type="table" target="#tab_3">Table 4</ref>, we can find that, for all the three test data sets, our reranking system can achieve better performance for more than 70% documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we propose a hierarchical recurren- t neural network language model for document modeling. We first built a RNNSLM to capture the information between sentences. Then we in- tegrate the hidden layer of RNNSLM into the in- put layer of word-level language model to form a hierarchical recurrent neural network. This en- ables the model be able to capture both in-sentence and cross-sentence information in a unified RN- N. Compared with conventional language models, our model can perceive a longer history than other language models and captures the context patterns in the previous sentences. At sentence level, we examine our model with sentence ordering task. At word level, we test the model perplexity. We also conduct a SMT rerank experiment on IWSLT 2014 data set. All these experimental results show that our hierarchical recurrent neural network has a satisfying performance.</p><p>In the future, we will explore better sentence representation such as distributed sentence repre- sentation as input for our sentence-level language model to better model document coherence. We can even update the gradient from different RNN to get a better performance. <ref type="bibr">Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Michaël Mathieu, and Yann L Cun. 2010</ref>. Learning convolutional feature hierarchies for visual recognition. In Advances in neural informa- tion processing systems, pages 1090-1098. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recurrent Neural Network for Sentencelevel Language Modeling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical recurrent neural network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU scores of SMT systems. The I-
WSLT is a public baseline which issued by the or-
ganizer of IWSLT 2014, as described in (Cettolo 
et al., 2012). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results to test BLEU in-
crease ratio after reranking 

From </table></figure>

			<note place="foot" n="4"> Sentence-level Language Model In this section, we describe how to leverage recurrent neural network for sentence-level language modeling. Mikolov et al. (2010) demonstrate a recurrent neural network language model (RNNLM)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the three anonymous reviewers for their helpful comments and suggestions. We also thank Dongdong Zhang, Lei Cui for useful discussions. This paper is supported by the project of National Natural Science Foundation of China (Grant No. 61272384, 61370170 &amp;61402134).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wit 3 : Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16 th Conference of the European Association for Machine Translation (EAMT)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in psychology</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="471" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual domain classification in spoken language understanding systems using recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word alignment modeling with context dependent deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
