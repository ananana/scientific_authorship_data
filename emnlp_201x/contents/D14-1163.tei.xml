<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
							<email>makoto-miwa@toyota-ti.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†The University of Tokyo</orgName>
								<address>
									<addrLine>3-7-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">‡Toyota Technological Institute</orgName>
								<address>
									<addrLine>2-12-1 Hisakata, Tempaku-ku</addrLine>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1544" to="1555"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel compositional language model that works on Predicate-Argument Structures (PASs). Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts. Unlike previous word-sequence-based models, our PAS-based model composes arguments into predicates by using the category information from the PAS. This enables our model to capture long-range dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations. We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results. Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus; despite this, for the subject-verb-object dataset our model improves upon the state of the art by as much as ∼10% in relative performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Studies on embedding single words in a vector space have made notable successes in capturing their syntactic and semantic properties <ref type="bibr" target="#b32">(Turney and Pantel, 2010)</ref>. These embeddings have also been found to be a useful component for Natural Language Processing (NLP) systems; for exam- ple, <ref type="bibr" target="#b31">Turian et al. (2010)</ref> and <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> demonstrated how low-dimensional word vectors learned by Neural Network Language Models (NNLMs) are beneficial for a wide range of NLP tasks.</p><p>Recently, the main focus of research on vector space representation is shifting from word repre- sentations to phrase representations ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b10">Grefenstette and Sadrzadeh, 2011;</ref><ref type="bibr" target="#b20">Mitchell and Lapata, 2010;</ref>. Combining the ideas of NNLMs and se- mantic composition, <ref type="bibr" target="#b30">Tsubaki et al. (2013)</ref> intro- duced a novel NNLM incorporating verb-object dependencies. More recently, <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> presented a NNLM that integrated syntac- tic dependencies. However, to the best of our knowledge, there is no previous work on integrat- ing a variety of syntactic and semantic dependen- cies into NNLMs in order to learn composition functions as well as word representations. The fol- lowing question thus arises naturally:</p><p>Can a variety of dependencies be used to jointly learn both stand-alone word vectors and their compositions, embedding them in the same vector space?</p><p>In this work, we bridge the gap between purely context-based ( <ref type="bibr" target="#b16">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b18">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b22">Mnih and Kavukcuoglu, 2013</ref>) and compositional ( <ref type="bibr" target="#b30">Tsubaki et al., 2013</ref>) NNLMs by using the flexible set of categories from Predicate-Argument-Structures (PASs). More specifically, we propose a Compositional Log-Bilinear Language Model using PASs (PAS- CLBLM), an overview of which is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The model is trained by maximizing the accuracy of predicting target words from their bag-of-words and dependency-based context, which provides information about selectional preference. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), one of the advantages of the PAS-CLBLM is that the model can treat not only word vectors but also composed vectors as contexts. Since the composed vectors The PAS-CLBLM predicts target words using not only context words but also composed vector representations derived from another level of predicate-argument structures. Underlined words are target words and we only depict the bag-of- words vector for the PAS-CLBLM.</p><p>are treated as input to the language model in the same way as word vectors, these composed vectors are expected to become similar to word vectors for words with similar meanings.</p><p>Our empirical results demonstrate that the pro- posed model has the ability to learn meaning- ful representations for adjective-noun, noun-noun, and (subject-) verb-object dependencies. On three tasks of measuring the semantic similarity be- tween short phrases (adjective-noun, noun-noun, and verb-object), the learned composed vectors achieve scores (Spearman's rank correlation ρ) comparable to or higher than those of previ- ous models. On a task involving more complex phrases (subject-verb-object), our learned com- posed vectors achieve state-of-the-art performance (ρ = 0.50) with a training corpus that is an order of magnitude smaller than that used by previous work <ref type="bibr" target="#b30">(Tsubaki et al., 2013;</ref><ref type="bibr" target="#b33">Van de Cruys et al., 2013)</ref>. Moreover, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large body of work on how to represent the meaning of a word in a vector space. Distri- butional approaches assume that the meaning of a word is determined by the contexts in which it appears <ref type="bibr" target="#b8">(Firth, 1957)</ref>. The context of a word is of- ten defined as the words appearing in a window of fixed-length (bag-of-words) and a simple ap- proach is to treat the co-occurrence statistics of a word w as a vector representation for w <ref type="bibr" target="#b19">(Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b20">Mitchell and Lapata, 2010)</ref>; al- ternatively, dependencies between words can be used to define contexts ( <ref type="bibr" target="#b9">Goyal et al., 2013;</ref><ref type="bibr" target="#b6">Erk and Padó, 2008;</ref><ref type="bibr" target="#b29">Thater et al., 2010)</ref>.</p><p>In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b4">Collobert et al., 2011</ref>). Recently, <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref> and <ref type="bibr" target="#b22">Mnih and Kavukcuoglu (2013)</ref> proposed highly scalable models to learn high-dimensional word vectors. <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> extended the model of <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref> by treating syntactic depen- dencies as contexts.</p><p>Mitchell and Lapata (2008) investigated a vari- ety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and mul- tiplication are now widely used to represent short phrases ( <ref type="bibr" target="#b20">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b3">Blacoe and Lapata, 2012)</ref>. The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost.</p><p>To incorporate syntactic information into com- position functions, a variety of compositional models have been proposed. These include recur- sive neural networks using phrase-structure trees <ref type="bibr" target="#b27">Socher et al., 2013b</ref>) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b10">Grefenstette and Sadrzadeh, 2011;</ref><ref type="bibr" target="#b11">Hashimoto et al., 2013;</ref><ref type="bibr">Hermann and Blunsom, 2013;</ref><ref type="bibr" target="#b26">Socher et al., 2013a</ref>).</p><p>More recently, syntactic dependency-based compositional models have been proposed <ref type="bibr" target="#b24">(Paperno et al., 2014;</ref><ref type="bibr" target="#b28">Socher et al., 2014;</ref><ref type="bibr" target="#b30">Tsubaki et al., 2013)</ref>. One of the advantages of these models is that they are less restricted by word or- der. Among these, <ref type="bibr" target="#b30">Tsubaki et al. (2013)</ref> intro- duced a novel compositional NNLM mainly fo- cusing on verb-object dependencies and achieved state-of-the-art performance for the task of mea- suring the semantic similarity between subject- verb-object phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures</head><p>In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks ( <ref type="bibr" target="#b17">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b22">Mnih and Kavukcuoglu, 2013)</ref>. More specifically, given tar- get words and their context words, the basic idea is to train classifiers to discriminate between each target word and artificially induced negative tar- get words. The feature vector of the classifiers are calculated using the context word vectors whose values are optimized during training. As a result, vectors of words in similar contexts become simi- lar to each other. Following these studies, we propose a novel model to jointly learn representations for words and their compositions by training word predic- tion classifiers using PASs. In this section, we first describe the predicate-argument structures as they serve as the basis of our model. We then introduce a Log-Bilinear Language Model us- ing Predicate-Argument Structures (PAS-LBLM) to learn word representations using both bag-of- words and dependency-based contexts. Finally, we propose integrating compositions of words into the model. <ref type="figure" target="#fig_0">Figure 1 (b)</ref> shows the overview of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predicate-Argument Structures</head><p>Due to advances in deep parsing technologies, syntactic parsers that can produce predicate- argument structures are becoming accurate and fast enough to be used for practical applications. In this work, we use the probabilistic HPSG parser Enju ( <ref type="bibr" target="#b21">Miyao and Tsujii, 2008)</ref> to obtain the predicate-argument structures of individual sen- tences. In its grammar, each word in a sentence is treated as a predicate of a certain category with zero or more arguments. One of the merits of using predicate-argument structures is that they can capture dependencies between more than two words, while standard syn- tactic dependency structures are limited to depen- dencies between two words. For example, one of the predicates in the phrase "heavy rain caused car accidents" is the verb "cause", and it has two ar- guments ("rain" and "accident"). Furthermore, ex- actly the same predicate-argument structure (pred- icate: cause, first argument: rain, second argu- ment: accident) is extracted from the passive form of the above phrase: "car accidents were caused by heavy rain". This is helpful when capturing semantic dependencies between predicates and ar- guments, and in extracting facts or relations de- scribed in a sentence, such as who did what to whom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Log-Bilinear Language Model Using</head><p>Predicate-Argument Structures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">PAS-based Word Prediction</head><p>The PAS-LBLM predicts a target word given its PAS-based context. We assume that each word w in the vocabulary V is represented with a d- dimensional vector v(w). When a predicate of category c is extracted from a sentence, the PAS- LBLM computes the predicted d-dimensional vec- tor p(w t ) for the target word w t from its context words w 1 , w 2 , . . . , w m :</p><formula xml:id="formula_0">p(w t ) = f ( m ∑ i=1 h c i ⊙ v(w i ) ) ,<label>(1)</label></formula><p>where h c i ∈ R d×1 are category-specific weight vectors and ⊙ denotes element-wise multiplica- tion. f is a non-linearity function; in this work we define f as tanh.</p><p>As an example following <ref type="figure" target="#fig_0">Figure 1</ref> (a), when the predicate "cause" is extracted with its first and second arguments "rain" and "accident", the PAS-LBLM computes p(cause) ∈ R d following Eq. <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_1">p(cause) = f (h verb arg12 arg1 ⊙ v(rain)+ h verb arg12 arg2 ⊙ v(accident)).<label>(2)</label></formula><p>In Eq. <ref type="formula" target="#formula_1">(2)</ref>, the predicate is treated as the target word, and its arguments are treated as the con- text words. In the same way, an argument can be treated as a target word:</p><formula xml:id="formula_2">p(rain) = f (h verb arg12 verb ⊙ v(cause)+ h verb arg12 arg2 ⊙ v(accident)).<label>(3)</label></formula><p>Relationship to previous work. If we omit the the category-specific weight vectors h c i in Eq. (1), our model is similar to the CBOW model in <ref type="bibr" target="#b17">Mikolov et al. (2013a)</ref>. CBOW predicts a tar- get word given its surrounding bag-of-words con- text, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of <ref type="bibr" target="#b22">Mnih and Kavukcuoglu (2013)</ref> uses different weight vec- tors depending on the position relative to the tar- get word. As with previous neural network lan- guage models <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr" target="#b13">Huang et al., 2012)</ref>, our model and vLBL can use weight ma- trices rather than weight vectors. However, as dis- cussed by <ref type="bibr" target="#b23">Mnih and Teh (2012)</ref>, using weight vec- tors makes the training significantly faster than us- ing weight matrices. Despite the simple formula- tion of the element-wise operations, the category- specific weight vectors efficiently propagate PAS- based context information as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training Word Vectors</head><p>To train the PAS-LBLM, we use a scoring function to evaluate how well the target word w t fits the given context:</p><formula xml:id="formula_3">s(w t , p(w t )) = ˜ v(w t ) T p(w t ),<label>(4)</label></formula><p>where˜vwhere˜ where˜v(w t ) ∈ R d×1 is the scoring weight vector for w t . Thus, the model parameters in the PAS- LBLM are (V, ˜ V , H). V is the set of word vec- tors v(w), and˜Vand˜ and˜V is the set of scoring weight vec- tors˜vtors˜ tors˜v(w). H is the set of the predicate-category- specific weight vectors h c i . Based on the objective in the model of <ref type="bibr" target="#b4">Collobert et al. (2011)</ref>, the model parameters are learned by minimizing the following hinge loss:</p><formula xml:id="formula_4">N ∑ n=1 max(1 − s(w t , p(w t )) + s(w n , p(w t )), 0), (5)</formula><p>where the negative sample w n is a randomly sam- pled word other than w t , and N is the number of negative samples. In our experiments we set N = 1. Following <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref>, nega- tive samples were drawn from the distribution over unigrams that we raise to the power 0.75 and then normalize to once again attain a probability distri- bution. We minimize the loss function in Eq. (5) using AdaGrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>). For further training details, see Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship to softmax regression models.</head><p>The model parameters can be learned by maximiz- ing the log probability of the target word w t based on the softmax function:</p><formula xml:id="formula_5">p(w t |context) = exp(s(w t , p(w t ))) ∑ |V| i=1 exp(s(w i , p(w t )))</formula><p>.</p><p>This is equivalent to a softmax regression model. However, when the vocabulary V is large, com- puting the softmax function in Eq. <ref type="formula" target="#formula_6">(6)</ref>  Intuition behind the PAS-LBLM. Here we briefly explain how each class of the model pa- rameters of the PAS-LBLM contributes to learning word representations at each stochastic gradient decent step. The category-specific weight vectors provide the PAS information for context word vec- tors which we would like to learn. During train- ing, context word vectors having the same PAS- based syntactic roles are updated similarly. The word-dependent scoring weight vectors propagate the information on which words should, or should not, be predicted. In effect, context word vectors making similar contributions to word predictions are updated similarly. The non-linear function f provides context words with information on the other context words in the same PAS. In this way, word vectors are expected to be learned efficiently by the PAS-LBLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Composition Functions</head><p>As explained in Section 3.1, predicate-argument structures inherently form graphs whose nodes are words in a sentence. Using the graphs, we can in- tegrate relationships between multiple predicate- argument structures into our model. When the context word w i in Eq. <ref type="formula" target="#formula_0">(1)</ref>, excluding predicate words, has another predicate-argument of category c ′ as a dependency, we replace v(w i ) with the vector produced by the composition func- tion for the predicate category c ′ . For example, as shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>, when the first argument "rain" of the predicate "cause" is also the argu- ment of the predicate "heavy", we first compute the d-dimensional composed vector representation for "heavy" and "rain":</p><formula xml:id="formula_7">g c ′ (v(heavy), v(rain)),<label>(7)</label></formula><p>where c ′ is the category adj arg1, and g c ′ is a func- tion to combine input vectors for the predicate- category c ′ . We can use any composition func- tion that produces a representation of the same dimensionality as its inputs, such as element- wise addition/multiplication (Mitchell and Lap- ata, 2008) or neural networks ( ). We then replace v(rain) in Eq. (2) with g c ′ (v(heavy), v(rain)). When the second argu- ment "accident" in Eq. <ref type="formula" target="#formula_1">(2)</ref> is also the argument of the predicate "car", v(accident) is replaced with g c ′′ (v(car), v(accident)). c ′′ is the predi- cate category noun arg1. These multiple relation- ships of predicate-argument structures should pro- vide richer context information. We refer to the PAS-LBLM with composition functions as PAS- CLBLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bag-of-Words Sensitive PAS-CLBLM</head><p>Both the PAS-LBLM and PAS-CLBLM can take meaningful relationships between words into ac- count. However, at times, the number of context words can be limited and the ability of other mod- els to take ten or more words from a fixed con- text in a bag-of-words (BoW) fashion could com- pensate for this sparseness. <ref type="bibr" target="#b13">Huang et al. (2012)</ref> combined local and global contexts in their neural network language models, and motivated by their work, we integrate bag-of-words vectors into our models. Concretely, we add an additional input term to Eq. <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_8">p(w t ) = f ( m ∑ i=1 h c i ⊙ v(w i ) + h c BoW ⊙ v(BoW) ) ,<label>(8)</label></formula><p>where h c BoW ∈ R d×1 are additional weight vec- tors, and v(BoW) ∈ R d×1 is the average of the word vectors in the same sentence. To construct the v(BoW) for each sentence, we average the word vectors of nouns and verbs in the same sen- tence, excluding the target and context words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Corpus</head><p>We used the British National Corpus (BNC) as our training corpus, extracted 6 million sentences from the original BNC files, and parsed them using the Enju parser described in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Sense Disambiguation Using Part-of-Speech Tags</head><p>In general, words can have multiple syntactic us- ages. For example, the word cause can be a noun or a verb depending on its context. Most of the previous work on learning word vectors ignores this ambiguity since word sense disam- biguation could potentially be performed after the word vectors have been trained ( <ref type="bibr" target="#b13">Huang et al., 2012;</ref>. Some re- cent work explicitly assigns an independent vec- tor for each word usage according to its part-of- speech (POS) tag ( <ref type="bibr" target="#b11">Hashimoto et al., 2013;</ref>. Alternatively, Baroni and Zamparelli (2010) assigned different forms of parameters to adjectives and nouns.</p><p>In our experiments, we combined each word with its corresponding POS tags. We used the base-forms provided by the Enju parser rather than the surface-forms, and used the first two charac- ters of the POS tags. For example, VB, VBP, VBZ, VBG, VBD, VBN were all mapped to VB. This resulted in two kinds of cause: cause NN and cause VB and we used the 100,000 most frequent lowercased word-POS pairs in the BNC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Selection of Training Samples Based on Categories of Predicates</head><p>To train the PAS-LBLM and PAS-CLBLM, we could use all predicate categories. However, our preliminary experiments showed that these cate- gories covered many training samples which are not directly relevant to our experimental setting, such as determiner-noun dependencies. We thus manually selected the categories used in our ex- periments. The selected predicates are listed in <ref type="table" target="#tab_0">Table 1</ref>: adj arg1, noun arg1, prep arg12, and verb arg12. These categories should provide meaningful information on selectional preference. For example, the prep arg12 denotes prepositions with two arguments, such as "eat at restaurant" which means that the verb "eat" is related to the noun "restaurant" by the preposition "at". Prepo- sitions are one of the predicates whose arguments can be verbs, and thus prepositions are important in training the composition functions for (subject-) verb-object dependencies as described in the next paragraph.</p><p>Another point we had to consider was how to construct the training samples for the PAS- CLBLM. We constructed compositional training samples as explained in Section 3.3 when c ′ was adj arg1, noun arg1, or verb arg12. <ref type="figure" target="#fig_1">Figure 2</ref> shows two examples in addition to the example in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. Using such training samples, the PAS-CLBLM could, for example, recognize from the two predicate-argument structures, "eat food" and "eat at restaurant", that eating foods is an ac- tion that occurs at restaurants. <ref type="table" target="#tab_2">Table 2</ref>: Composition functions used in this work. The examples are shown as the adjective-noun de- pendency between w 1 ="heavy" and w 2 ="rain".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Composition Function Add</head><formula xml:id="formula_9">l v(w 1 ) + v(w 2 ) Add nl tanh(v(w 1 ) + v(w 2 )) Wadd l m c adj ⊙ v(w 1 ) + m c arg1 ⊙ v(w 2 ) Wadd nl tanh(m c adj ⊙ v(w 1 ) + m c arg1 ⊙ v(w 2 ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Selection of Composition Functions</head><p>As described in Section 3.3, we are free to se- lect any composition functions in Eq. <ref type="formula" target="#formula_7">(7)</ref>. To maintain the fast training speed of the PAS- LBLM, we avoid dense matrix-vector multiplica- tion in our composition functions. In <ref type="table" target="#tab_2">Table 2</ref>, we list the composition functions used for the PAS-CLBLM. Add l is element-wise addition and Add nl is element-wise addition with the non- linear function tanh. The subscripts l and nl de- note the words linear and non-linear. Similarly, Wadd l is element-wise weighted addition and Wadd nl is element-wise weighted addition with the non-linear function tanh. The weight vec- tors m c i ∈ R d×1 in <ref type="table" target="#tab_2">Table 2</ref> are predicate-category- specific parameters which are learned during train- ing. We investigate the effects of the non-linear function tanh for these composition functions. In the formulations of the backpropagation algo- rithm, non-linear functions allow the input vectors to weakly interact with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Initialization and Optimization of Model Parameters</head><p>We assigned a 50-dimensional vector for each word-POS pair described in Section 4.2 and ini- tialized the vectors and the scoring weight vec- tors using small random values. In part inspired by the initialization method of the weight matrices in <ref type="bibr" target="#b26">Socher et al. (2013a)</ref>, we initialized all values in the compositional weight vectors of the Wadd l and Wadd nl as 1.0. The context weight vectors were initialized using small random values. We minimized the loss function in Eq. (5) us- ing mini-batch SGD and AdaGrad <ref type="bibr" target="#b5">(Duchi et al., 2011</ref>). Using AdaGrad, the SGD's learning rate is adapted independently for each model parame- ter. This is helpful in training the PAS-LBLM and PAS-CLBLM, as they have conditionally depen- dent model parameters with varying frequencies.</p><p>The mini-batch size was 32 and the learning rate was 0.05 for each experiment, and no regulariza- tion was used. To verify the semantics captured by the proposed models during training and to tune the hyperparameters, we used the WordSim-353 2 word similarity data set ( <ref type="bibr" target="#b7">Finkelstein et al., 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on Phrase Similarity Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Settings</head><p>The learned models were evaluated on four tasks of measuring the semantic similarity between short phrases. We performed evaluation using the three tasks (AN, NN, and VO) in the dataset 3 pro- vided by <ref type="bibr" target="#b20">Mitchell and Lapata (2010)</ref>, and the SVO task in the dataset <ref type="bibr">4</ref> provided by <ref type="bibr" target="#b10">Grefenstette and Sadrzadeh (2011)</ref>.</p><p>The datasets include pairs of short phrases ex- tracted from the BNC. AN, NN, and VO con- tain 108 phrase pairs of adjective-noun, noun- noun, and verb-object. SVO contains 200 pairs of subject-verb-object phrases. Each phrase pair has multiple human-ratings: the higher the rating is, the more semantically similar the phrases. For ex- ample, the subject-verb-object phrase pair of "stu- dent write name" and "student spell name" has a high rating. The pair "people try door" and "peo- ple judge door" has a low rating.</p><p>For evaluation we used the Spearman's rank correlation ρ between the human-ratings and the cosine similarity between the composed vector pairs. We mainly used non-averaged human- ratings for each pair, and as described in Section 5.3, we also used averaged human-ratings for the SVO task. Each phrase pair in the datasets was an- notated by more than two annotators. In the case of averaged human ratings, we averaged multiple human-ratings for each phrase pair, and in the case of non-averaged human-ratings, we treated each human-rating as a separate annotation.</p><p>With the PAS-CLBLM, we represented each phrase using the composition functions listed in n/a n/a 0.45 K w/ BNC n/a n/a 0.41 Human agreement 0.52 0.49 0.55 the composed vector for each phrase was com- puted using the Wadd nl function, and when we trained the PAS-LBLM, we used the element-wise addition function. To compute the composed vec- tors using the Wadd l and Wadd nl functions, we used the categories of the predicates adj arg1, noun arg1, and verb arg12 listed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>As a strong baseline, we trained the Skip-gram model of <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref> using the pub- licly available word2vec 5 software. We fed the POS-tagged BNC into word2vec since our models utilize POS tags and trained 50-dimensional word vectors using word2vec. For each phrase we then computed the representation using vector addition. <ref type="table" target="#tab_3">Table 3</ref> shows the correlation scores ρ for the AN, NN, and VO tasks. Human agreement denotes the inter-annotator agreement. The word2vec baseline achieves unexpectedly high scores for these three tasks. Previously these kinds of models ( <ref type="bibr" target="#b18">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b22">Mnih and Kavukcuoglu, 2013</ref>) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic sim- ilarity between phrases. However, this experimen- tal result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addi- tion to word analogy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">AN, NN, and VO Tasks</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, BL, HB, KS, and K denote the work of <ref type="bibr" target="#b3">Blacoe and Lapata (2012)</ref>, <ref type="bibr">Hermann and Blunsom (2013)</ref>, , and  respectively. Among these,</p><formula xml:id="formula_10">Averaged Non-averaged Model Corpus SVO-SVO SVO-V SVO-SVO SVO-V PAS-CLBLM (Add l ) 0.29 0.34 0.24 0.28 PAS-CLBLM (Add nl ) 0.27 0.32 0.24 0.28 PAS-CLBLM (Wadd l ) BNC 0.25 0.26 0.21 0.23 PAS-CLBLM (Wadd nl ) 0.42</formula><note type="other">0.50 0.34 0.41 PAS-LBLM 0.21 0.06 0.18 0.08 word2vec BNC 0.12 0.32 0.12 0.28 Grefenstette and Sadrzadeh (2011) BNC</note><p>n/a n/a 0.21 n/a <ref type="bibr" target="#b30">Tsubaki et al. (2013)</ref> ukWaC n/a 0.47 n/a n/a Van de Cruys et al. <ref type="formula" target="#formula_0">(2013)</ref> ukWaC n/a n/a 0.32 0.37 Human agreement 0.75 0.62 <ref type="table" target="#tab_4">Table 4</ref>: Spearman's rank correlation scores ρ for the SVO task. Averaged denotes the ρ calculated by averaged human ratings, and Non-averaged denotes the ρ calculated by non-averaged human ratings.</p><p>only  used the ukWaC corpus ( <ref type="bibr" target="#b1">Baroni et al., 2009)</ref> which is an or- der of magnitude larger than the BNC. As we can see in <ref type="table" target="#tab_3">Table 3</ref>, the PAS-CLBLM (Add nl ) achieves scores comparable to and higher than those of the baseline and the previous state-of-the-art results. In relation to these results, the Wadd l and Wadd nl variants of the PAS-CLBLM do not achieve great improvements in performance. This indicates that simple word vector addition can be sufficient to compose representations for phrases consisting of word pairs. <ref type="table" target="#tab_4">Table 4</ref> shows the correlation scores ρ for the SVO task. The scores ρ for this task are reported for both averaged and non-averaged human ratings. This is due to a disagreement in previous work regarding which metric to use when reporting re- sults. Hence, we report the scores for both settings in  <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SVO Task</head><p>The results show that the weighted addition model with the non-linear function tanh (PAS- CLBLM (Wadd nl )) is effective for the more com- plex phrase task. While simple vector addition is sufficient for phrases consisting of word pairs, it is clear from our experimental results that they fall short for more complex structures such as those involved in the SVO task.</p><p>Our PAS-CLBLM (Wadd nl ) model outperforms the previous state-of-the-art scores for the SVO task as reported by <ref type="bibr" target="#b30">Tsubaki et al. (2013)</ref> and Van de <ref type="bibr" target="#b33">Cruys et al. (2013)</ref>. As such, there are three key points that we would like to emphasize:</p><p>(1) the difference of the training corpus size, (2) the necessity of the pre-trained word vectors, (3) the modularity of deep learning models.  <ref type="formula" target="#formula_0">(2013)</ref> is based on neural network language models which use syn- tactic dependencies between verbs and their ob- jects. While their novel model, which incorpo- rates the idea of co-compositionality, works well with pre-trained word vectors produced by exter- nal models, it is not clear whether the pre-trained vectors are required to achieve high scores. In contrast, we have achieved state-of-the-art results without the use of pre-trained word vectors.</p><p>Despite our model's scalability, we trained 50- dimensional vector representations for words and their composition functions and achieved high scores using this low dimensional vector space. model d AN NN VO SVO Add l 50 0.52 0.44 0.35 0.24 1000 0.51 0.51 0.43 0.31 Add nl 50 0.52 0.46 0.45 0.24 1000 0.51 0.50 0.45 0.31 Wadd l 50 0.48 0.39 0.34 0.21 1000 0.50 0.49 0.43 0.32 Wadd nl 50 0.48 0.40 0.39 0.34 1000 0.51 0.48 0.48 0.34 <ref type="table">Table 5</ref>: Comparison of the PAS-CLBLM between d = 50 and d = 1000.</p><p>This maintains the possibility to incorporate re- cently developed deep learning composition func- tions into our models, such as recursive neural tensor networks <ref type="bibr" target="#b27">(Socher et al., 2013b</ref>) and co- compositional neural networks ( <ref type="bibr" target="#b30">Tsubaki et al., 2013)</ref>. While such complex composition functions slow down the training of compositional models, richer information could be captured during train- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effects of the Dimensionality</head><p>To see how the dimensionality of the word vectors affects the scores, we trained the PAS-CLBLM for each setting using 1,000-dimensional word vectors and set the learning rate to 0.01. <ref type="table">Table 5</ref> shows the scores for all four tasks. Note that we only re- port the scores for the setting non-averaged SVO- SVO here. As shown in <ref type="table">Table 5</ref>, the scores consis- tently improved with a few exceptions. The scores ρ = 0.51 for the NN task and ρ = 0.48 for the VO task are the best results to date. However, the score ρ = 0.34 for the SVO task did not improve by increasing the dimensionality. This means that simply increasing the dimensionality of the word vectors does not necessarily lead to better results for complex phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effects of Bag-of-Words Contexts</head><p>Lastly, we trained the PAS-CLBLM without the bag-of-words contexts described in Section 3.4 and used 50-dimensional word vectors. As can be seen in <ref type="table">Table 6</ref>, large score improvements were observed only for the VO and SVO tasks by in- cluding the bag-of-words contexts and the non- linearity function. It is likely that the results de- pend on how the bag-of-words contexts are con- structed. However, we leave this line of analysis as future work. Both adjective-noun and noun-  <ref type="table">Table 6</ref>: Scores of the PAS-CLBLM with and without BoW contexts. noun phrase are noun phrases, and (subject-) verb- object phrases can be regarded as complete sen- tences. Therefore, different kinds of context infor- mation might be required for both groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Analysis on Composed Vectors</head><p>An open question that remains is to what ex- tent composition affects the representations pro- duced by our PAS-CLBLM model. To evalu- ate this we assigned a vector for each composed representation. For example, the adjective-noun dependency "heavy rain" would be assigned an independent vector. We added the most fre- quent 100,000 adjective-noun, noun-noun, and (subject-) verb-object tuples to the vocabulary and the resulting vocabulary contained 400,000 to- kens (100,000+3×100,000). A similar method for treating frequent neighboring words as single words was introduced by <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref>. However, some dependencies, such as (subject-) verb-object phrases, are not always captured when considering only neighboring words. <ref type="table" target="#tab_7">Table 7</ref> (No composition) shows some examples of predicate-argument dependencies with their closest neighbors in the vector space according to the cosine similarity. The table shows that the learned vectors of multiple words capture seman- tic similarity. For example, the vector of "heavy rain" is close to the vectors of words which ex- press the phenomena heavily raining. The vector of "new york" captures the concept of a major city. The vectors of (subject-) verb-object dependencies also capture the semantic similarity, which is the main difference to previous approaches, such as that of <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref>, which only consider neighboring words. These results suggest that the PAS-CLBLM can learn meaningful composition <ref type="table">Query   No composition  Composition  rain  rain  (AN)  thunderstorm  sunshine  heavy  downpour  storm  rain  blizzard  drizzle  much rain  chill  general manager  executive  (AN)  vice</ref>    functions since the composition layers receive the same error signal via backpropagation.</p><p>We then trained the PAS-CLBLM using Wadd nl to learn composition functions. <ref type="table" target="#tab_7">Table 7</ref> (Compo- sition) shows the nearest neighbor words for each composed vector, and as we can see, the learned composition function emphasizes the head words and captures some sort of semantic similarity. We then inspected the L2-norms of the weight vectors of the composition function. As shown in <ref type="table" target="#tab_8">Table 8</ref>, head words are strongly emphasized. Emphasiz- ing head words is helpful in representing com- posed meanings, but in the case of verbs it may not always be sufficient. This can be observed in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, which demonstrates that verb- related tasks are more difficult than noun-phrase tasks.</p><p>While No composition captures the seman- tic similarity well using independent parameters, there is the issue of data sparseness. As the size of the vocabulary increases, the number of tuples of word dependencies increases rapidly. In this ex- periment, we used only the 300,000 most frequent tuples. In contrast to this, the learned composi- tion functions can capture similar information us- ing only word vectors and a small set of predicate categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented a compositional log-bilinear language model using predicate-argument struc- tures that incorporates both bag-of-words and dependency-based contexts. In our experiments the learned composed vectors achieve state-of-the- art scores for the task of measuring the semantic similarity between short phrases. For the subject- verb-object phrase task, the result is achieved without any pre-trained word vectors using a cor- pus an order of magnitude smaller than that of the previous state of the art. For future work, we will investigate how our models and the resulting vec- tor representations can be helpful for other unsu- pervised and/or supervised tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the proposed model: PAS-CLBLM. (a) The PAS-LBLM predicts target words from their bag-of-words and dependency-based context words. (b) The PAS-CLBLM predicts target words using not only context words but also composed vector representations derived from another level of predicate-argument structures. Underlined words are target words and we only depict the bag-ofwords vector for the PAS-CLBLM.</figDesc><graphic url="image-1.png" coords="2,72.14,62.77,453.46,124.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two PAS-CLBLM training samples.</figDesc><graphic url="image-2.png" coords="6,76.39,62.77,209.69,64.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Tsubaki et al. (2013) and Van de Cruys et al. (2013) used the ukWaC corpus. This means our model works better, despite using a considerably smaller corpora. It should also be noted that, like us, Grefenstette and Sadrzadeh (2011) used the BNC corpus. The model of Tsubaki et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 shows some ex-</head><label>1</label><figDesc></figDesc><table>Category 
predicate arg1 
arg2 
adj arg1 
heavy 
rain 
noun arg1 car 
accident 
verb arg12 cause 
rain 
accident 
prep arg12 at 
eat 
restaurant 

Table 1: Examples of predicates of different cate-
gories from the grammar of the Enju parser. arg1 
and arg2 denote the first and second arguments. 

amples of predicates of different categories. 1 For 
example, a predicate of the category verb arg12 
expresses a verb with two arguments. A graph can 
be constructed by connecting words in predicate-
argument structures in a sentence; in general, these 
graphs are acyclic. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>When there was no composition present, we represented the phrase using element-wise ad- dition. For example, when we trained the PAS- CLBLM with the composition function Wadd nl ,</figDesc><table>Model 
AN NN VO 
PAS-CLBLM (Add l ) 
0.52 0.44 0.35 
PAS-CLBLM (Add nl ) 
0.52 0.46 0.45 
PAS-CLBLM (Wadd l ) 
0.48 0.39 0.34 
PAS-CLBLM (Wadd nl ) 0.48 0.40 0.39 
PAS-LBLM 
0.41 0.44 0.39 
word2vec 
0.52 0.48 0.42 
BL w/ BNC 
0.48 0.50 0.35 
HB w/ BNC 
0.41 0.44 0.34 
KS w/ ukWaC 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spearman's rank correlation scores ρ for 
the three tasks: AN, NN, and VO. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .SVO-SVO and SVO-V in</head><label>4</label><figDesc></figDesc><table>Another point we should consider is 
that some previous work reported scores based on 
the similarity between composed representations 
(Grefenstette and Sadrzadeh, 2011; Van de Cruys 
et al., 2013), and others reported scores based on 
the similarity between composed representations 
and word representations of landmark verbs from 
the dataset (Tsubaki et al., 2013; Van de Cruys et 
al., 2013). For completeness, we report the scores 
for both settings: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Nearest neighbor vectors for multiple 
words. POS-tags are not shown for simplicity. 

category 
predicate arg1 arg2 
adj arg1 
2.38 
6.55 
-
noun arg1 
3.37 
5.60 
-
verb arg12 
6.78 
2.57 2.18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>L2-norms of the 50-dimensional weight 
vectors of the composition function Wadd nl . 

</table></figure>

			<note place="foot" n="1"> The categories of the predicates in the Enju parser are summarized at http://kmcs.nii.ac.jp/ ˜ yusuke/ enju/enju-manual/enju-output-spec.html.</note>

			<note place="foot" n="2"> http://www.cs.technion.ac.il/ ˜ gabr/ resources/data/wordsim353/ 3 http://homepages.inf.ed.ac.uk/ s0453356/share 4 http://www.cs.ox.ac.uk/activities/ compdistmeaning/GS2011data.txt</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their help-ful comments and suggestions. This work was supported by JSPS KAKENHI Grant Number 13F03041.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Comparison of Vector-based Representations for Semantic Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Structured Vector Space Model for Word Meaning in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Placing Search in Context: The Concept Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrilovich</forename><surname>Evgenly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Yossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rivlin</forename><surname>Ehud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solan</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfman</forename><surname>Gadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruppin</forename><surname>Eytan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International World Wide Web Conference</title>
		<meeting>the Tenth International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 1930-55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis</title>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Structured Distributional Semantic Model : Integrating Structure with Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experimental Support for a Categorical Compositional Distributional Model of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple Customization of Recursive Neural Networks for Semantic Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Role of Syntax in Vector Space Models of Compositional Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving Word Representations via Global Context and Multiple Word Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prior Disambiguation of Word Tensors for Constructing Sentence Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1590" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Separating Disambiguation from Composition in Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th Conference on Natural Language Learning (CoNLL)</title>
		<meeting>17th Conference on Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at the International Conference on Learning Representations</title>
		<meeting>Workshop at the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composition in Distributional Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature forest models for probabilistic HPSG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML &apos;12</title>
		<editor>John Langford and Joelle Pineau</editor>
		<meeting>the 29th International Conference on Machine Learning (ICML-12), ICML &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsing with Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="948" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Tensor-based Factorization Model of Semantic Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1142" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
