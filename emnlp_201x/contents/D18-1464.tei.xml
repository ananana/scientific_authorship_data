<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Local Coherence Model for Text Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Heidelberg Institute for Theoretical Studies (HITS) and Research Training Group AIPHES</orgName>
								<orgName type="laboratory" key="lab2">Heidelberg Institute for Theoretical Studies (HITS)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
							<email>michael.strube@h-its.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Heidelberg Institute for Theoretical Studies (HITS) and Research Training Group AIPHES</orgName>
								<orgName type="laboratory" key="lab2">Heidelberg Institute for Theoretical Studies (HITS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Local Coherence Model for Text Quality Assessment</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">4328</biblScope>
							<biblScope unit="page" from="4328" to="4339"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text. We represent the semantics of a sentence by a vector and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other task-dependent features significantly improves the performance of a strong essay scorer.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coherence is a key factor that distinguishes well-written texts from random collections of sen- tences. A potential application of coherence mod- els is text quality assessment. Examples include readability assessment <ref type="bibr" target="#b35">(Pitler and Nenkova, 2008;</ref><ref type="bibr" target="#b25">Li and Hovy, 2014</ref>) and essay scoring ( <ref type="bibr" target="#b32">Miltsakaki and Kukich, 2004;</ref><ref type="bibr" target="#b5">Burstein et al., 2010)</ref>. Here, we address the problem of local coherence model- ing, which captures text relatedness at the level of sentence-to-sentence transitions.</p><p>Several approaches to local coherence model- ing have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b11">Elsner and Charniak, 2011</ref>; Guinaudeau and Strube, * This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit√§t Darmstadt, https://www.ukp.tu-darmstadt. <ref type="bibr">de. 2013;</ref><ref type="bibr" target="#b40">Tien Nguyen and Joty, 2017)</ref>. Lexical mod- els connect sentences based on semantic relations between words in sentences (Beigman <ref type="bibr" target="#b4">Klebanov and Shamir, 2006;</ref><ref type="bibr" target="#b16">Heilman et al., 2007;</ref><ref type="bibr" target="#b31">Mesgar and Strube, 2016)</ref>. Both of these approaches suf- fer from different weaknesses. The entity-based models require an entity detection system, a coref- erence model, and a syntactic parser. These subsystems need to be perfect to gain the best performance of entity-based coherence models. The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear. More concretely, such lexical models take sentences as a bag of words. Recent deep learning coherence work ( <ref type="bibr" target="#b25">Li and Hovy, 2014;</ref><ref type="bibr" target="#b26">Li and Jurafsky, 2017</ref>) adopts recursive and recurrent neural networks for com- puting semantic vectors for sentences. Coherence models that use recursive neural networks suffer from a severe dependence on external resources, e.g. a syntactic parser to construct their recursion structure. Coherence models that purely rely on the recurrent neural networks process words se- quentially within a text. However, in such models, long-distance dependencies between words can- not be captured effectively due to the limits of the memorization capability of recurrent networks.</p><p>Our motivation is to overcome these limitations. We use the advantages of distributional represen- tations in order to, first, identify and represent salient semantic information that connects sen- tences, and second, extract patterns of changes in such information as a text progresses. By repre- senting words of sentences with their pre-trained embeddings, we take lexical semantic relations be- tween words into account. We employ a Recur- rent Neural Network (RNN) layer to combine in- formation in word embeddings and actual context information of words in sentences. Our model encodes salient information that relates two adja-cent sentences based on the two most similar RNN states in sentences. We accumulate two identified RNN states to represent semantic information that connects two adjacent sentences. We encode pat- tern of semantic information changes across sen- tences in a text by a convolutional neural network to represent coherence. Our end-to-end coherence model is superior to previous work because it re- lates sentences based on two semantic information states in sentences that are highly similar. So it does not need extra tools such as coreference res- olution systems. Furthermore, our model incorpo- rates words in their sentence context and models (roughly) distant relations between words.</p><p>We evaluate our model on two tasks: readabil- ity assessment and essay scoring. Both have been frequently used for coherence evaluation ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b32">Miltsakaki and Kukich, 2004</ref>). Readability assessment is a ranking task where we compare the rankings given by the model against human judgments. Essay scoring is a regression task, in which we investigate if the combination of coherence vectors produced by our model and other essay scoring features proposed by <ref type="bibr" target="#b34">Phandi et al. (2015)</ref> improves the performance of the es- say scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset <ref type="bibr" target="#b8">(De Clercq and Hoste, 2016)</ref>; and the combination of our coherence features with other essay scoring features significantly improves the performance of the examined essay scorer ( <ref type="bibr" target="#b34">Phandi et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early work on coherence captures different types of relations: entity-based ( <ref type="bibr" target="#b14">Grosz et al., 1995;</ref><ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, lexical-based <ref type="bibr" target="#b3">(Beigman Klebanov and Flor, 2013;</ref><ref type="bibr" target="#b38">Somasundaran et al., 2014;</ref><ref type="bibr" target="#b46">Zhang et al., 2015)</ref>, etc. Among these models, the entity-grid model ( <ref type="bibr">Lapata, 2005, 2008)</ref> has received a lot of attention. In this model, entities are defined, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, defines all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its en- tity extraction phase ( <ref type="bibr" target="#b11">Elsner and Charniak, 2011;</ref><ref type="bibr" target="#b13">Feng and Hirst, 2012)</ref> and by defining other types of patterns ( <ref type="bibr" target="#b27">Lin et al., 2011;</ref><ref type="bibr" target="#b28">Louis and Nenkova, 2012;</ref><ref type="bibr" target="#b18">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b15">Guinaudeau and Strube, 2013)</ref>. Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predefined coher- ence patterns and extract patterns automatically. However, all of these models limit relations be- tween sentences to entities that are shared by sen- tences. This makes the performance of these mod- els dependent on the performance of other tools like coreference resolution systems and syntac- tic parsers. Our coherence model, in contrast, is based on relations between any embedded seman- tic information in sentences, and does not require entity annotations. A similar approach to ours is proposed by <ref type="bibr" target="#b31">Mesgar and Strube (2016)</ref>. Their approach encodes lexical relations between sen- tences in a text via a graph. Sentences are en- coded by nodes, and lexical semantic relations be- tween sentences are represented by edges. Coher- ence patterns are obtained by applying a subgraph mining method to graph representations of all texts in a corpus. This model involves words individ- ually and independent of their sentence context. Our model uses a RNN layer over words in sen- tences to incorporate context information. Our ap- proach for extracting coherence patterns also dif- fers from this model as we employ CNNs rather than graph mining. <ref type="bibr" target="#b25">Li and Hovy (2014)</ref> model sentences as vectors derived from RNNs and train a feed-forward neural network that takes an input window of sentence vectors and assigns a proba- bility which represents the coherence of the sen- tences in the window. Text coherence is evalu- ated by sliding the window over sentences and ag- gregating their coherence probabilities. Similarly, <ref type="bibr" target="#b26">Li and Jurafsky (2017)</ref> study the same model at a larger scale and use a sequence-to-sequence ap- proach in which the model is trained to generate the next sentence given the current sentence and vice versa. Our approach differs from these meth- ods; we represent coherence by a vector of co- herence patterns. Moreover, our model takes dis- tant relations between words in a text into account by relating two semantic states of sentences that are highly similar. <ref type="bibr" target="#b23">Lai and Tetreault (2018)</ref> com- pare the performance of the aforementioned co- herence models on texts from different domains. They conclude that the neural coherence models, which are explained above, surpass examined non- neural coherence models such as the entity-based models and the lexical-based model. Unlike their evaluation method, which predicts the coherence level of a text, we rank two texts with respect to their coherence levels for the readability assess- ment task. We also show that integrating our co- herence model into an essay scorer improves its performance.</p><p>An important task for evaluating a coherence model is readability assessment ( <ref type="bibr" target="#b25">Li and Hovy, 2014;</ref><ref type="bibr" target="#b33">Petersen et al., 2015;</ref><ref type="bibr" target="#b41">Todirascu et al., 2016)</ref>. The more coherent a text, the faster to read and easier to understand it is. Early readability formu- las were based on superficial text features such as average word lengths ( <ref type="bibr" target="#b22">Kincaid et al., 1975</ref>). These formulas systematically ignore many important factors that affect readability such as discourse co- herence ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>. <ref type="bibr" target="#b37">Schwarm and Ostendorf (2005)</ref> and <ref type="bibr" target="#b12">Feng et al. (2010)</ref> re- cast readability assessment as a ranking task, and employ different semantic (e.g. language model perplexity scores) and syntactic (e.g. the average number of NPs) features to solve this task. <ref type="bibr" target="#b35">Pitler and Nenkova (2008)</ref> show that discourse coher- ence features are more informative than other fea- tures for ranking texts with respect to their read- ability. Following the related work on coherence modeling ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b30">Mesgar and Strube, 2015)</ref>, we evaluate our coherence model on this task.</p><p>Another popular task for evaluating coherence models is essay scoring <ref type="bibr" target="#b3">(Beigman Klebanov and Flor, 2013;</ref><ref type="bibr" target="#b38">Somasundaran et al., 2014</ref>). Milt- sakaki and <ref type="bibr" target="#b32">Kukich (2004)</ref> employ an essay scoring system to examine whether local coherence fea- tures, as defined by a measure of Centering The- ory's Rough-Shift transitions ( <ref type="bibr" target="#b14">Grosz et al., 1995)</ref>, might be a significant contributor to the evaluation of essays. They show that adding such features to their essay scorer improves its performance sig- nificantly. <ref type="bibr" target="#b5">Burstein et al. (2010)</ref> specifically fo- cus on the impact of entity transition features, as proposed by the entity-grid model for coherence modeling, on the essay scoring task. They demon- strate that by combining these features with other features related to grammar errors and word usage, the performance of their automated essay scoring system improves. Likewise, we combine our co- herence vectors with other features that are used by a strong essay scorer ( <ref type="bibr" target="#b34">Phandi et al., 2015)</ref> and show that our coherence vectors improve the per- formance of this system significantly. </p><formula xml:id="formula_0">e0 e1 e2 E0 e3 e4 e5 E1 e6 e7 e8 E2 e9 e10 e12 E3 e13 e14 e15 E4 s 0 s 1 s 2 s 3 s 4 LOOKUP LOOKUP LOOKUP LOOKUP LOOKUP LSTM LSTM LSTM LSTM LSTM h 0 0 h 1 0 h 2 0 h 0 1 h 1 1 h 2 1 h 0 2 h 1 2 h 2 2 h 0 3 h 1 3 h 2 3 h 0 4 h 1 4 h 2 4 H0 H1 H2 H3 H4 ÔøΩ f1 ÔøΩ f2 ÔøΩ f3 ÔøΩ f4 d 23 CNN ÔøΩ p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coherence Model</head><p>In this section, we describe details of our model. First, we explain how we encode words in their context (Section 3.1). Then we show how we re- late sentences (Section 3.2), and finally we explain how we represent coherence based on sentence re- lations (Section 3.3). A general formulation of our model is a parametric function,</p><formula xml:id="formula_1">ÔøΩ p = L Œ∏ (d),</formula><p>where d is an input document, Œ∏ indicates parameters of neural modules, and ÔøΩ p is a vector representation for the coherence of d. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word and Context Representations</head><p>We use a lookup table to associate all words in the vocabulary with word embeddings. The lookup ta- ble is initialized by existing pre-trained word em- beddings because they capture lexical semantic re-lations between words. For sentence s i , the lookup table returns matrix E i whose rows are embed- dings of words in s i . A weakness of former lexi- cal coherence models ( <ref type="bibr" target="#b38">Somasundaran et al., 2014;</ref><ref type="bibr" target="#b31">Mesgar and Strube, 2016</ref>) is that they only rely on semantic relations between words in sentences, regardless of the current context of words. In or- der to overcome this limitation, we use a standard unidirectional 1 RNN with Long Short-Term Mem- ory (LSTM) cells to encode the current context of words in sentences. For embedding matrix E i :</p><formula xml:id="formula_2">H i = LSTM ÔøΩ E i , h n‚àí1 i‚àí1 ÔøΩ ,</formula><p>where H i is a list of LSTM states, and h n‚àí1 i‚àí1 is the last LSTM state of sentence s i‚àí1 . Parameter n is the number of words in a sentence. We take state vector h j i ‚àà H i as a representation of its in- put word embedding, e j , that is combined with its preceding word vectors in sentence s i . For sake of brevity, the details of LSTM formulations are ex- plained in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Relation Representations</head><p>The relation between sentences is encoded by the most similar semantic states of sentences. Given two adjacent sentences, two of their LSTM states that have the highest similarity are selected to con- nect them. Those LSTM states refer to the salient semantic information that is shared between sen- tences. To model this, we follow attention compo- nents in neural language models ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b42">Vaswani et al., 2017)</ref> where the similarity between the last LSTM state and each of its pre- ceding states is computed to measure the amount of attention that the model should give to its pre- ceding context for generating the next word. More formally, for two adjacent sentences s i and s i‚àí1 , one LSTM state in H i and one LSTM state in H i‚àí1 that have the maximum similarity are se- lected to represent the relation between the sen- tences:</p><formula xml:id="formula_3">(ÔøΩ u, ÔøΩ v) = argmax ( ÔøΩ hm‚ààH i ) ( ÔøΩ hn‚ààH i‚àí1 ) (sim( ÔøΩ h m , ÔøΩ h n )),</formula><p>where H i and H i‚àí1 are LSTM states correspond- ing to sentences s i and s i‚àí1 . The similarity func- tion, sim, returns the absolute value of the dot product between input vectors,</p><formula xml:id="formula_4">sim( ÔøΩ h m , ÔøΩ h n ) = | ÔøΩ h m ¬∑ ÔøΩ h n |,<label>(1)</label></formula><p>where the function |.| computes the absolute value of its input 2 . We use the dot product function be- cause in practice it enables our model to calculate the above equations efficiently in parallel and in matrix-space, i.e., directly on H i and H i‚àí1 . Since this is the details of implementation, we explain matrix-based equations in Appendix B. The abso- lute value in the similarity function is used to en- code semantic relatedness between associated in- formation with vectors, which is independent of the sign of the similarity function <ref type="bibr" target="#b29">(Manning and Sch√ºtze, 1999</ref>). We represent semantic information that relates two adjacent sentences by accumulating its se- lected LSTM states in the corresponding sen- tences. Since averaging in the vector space is an effective way to accumulate information rep- resented in some vectors ( <ref type="bibr" target="#b17">Iyyer et al., 2015;</ref><ref type="bibr" target="#b43">Wieting et al., 2016)</ref>, we compute the average of two identified vectors among the LSTM states of two adjacent sentences to represent semantic informa- tion shared by the sentences. More concretely, the vector representation of what relates sentence s i to its immediately preceding sentence is obtained by averaging a vector of H i and a vector of H i‚àí1 that are identified as highly similar:</p><formula xml:id="formula_5">ÔøΩ f i = avg(ÔøΩ u, ÔøΩ v) = ÔøΩ u + ÔøΩ v 2 ,</formula><p>where ÔøΩ u and ÔøΩ v are selected vectors. ÔøΩ f i is the vector representation of what connects s i to its immedi- ately preceding sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coherence Representations</head><p>Since sentences in a coherent text are about sim- ilar topics and share some semantic information, we compute semantic similarity between adjacent information states, i.e. ÔøΩ f i s, to capture how they are changing through a text. We propose to encode changes by a continuous value between 0 and 1, where 1 shows that there is no change and 0 in- dicates that there is a big semantic drift in a text. Any value in between depicts how far a text is se- mantically changing. Given two adjacent vectors ÔøΩ f i and ÔøΩ f i+1 , the degree of continuity between them is:</p><formula xml:id="formula_6">d = sim( ÔøΩ f i , ÔøΩ f i+1 ) l ,</formula><p>where l is the length of input vectors, which is used to prevent large numbers ( <ref type="bibr" target="#b42">Vaswani et al., 2017)</ref>, and sim is the similarity function (Sec- tion 3.2). The task of this layer is to check if the salient information that is shared by two adjacent sentences is salient in the subsequent sentence or not.</p><p>The last layer of our model is a convolutional layer to automatically extract and represent pat- terns of semantic changes in a text. CNNs have proven useful for various NLP tasks <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b21">Kim, 2014;</ref><ref type="bibr" target="#b19">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b6">Cheng and Lapata, 2016</ref>) because of their effec- tiveness in identifying patterns in their input ( <ref type="bibr" target="#b44">Xu et al., 2015</ref>). In the case of coherence, the con- volution layer can identify coherence patterns that correlate with final tasks <ref type="bibr" target="#b40">(Tien Nguyen and Joty, 2017)</ref>. We use a temporal narrow convolution by applying a kernel filter k of width h to a window of h adjacent transitions over sentences to pro- duce a new coherence feature. This filter is ap- plied to each possible window of transitions in a text to produce a feature map ÔøΩ p, which is a coher- ence vector. Since we use a standard convolution layer, we explain details of the CNN formulations in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variants of Our Model</head><p>In our experiments, we consider two variants of our model: CohLSTM that is the full version of our model as described above; and CohEmb that is an ablation. CohEmb has no RNN layer, so the model is built directly on word embeddings. In this model, relations between sentences are made over only content words by eliminating all stop words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>Model configurations. Our model is imple- mented in PyTorch 3 with CUDA 8.0 support. In preprocessing we apply zero-padding to all sen- tences and documents to make their length equal. The vocabulary is limited to the 4000 most fre- quent words in the training data and all other words are replaced with the unknown token. We use the pre-trained word embeddings released by <ref type="bibr" target="#b47">Zou et al. (2013)</ref>, which are employed by <ref type="bibr">3</ref> https://pytorch.org state-of-the-art essay scoring systems. The dimen- sions of word embeddings and LSTM cells are 50 and 300, respectively. The convolution layer uses one filter with size 4. However, optimizing hyper- parameters for each task may lead to better perfor- mance. For selecting two vectors with the highest similarity from the LSTM states of two adjacent sentences, we capture the similarity between any pair of LSTM states of the sentences as an element in a vector, and then apply a max-pooling layer to this vector of similarities to identify the pair with maximally similar LSTM states. Selected LSTM states are used for representing salient information shared by the sentences. In CohEmb, stop words are removed by the SMART English stop word list <ref type="bibr" target="#b36">(Salton, 1971)</ref>.</p><p>Training setup. We set the mini-batch size to 32 and train the network for 100 epochs. At each epoch we evaluate the model on the validation set and select the one with the best performance for test evaluations. We optimize with Adam, with an initial learning rate of 0.01. Word vectors are up- dated during training. The dropout method with rate 0.5 is employed for regularization. Loss func- tions are specifically defined for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model on two downstream tasks: readability assessment (Section 5.1), in which co- herence representations of documents are mapped to coherence scores, and then documents are ranked based on these scores; and essay scoring (Section 5.2), in which the coherence representa- tion of an essay is combined with other features for essay scoring to quantify the quality of the essay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Readability Assessment</head><p>Readability assessment -How difficult is a text to read and understand? -depends on many factors one of which is coherence. Texts that are more co- herent are supposed to be faster to read and easier to understand. Following earlier research on local coherence ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b35">Pitler and Nenkova, 2008;</ref><ref type="bibr" target="#b15">Guinaudeau and Strube, 2013)</ref>, we evaluate our coherence model on this task by ranking texts with respect to readability, instead of predicting readability scores. More formally, we approach readability assessment as follows: Given a text-pair, which text is easier to read? Compared models. We compare the two vari- ants of our model as described in Section 3.4 with two following state-of-the-art systems:</p><p>Mesgar and Strube <ref type="bibr">(2016)</ref>. This is a graph-based coherence model, in which nodes of a graph indicate sentences of a text, and an edge between two sentence nodes represents the exis- tence of a lexico-semantic relation between two words in the sentences. Semantic relations be- tween words are measured by the absolute value of the cosine function over their corresponding pre-trained word embeddings. If the similarity value for two word vectors is below a certain threshold 4 then the connection between these two words is omitted. Given the graph representa- tion of a text, its coherence is encoded as a vec- tor whose elements are frequencies of different subgraphs in the graph. The size of subgraphs is defined by the number of their nodes and is set to five. Subgraphs are extracted by a ran- dom sampling approach. We choose this model for comparison because its intuition is similar to our model. However, this model suffers from the following limitations: word embeddings are con- sidered independently, not in their current context; and a manual threshold is used for connection fil- tering. We overcome these two weaknesses using the RNN and CNN layers in our model, respec- tively.</p><p>De <ref type="bibr" target="#b8">Clercq and Hoste (2016)</ref>. This is the state-of-the-art readability system on the exam- ined dataset. It uses a rich set of readability fea- tures ranging from surface to semantic text fea- tures. The ranking is performed by LibSVM in their model. We report their best performance that is achieved by extensive feature engineering and SVM's parameter optimization. ranking approach and define the loss function as:</p><formula xml:id="formula_7">loss = max {0, 1 ‚àí s d + s d ÔøΩ } .</formula><p>The parameters of the model are shared to obtain the scores for texts in pair <ref type="bibr">(d, d ÔøΩ )</ref>.</p><p>Data. We use the readability dataset proposed by De <ref type="bibr" target="#b9">Clercq et al. (2014)</ref>. It consists of 105 texts collected from the British National Corpus and Wikipedia in four different genres: adminis- trative (e.g. reports and surveys), informative (e.g. articles of newspapers and Wikipedia entries), in- structive (e.g. user manuals and guidelines), and miscellaneous (e.g., very technical texts and chil- dren's literature). The average number of sen- tences is about 12 per text. 10, 907 pairs of texts are labeled with five fine-grained categories: {‚àí100, ‚àí50, 0, 50, 100} indicating that the first text in a pair is respectively much easier, some- what easier, equally difficult, somewhat difficult, more difficult to read than the second text in the pair. Labels of text-pairs are assigned by human judges. <ref type="bibr">Similar to De Clercq and Hoste (2016)</ref>, we evaluate on the positive and negative labels as two sets of classes resulting in 6, 290 text-pairs in total. The original readability dataset does not pro- vide any standard training/validation/test sets. We apply 5-fold cross-validation over this dataset.</p><p>Evaluation metric. The quality of a model is measured in terms of accuracy, which is the frac- tion of pairs that are correctly ranked by a model divided by the total number of document-pairs. We report the average accuracy over all runs of cross-validation as the final result. We perform a paired t-test to determine if improvements are sta- tistically significant (p &lt; .05).</p><p>Results. <ref type="table">Table 1</ref> summarizes the results of dif- ferent systems for the readability assessment task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy (%) <ref type="bibr" target="#b31">Mesgar and Strube (2016)</ref> 85  <ref type="table">Table 1</ref>: Results on readability assessment. The first system is the state-of-the-art coherence model on this dataset. The last one is a full readability system. " * " in- dicates statistically significant difference with the bold result.</p><p>CohEmb significantly outperforms the graph-based coherence model proposed by <ref type="bibr" target="#b31">Mesgar and Strube (2016)</ref> by a large margin (6%), showing that our model captures coherence better than their model. In our model, the CNN layer automatically learns which connections are important to be considered for coherence patterns, whereas this is performed in <ref type="bibr" target="#b31">Mesgar and Strube (2016)</ref> by defining a threshold for eliminating connections.</p><p>CohLSTM significantly outperforms both the coherence model proposed by <ref type="bibr" target="#b31">Mesgar and Strube (2016)</ref> and the CohEmb model by 11% and 5%, respectively, and defines a new state-of-the-art on this dataset. CohLSTM, unlike <ref type="bibr" target="#b31">Mesgar and Strube (2016)</ref>'s model and CohEmb, considers words of sentences in their sentence context. This sup- ports our intuition that actual context information of words contributes to the perceived coherence of texts.</p><p>CohLSTM, which captures exclusively local coherence, even outperforms the readability sys- tem proposed by <ref type="bibr" target="#b8">De Clercq and Hoste (2016)</ref>, which relies on a wide range of lexical, syntactic and semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Essay Scoring</head><p>One part of the student assessment process is es- say writing where students are asked to write an essay about a given topic known as a prompt. An essay scoring system assigns an essay a score re- flecting the quality of the essay. The quality of an essay depends on various factors including co- herence. Following previous studies <ref type="bibr" target="#b32">(Miltsakaki and Kukich, 2004;</ref><ref type="bibr" target="#b24">Lei et al., 2014;</ref><ref type="bibr" target="#b38">Somasundaran et al., 2014;</ref><ref type="bibr" target="#b45">Zesch et al., 2015</ref>), we approach this task by combining the coherence vector produced by our model and the feature vector developed by an open-source essay scorer to represent an essay by a vector. The final vector representation of an essay, ÔøΩ x, is mapped to a score by a simple neural regression method as follows:</p><formula xml:id="formula_8">s = sigmoid(ÔøΩ u ¬∑ ÔøΩ x + b),</formula><p>where ÔøΩ u and b are the weight vector and the bias, respectively. We exactly define vector ÔøΩ x for dif- ferent examined systems, where we explain com- pared models for essay scoring.</p><p>Compared models. We compare variations of our model (Section 3.4) with the following mod- els: EASE (BLRR). As a baseline we use an open-source essay scoring system, Enhanced AI Scoring Engine 5 (EASE) ( <ref type="bibr" target="#b34">Phandi et al., 2015)</ref>. This system was ranked third among all 154 par- ticipating teams in the Automated Student As- sessment Prize (ASAP) competition and is the best among all open-source participating systems. It employs Bayesian Linear Ridge Regression (BLRR) as its regression method applied to a set of linguistic features grouped in four categories: (i) Frequency-based features: such as the number of characters, the number of words, the number of commas, etc; (ii) POS-based features: the number of POS n-grams; (iii) Word overlap with prompt; (iv) Bag of n-grams: the number of uni-grams and bi-grams. EASE. The difference between this system and EASE (BLRR) is in the employed regression method. This system uses a neural regression method as described above. In order to have a similar experimental settings for this task, here, we use feature vectors generated by <ref type="bibr" target="#b34">Phandi et al. (2015)</ref> to train our neural regression system. The input of the neural regression function is a nonlin- ear transformation of the feature vector produced by EASE,</p><formula xml:id="formula_9">ÔøΩ f . Therefore ÔøΩ x = tanh( ÔøΩ w ¬∑ ÔøΩ f + b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EASE &amp; CohEmb. This model combines the feature vector computed by EASE, ÔøΩ</head><p>f , and the co- herence vector produced by CohEmb, ÔøΩ p, to have a more reliable representation of an essay. More concretely, the input to our regression function, ÔøΩ x, is obtained as follows:</p><formula xml:id="formula_10">ÔøΩ h 1 = tanh( ÔøΩ w 1 ¬∑ ÔøΩ f + b 1 ), ÔøΩ h 2 = ÔøΩ h 1 ‚äï ÔøΩ p, ÔøΩ x = tanh( ÔøΩ w 2 ¬∑ ÔøΩ h 2 + b 2 ),</formula><p>where ‚äï indicates the concatenation operatation. EASE &amp; CohLSTM. The structure of this model is the same as the EASE &amp; CohEmb struc- ture. But the input coherence vector, ÔøΩ p, is pro- duced by CohLSTM. <ref type="bibr" target="#b10">Dong et al. (2017)</ref>. It is a sentence-document model, which is especially designed for this task. It first encodes each sentence by a vector, which represents whole sentence meanings, and then use an RNN to embed vectors of sentences into a doc- ument vector.</p><p>Experimental setup. The size of the input vec- tor for the regression method, ÔøΩ x, is fixed to 100 and its output size is fixed to 1. Dimensions of other parameters, ÔøΩ w 1 and ÔøΩ w 2 , are set accordingly. The loss function is the Mean Squared Error (MSE) between human scores, ÔøΩ H, and scores predicted by our system, ÔøΩ S:</p><formula xml:id="formula_11">M SE( ÔøΩ H, ÔøΩ S) = 1 N ÔøΩ ( ÔøΩ H ‚àí ÔøΩ S) 2 .</formula><p>The models are compared for each prompt by run- ning 5-fold cross-validation ( <ref type="bibr" target="#b10">Dong et al., 2017</ref>).</p><p>Data. We apply our model to a dataset used in the Automated Student Assessment Prize (ASAP) competition run by Kaggle 6 . The essays are as- sociated with scores given by humans and catego- rized in eight prompts. Each prompt can be inter- preted as a different essay topic along with differ- ent genres.  Evaluation metric. ASAP adopted Quadratic Weighted Kappa (QWK) as the official evaluation metric. This metric measures the agreement be- tween scores predicted by a system and scores as- signed by humans. QWK considers chance agree- ments and penalizes large disagreements more than small agreements. We use an implementation of QWK that is described in <ref type="bibr" target="#b39">Taghipour and Ng (2016)</ref>. The formulation of QWK are explained in Appendix D. The final reported QWK is the av- erage over QWKs of all prompts. We perform a paired t-test to determine if improvements are sta- tistically significant (p &lt; .05).</p><p>Results. <ref type="table">Table 3</ref> shows the results of different systems for the essay scoring task. Both EASE &amp; CohEmb, and EASE &amp; CohLSTM significantly improve EASE, confirming that our proposed representation for coherence is beneficial for essay scoring and improves the performance of the examined essay scoring system. Our model does not beat the state-of-the-art essay scoring system ( <ref type="bibr" target="#b10">Dong et al., 2017)</ref>, which is especially designed for this task and is tuned on this dataset. This model learns a vector representation for an input essay so that the vector performs the best for this regression task. In contrast, the core of our best performing essay scoring system, i.e. EASE &amp; CohLSTM, is the feature vector generated by EASE, which has less modeling capacity than a deep learning model like the model proposed by <ref type="bibr" target="#b10">Dong et al. (2017)</ref>. The reason that we combine our coherence model with EASE, rather than the model proposed by <ref type="bibr" target="#b10">Dong et al. (2017)</ref>, is that EASE has no notion of coherence. By combining our coherence model with it, we examine if our coherence vector improves its performance or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surprisingly, EASE &amp; CohLSTM works on par</head><p>with EASE &amp; CohEmb. To gain a better in- sight, we ablate EASE feature vectors and com- pare the performance of the coherence models, i.e., CohLSTM, and CohEmb. Of course, coher- ence vectors on their own are not sufficient for predicting essay scores but this setup shows how much each variant of our model contributes to this task. The two last rows in <ref type="table">Table 3</ref> show the results.</p><p>CohLSTM outperforms CohEmb on all prompts, which matches the results for readability assessment. This confirms our intuition that integrating the information of the current context of words contributes to coherence measurement.</p><p>In terms of average QWK, CohLSTM works similar to EASE; however they behave differently on different prompts. The largest improvement for CohLSTM, with respect to EASE, is obtained on prompt 7 and 8. These two prompts ask for stories about laughter and patience, so corresponding es- says can be categorized in the narrative genre (see <ref type="table" target="#tab_1">Table 2</ref>). The guidelines of these two prompts, which are publicly available in the Kaggle data, ask human annotators to assign the highest score to essays that are coherent and hold the attention of readers through an essay. This is what our model captures: the sequence of semantic changes in a text, or coherence.</p><p>On prompt 5, in contrast, we see the largest de- terioration in performance of CohLSTM in com- parison to EASE. This prompt asks students to de- scribe the mood created by the author of a mem- oir. Essays are expected to contain specific infor-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Prompts <ref type="table" target="#tab_1">Avg QWK  1  2  3  4  5  6  7  8  EASE (BLRR)</ref> 0.761 0.606 0.621 0.742 0.784 0.775 0.730 0.617 0. <ref type="bibr">705 * Dong et al. (2017)</ref> 0.822 0.682 0.672 0.814 0.803 0.811 0.801 0.705 0.764 * EASE 0.702 0.572 0.620 0.731 0.752 0.758 0.648 0.530 0.664 * EASE &amp; CohEmb 0.783 0.646 0.664 0.776 0.777 0.776 0.744 0.632 0.725 * EASE &amp; CohLSTM 0.784 0.654 0.663 0.788 0.793 0.794 0.756 0.646 0.728 * CohEmb 0.625 0.523 0.501 0.570 0.581 0.578 0.661 0.472 0.564 * CohLSTM 0.669 0.634 0.591 0.710 0.639 0.716 0.729 0.641 0.666 * <ref type="table">Table 3</ref>: Results on essay scoring. " * " shows significant improvements with respect to the underlined score (p &lt; .05). Bold numbers show the best results among different variants of our model. mation from the memoir so that an essay with the highest score has the highest coverage of all rel- evant and specific information from the memoir. Therefore, mentioning the details of the memoir in essays of prompt 5 is more important than co- herence for this prompt. This also shows that our model exclusively captures the coherence of a text, which is the goal of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We developed a local coherence model that en- codes patterns of changes in what semantically relates adjacent sentences. The main novelty of our approach is that it defines sentence connec- tions based on any semantic concept in sentences. In this sense, our model goes beyond entity-based coherence models, which need extra dependencies such as coreference resolution systems. Moreover, in contrast to lexical cohesion models, which take words individually, our model encodes words in their sentence context. Our model relates sen- tences by means of distant relations between word representations. The most similar LSTM states in two adjacent sentences are selected to encode the salient semantic concept that relates the sentences. The model finally employs a convolutional layer to extract and represent patterns of topic changes across sentences in a text as a coherence vector. We evaluate coherence vectors generated by our model on the readability assessment and essay scoring tasks. On the former, our model achieves new state-of-the-art results. On the latter, it signif- icantly improves the performance of a strong es- say scorer. We believe the reason that our system works is that it learns which semantic concepts of sentences should be used to relate sentences, and which information about concepts is required to model sentence-to-sentence transitions. In future work we intend to run qualitative experiments on patterns that are extracted by our model to see if they are also linguistically interpretable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of our model. e k is word embeddings associated with the k th word in an input text. h j i depicts the j th hidden state in LSTM states of sentence s i. Two states in LSTM states of sentence s i and sentence s i‚àí1 that have the highest similarity are selected to connect sentences. Vector ÔøΩ f i captures information about the salient topic that relates sentence s i to sentence s i‚àí1. d 23 represents the similarity between ÔøΩ f 2 and ÔøΩ f 3 or the degree of continuity of the topic over adjacent sentences. Different shades of gray show different degrees of similarity. The CNN encodes patterns of changes as coherence vector ÔøΩ p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Experimental setup.</head><label></label><figDesc>In Section 3, we formu- lated our model as ÔøΩ p = L Œ∏ (d) where Œ∏ repre- sents parameters of the neural modules (i.e. the CNN and RNN layers) in our model. For this task, we use an output layer to map coherence vector ÔøΩ p to score s which quantifies the degree of the per- ceived coherence of document d. Formally, the output layer is s d = ÔøΩ u ¬∑ ÔøΩ p + b where ÔøΩ u and b are the weight vector and bias, respectively. Let doc- ument d be more readable than document d ÔøΩ , then the model should ideally produce s d &gt; s ÔøΩ d . We train the parameters of the model by a pairwise 4 Like Mesgar and Strube (2016), we set this threshold to 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 summarizes</head><label>2</label><figDesc></figDesc><table>some properties 
of this dataset. 

Prompt # Essays Genre 
Avg. Len. 
1 
1783 
argumentative 350 
2 
1800 
argumentative 350 
3 
1726 
response 
150 
4 
1772 
response 
150 
5 
1805 
response 
150 
6 
1800 
response 
150 
7 
1569 
narrative 
250 
8 
723 
narrative 
650 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Some properties of the dataset used for the 
essay scoring experiment. 

</table></figure>

			<note place="foot" n="1"> We use unidirectional RNN to model the way that an English text is read.</note>

			<note place="foot" n="2"> In practice, the absolute function is implemented as g(z) = max(0, z) ‚àí min(0, z) to be differentiable.</note>

			<note place="foot" n="5"> https://github.com/edx/ease</note>

			<note place="foot" n="6"> https://www.kaggle.com/c/asap-aes/ data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Re-search Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Informa-tion from Heterogeneous Sources (AIPHES) un-der grant No. GRK 1994/1 and the Klaus Tschira Foundation, Heidelberg, Germany. We thank Mohammad Taher Pilehvar, Ines Rehbein, and Mark-Christoph M√ºller for their valuable feed-back on earlier drafts of this paper. We also thank anonymous reviewers for their useful suggestions for improving the quality of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-30" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word association profiles and their use for automated scoring of essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reader-based exploration of lexical cohesion. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using entity-based features to model coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Andreyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies 2010: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies 2010: The Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Los Angeles, Cal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">All mixed up? Finding the optimal feature set for general readability prediction and its application to English and Dutch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V√©ronique</forename><surname>Orph√©e De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="457" to="490" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using the crowd for readability prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V√©ronique</forename><surname>Orph√©e De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martine</forename><forename type="middle">De</forename><surname>Van Oosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieve</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="325" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-based recurrent convolutional neural network for automatic essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-04" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2011-06-24" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparison of features for automatic readability assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No√©mie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling 2010: Poster Volume</title>
		<meeting>Coling 2010: Poster Volume<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extending the entity-based coherence model with multiple ranks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph-based local coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining lexical and grammatical features to improve readability measures for first and second language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Rochester, N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Md</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Md</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, Fog count and Flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Chisson</surname></persName>
		</author>
		<idno>8-75</idno>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
		<respStmt>
			<orgName>Naval Technical Training Command, Naval Air Station Memphis-Millington, Tenn</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discourse coherence in the wild: A dataset, evaluation and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGdial 2018 Conference: The 19th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the SIGdial 2018 Conference: The 19th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using learning analytics to analyze writing skills of students: A case study in a technological common core curriculum course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Un</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Lok</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">To</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IAENG International Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="41" to="45" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-11" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2011-06-24" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning</title>
		<meeting>the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch√ºtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph-based coherence modeling for assessing readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of STARSEM 2015: The Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>STARSEM 2015: The Fourth Joint Conference on Lexical and Computational Semantics<address><addrLine>Denver, Col</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lexical coherence graph modeling using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, Cal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1414" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluation of text coherence for electronic essay scoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="55" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Entropy and graph based modelling of document coherence using discourse entities: An application to IR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birger</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR International Conference on the Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on the Theory of Information Retrieval<address><addrLine>Northhampton, Mass</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-30" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">A</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting readability: A unified framework for predicting text quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The SMART Retrieval System-Experiments in Automatic Document Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, N.J.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reading level assessment using support vector machines and statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-30" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lexical chaining for measuring discourse coherence quality in test-taker essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="950" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A neural approach to automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016-01-05" />
			<biblScope unit="page" from="1882" to="1891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A neural local coherence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Dat Tien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1320" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are cohesive features relevant for text readability evaluation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amalia</forename><surname>Todirascu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Gala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="987" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Neural Information Processing Systems (NIPS 2017)</title>
		<meeting><address><addrLine>Long Beach, CA., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Tex</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-01-05" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Task-independent features for automated essay grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Scholten-Akoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 10th Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Denver, Col</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Encoding world knowledge in the evaluation of local coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><forename type="middle">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Col.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Wash.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
