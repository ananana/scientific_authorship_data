<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Sony Corp</orgName>
								<address>
									<settlement>Tokyo, Saarbr√ºcken</settlement>
									<country>Japan, Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley EECS</orgName>
								<address>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="457" to="468"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
					<note>3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as con-catenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multi-modal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model out-performs the state-of-the-art on the Visual7W dataset and the VQA challenge.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning for text and images has been extensively studied in recent years. Recurrent neural networks (RNNs) are often used to represent sen- tences or phrases <ref type="bibr" target="#b33">(Sutskever et al., 2014;</ref>   2015), and convolutional neural networks (CNNs) have shown to work best to represent images <ref type="bibr" target="#b4">(Donahue et al., 2013;</ref><ref type="bibr" target="#b41">He et al., 2015</ref>). For tasks such as visual question answering (VQA) and visual ground- ing, most approaches require joining the represen- tation of both modalities. For combining the two vector representations (multimodal pooling), current approaches in VQA or grounding rely on concatenat- ing vectors or applying element-wise sum or product. While this generates a joint representation, it might not be expressive enough to fully capture the complex associations between the two different modalities.</p><p>In this paper, we propose to rely on Multimodal Compact Bilinear pooling (MCB) to get a joint repre- sentation. Bilinear pooling computes the outer prod- uct between two vectors, which allows, in contrast to element-wise product, a multiplicative interaction between all elements of both vectors. Bilinear pool- ing models <ref type="bibr" target="#b34">(Tenenbaum and Freeman, 2000</ref>) have recently been shown to be beneficial for fine-grained classification for vision only tasks ( <ref type="bibr" target="#b23">Lin et al., 2015)</ref>. However, given their high dimensionality (n 2 ), bi- linear pooling has so far not been widely used. In this paper, we adopt the idea from  which shows how to efficiently compress bilinear pooling for a single modality. In this work, we dis- cuss and extensively evaluate the extension to the multimodal case for text and visual modalities. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, Multimodal Compact Bilinear pooling (MCB) is approximated by randomly pro- jecting the image and text representations to a higher dimensional space (using Count Sketch ( <ref type="bibr" target="#b3">Charikar et al., 2002)</ref>) and then convolving both vectors effi- ciently by using element-wise product in Fast Fourier Transform (FFT) space. We use MCB to predict an- swers for the VQA task and locations for the visual grounding task. For open-ended question answering, we present an architecture for VQA which uses MCB twice, once to predict spatial attention and the second time to predict the answer. For multiple-choice ques- tion answering we introduce a third MCB to relate the encoded answer to the question-image space. Addi- tionally, we discuss the benefit of attention maps and additional training data for the VQA task. To sum- marize, MCB is evaluated on two tasks, four datasets, and with a diverse set of ablations and comparisons to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multimodal pooling. Current approaches to mul- timodal pooling involve element-wise operations or vector concatenation. In the visual question answer- ing domain, a number of models have been proposed. Simpler models such as iBOWIMG baseline ( <ref type="bibr" target="#b41">Zhou et al., 2015</ref>) use concatenation and fully connected layers to combine the image and question modali- ties. Stacked Attention Networks ( <ref type="bibr" target="#b41">Yang et al., 2015)</ref> and Spatial Memory Networks ( <ref type="bibr" target="#b40">Xu et al., 2015)</ref> use LSTMs or extract soft-attention on the image fea- tures, but ultimately use element-wise product or element-wise sum to merge modalities. D-NMN <ref type="bibr" target="#b0">(Andreas et al., 2016a</ref>) introduced REINFORCE to dy- namically create a network and use element-wise product to join attentions and element-wise sum pre- dict answers. Dynamic Memory Networks (DMN) ( <ref type="bibr">Xiong et al., 2016)</ref> pool the image and question with element-wise product and sum, attending to part of the image and question with an Episodic Mem- ory Module ( <ref type="bibr">Kumar et al., 2016</ref>). DPPnet ( <ref type="bibr" target="#b27">Noh et al., 2015</ref>) creates a Parameter Prediction Network which learns to predict the parameters of the second to last visual recognition layer dynamically from the question. Similar to this work, DPPnet allows mul- tiplicative interactions between the visual and ques- tion encodings. <ref type="bibr" target="#b24">Lu et al. (2016)</ref> recently proposed a model that extracts multiple co-attentions on the image and question and combines the co-attentions in a hierarchical manner using element-wise sum, concatenation, and fully connected layers. For the visual grounding task, <ref type="bibr" target="#b25">Rohrbach et al. (2016)</ref> propose an approach where the language phrase embedding is concatenated with the visual features in order to predict the attention weights over multiple bounding box proposals. Similarly, <ref type="bibr" target="#b13">Hu et al. (2016a)</ref> concatenate phrase embeddings with vi- sual features at different spatial locations to obtain a segmentation.</p><p>Bilinear pooling. Bilinear pooling has been ap- plied to the fine-grained visual recognition task. <ref type="bibr" target="#b23">Lin et al. (2015)</ref> use two CNNs to extract features from an image and combine the resulting vectors using an outer product, which is fully connected to an output layer.  address the space and time complexity of bilinear features by viewing the bilin- ear transformation as a polynomial kernel. <ref type="bibr" target="#b29">Pham and Pagh (2013)</ref> describe a method to approximate the polynomial kernel using Count Sketches and convo- lutions.</p><p>Joint multimodal embeddings. In order to model similarities between two modalities, many prior works have learned joint multimodal spaces, or em- beddings. Some of such embeddings are based on Canonical Correlation Analysis ( <ref type="bibr" target="#b10">Hardoon et al., 2004</ref>) e.g. ( <ref type="bibr" target="#b8">Gong et al., 2014;</ref><ref type="bibr" target="#b20">Klein et al., 2015;</ref><ref type="bibr" target="#b29">Plummer et al., 2015</ref>), linear models with ranking loss <ref type="bibr" target="#b5">(Frome et al., 2013;</ref><ref type="bibr">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b37">Weston et al., 2011</ref>) or non-linear deep learning models ( <ref type="bibr" target="#b19">Kiros et al., 2014;</ref><ref type="bibr">Mao et al., 2015;</ref><ref type="bibr" target="#b26">Ngiam et al., 2011</ref>). Our multimodal com- pact bilinear pooling can be seen as a complementary operation that allows us to capture different interac- tions between two modalities more expressively than e.g. concatenation. Consequently, many embedding learning approaches could benefit from incorporating such interactions.</p><p>0 -x n1 ... 0 -x 1 0 0 x 2 -q 2 0 ... q 4 0 0 q n2 q 9 q 1 q 2 ... q n2</p><p>x 1 x 2 ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal Compact Bilinear Pooling for Visual and Textual Embeddings</head><p>For the task of visual question answering (VQA) or visual grounding, we have to predict the most likely answer or location√¢locationÀÜlocation√¢ for a given image x and question or phrase q. This can be formulated as√¢</p><formula xml:id="formula_0">asÀÜas√¢ = argmax a‚ààA p(a|x, q; Œ∏)<label>(1)</label></formula><p>with parameters Œ∏ and the set of answers or loca- tions A. For an image embedding x = Œû(x) (i.e. a CNN) and question embedding q = ‚Ñ¶(q) (i.e. an LSTM), we are interested in getting a good joint rep- resentation by pooling both representations. With a multimodal pooling Œ¶(x, q) that encodes the relation- ship between x and q well, it becomes easier to learn a classifier for Equation (1). In this section, we first discuss our multimodal pooling Œ¶ for combining representations from differ- ent modalities into a single representation (Sec. 3.1) and then detail our architectures for VQA (Sec. 3.2) and visual grounding (Sec. 3.3), further explaining how we predict√¢predictÀÜpredict√¢ with the given image representation Œû and text representation ‚Ñ¶.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimodal Compact Bilinear</head><p>Pooling (MCB)</p><p>Bilinear models <ref type="bibr" target="#b34">(Tenenbaum and Freeman, 2000</ref>) take the outer product of two vectors x ‚àà R n 1 and q ‚àà R n 2 and learn a model W (here linear), i.e.</p><formula xml:id="formula_1">z = W [x ‚äó q]</formula><p>, where ‚äó denotes the outer product (xq T ) and [ ] denotes linearizing the matrix in a vec- tor. As discussed in the introduction, bilinear pooling is interesting because it allows all elements of both vectors to interact with each other in a multiplicative</p><formula xml:id="formula_2">Algorithm 1 Multimodal Compact Bilinear 1: input: v 1 ‚àà R n1 , v 2 ‚àà R n2 2: output: Œ¶(v 1 , v 2 ) ‚àà R d 3: procedure MCB(v 1 , v 2 , n 1 , n 2 , d) 4: for k ‚Üê 1 . . . 2 do 5:</formula><p>if h k , s k not initialized then 6:</p><formula xml:id="formula_3">for i ‚Üê 1 . . . n k do 7: sample h k [i] from {1, . . . , d} 8: sample s k [i] from {‚àí1, 1} 9: v k = Œ®(v k , h k , s k , n k )</formula><p>10:</p><formula xml:id="formula_4">Œ¶ = FFT ‚àí1 (FFT(v 1 ) FFT(v 2 )) 11: return Œ¶ 12: procedure Œ®(v, h, s, n) 13: y = [0, . . . , 0] 14: for i ‚Üê 1 . . . n do 15: y[h[i]] = y[h[i]] + s[i] ¬∑ v[i] 16:</formula><p>return y way. However, the high dimensional representation (i.e. when n 1 and n 2 are large) leads to an infeasible number of parameters to learn in W . For example, we use n 1 = n 2 = 2048 and z ‚àà R 3000 for VQA. W thus would have 12.5 billion parameters, which leads to very high memory consumption and high computation times.</p><p>We thus need a method that projects the outer prod- uct to a lower dimensional space and also avoids computing the outer product directly. As suggested by  for a single modality, we rely on the Count Sketch projection function Œ® ( <ref type="bibr" target="#b3">Charikar et al., 2002</ref>), which projects a vector v ‚àà R n to y ‚àà R d . We initialize two vectors s ‚àà {‚àí1, 1} n and h ‚àà {1, ..., d} n : s contains either 1 or ‚àí1 for each index, and h maps each index i in the input v to an index j in the output y. Both s and h are initialized randomly from a uniform distribution and remain constant for future invocations of count sketch. y is initialized as a zero vector. This allows us to project the outer product to a lower dimensional space, which reduces the number of parameters in W . To avoid computing the outer product explicitly, <ref type="bibr" target="#b29">Pham and Pagh (2013)</ref> showed that the count sketch of the outer product of two vectors can be expressed as convolution of both count sketches:  where * is the convolution operator. Additionally, the convolution theorem states that convolution in the time domain is equivalent to element-wise product in the frequency domain. The convolution x * q can be rewritten as FFT ‚àí1 (FFT(x ) FFT(q )), where refers to element-wise product. These ideas are summarized in <ref type="figure">Figure 2</ref> and formalized in Algorithm 1, which is based on the Tensor Sketch algorithm of <ref type="bibr" target="#b29">Pham and Pagh (2013)</ref>. We invoke the algorithm with v 1 = x and v 2 = q. We note that this easily extends and remains efficient for more than two multi-modal inputs as the combination happens as element-wise product.</p><formula xml:id="formula_5">Œ®(x ‚äó q, h, s) = Œ®(x, h, s) * Œ®(q, h, s), 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architectures for VQA</head><p>In VQA, the input to the model is an image and a question, and the goal is to answer the question. Our model extracts representations for the image and the question, pools the vectors using MCB, and arrives at the answer by treating the problem as a multi-class classification problem with 3,000 possible classes.</p><p>We extract image features using a 152-layer Resid- ual Network ( <ref type="bibr" target="#b41">He et al., 2015</ref>) that is pretrained on ImageNet data <ref type="bibr" target="#b3">(Deng et al., 2009</ref>). Images are re- sized to 448 √ó 448, and we use the output of the layer ("pool5") before the 1000-way classifier. We then perform L 2 normalization on the 2048-D vector.</p><p>Input questions are first tokenized into words, and the words are one-hot encoded and passed through a learned embedding layer. The tanh nonlinearity is used after the embedding. The embedding layer is followed by a 2-layer LSTM with 1024 units in each layer. The outputs of each LSTM layer are concatenated to form a 2048-D vector.</p><p>The two vectors are then passed through MCB. The MCB is followed by an element-wise signed square-root and L 2 normalization. After MCB pool- ing, a fully connected layer connects the resulting 16,000-D multimodal representation to the 3,000 top answers.</p><p>Attention. To incorporate spatial information, we use soft attention on our MCB pooling method. Ex- plored by ( <ref type="bibr" target="#b40">Xu et al., 2015</ref>) for image captioning and by <ref type="bibr" target="#b39">(Xu and Saenko, 2016)</ref> and <ref type="bibr" target="#b41">(Yang et al., 2015)</ref> for VQA, the soft attention mechanism can be easily integrated in our model.</p><p>For each spatial grid location in the visual rep- resentation (i.e. last convolutional layer of ResNet [res5c], last convolutional layer of VGG [conv5]), we use MCB pooling to merge the slice of the visual feature with the language representation. As depicted in <ref type="figure" target="#fig_3">Figure 3</ref>, after the pooling we use two convolu- tional layers to predict the attention weight for each grid location. We apply softmax to produce a nor- malized soft attention map. We then take a weighted sum of the spatial vectors using the attention map to create the attended visual representation. We also ex- periment with generating multiple attention maps to allow the model to make multiple "glimpses" which are concatenated before being merged with the lan- guage representation through another MCB pooling for prediction. Predicting attention maps with MCB pooling allows the model to effectively learn how to attend to salient locations based on both the visual and language representations.</p><p>Answer Encoding. For VQA with multiple choices, we can additionally embed the answers. We Q : "What do you see?" (Ground Truth : a3) a1 : "A courtyard with flowers" a2 : "A restaurant kitchen" a3 : "A family with a stroller, tables for dining" a4 : "People waiting on a train" a1 a2 a3 a4 In addition to using MCB with attention, we use an additional MCB pooling to merge the encoded an- swer choices with the multimodal representation of the original pipeline. The resulting embedding is projected to a classification vector with a dimension equal to the number of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture for Visual Grounding</head><p>We base our grounding approach on the fully- supervised version of GroundeR ( <ref type="bibr" target="#b25">Rohrbach et al., 2016)</ref>. The overview of our model is shown in <ref type="figure">Fig- ure 5</ref>. The input to the model is a query natural language phrase and an image along with multiple proposal bounding boxes. The goal is to predict a bounding box which corresponds to the query phrase. We replace the concatenation of the visual representa- tion and the encoded phrase in GroundeR with MCB to combine both modalities. In contrast to <ref type="bibr" target="#b25">Rohrbach et al. (2016)</ref>, we include a linear embedding of the visual representation and L 2 normalization of both in- put modalities, instead of batch normalization (Ioffe and Szegedy, 2015), which we found to be beneficial when using MCB for the grounding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation on Visual Question Answering</head><p>We evaluate the benefit of MCB with a diverse set of ablations on two visual question answering datasets.  what, where, when, who, why, and how). Compared to the VQA dataset, Vi- sual Genome represents a more balanced distribu- tion of the 6W question types. Moreover, the aver- age question and answer lengths for Visual Genome are larger than the VQA dataset. To leverage the Visual Genome dataset as additional training data, we remove all the unnecessary words such as "a", "the", and "it is" from the answers to decrease the length of the answers and extract QA pairs whose answers are single-worded. The extracted data is fil- tered again based on the answer vocabulary space created from the VQA dataset, leaving us with addi- tional 1M image-QA triplets.</p><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We use the Adam solver with = 0.0007, Œ≤ 1 = 0.9, Œ≤ 2 = 0.999. We use dropout after the LSTM layers and in fully connected layers. For the experiments in <ref type="table" target="#tab_7">Table 1</ref> and 2, we train on the VQA train split, vali- date on the VQA validation split, and report results on the VQA test-dev split. We use early stopping: if the validation score does not improve for 50,000 iter- ations, we stop training and evaluate the best iteration on test-dev.</p><p>For the Visual7W task, we use the same hyperpa- rameters and training settings as in the VQA exper- iments. We use the splits from ( <ref type="bibr" target="#b21">Zhu et al., 2016</ref>) to train, validate, and test our models. We also compute accuracies on this data using their evaluation code.</p><p>For VQA multiple choice, we train the open-ended models and take the argmax over the multiple choice     answers at test time. For Visual7W, we use the an- swer encoding as described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Results</head><p>We compare the performance of non-bilinear and bilinear pooling methods in <ref type="table" target="#tab_7">Table 1</ref>. We see that MCB pooling outperforms all non-bilinear pooling methods, such as eltwise sum, concatenation, and eltwise product. One could argue that the compact bilinear method simply has more parameters than the non-bilinear pooling methods, which contributes to its perfor- mance. We compensated for this by stacking fully connected layers (with 4096 units per layer, ReLU activation, and dropout) after the non-bilinear pool- ing methods to increase their number of parameters. However, even with similar parameter budgets, non- bilinear methods could not achieve the same accuracy as the MCB method. For example, the "Concatena- tion + FC + FC" pooling method has approximately 4096 2 + 4096 2 + 4096 √ó 3000 ‚âà 46 million pa- rameters, which matches the 48 million parameters available in MCB with d = 16000. However, the per- formance of the "Concatenation + FC + FC" method is only 57.10% compared to MCB's 59.83%.</p><p>Section 2 in  linear pooling has no impact on accuracy compared to full bilinear pooling. Section 3 in <ref type="table" target="#tab_7">Table 1</ref> demon- strates that the MCB brings improvements regardless of the image CNN used. We primarily use ResNet- 152 in this paper, but MCB also improves perfor- mance if VGG-19 is used. Section 4 in <ref type="table" target="#tab_7">Table 1</ref> shows that our soft attention model works best with MCB pooling. In fact, attending to the Concatenation + FC layer has the same performance as not using attention at all, while attending to the MCB layer improves performance by 2.67 points. <ref type="table" target="#tab_5">Table 2</ref> compares different values of d, the output dimensionality of the multimodal compact bilinear feature. Approximating the bilinear feature with a 16,000-D vector yields the highest accuracy.</p><p>We also evaluated models with multiple atten- tion maps or channels. One attenion map achieves 64.67%, two 65.08% and four 64.24% accuracy (trained on train+val). Visual inspection of the gen- erated attention maps reveals that an ensembling or smoothing effect occurs when using multiple maps. <ref type="table" target="#tab_6">Table 3</ref> presents results for the Visual7W multiple- choice QA task. The MCB with attention model out- performs the previous state-of-the-art by 7.9 points overall and performs better in almost every category. <ref type="table" target="#tab_8">Table 4</ref> compares our approach with the state-of-the- art on VQA test set. Our best single model uses MCB pooling with two attention maps. Additionally, we augment our training data with images and QA pairs from the Visual Genome dataset. We also con- catenate the learned word embedding with pretrained GloVe vectors ( <ref type="bibr" target="#b28">Pennington et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to State-of-the-Art</head><p>Each model in our ensemble of 7 models uses MCB with attention. Some of the models were trained with data from Visual Genome, and some were trained with two attention maps. This ensem-Method Accuracy, % <ref type="bibr" target="#b29">Plummer et al. (2015)</ref> 27.42 <ref type="bibr" target="#b14">Hu et al. (2016b)</ref> 27.80 <ref type="bibr" target="#b30">Plummer et al. (2016)</ref>  <ref type="bibr">1</ref> 43.84  43.89 <ref type="bibr" target="#b25">Rohrbach et al. (2016)</ref> 47   ble is 1.8 points above the next best approach on the VQA open-ended task and 0.8 points above the next best approach on the multiple-choice task (on Test- dev). Even without ensembles, our "MCB + Genome + Att. + GloVe" model still outperforms the next best result by 0.5 points, with an accuracy of 65.4% versus 64.9% on the open-ended task (on Test-dev).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on Visual Grounding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our visual grounding approach on two datasets. al., 2013) object proposals and the Fast R-CNN <ref type="bibr" target="#b7">(Girshick, 2015</ref>) fine-tuned VGG16 features <ref type="bibr" target="#b31">(Simonyan and Zisserman, 2014</ref>). The second dataset is Refer- ItGame ( <ref type="bibr" target="#b17">Kazemzadeh et al., 2014)</ref>, which contains 20K images from IAPR TC-12 dataset <ref type="bibr" target="#b9">(Grubinger et al., 2006</ref>) with segmented regions from SAIAPR-12 dataset <ref type="bibr" target="#b5">(Escalante et al., 2010</ref>) and 120K associated natural language referring expressions. For Refer- ItGame we follow the experimental setup of <ref type="bibr" target="#b14">Hu et al. (2016b)</ref> and rely on their ground-truth bound- ing boxes extracted around the segmentation masks. We use the Edge <ref type="bibr">Box (Zitnick and Doll√°r, 2014</ref>) ob- ject proposals and visual features (VGG16 combined with the spatial features, which encode bounding box relative position) from <ref type="bibr" target="#b14">Hu et al. (2016b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>In all experiments we use Adam solver <ref type="bibr" target="#b18">(Kingma and Ba, 2014</ref>) with learning rate = 0.0001. The embed- ding size is 500 both for visual and language embed- dings. We use d = 2048 in the MCB pooling, which we found to work best for the visual grounding task. The accuracy is measured as percentage of query phrases which have been localized correctly. The phrase is localized correctly if the predicted bound- ing box overlaps with the ground-truth bounding box by more than 50% intersection over union (IOU). <ref type="table" target="#tab_10">Tables 5 and 6</ref> summarize our results in the visual grounding task. We present multiple ablations of our proposed architecture. First, we replace the MCB with simple concatenation of the embedded visual feature and the embedded phrase, resulting in 46.5% on the Flickr30k Entities and 25.48% on the Refer- ItGame datasets. The results can be improved by replacing the concatenation with the element-wise product of both embedded features (47.41% and 27.80%). We can further slightly increase the per- formance by introducing additional 2048-D convo- lution after the element-wise product (47.86% and 27.98%). However, even with fewer parameters, our MCB pooling significantly improves over this base- line on both datasets, reaching state-of-the-art accu- racy of 48.69% on Flickr30k Entities and 28.91% on ReferItGame dataset. <ref type="figure" target="#fig_8">Figure 6 (</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose the Multimodal Compact Bilinear Pool- ing (MCB) to combine visual and text representa- tions. For visual question answering, our architecture with attention and multiple MCBs gives significant improvements on two VQA datasets compared to state-of-the-art. In the visual grounding task, in- troducing MCB pooling leads to improved phrase localization accuracy, indicating better interaction between query phrase representations and visual rep- resentations of proposal bounding boxes. The code to replicate our experiments is available at https: //github.com/akirafukui/vqa-mcb.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multimodal Compact Bilinear Pooling for visual question answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Multimodal Compact Bilinear Pooling (MCB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For every element v[i] its destination index j = h[i] is looked up using h, and s[i] ¬∑ v[i] is added to y[j]. See lines 1-9 and 12-16 in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our architecture for VQA: Multimodal Compact Bilinear (MCB) with Attention. Conv implies convolutional layers and FC implies fully connected layers. For details see Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our architecture for VQA: MCB with Attention and Answer Encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Q</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1024</head><label>1024</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top: predicted answers and attention maps from MCB model on VQA images. Bottom: predicted grounding from MCB model (left) and Eltwise Product + Conv model (right) on Flickr30k Entities images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>: "Person in blue checkered shirt"</head><label></label><figDesc></figDesc><table>b1 
b2 
b3 
b4 

Q 

Conv 

b4 

Multimodal 
Compact 
Bilinear 
Tile 

Relu 
Conv 
Softmax 

CNN 

CNN 

CNN 

CNN 

b3 
b2 
b1 

b3 

WE 
LSTM 
L2 
norm 

Conv L2 norm 

Conv L2 norm 

Conv L2 norm 

Conv L2 norm 

Figure 5: Our Architecture for Grounding with MCB 
(Sec. 3.3) 

4.1 Datasets 

The Visual Question Answering (VQA) real-image 
dataset (Antol et al., 2015) consists of approximately 
200,000 MSCOCO images (Lin et al., 2014), with 
3 questions per image and 10 answers per question. 
There are 3 data splits: train (80K images), validation 
(40K images), and test (80K images). Additionally, 
there is a 25% subset of test named test-dev. Ac-
curacies for ablation experiments in this paper are 
reported on the test-dev data split. We use the VQA 
tool provided by Antol et al. (2015) for evaluation. 
We conducted most of our experiments on the open-
ended real-image task. In Table 4, we also report our 
multiple-choice real-image scores. 
The Visual Genome dataset (Krishna et al., 
2016) uses 108,249 images from the intersection of 
YFCC100M (Thomee et al., 2015) and MSCOCO. 
For each image, an average of 17 question-answer 
pairs are collected. There are 1.7 million QA pairs 
of the 6W question types (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracies for different values of d, the 
dimension of the compact bilinear feature. Models 
are trained on the VQA train split and tested on test-
dev. Details in Sec. 4.3. 

Method 
What Where When Who Why How Avg 

Zhu et al. 
51.5 57.0 75.0 59.5 55.5 49.8 54.3 
Concat+Att. 47.8 56.9 74.1 62.3 52.7 51.2 52.8 
MCB+Att. 
60.3 70.4 79.5 69.2 58.2 51.1 62.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Multiple-choice QA tasks accuracy (%) on 
Visual7W test set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 1 also shows that compact bi-</head><label>1</label><figDesc></figDesc><table>Test-dev 

Test-standard 

Open Ended 
MC 
Open Ended 
MC 

Y/N No. Other All 
All Y/N No. Other All 
All 

MCB 
81.2 35.1 49.3 60.8 65.4 
-
-
-
-
-
MCB + Genome 
81.7 36.6 51.5 62.3 66.4 
-
-
-
-
-
MCB + Att. 
82.2 37.7 54.8 64.2 68.6 
-
-
-
-
-
MCB + Att. + GloVe 
82.5 37.6 55.6 64.7 69.1 
-
-
-
-
-
MCB + Att. + Genome 
81.7 38.2 57.0 65.1 69.5 
-
-
-
-
-
MCB + Att. + GloVe + Genome 82.3 37.2 57.4 65.4 69.9 
-
-
-
-
-
Ensemble of 7 Att. models 
83.4 39.8 58.5 66.7 70.2 83.2 39.5 58.0 66.5 70.1 

Naver Labs (challenge 2nd) 
83.5 39.8 54.8 64.9 69.4 83.3 38.7 54.6 64.8 69.3 
HieCoAtt (Lu et al., 2016) 
79.7 38.7 51.7 61.8 65.8 
-
-
-
62.1 66.1 
DMN+ (Xiong et al., 2016) 
80.5 36.8 48.3 60.3 
-
-
-
-
60.4 
-
FDA (Ilievski et al., 2016) 
81.1 36.2 45.8 59.2 
-
-
-
-
59.5 
-
D-NMN (Andreas et al., 2016a) 81.1 38.6 45.5 59.4 
-
-
-
-
59.4 
-
AMA (Wu et al., 2016) 
81.0 38.4 45.2 59.2 
-
81.1 37.1 45.8 59.4 
-
SAN (Yang et al., 2015) 
79.3 36.6 46.1 58.7 
-
-
-
-
58.9 
-
NMN (Andreas et al., 2016b) 
81.2 38.0 44.0 58.6 
-
81.2 37.7 44.0 58.7 
-
AYN (Malinowski et al., 2016) 
78.4 36.4 46.3 58.4 
-
78.2 36.3 46.3 58.4 
-
SMem (Xu and Saenko, 2016) 
80.9 37.3 43.1 58.0 
-
80.9 37.5 43.5 58.2 
-
VQA team (Antol et al., 2015) 
80.5 36.8 43.1 57.8 62.7 80.6 36.5 43.7 58.2 63.1 
DPPnet (Noh et al., 2015) 
80.7 37.2 41.7 57.2 
-
80.3 36.9 42.2 57.4 
-
iBOWIMG (Zhou et al., 2015) 
76.5 35.0 42.6 55.7 
-
76.8 35.0 42.6 55.9 62.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Open-ended and multiple-choice (MC) results on VQA test set (trained on train+val set) compared 
with state-of-the-art: accuracy in %. See Sec. 4.4. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>. 81</head><label>81</label><figDesc></figDesc><table>Concatenation 
46.50 
Element-wise Product 
47.41 
Element-wise Product + Conv 
47.86 
MCB 
48.69 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Grounding accuracy on Flickr30k Entities 
dataset. 

Method 
Accuracy, % 

Hu et al. (2016b) 
17.93 
Rohrbach et al. (2016) 
26.93 

Concatenation 
25.48 
Element-wise Product 
27.80 
Element-wise Product + Conv 
27.98 
MCB 
28.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Grounding accuracy on ReferItGame 
dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head></head><label></label><figDesc>bottom) shows examples of improved phrase localization.</figDesc><table>What vegetable is the dog 
chewing on? 
MCB: carrot 
GT: carrot 

What kind of dog is this? 
MCB: husky 
GT: husky 

What kind of flooring does 
the room have? 
MCB: carpet 
GT: carpet 

What color is the traffic 
light? 
MCB: green 
GT: green 

Is this an urban area? 
MCB: yes 
GT: yes 

Where are the buildings? 
MCB: in background 
GT: on left 

MCB 
Eltwise Product + Conv MCB 
Eltwise Product + Conv 

A tattooed woman with a green dress and yellow back-
pack holding a water bottle is walking across the street. 
A dog distracts his owner from working at her computer. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yang Gao and Oscar Beijbom for helpful discussions about Compact Bilinear Pool-ing. This work was supported by DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Ar-tificial Intelligence Research (BAIR) Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A LargeScale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
	<note>Automata, languages and programming</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The segmented and annotated iapr tc-12 benchmark. Computer Vision and Image Understanding</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
	<note>Devise: A deep visual-semantic embedding model</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop OntoImage</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hardoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hodosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ilievski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>A focused dynamic attention model for visual question answering</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Karpathy and Fei-Fei2015] Andrej Karpathy and Li FeiFei</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kazemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] Diederik</forename><surname>Ba2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<editor>Kiros et al.2015] Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler</editor>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<editor>Kumar et al.2016] Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher</editor>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ask me anything: Dynamic memory networks for natural language processing</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for finegrained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical Co-Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Malinowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02697</idno>
		<title level="m">Ask Your Neurons: A Deep Learning Approach to Visual Question Answering</title>
		<editor>Mao et al.2015] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Learning Representations. ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pagh ; Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Plummer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04870v3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<editor>Ronghang Hu, Trevor Darrell, and Bernt Schiele</editor>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Anna Rohrbach, Marcus Rohrbach</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Grounding of textual phrases in images by reconstruction</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Sutskever et al.2014</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freeman2000] Joshua B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 467 new data and new challenges in multimedia research. CoRR, abs/1503.01817</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
	<note>Uijlings et al.2013</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Samy Bengio, and Nicolas Usunier</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>Stephen Merity, and Richard Socher</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proc. IEEE Conf. Computer Vision Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko2016] Huijuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<idno>arXiv:1512.02167</idno>
	</analytic>
	<monogr>
		<title level="m">Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. Simple baseline for visual question answering</title>
		<editor>Oliver Groth, Michael Bernstein, and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doll√°r2014] C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
