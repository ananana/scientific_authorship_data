<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Dependent Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfei</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Dependent Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns , but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a two-stage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) like WordNet <ref type="bibr" target="#b12">(Miller, 1995)</ref>, <ref type="bibr">Freebase (Bollacker et al., 2008)</ref>, and DB- pedia ( <ref type="bibr" target="#b9">Lehmann et al., 2014</ref>) have become ex- tremely useful resources for many NLP-related ap- plications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Al- though powerful in representing complex data, the symbolic nature makes KGs hard to manipulate.</p><p>Recently, knowledge graph embedding has at- tracted much attention <ref type="bibr" target="#b1">(Bordes et al., 2011;</ref><ref type="bibr" target="#b2">Bordes et al., 2013;</ref><ref type="bibr" target="#b16">Socher et al., 2013;</ref>. It attempts to embed entities and relations in a KG into a continuous vector space, so as to simplify the manipulation while preserving the in- herent structure of the original graph.</p><p>Most of the existing KG embedding methods model triples individually, ignoring the fact that * Corresponding author: Quan Wang. entities connected to a same node are usually im- plicitly related to each other, even if they are not directly connected. <ref type="figure" target="#fig_0">Figure 1</ref> gives two examples.</p><p>Shaquille O Neal and NBA in the former ex- ample and Nevada and Utah in the latter exam- ple are implicitly related to each other, through the intermediate nodes Phoenix Suns and USA re- spectively. We refer to such implicit relationships as contextual connectivity patterns (CCPs). Re- lationships explicitly represented in triples are re- ferred to as local connectivity patterns (LCPs). In most of the existing methods, only LCPs are ex- plicitly modeled. This paper proposes a two-stage embedding scheme that explicitly takes into account both C- CPs and LCPs, called context-dependent KG em- bedding. In the first stage, each CCP is formalized as a knowledge path, i.e., a sequence of entities and relations occurring in the pattern. A word em- bedding model is adopted to learn embeddings of entities and relations, by taking them as pseudo- words. The embeddings are enforced compatible within each knowledge path, and hence can cap- ture CCPs. In the second stage, the learned em- beddings are fine-tuned by an existing KG embed- ding technique. Since such a technique requires the embeddings to be compatible on each individ- ual triple, LCPs are also encoded.</p><p>The advantages of our approach are three-fold. 1) It fully exploits both CCPs and LCPs, and can obtain more accurate embeddings. 2) It is a gen- eral scheme, applicable to a wide variety of word embedding models in the first stage and KG em- bedding models in the second. 3) No auxiliary data is further required in the two-stage process, except for the original graph.</p><p>We evaluate our approach on two publicly avail- able data sets, and achieve significant and consis- tent improvements over state-of-the-art methods in the link prediction and triple classification tasks. The learned embeddings are not only more accu- rate but also more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Context-Dependent KG Embedding</head><p>We are given a KG with nodes corresponding to entities and edges to relations. Each edge is denot- ed by a triple (h, r, t), where h is the head entity, t the tail entity, and r the relation between them. Entities and relations are represented as vectors, matrices, or tensors in a continuous vector space. Context-dependent KG embedding aims to auto- matically learn entity and relation embeddings, by using observed triples O in a two-stage process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling CCPs</head><p>The first stage models CCPs conveyed in the KG. Each CCP is formalized as a knowledge path, i.e., a sequence of entities and relations occurring in the pattern. For the CCPs in <ref type="figure" target="#fig_0">Figure 1</ref>, the associ- ated knowledge paths are: We fix the length of knowledge paths to 5. Dur- ing path extraction, we ignore the directionality of edges, and treat the KG as an undirected graph. <ref type="bibr">1</ref> Given the extracted knowledge paths, we em- ploy word embedding models to pre-train the em- beddings of entities and relations, by taking them as pseudo-words. We use two word embedding models: CBOW and Skip-gram ( <ref type="bibr" target="#b10">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013b</ref>). In CBOW, words in the context are projected to their embeddings and then summed. Based on the summed embedding, log-linear classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classi- fiers are further adopted to predict its context. We restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and re- lation embeddings pre-trained in this way are re- quired to be compatible within each knowledge path, and thus can encode CCPs. <ref type="bibr" target="#b14">Perozzi et al. (2014)</ref> and <ref type="bibr" target="#b7">Goikoetxea et al. (2015)</ref> have proposed similar ideas, i.e., to gener- ate random walks from online social networks or from the WordNet knowledge base, and then em- ploy word embedding techniques on these random walks. But our approach has two differences. 1) It deals with heterogeneous graphs with differen- t types of edges. Both nodes (entities) and edges (relations) are included during knowledge path ex- traction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embeddings as final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modeling LCPs</head><p>The second stage models LCPs conveyed in the KG. We employ three state-of-the-art KG embed- ding models, namely SME ( <ref type="bibr" target="#b3">Bordes et al., 2014</ref>), <ref type="bibr">TransE (Bordes et al., 2013)</ref>, and SE <ref type="bibr" target="#b1">(Bordes et al., 2011</ref>) to fine-tune the pre-trained embeddings. These three models work in the following way. First, entities are represented as vectors, and re- lations as operators in an embedding space, char- acterized by vectors (SME and TransE) or matri- ces (SE). Then, for each triple (h, r, t), an energy function f r (h, t) is defined to measure its plausi- bility. Plausible triples are assumed to have low energies. Finally, to obtain entity and relation em- beddings, a margin-based ranking loss, i.e.,</p><formula xml:id="formula_0">L = t + ∈O t − ∈N t + γ + f r (h, t) − f r (h , t ) + ,</formula><p>is minimized. Here, t + = (h, r, t) ∈ O is an ob- served (positive) triple; N t + is the set of negative triples constructed by replacing entities in t + , and t − = (h , r, t ) ∈ N t + ; γ is a margin separating positive and negative triples; [x] + = max(0, x). <ref type="table" target="#tab_0">Table 1</ref> summarizes the entity/relation embed- dings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to <ref type="bibr" target="#b13">(Nickel et al., 2011;</ref><ref type="bibr" target="#b15">Riedel et al., 2013;</ref><ref type="bibr" target="#b17">Wang et al., 2014;</ref><ref type="bibr" target="#b6">Chang et al., 2014</ref>).</p><p>We adopt stochastic gradient descent to solve the minimization problem, by taking entity and re- lation embeddings pre-trained in the first stage as Method Entity/Relation embedding Energy function SME (linear) ( <ref type="bibr" target="#b3">Bordes et al., 2014</ref>) h, t ∈ R k , r ∈ R k fr (h, t) = (Wu1r + Wu2h + bu) T (Wv1r + Wv2t + bv) SME (bilinear) ( <ref type="bibr" target="#b3">Bordes et al., 2014)</ref> h  initial values. <ref type="bibr">2</ref> The entity and relation embeddings fine-tuned in this way are required to be compati- ble within each triple, and thus can encode LCPs. <ref type="bibr" target="#b16">Socher et al. (2013)</ref> have proposed a similar idea, i.e., to use embeddings learned from an aux- iliary corpus as initial values. However, linking entities recognized in an auxiliary corpus to those occurring in the KG is always a non-trivial task. Our approach requires no auxiliary data, and nat- urally avoids the entity linking task.</p><formula xml:id="formula_1">, t ∈ R k , r ∈ R k fr (h, t) = ((W u ¯ × 3 r) h + bu) T ((W v ¯ × 3 r) t + bv) TransE (Bordes et al., 2013) h, t ∈ R k , r ∈ R k fr (h, t) = h + r − t 1 SE (Bordes et al., 2011) h, t ∈ R k , Ru, Rv ∈ R k×k fr (h, t) = Ruh − Rvt 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We test our approach on the tasks of link predic- tion and triple classification. Two publicly avail- able data sets are used. The first is WN18 released by <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>  <ref type="bibr">3</ref> . It is a subset of Word- Net, consisting of 18 relations and the entities con- nected by them. The second is NELL186 released by   <ref type="bibr">4</ref> , containing the most fre- quent 186 relations in NELL ( <ref type="bibr">Carlson et al., 2010)</ref> and the associated entities. Triples are split into training/validation/test sets, used for model train- ing, parameter tuning, and evaluation respectively. Knowledge paths are extracted from training sets. <ref type="table" target="#tab_1">Table 2</ref> gives some statistics of the data sets.</p><p>To perform context-dependent KG embedding, we use CBOW and Skip-gram in the pre-training stage, and SME, TransE, and SE in the fine-tuning stage. We take randomly initialized SME, TransE, and SE as baselines, denoted as *-Random. We do not compare to the setting that employs only CBOW or Skip-gram, since it does not provide an energy function to calculate triple plausibility, which hinders the evaluation of both tasks. <ref type="bibr">2</ref> For SE, only entity vectors are initialized by pre-trained embeddings. Relation matrices are randomly initialized.</p><p>3 https://everest.hds.utc.fr/doku.php?id=en:smemlj12 4 http://www.aclweb.org/anthology/P/P15/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Link Prediction</head><p>Link prediction is to predict whether there is a spe- cific relation between two entities.</p><p>Evaluation Protocol. For each test triple, the head is replaced by every entity in the KG, and the energy is calculated for each corrupted triple.</p><p>Ranking the energies in ascending order, we get the rank of the correct answer. We can get another rank by corrupting the tail. We report two metrics on the test sets: Mean (averaged rank) and Hit- s@10 (proportion of ranks no larger than 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details. To train CBOW and</head><p>Skip-gram, we use the word2vec implementation- s 5 . 20 negative samples are drawn for each pos- itive one. The context size is fixed to 5. To train SME, TransE, and SE, we use the implementation- s provided by the authors <ref type="bibr">6</ref> , with 100 mini-batches. We vary the learning rate in {0.01, 0.1, 1, 10}, the dimension k in {20, 50}, and the margin γ in {1, 2, 4}. The best model is selected by monitor- ing Hits@10 on the validation sets, with a total of at most 1000 iterations over the training sets.</p><p>Results. <ref type="table" target="#tab_3">Table 3</ref> reports the results on the test sets of WN18 and NELL186. The improvements of CBOW/Skip-gram over Random are also given. Statistically significant improvements are marked by ‡ (sign test, significance level 0.05). The result- s show that a pre-training stage consistently im- proves over the baselines for all the methods on both data sets. Almost all of the improvements are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triple Classification</head><p>Triple classification aims to verify whether an un- seen triple is correct or not.   classification, a triple is predicted to be positive if the energy is below a relation-specific thresh- old δ r ; otherwise negative. We report two metric- s on the test sets: micro-averaged accuracy (per- instance average) and macro-averaged accuracy (per-relation average).</p><p>Implementation Details. We use the same pa- rameter settings as in the link prediction task. The relation-specific threshold δ r is determined by maximizing Micro-ACC on the validation sets.</p><p>Results. <ref type="table" target="#tab_4">Table 4</ref> reports the results on the test sets of WN18 and NELL186. The results again demonstrate both the superiority and the generali- ty of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>This section is to explore why pre-training helps in KG embedding, specifically in link prediction. We first test different random initializations in traditional KG embedding models. We run SME (linear) twice on WN18, with two different initial- ization settings. Both are randomly sampled from the same uniform distribution, but with differen- t seeds, referred to as Random-I and Random- II. Each setting finally gets 10,000 ranks on the test set. <ref type="bibr">7</ref> To better understand the difference be- <ref type="bibr">7</ref> For each of the 5,000 test triples, both the head and the tween the two settings, we analyze the ranks indi- vidually, rather than reporting aggregated metrics (Mean and Hits@10). Specifically, we distribute the 10,000 instances into different bins according to the ranks given by one setting (e.g. Random- I). Instances assigned to the i-th bin have the same rank of i, that means, they are all ranked in the i-th position by this setting. Then, within each bin, we calculate the average rank of the instances given by the other setting (e.g. Random-II). If the av- erage rank differs drastically from the bin ID, the instances in this bin are ranked significantly dif- ferently by the two settings. <ref type="figure" target="#fig_6">Figures 2(a) and 2(b)</ref> show the results, with the instances distributed ac- cording to Random-I and Random-II respectively. In both cases, we retain the bins with ID no larger than 50, covering about 85% of the instances. In most of the bins, the average rank (red bars in the figures) differs drastically from the bin ID (black bars in the figures), indicating that the ranks giv- en by Random-I and Random-II are significantly different at the instance level. The results demon- strate the non-convexity of SME (linear): different initial values lead to different local minimum.</p><p>We further compare the settings of initial val- ues 1) randomly sampled from a uniform distri- bution (Random) and 2) pre-trained by Skip-gram tail are corrupted and ranked.      <ref type="figure" target="#fig_6">(Figure 2(c)</ref>), while Random has an average rank much higher than the bin ID <ref type="figure" target="#fig_6">(Figure 2(d)</ref>), implying that Skip- gram performs better than Random-I at the in- stance level. The results indicate that pre-training might help in finding better initial values which lead to better local minimum.</p><p>Finally we test our two-stage KG embedding scheme where the skip-gram model itself is giv- en two different initialization settings, say Skip- gram-I and Skip-gram-II. The results are given in Figures 2(e) and 2(f). In each of the first 20 bins, Skip-gram-I and Skip-gram-II get an average rank almost the same with the bin ID, implying that the two settings perform quite similarly, particularly at the highest ranking levels. The results indicate that a pre-training stage might help in obtaining more stable embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed a novel two-stage scheme for KG embedding, called context-dependent KG em- bedding. In the pre-training stage CCPs are encod- ed by a word embedding model, and in the fine- tuning stage LCPs are encoded by a traditional KG embedding model. Since both types of connectiv- ity patterns are explicitly taken into account, our approach can obtain more accurate embeddings. Moreover, our approach is quite general, applica- ble to various word embedding and KG embed- ding models. Experimental results on link predic- tion and triple classification demonstrate the supe- riority, generality, and stability of our approach.</p><p>As future work, we plan to 1) Investigate the ef- ficacy of longer CCPs (i.e. knowledge paths with lengths longer than 5). 2) Design a joint model that encodes LCPs and CCPs simultaneously. More- over, our approach actually reveals the possibili- ty of a broad idea, i.e., initializing an embedding model by another embedding model. We would also like to test the feasibility of other such strate- gies, e.g., initializing SME by TransE, so as to combine the benefits of both models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: LCPs and CCPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"</head><label></label><figDesc>Shaquille O Neal, AthletePlaysForTeam, Phoenix Suns, TeamPlaysInLeague, NBA" "Nevada, StateLocatedInCountry, USA, StateLocatedInCountry, Utah".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ranks obtained by different initialization strategies (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Entity/Relation embeddings and energy functions used in KG embedding methods.</head><label>1</label><figDesc></figDesc><table># rel. # ent. # trip. (train/valid/test) # path 

WN18 
18 40,943 141,442 5,000 5,000 5,674,308 
NELL186 186 14,463 31,134 5,000 5,000 1,914,475 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the data sets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Link prediction results on the test sets of WN18 and NELL186. 

Micro-ACC (%) 
Macro-ACC (%) 

Random 
CBOW 
Skip-gram 
Random 
CBOW 
Skip-gram 

WN18 
SME (linear) 
84.70 
89.54 (↑6%) 
89.16 (↑5%) 
85.11 
89.11 (↑5%) 
90.57 (↑6%) 
SME (bilinear) 
84.30 
91.83 (↑9%) 
90.68 (↑8%) 
85.36 
90.49 (↑6%) 
89.89 (↑5%) 
TransE 
94.60 
96.98 (↑3%) 
97.23 (↑3%) 
86.74 
93.46 (↑8%) 
94.49 (↑9%) 
SE 
94.71 
96.46 (↑2%) 
96.42 (↑2%) 
87.99 
92.05 (↑5%) 
91.70 (↑4%) 

NELL186 
SME (linear) 
88.59 
89.95 (↑2%) 
91.19 (↑3%) 
84.42 
85.70 (↑2%) 
86.67 (↑3%) 
SME (bilinear) 
88.74 
93.22 (↑5%) 
92.86 (↑5%) 
83.41 
89.70 (↑8%) 
89.65 (↑7%) 
TransE 
82.54 
85.65 (↑4%) 
85.33 (↑3%) 
76.74 
80.06 (↑4%) 
80.06 (↑4%) 
SE 
89.00 
93.37 (↑5%) 
93.07 (↑5%) 
83.01 
87.89 (↑6%) 
87.98 (↑6%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Triple classification results on the test sets of WN18 and NELL186.</figDesc><table></table></figure>

			<note place="foot" n="1"> Two entities connected to a same node are always expected to have some implicit relationships, no matter how they are connected to the intermediate node.</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/ 6 https://github.com/glorotxa/SME</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments and suggestions. This work is supported by the National Natural Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 24th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1568" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random walks and neural network language models on knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josu</forename><surname>Goikoetxea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1434" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantically smooth knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dbpedia: A largescale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Semantic Web Journal</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at International Conference on Learning Representations</title>
		<meeting>Workshop at International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1859" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
