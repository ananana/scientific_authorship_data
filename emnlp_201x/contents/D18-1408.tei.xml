<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phrase-level Self-Attention Networks for Universal Sentence Encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><surname>Key</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Phrase-level Self-Attention Networks for Universal Sentence Encoding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3729" to="3738"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3729</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Universal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly paral-lelizable computation and significantly less training time. However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected. To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word&apos;s representation hierarchically with longer-term context dependencies captured in a larger phrase. As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. At the same time, syntactic information can be easily integrated in the model. Experiment results show that PSAN can achieve the state-of-the-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence tex-tual similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Following the success of word embeddings <ref type="bibr" target="#b0">(Bengio et al., 2003;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013)</ref>, one of NLP's next challenges has become the hunt for universal sentence encoders. The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks. The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for downstream NLP tasks, especially for those with relatively small datasets.</p><p>Previous models for sentence encoding typi- cally rely on Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b7">Chung et al., 2014</ref>) or Convolutional Neural Networks (CNNs) ( <ref type="bibr" target="#b14">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b30">dos Santos and Gatti, 2014;</ref><ref type="bibr" target="#b16">Kim, 2014;</ref><ref type="bibr" target="#b23">Mou et al., 2016</ref>) to pro- duce context-aware representation. RNNs encode a sentence by reading words in sequential or- der, they are capable of learning long-term de- pendencies but are hard to parallelize and not time-efficient. CNNs focus on local or position- invariant dependencies but do not perform well on many tasks <ref type="bibr" target="#b31">(Shen et al., 2017)</ref>.</p><p>Fully attention-based neural networks have at- tracted wide interest recently, because they can model both dependencies while being more par- allelizable and requiring significantly less time to train. <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref> proposed the multi- head attention to project a sentence to multiple semantic subspaces, then apply self-attention in each subspace and concatenate the attention re- sults. <ref type="bibr" target="#b31">Shen et al. (2017)</ref> proposed the directional self-attention, they apply forward and backward masks to the alignment score matrix to encode temporal order information, and computed atten- tion at feature level to select the features that can best describe the word's meaning in given context. Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence length. Fur- thermore, the syntactic property that is intrinsic to natural language is not considered at all.</p><p>Language is inherently tree structured, and the meaning of a sentence comes largely from com- posing the meanings of subtrees <ref type="bibr" target="#b6">(Chomsky, 1957)</ref>. Previous syntactic tree-based sentence encoders <ref type="bibr" target="#b32">(Socher et al., 2013;</ref><ref type="bibr" target="#b36">Tai et al., 2015</ref>) mainly rely on recursive networks. Although the composition-ality can be explicitly modeled, their models need expensive recursion computation and are hard to be trained by batched gradient descent methods.</p><p>In this paper, we propose the Phrase-level Self- Attention Networks (PSAN), for RNN/CNN-free sentence encoding, it inherits all the advantages of fully attention-based models while requires much less memory consumption. In addition, syntac- tic information can be incorporated into the model more easily. In our model, every sentence is split into multiple phrases based on parse tree, self- attention is performed at the phrase level instead of the sentence level, thus the memory consump- tion reduces rapidly as the number of phrases in- creases. Furthermore, a gated memory component is employed to refine word representations hierar- chically by incorporating longer-term context de- pendencies. As a result, syntactic information can be integrated into the model without expensive re- cursion computation. At last, multi-dimensional attention is applied on the refined word represen- tations to obtain the final sentence representation.</p><p>Following <ref type="bibr" target="#b10">Conneau et al. (2017)</ref>, we trained our sentence encoder on the SNLI ( <ref type="bibr" target="#b2">Bowman et al., 2015</ref>) dataset, and evaluate the quality of the obtained universal sentence representations on a wide range of transfer tasks. The SNLI dataset is extremely suitable for training sentence encoders because it is the largest high-quality human- annotated dataset that involves reasoning about the semantic relationships within sentences.</p><p>The main contributions of our work can be sum- marized as follows:</p><p>• We propose the Phrase-level Self-Attention mechanism (PSA) for contextualization. The memory consumption can be reduced be- cause self-attention is performed at the phrase level instead of the sentence level.</p><p>• A gated memory updating mechanism is pro- posed to refine each word representation hier- archically by incorporating different levels of contextual information along the parse tree.</p><p>• Our proposed PSAN model outperforms the state-of-the-art supervised sentence encoders on a wide range of transfer tasks with signif- icantly less memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Model</head><p>In this section, we introduce the Phrase-level Self- Attention Networks (PSAN) for sentence encod- ing. A phrase is a group of words that carry a specific idiomatic meaning and function as a con- stituent in the syntax of a sentence. Words in a phrase are syntactically and semantically related to each other. Therefore, it can be advantageous to learn a context-aware representation inside a phrase while filtering out information from outside the phrase using self-attention mechanism. In an attempt to better utilize the tree structure which is intrinsic to language, we propose the gated mem- ory updating mechanism to combine different lev- els of context information. At last, an attention mechanism is utilized to summarize all the token representations into a fixed-length sentence vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phrase Division</head><p>The phrase structure organizes words into nested constituents which can be successively divided into their parts as we move down the constituency- based parse trees. One phrase division shows only one aspect of context dependency. In order to capture different levels of context dependencies, we can split a sentence at different granularities. The number of levels T is a hyper-parameter to be tuned.</p><p>We can break down the nodes at T different lay- ers in the parse tree to capture T levels of context dependencies 1 , as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phrase-level Self-Attention</head><p>This is the core component of our model. It aims to learn a context-aware representation for each to- ken inside a phrase. In order to filter out informa- tion that is semantically or syntactically distant, self-attention is performed at the phrase level in- stead of the sentence level.</p><p>Similar to directional self-attention network (DiSAN) <ref type="bibr" target="#b31">(Shen et al., 2017)</ref>, Phrase-level Self- Attention uses multi-dimensional attention to compute the alignment score for each dimension of token embedding. Therefore, it can select the features that can best describe a word's specific meaning in any given context. Given a phrase P ∈ R l×d represented as a se- quence of word embeddings [p 1 , . . . , p l ], where l is the length of the phrase and d is the dimension of word embedding representation, we first com- pute the alignment score for each token pair in the phrase:</p><formula xml:id="formula_0">a ij = σ W a1 p i + W a2 p j + b a + M ij M ij = 0, i = j −∞, i = j (1)</formula><p>where σ (·) is an activation function, W a1 , W a2 ∈ R d×d and b a ∈ R d are parameters to be learned, and M is a diagonal-diabled mask ( <ref type="bibr" target="#b13">Hu et al., 2017</ref>) that aims to prevent a word from being aligned with itself.</p><p>The output of the attention mechanism is a weighted sum of embeddings from all tokens for each token in the phrase:</p><formula xml:id="formula_1">˜ p i = l j=1 exp (a ij ) l k=1 exp (a ik ) p j (2)</formula><p>where means point-wise product. Note that the alignment score for each token pair is a vector rather than a scalar in the multi-dimensional atten- tion.</p><p>The final output of Phrase-level Self-Attention is obtained by comparing each input representa- tion with its attention-weighted counterpart. We use a comparison function based on absolute dif- ference and element-wise multiplication which was similar to <ref type="bibr" target="#b38">Wang and Jiang (2016)</ref>. This com- parison function has the advantage of measuring the semantic similarity or relatedness of two se- quences.</p><formula xml:id="formula_2">c i = σ (W c [|p i − ˜ p i | ; p i ˜ p i ] + b c ) (3)</formula><p>where W c ∈ R d×2d and b a ∈ R d are parameters to be learned. c i is the representation for the i-th word in the phrase that captures local dependen- cies within the phrase. At last, we put together the Phrase-level Self- Attention results for non-overlapping phrases from the same phrase division of a sentence. For the t-th phrase division we can get C (t) = [c 1 , . . . , c ls ], the phrase-level self-attention results for the sentence from the t-th layer split, where l s is the sentence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gated Memory Updating</head><p>Above describes the Phrase-level Self-Attention (PSA) for one split of the parse tree. The parse tree can be split at different granularities. We pro- pose a novel gated memory updating mechanism to refine each word representation hierarchically with longer-term dependencies captured in a larger granularity. Inspired by the idea of adaptive gate in highway networks ( <ref type="bibr" target="#b34">Srivastava et al., 2015)</ref>, our memory mechanism add a gate to original memory networks ( <ref type="bibr" target="#b39">Weston et al., 2014;</ref><ref type="bibr" target="#b35">Sukhbaatar et al., 2015)</ref>. This gate has the ability to determine the importance of the new input and the original mem- ory in the memory updating.</p><formula xml:id="formula_3">C (t) = P SA M (t−1) G (t) = sigmoid W g M (t−1) ; C (t) + b g M (t) = G (t) σ W m M (t−1) ; C (t) + b m (4)</formula><p>where W g , W m ∈ R d×2d and b g , b m ∈ R d are pa- rameters to be learned. Note that in order to share representation power and to reduce the number of parameters, the parameters of gated memory up- dating are shared among different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sentence Summarization</head><p>In this layer, self-attention mechanism is em- ployed to summarize the refined representation of a sentence into a fixed-length vector. The self- attention mechanism can explore the dependencies among tokens within the whole sentence. As a re- sult, global dependencies can also be incorporated in the model.</p><formula xml:id="formula_4">e i = W e2 σ W e1 m (T ) i + b e1 + b e2 v = l i=1 exp (e i ) l j=1 exp (e j ) m (T ) i<label>(5)</label></formula><p>where W g , W m ∈ R d×d and b g , b m ∈ R d are parameters to be learned. After this step, the refined context-aware sentence representation is compressed into a fixed-length vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we conduct a plethora of exper- iments to study the effectiveness of the PSAN model. Following <ref type="bibr" target="#b10">Conneau et al. (2017)</ref>, we train our sentence encoder using the SNLI dataset, and evaluate it across a variety of NLP tasks including sentence classification, natural language inference and sentence textual similarity. To train the model, Adadelta optimizer (Zeiler, 2012) with a learning rate of 0.75 is used on the SNLI dataset. The dropout ( <ref type="bibr" target="#b33">Srivastava et al., 2014</ref>) rate and L2 regularization weight decay fac- tor γ are set to 0.5 and 5e-5. To test the model, the SentEval toolkit ( <ref type="bibr" target="#b9">Conneau and Kiela, 2018</ref>) is used as the evaluation pipeline for fairer compari- son.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Setting</head><p>Natural language inference (NLI) is a fundamental task in the field of natural language processing that involves reasoning about the semantic relationship between two sentences, which makes it a suitable task to train sentence encoding models.</p><p>We conduct experiments on the Stanford Nat- ural Language Inference (SNLI) dataset <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>.</p><p>The dataset has 570k human-annotated sentence pairs, each labeled with one of the following pre-defined relation- ships: Entailment (the premise entails the hy- pothesis), Contradiction (they contradict each other) and N eutral (they are irrelevant). </p><formula xml:id="formula_5">v inp = v p ; v h ; v p − v h ; v p v h<label>(6)</label></formula><p>At last, we feed the sentence-pair representation v inp into a two layer feed-forward network and use a sof tmax layer to make the classification. This is the de facto scheme for sentence encoders trained on SNLI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Setting</head><p>To show the modeling capacity and robustness of our proposed model, we evaluate our model across a wide range of tasks that can be solved purely based on the encoded semantics.  The transfer tasks used in evaluation can be concluded in the following classes: sentence classification (MR, CR, MPQA, SUBJ, SST2, SST5, TREC), natural language inference (SICK- E, SICK-R), semantic relatedness (STS14) and paraphrase detection (MRPC). <ref type="table" target="#tab_2">Table 1</ref> presents some statistics about the datasets 2 .</p><note type="other">70042 sentiment 2 1.95 3.35 5.03 5.53 3.22 2.15 SST5 11855 sentiment 5 2.00 3.53 6.10 10.08 5.71 3.31 TREC 5952 question type 6 2.00 3.73 5.59 5.03 2.98 1.99 SICK-E 9930 inference 3 1.93 3.40 4.92 5.01 2.85 1.97 SICK-R 9930 inference [0, 5] 1.93 3.40 4.92 5.01 2.85 1.97 STS14 4500 semantic similarity [0, 5] 1.96 3.58 5.12</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Baselines</head><p>We compare our model with the following super- vised sentence encoders:</p><p>• BiLSTM-Max ( <ref type="bibr" target="#b10">Conneau et al., 2017</ref>) is a simple but effective baseline that performs max-pooling over a bi-directional LSTM.</p><p>• AdaSent (Zhao et al., 2015) forms a hierar- chy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments.</p><p>•  • <ref type="bibr">DiSAN (Shen et al., 2017</ref>) is composed of a directional self-attention block with temporal order encoded, and a multi-dimensional at- tention that compresses the sequence into a vector representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Performance</head><p>Experiment results of our model and four base- lines are shown in <ref type="table" target="#tab_4">Table 2</ref>. Micro and macro accu- racies are two composite indicators for evaluating transfer performance of tasks whose metric is clas- sification accuracy. Macro accuracy is the propor- tion of true results in the population of instances from all tasks. Micro accuracy is the arithmetic mean of dev accuracies for each task. PSAN achieves the state-of-the-art performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 BiLSTM-max <ref type="bibr">79</ref>   with considerably fewer parameters, outperform- ing a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information. Especially when compared with previous best model BiLSTM-Max, PSAN can outperform their model with only 5% of their parameter numbers, demonstrating the ef- fectiveness of our model at extracting semantically important information from a sentence.</p><p>In <ref type="table" target="#tab_6">Table 3</ref>, we compare our model with baseline sentence encoders in each transfer task. PSAN can consistently outperform the baselines in almost ev- ery task considered. On the SICK dataset, which can be seen as an out-domain version of SNLI, our model can outperform the baselines by a large margin, demonstrating the semantic relationship learned on the SNLI can be well transfered to other domains. On the STS14 dataset, where sentence vectors can be more directly measured by the co- sine distance, our model can also achieve the state- of-the-art performance, indicating that our learned sentence representations are of high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>For thorough comparison, we implement seven ex- tra baselines to analyze the improvements con- tributed by each part of our PSAN model:</p><p>• PSA on the first/second/third layer only only uses the Phrase-level Self-Attention at the first/second/third layer of phrase division.</p><p>• w/o PSA applies self-attention at the sen- tence level and uses the gated memory updat- ing mechanism to refine each token represen- tation hierarchically.</p><p>• w/o syntactic division divides each sentence equally into small blocks, and applies PSA within each block. The number of blocks equals the number of phrases in that layer.</p><p>• w/o gated memory updating concatenates the outputs of Phrase-level Self-Attention from three layers of phrase division and feeds the result to a feed-forward layer.</p><p>• w/o both applies self-attention at the sen- tence level, and uses sentence summarization to summarize the attention results into a fixed length vector.</p><p>The results are listed in <ref type="table" target="#tab_7">Table 4</ref>. We can see that (2) performs best among (1), <ref type="formula" target="#formula_6">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref>, demon- strating that the second layer split is more expres- sive, because the number of words per phrase in the second layer is the most suitable. It is neither too small to capture context dependencies, nor too large to filter out irrelevant noise. <ref type="formula" target="#formula_6">(8)</ref> outperforms <ref type="formula" target="#formula_6">(1)</ref>, <ref type="formula" target="#formula_6">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref>, showing that combining phrase- level information from different granularities can further improve performance. We also experiment on models where the align- ment matrix is calculated at the sentence level or at the syntactic-irrelevant block level. <ref type="formula" target="#formula_4">(5)</ref>   <ref type="formula" target="#formula_6">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref>, demonstrating syntactic information helps in sen- tence representation.</p><p>When comparing <ref type="formula" target="#formula_5">(6)</ref> with <ref type="formula" target="#formula_6">(8)</ref>, we can tell that gated memory updating is a better method when used to refine token representation along the parse tree. We assume that memory updating resembles the tree structure of language in that larger phrase is composed in the knowledge of how smaller phrases are composed inside it.</p><p>Comparing <ref type="formula" target="#formula_6">(7)</ref> with <ref type="formula" target="#formula_6">(1)</ref>, <ref type="formula" target="#formula_6">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref>, we can find that performing self-attention at the phrase level is generally better than at the sentence level, indi- cating that reducing attention context into phrase level can effectively filter out words that are syn- tactically and semantically distant, thus focusing on the interaction with important words. Compar- ing <ref type="formula" target="#formula_6">(7)</ref> with <ref type="formula" target="#formula_6">(4)</ref>, we can draw the conclusion that memory updating is effective even when the inputs to each layer are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Sentence Length</head><p>Long-term dependencies are typically hard to cap- ture for sequential models like RNNs ( <ref type="bibr" target="#b1">Bengio et al., 1994;</ref><ref type="bibr" target="#b12">Hochreiter and Schmidhuber, 1997</ref>).</p><p>We conduct experiments to see how performance changes as the sentence length increases. In <ref type="figure" target="#fig_3">Fig- ure 2</ref>, we show the relationship between classifi- cation accuracy and the average length of sentence pair on the SNLI dataset. Sentence-level Self- Attention (w/o PSA model described in subsec- tion 4.2) is used as a baseline for our model. PSAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Memory(MB) Acc(%)  outperforms Sentence-level Self-Attention model consistently for longer sentences of length 14 to 20. This demonstrates that incorporating syntac- tic information by performing self-attention at the phrase level and refining each word's representa- tion hierarchically can help to capture long-term dependencies across words in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Memory Consumption</head><p>We conduct experiments to analyze the memory consumption reduction resulted from Phrase-level Self-Attention. To this end, we re-implement two fully attention-based models ( <ref type="bibr" target="#b37">Vaswani et al., 2017;</ref><ref type="bibr" target="#b31">Shen et al., 2017</ref>) on the TREC dataset. To make fair comparison, the dimensions of sentence vec- tors are set to 300, the same number as our model. <ref type="table" target="#tab_9">Table 5</ref> lists the results. Our PSAN model can out- perform the other two fully attention-based mod- els, while being more memory efficient. reducing more than 20% of memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization and Case Study</head><p>In order to analyze the attention changing pro- cess and the importance of each word in the sen- tence vector, we visualize the attention scores in the alignment matrix of each layer in Phrase- level Self-Attention and sentence summarization layer. To facilitate the visualization of the multi- dimension attention vector, we use the l2 norm of the attention vector for representation. In <ref type="figure" target="#fig_4">Figure 3</ref>, we can see that, the difference in attention weights between semantically important and unimportant words gets larger as the context becomes larger. This implies that token represen- tation can be gradually refined by the gated mem- ory updating mechanism. Furthermore, the align- ment matrix of a phrase can be refined even if the phrase division does not change between lay- ers. For instance, the word "girl" gets larger at- tention weight in the second layer division than in the first layer. This demonstrates that the <ref type="table">memory a  little  girl  wearing   a  green  dress  with  blue  dots  carries   a  box  by  its  handle   a  little  girl  wearing  a  green  dress  with  blue  dots  carries  a  box  by  its  handle   (a)   a  little  girl  wearing   a  green  dress  with  blue  dots  carries   a  box  by  its  handle   a  little  girl  wearing  a  green  dress  with  blue  dots  carries  a  box  by  its  handle   (b)   a  little  girl  wearing   a  green  dress  with  blue  dots  carries   a  box  by  its  handle   a  little  girl  wearing  a  green  dress  with  blue  dots  carries  a  box  by  its  handle   (c)   a</ref>  updating mechanism can gradually pick out im- portant words for sentence representation. Finally, nouns and verbs dominate the attention weights, while stop words like "a" and "its", contribute lit- tle to the final sentence representation, this indi- cates that PSAN can effectively pick out semanti- cally important words that are most representative for the meaning of the whole sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, self-attention mechanism has been suc- cessfully applied to the field of sentence encod- ing, it utilizes the attention mechanism to relate elements at different positions from a single sen- tence. Due to its direct access to each token repre- sentation, both long-term and local dependencies can be modeled flexibly. <ref type="bibr" target="#b20">Liu et al. (2016)</ref> lever- aged the average-pooled word representation to at- tend words appear in the sentence itself. <ref type="bibr" target="#b4">Cheng et al. (2016)</ref> proposed the LSTMN model for ma- chine reading, an attention vector is produced for each of its hidden states during the recurrent itera- tion, thus empowering the recurrent network with stronger memorization capability and the ability to discover relations among tokens. <ref type="bibr" target="#b19">Lin et al. (2017)</ref> obtained a fixed-size sentence embedding matrix by introducing self-attention. Different from the feature-level attention used in our model, their at- tention mechanism extracted different aspects of the sentence into multiple vector representations, and utilized a penalization term to encourage the diversity of different attention results.</p><p>Syntactic information can be useful for under- standing a natural language sentence. Many pre- vious researches utilized syntactic information to build sentence encoder from composing the mean- ings of subtrees. Tree-LSTM ( <ref type="bibr" target="#b36">Tai et al., 2015;</ref>) composed its hidden state from an input vector and the hidden states of arbitrar- ily many child units. In Tree-based CNN ( <ref type="bibr" target="#b24">Mou et al., 2015</ref><ref type="bibr" target="#b23">Mou et al., , 2016</ref>, a set of subtree feature detec- tors slide over the parse tree of a sentence, and a max-pooling layer is utilized to aggregate infor- mation along different parts of the tree.</p><p>Apart from the models that use parse infor- mation, there have been several researches that aimed to learn the hierarchical latent structure of text by recursively composing words into sen- tence representation. Among them, neural tree in- dexer <ref type="bibr" target="#b26">(Munkhdalai and Yu, 2017b</ref>) utilized LSTM or attentive node composition function to con- struct full n-ary tree for input text. Gumbel Tree- LSTM ( <ref type="bibr" target="#b5">Choi et al., 2018</ref>) used Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically. A major drawback of these models is that the recursion computation can be expensive and hard to be pro- cessed in batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose the Phrase-level Self-Attention Net- works (PSAN), a fully attention-based model that can utilize syntactic information for universal sen- tence encoding. By applying self-attention at the phrase level, we can filter out distant and unrelated words and focus on modeling interaction between semantically and syntactically important words, a gated memory updating mechanism is utilized to incorporate different levels of contextual informa- tion along the parse tree. Empirical results on a wide range of transfer tasks demonstrate the effec- tiveness of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>girl wearing a green dress with blue dots carries a box by its handle wearing a green dress with blue dots carries a box by its handle A little girl wearing a green dress with blue dots carries a box by its handleFigure 1 :</head><label>1</label><figDesc>Figure 1: An example of phrase division, the sentence and its parse tree are from the SNLI training data. The division is started from the root of a parse tree. In this example, a phrase will not be further divided if it contains 3 or less words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fol- lowing previous work (Bowman et al., 2015; Mou et al., 2016), we remove the instances which an- notators can not reach consensus on. In this way we get 549367/9842/9824 sentence pairs for train/validation/test set. Following the siamese architecture (Bromley et al., 1993), we apply PSAN to both the premise and the hypothesis with their parameters tied. v p and v h are fixed-length vector representations for the premise and the hypothesis respectively. The final sentence-pair representation is formed by concatenating the original vectors with the ab- solute difference and element-wise multiplication between them:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(Mou et al., 2016; Liu et al., 2016; Shen et al., 2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fine-grained classification accuracies for PSAN and Sentence-level Self-Attention on the SNLI dataset are compared on the left, how data are distributed along sentence length is shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) / (b) / (c): attention weights of Phrase-level Self-Attention mechanism in the third / second / first layer phrase division; (d): attention weights of the sentence summarization layer.</figDesc><graphic url="image-2.png" coords="8,229.12,79.97,112.54,112.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets. If the output is an integer, it represents the number of classes of the classification task. If the output is an interval, it represents the output range of the regression task. # phrases / sent. represents the average number of phrases per sentence for each layer of phrase division. # words / phrase represents the average number of words per phrase for each layer of phrase division. was selected based on what appears to be the com- munity consensus regarding the appropriate eval- uations for universal sentence representations. To facilitate comparison, we use the same sentence evaluation tool as Conneau et al. (2017) to auto- mate evaluation on all the tasks mentioned in this paper.</figDesc><table>5.34 2.92 2.04 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on SNLI and transfer tasks of 
various sentence encoders. dim: the size of sen-
tence representation. |θ|: the number of param-
eters. Test accuracies on SNLI, micro and macro 
averages of accuracies of dev set on transfer tasks 
are chosen as evaluation metrics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Transfer test results for our model and various baselines. Classification accuracy is chosen as 
evaluation metric for datasets including MR, CR, SUBJ, MPQA, SST, TREC and SICK-E; Classification 
accuracy and F1-score are chosen for MRPC; Pearson correlation is chosen for SICK-R; Pearson and 
Spearman correlations are chosen for STS-14. 

Model 
Acc(%) 
(1) PSA on the first layer only 
84.9 
(2) PSA on the second layer only 
85.3 
(3) PSA on the third layer only 
84.6 
(4) w/o PSA 
85.3 
(5) w/o syntactic division 
85.5 
(6) w/o gated memory updating 
85.2 
(7) w/o both 
84.7 
(8) Full Model 
86.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the SNLI dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Memory consumption and test accuracy 
of three fully attention-based models on the TREC 
dataset. 

</table></figure>

			<note place="foot" n="1"> To avoid the situation that the produced phrases are too small, a phrase will not be further divided if its length is smaller than 4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our work is supported by National Natu-ral Science Foundation of China under Grant No.61433015 and the National Key Research and Development Program of China under Grant</head><p>No.2017YFB1002101. The corresponding au-thors of this paper are Houfeng Wang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 6, [7th NIPS Conference</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to compose task-specific tree structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntactic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noem</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of American Linguistics</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">174196</biblScope>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Senteval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05-13" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno>abs/1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-712" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
	<note>Skip-thought vectors</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional LSTM model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1605.09090</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shortcutstacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-08" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-712" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1611.01747</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobihani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
