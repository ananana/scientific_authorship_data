<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1299" to="1308"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel multi-task attention-based neural network model to address implicit discourse relationship representation and identification through two types of representation learning, an attention-based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora. The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our proposed model out-performs the state-of-the-art systems on benchmark corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of implicit discourse relation (or rhetor- ical relation) identification is to recognize how t- wo adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) be- tween them are logically connected to one anoth- er (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and lan- guage generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analy- sis, machine comprehension, etc.</p><p>With the release of PDTB 2.0 ( <ref type="bibr" target="#b13">Prasad et al., 2008)</ref>, lots of work has been done for discourse re- lation identification on natural (i.e., genuine) dis- course data <ref type="bibr" target="#b12">(Pitler et al., 2009;</ref><ref type="bibr" target="#b8">Lin et al., 2009</ref>; <ref type="bibr" target="#b22">Wang et al., 2010;</ref><ref type="bibr" target="#b28">Zhou et al., 2010;</ref><ref type="bibr" target="#b0">Braud and Denis, 2015;</ref><ref type="bibr" target="#b5">Fisher and Simmons, 2015</ref>) with the use of traditional NLP linguistically informed fea- tures and machine learning algorithms. Recently, more and more researchers resorted to neural net- works for implicit discourse recognition <ref type="bibr" target="#b26">(Zhang et al., 2015;</ref><ref type="bibr" target="#b14">Qin et al., 2016a</ref>; <ref type="bibr" target="#b10">Liu and Li, 2016;</ref><ref type="bibr" target="#b1">Braud and Denis, 2016;</ref><ref type="bibr" target="#b24">Wu et al., 2016)</ref>. Meanwhile, to alleviate the shortage of labeled data, researcher- s explored multi-task learning with the aid of u- nannotated data for implicit discourse recognition either in traditional machine learning framework <ref type="bibr" target="#b3">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b7">Lan et al., 2013)</ref> or recently in neural network framework ( <ref type="bibr" target="#b24">Wu et al., 2016;</ref>.</p><p>In this work, we present a novel multi-task attention-based neural network to address implic- it discourse relationship representation and recog- nition. It performs two types of representation learning at the same time. An attention-based neu- ral network conducts discourse relationship repre- sentation learning through interaction between t- wo discourse arguments. Meanwhile, a multi-task learning framework leverages knowledge from auxiliary task to enhance the performance of main task. Furthermore, these two types of learning are integrated into one neural network framework and work together to maximize the overall perfor- mance.</p><p>The contributions of this work are listed as fol- lows.</p><p>• We propose a multi-task attention-based neu- ral network model to address implicit dis- course relationship representation and recog- nition, which benefits from both the interac- tion between discourse arguments and the in- teraction between different learning tasks;</p><p>• Our method achieves the best results on two benchmark corpora in comparison with the state-of-the-art systems so far.</p><p>The organization of this work is as follows. Section 2 describes the proposed novel multi-task neural network. Section 3 introduces the exper-imental settings in detail. Section 4 reports the comprehensive experimental results on two bench- mark corpora. Section 5 summarized related work. Finally, Section 6 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-task Attention-based Neural</head><p>Networks Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>The idea of learning two types of interactive knowledge from arguments and from multi-tasks is motivated by the following observations and analysis.</p><p>On the one hand, to recognize the discourse re- lationships, our system needs to understand the meaning of each argument and infer the discourse sense transferred between two arguments (denoted as Arg-1 and Arg-2). Learning the semantic rep- resentation of each argument (sentence) has been studied with the use of many neural network mod- els and their variants (e.g., CNN, RNN, LSTM, Bi-LSTM, ect). However, learning the complicat- ed and various types of discourse relationships be- tween arguments may not be performed by simply summing up or concatenating two argument repre- sentations. We analyze the discourse with contrast relationship and find that the contrast information may result from different parts of sentence, e.g., tenses (e.g., previous vs. now), entities (their vs our), or even the whole arguments, etc. There- fore, in order to learn the relationship represen- tation between two arguments, we propose an at- tention mechanism that can select out the most im- portant part from two arguments and perform the information interaction between two arguments.</p><p>On the other hand, one common issue involved in implicit discourse relationship identification is the lack of labeled data. In this work, we state that the relevant information from unlabelled da- ta might be helpful and we present a novel multi- task learning framework. In contrast with previ- ous multi-task learning framework in traditional machine learning, we improve multi-task learning framework with representation learning for better discourse relationship representation.</p><p>Inspired by the above considerations, we present a novel multi-task attention-based neural network model by integrating attention mechanis- m with multi-task learning for information inter- action between arguments and between tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Discourse Representation</head><p>To learn the semantic representation of each argu- ment in discourse, a lot of neural network mod- els and their variants have been proposed, such as, convolutional neural network (CNN), recurren- t neural network (RNN) and so on. As a variant of RNN, long-short term memory (LSTM) neural network specifically addresses the issue of learn- ing long-term dependencies and is good at model- ing over a sequence of words with consideration of the contextual information. Therefore, in this work we adopt LSTM to model discourse argu- ment. l, through the embedding layer, we associate each word w in the vocabulary with a vector represen- tation x w ∈ R dw . Let x 1 i (x 2 i ) be the i-th word vector in Arg-1 (Arg-2), then these two discourse arguments are represented as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">LSTM for Argument Representation</head><formula xml:id="formula_0">Arg-1: [x 1 1 , x 1 2 , · · · , x 1 L 1 ] (1) Arg-2: [x 2 1 , x 2 2 , · · · , x 2 L 2 ]<label>(2)</label></formula><p>where</p><formula xml:id="formula_1">Arg-1 (Arg-2) has L 1 (L 2 ) words.</formula><p>Given the word representations of the argument</p><formula xml:id="formula_2">[x 1 , x 2 , · · · , x L ] as the input sequence, an LSTM computes the state sequence [h 1 , h 2 , · · · , h L ] for</formula><p>each time step i using the following formulation:</p><formula xml:id="formula_3">i i = σ(W i [x i , h i−1 ] + b i )<label>(3)</label></formula><formula xml:id="formula_4">f i = σ(W f [x i , h i−1 ] + b f )<label>(4)</label></formula><formula xml:id="formula_5">o i = σ(W o [x i , h i−1 ] + b o )<label>(5)</label></formula><formula xml:id="formula_6">˜ c i = tanh(W c [x i , h i−1 ] + b c )<label>(6)</label></formula><formula xml:id="formula_7">c i = i i ˜ c i + f i c i−1 (7) h i = o i tanh(c i )<label>(8)</label></formula><p>where [ ] means the concatenation operation of vectors, σ denotes the sigmoid function and de- notes element-wise product. Besides, i i , f i , o i and c i denote the input gate, forget gate, output gate and memory cell, respectively. Moreover, we also use bidirectional LSTM (Bi-LSTM) which is able to capture the context from both past and fu- ture rather than LSTM which only considers the context information from the past. Therefore, at each position i of the sequence, we obtain two s- tates</p><formula xml:id="formula_8">− → h i and ← − h i , where − → h i , ← − h i ∈ R d h . Then we concatenate them to get the intermediate state, i.e. h i = [ − → h i , ← − h i ]. After that, we sum up the sequence states [h 1 , h 2 , · · · , h L ]</formula><p>to get the repre- sentations of Arg-1 and Arg-2 respectively as fol- lows:</p><formula xml:id="formula_9">R Arg 1 = L 1 i=1 h 1 i (9) R Arg 2 = L 2 i=1 h 2 i (10)</formula><p>Finally we concatenate the two argument repre- sentations R Arg 1 and R Arg 2 as the argument pair representation, i.e.,</p><formula xml:id="formula_10">R pair = [R Arg 1 , R Arg 2 ].</formula><p>Clearly, in this way, there is no any correla- tion and interaction between the two arguments. That is, whatever the types of discourse relation- ship they hold, the argument pair representation R pair is independent from R Arg 1 or R Arg 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Attention Neural Network for Relationship Representation</head><p>In order to effectively capture the complicated and various types of relationships between arguments, we proposed a novel attention-based neural net- work model shown in <ref type="figure">Figure 2</ref>.</p><p>To do it, we first compute the match between R Arg 1 (R Arg 2 ) and each state h 2 i (h 1 i ) of Arg-2 (Arg-1) by taking the inner product followed by a word embedding word embedding</p><formula xml:id="formula_11">x x x x x Arg-1 Arg-2 x x x x + softmax LSTM LSTM Loss R Arg 2 R Arg1 p 1 p 2 ′ R Arg1 ′ R Arg 2</formula><p>Figure 2: Attention Neural Network for represen- tation learning of arguments.</p><p>softmax as follows:</p><formula xml:id="formula_12">p 1 i = Softmax(R T Arg 2 h 1 i )<label>(11)</label></formula><formula xml:id="formula_13">p 2 i = Softmax(R T Arg 1 h 2 i )<label>(12)</label></formula><p>where Softmax(</p><formula xml:id="formula_14">z i ) = e z i / j e z i .</formula><p>Here p is an attention (probability) vector over the inputs and can be viewed as the weights of the words mea- suring to what degree our model should pay atten- tion to. It is worth noting that p 1 and p 2 are de- termined by R Arg 2 and R Arg 1 respectively, which means the representation of one argument depends on the representation of the other.</p><p>Next, we sum over the state h i weighted by the attention vector p to compute the new representa- tions for Arg-1 and Arg-2 respectively as below:</p><formula xml:id="formula_15">R Arg 1 = L 1 i=0 h 1 i p 1 i (13) R Arg 2 = L 2 i=0 h 2 i p 2 i (14)</formula><p>The representation of Arg-2 (R Arg 2 ) is used to compute the weights of words in Arg-1 (i.e., p 1 ) and R Arg 1 is used to compute the weights of words in Arg-2 (i.e., p 2 ). In this way, the new representations of the two arguments interact with each other. Therefore, this attention mechanism enables our model to focus on specific spans in the two arguments, which is crucial to recognize the discourse relations. We then concatenate R to get the argument pair representation</p><formula xml:id="formula_16">R pair = [R Arg 1 , R Arg 2 ].</formula><p>Finally, we feed the argument pair vector R pair to a fully-connected softmax layer which outputs the probabilities of different classes for the clas- sification task. Here we choose the cross-entropy loss between the outputs of the softmax layer and the ground-truth class labels as our loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-task Attention-based Neural Networks</head><p>The model presented in Section 2.2 can perfor- m implicit discourse relation recognition in it- self. However, similar with many models in deep learning, one big issue is the lack of labeled da- ta. Therefore, we propose a multi-task attention- based neural network by integrating the aforemen- tioned model into a multi-task learning framework to address the implicit discourse relation recogni- tion with the aid of large amount of unlabelled da- ta. <ref type="figure">Figure 3</ref> shows the general framework of our proposed multi-task attention-based neural net- work model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arg Pair representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arg Pair representation</head><p>Arg-1</p><p>Arg  <ref type="figure">Figure 3</ref>: The framework of our proposed multi- task attention-based neural network model.</p><p>We use the aforementioned attention-based neural network to map the argument pair in- to a low-dimensional vector (R pair ) denoted as Arg Pair representation componen- t in <ref type="figure">Figure 3</ref>. Under the multi-task learning framework, the parameters of the Arg Pair representation components are shared be- tween the main task and the auxiliary tasks. We denote R main and R aux as the representations of argument pair for main and auxiliary tasks, respec- tively. And we add a hidden layer after R main and R aux to learn the task-specific representations fol- lowed by the softmax layers used to compute the loss of the main task (Loss main ) and the loss of the auxiliary task (Loss aux ), respectively.</p><p>Regarding the strategy of sharing knowledge learnt from auxiliary to main task, we propose the following three methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Equal Share</head><p>A simple and straightforward way is to equally share the knowledge learned from main task and auxiliary task. Therefore, the total loss of the multi-task neural network is calculated as:</p><formula xml:id="formula_17">Loss = Loss main + Loss aux (15)</formula><p>where Loss aux has the same weight as Loss main .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Weighted Share</head><p>Another method is to give different weights to the main and auxiliary task as below:</p><formula xml:id="formula_18">Loss = Loss main + w * Loss aux<label>(16)</label></formula><p>where w ∈ (0, 1] is a weight parameter. Clearly, a lower value of w means less importance of auxil- iary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Sigmoid (Gated) Interaction</head><p>The above two ways of sharing knowledge actual- ly have no deep interaction between the main and auxiliary tasks. They only share equal or weighted contributions from tasks to final result. Therefore, we propose a model that can perform interaction between tasks, which is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. We introduce two important parameters W inter ∈ R d pair ×d pair and b inter ∈ R d pair (d pair is the length of the argument pair repre- sentation vector R pair ) to fulfil the interaction between main and auxiliary tasks. As shown in the following Formula <ref type="formula" target="#formula_19">(17)</ref> and <ref type="formula" target="#formula_7">(18)</ref>, the new representation of argument pair R main is updated by the combination of W inter and R aux using a Sigmoid function.</p><formula xml:id="formula_19">R main = R main σ(W inter R aux + b inter )<label>(17)</label></formula><formula xml:id="formula_20">R aux = R aux σ(W inter R main + b inter )<label>(18)</label></formula><p>W inter and the Sigmoid function (σ) work to- gether to make information interacted between t- wo tasks and select useful relevant information out of the opposite tasks as <ref type="bibr">well</ref>  a parameter to be trained. This mechanism act- s as a gate to determine how much the informa- tion would pass through to the final result. There- fore, under the framework of multi-task and gated mechanism, the main and auxiliary tasks are capa- ble of not only sharing the parameters of learning argument pair representation but also interacting the representations learning from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Parameter Learning</head><p>We We applied dropout to the penultimate layer and set the dropout rate as 0.5. These parameters remain the same in experiments except the share weight w varies which will be discussed later. We chose the cross-entropy loss as loss function and adopt- ed AdaGrad (Duchi et al., 2011) with a learning rate of 0.001 and a minibatch size of 64 to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We adopted three corpora: PDTB 2.0 and CoNLL- 2016 datasets are annotated for discourse relation recognition evaluation, and the BLLIP corpus is unlabeled and used as auxiliary task. PDTB 2.0 is the largest annotated corpus of dis- course relations, which contains 2, 312 Wall Street Journal (WSJ) articles. The sense label of dis- course relations is hierarchically with three lev- els, i.e., class, type and sub-type. The top level contains four major semantic classes: Comparison (denoted as Comp.), Contingency (Cont.), Expan- sion (Exp.) and Temporal (Temp.). For each class, a set of types is used to refine relation sense. The set of subtypes is to further specify the semantic contribution of each argument. We focus on the top level (class) relations. Following <ref type="bibr" target="#b12">(Pitler et al., 2009)</ref>, we used sections 2-20 as training set, sec- tions 21-22 as test set, and sections 0-1 as develop- ment set. <ref type="table">Table 1</ref>   <ref type="table">Table 1</ref>: The statistics of four top level implicit discourse relations in PDTB 2.0.</p><p>The CoNLL-2016 Shared Task focuses on shallow discourse parsing, which provides two test datasets, i.e., one from PDTB section 23 denot- ed as CoNLL-Test set, and the other from a sim- ilar source and domain (English Wikinews 2 ) de- noted as CoNLL-Blind test set. Different from the sense labels in PDTB, the CoNLL-Test set has three sense levels and the EntRel label. Moreover, it merges several labels in the original annotation to reduce some sparsity without losing too much of the utility and the semantics of the sense.</p><p>BLLIP The North American News Text (Com- plete) is used as unlabeled data source to generate synthetic labeled data for auxiliary task. We re- move the explicit discourse connectives from raw texts and grant the explicit relations as the synthet- ic implicit relations. We obtain a resulting corpus with 100, 000 implicit relations by random sam- pling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Measures</head><p>We adopt precision (P), recall (R) and their har- monic mean, i.e., F 1 for performance evaluation. We also report accuracy for direct comparison with previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on PDTB in multiple binary classification</head><p>To be consistent with previous work, we first per- form multiple binary classification (one-versus- other) on the four top level classes in PDTB. Sev- eral previous studies merged EntRel with Expan- sion, which is also explored in our study and noted as Exp+. <ref type="table" target="#tab_4">Table 2</ref> shows the results of our proposed three models in terms of F 1 (%) on PDTB using multiple binary classification, where STL means single task learning, Eshare, Wshare and Gshare denote the equal share, weighted share and gated interaction share under multi-task framework re- spectively, Imp denotes the standard implicit re- lations dataset in PDTB (similarly, Imp denotes standard implicit relations dataset in the CoNLL dataset when we perform experiments on the CoN- LL dataset) used for training, Exp denotes all ex- plicit relations in sections 00-24 in PDTB (similar- ly, all explicit relations in the CoNLL dataset when we perform experiments on the CoNLL dataset), and BLLIP denotes the synthetic implicit relations extracted from BLLIP. For example, Imp + BLLIP indicates that Imp is used for main task and BLLIP is for auxiliary task. The first three rows in <ref type="table" target="#tab_4">Table 2</ref> list the result- s of LSTM, Bi-LSTM and attention neural net- work in the single task learning (STL) framework, which act as baselines for comparison with multi- task learning. We see that Bi-LSTM achieve s- lightly better performance than LSTM, which is consistent with previous work as Bi-LSTM con- siders the forward and backward direction contex- tual information while LSTM only considers the forward information. Compared with LSTM and Bi-LSTM, the attention neural network achieves much better performance. This indicates the effec- tiveness of attention mechanism for capturing the interaction between discourse arguments, which is crucial for relationship representation.</p><p>Generally, under the multi-task neural network framework, the three proposed multi-task neural networks, i.e., Eshare, Wshare and Gshare, out- perform the single task learning methods. Com- paring with Eshare and Wshare, we see that us- ing a low value of w is able to boost the perfor- mance and reduce the negative influence brought by auxiliary task. We then use the best w value in Wshare to construct the loss of Gshare and the Gshare achieves the best performance among all methods through information interaction between main and auxiliary tasks.</p><p>Comparing Imp + Exp with Imp + BLLIP, we see that using Exp as auxiliary task achieves low- er performance than using BLLIP and even hurt- s the performance compared with the single task. The possible reasons may result from (1) there is difference between explicit and implicit discourse relations and (2) the size of Exp dataset is much smaller than that of BLLIP and thus it is not large enough to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on PDTB and CoNLL-2016 in multi-class classification</head><p>We also perform multi-class classification on PDTB and CoNLL-2016. That is, a four-way clas- sification on the four top-level classes in PDTB and a 15-way classification on the 15 sense label- s in CoNLL dataset. <ref type="table" target="#tab_5">Table 3</ref> shows the results of multi-class classification on PDTB and CoNLL- 2016 corpora in terms of accuracy (%) and macro- averaged F 1 (%).</p><p>The results of multi-class classification are con- sistent with the results of binary classification. First, the attention neural network achieves bet- ter performance than LSTM and Bi-LSTM. Sec- ond, the multi-task learning methods outperform the single-task learning method. Thrid, the Gshare method achieves the best performance. <ref type="table" target="#tab_6">Table 4</ref> lists the performance of our best mod- el with the reported state-of-the-art systems on PDTB and CoNLL-2016. We see that our mod- el achieves F 1 improvements of 1.64% on Con- t., 0.97% on Exp., and 1.35% on Exp.+ against the best reported systems in binary classification. And in multi-class classification, our model also achieves the best performance of F 1 in four-way classification and accuracy in CoNLL-2016 Blind test set, which indicates that our model has good generality. Specially, ( ) and (Liu and Li, 2016) listed in <ref type="table" target="#tab_6">Table 4</ref>, which adopted neu- ral network-based multi-task framework, are quite    relevant to this work. ( ) present- ed a multi-task neural network, which considered information sharing between the main and auxil- iary task. Different from their work, our work inte- grates the attention-based interaction between ar- guments and the multi-task based interaction be- tween tasks into the final model. This is the main reason why our model achieves better per- formance in all types of relations, which shows the effectiveness of integrating gated mechanism into multi-task framework. Besides, ( <ref type="bibr" target="#b10">Liu and Li, 2016</ref>) used a complicated multi-level attention mecha- nism and the performance of our attention neural network in the single task is comparable to their results. Our multi-task attention model achieves better performance in most types with the aid of multi-task framework. Besides, our previous work in ( <ref type="bibr" target="#b7">Lan et al., 2013</ref>) listed in <ref type="table" target="#tab_6">Table 4</ref>, also presented a multi- task framework with traditional machine learning method to address implicit discourse recognition using BLLIP to obtain synthetic data. Clearly, un- der neural network-based multi-task framework, the attention and gated mechanism significantly improved the results and outperformed traditional machine learning method in all types of relations. <ref type="figure" target="#fig_5">Figure 5</ref> shows the performance of four binary classification on four top level classes influenced by different share weights w in Wshare multi-task framework. We see that the best performance is achieved when we use a lower value of w. This  indicates that a low value of w can boost perfor- mance and reduce the negative influence brought by auxiliary task and enable our model to pay more attention to the main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state-of-the-art Systems</head><note type="other">Comp. Cont. Exp. Exp+</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects of parameters w</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implicit Discourse</head><p>With the release of PDTB 2.0, a number of studies performed discourse relation recognition on nat- ural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learn- ing algorithms <ref type="bibr" target="#b12">(Pitler et al., 2009;</ref><ref type="bibr" target="#b8">Lin et al., 2009;</ref><ref type="bibr" target="#b22">Wang et al., 2010;</ref><ref type="bibr" target="#b0">Braud and Denis, 2015;</ref><ref type="bibr" target="#b5">Fisher and Simmons, 2015)</ref>.</p><p>Later, to make a full use of unlabelled data, sev- eral studies performed multi-task or unsupervised learning methods ( <ref type="bibr" target="#b7">Lan et al., 2013;</ref><ref type="bibr" target="#b0">Braud and Denis, 2015;</ref><ref type="bibr" target="#b5">Fisher and Simmons, 2015;</ref><ref type="bibr" target="#b16">Rutherford and Xue, 2015)</ref>.</p><p>Recently, with the development of deep learn- ing, researchers resorted to neural networks meth- ods ( <ref type="bibr" target="#b6">Ji and Eisenstein, 2015;</ref><ref type="bibr" target="#b26">Zhang et al., 2015;</ref><ref type="bibr" target="#b14">Qin et al., 2016a</ref>; <ref type="bibr" target="#b10">Liu and Li, 2016;</ref><ref type="bibr" target="#b1">Braud and Denis, 2016;</ref><ref type="bibr" target="#b24">Wu et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-task learning</head><p>Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. <ref type="bibr" target="#b3">(Collobert and Weston, 2008</ref>) proposed a multi- task neural network trained jointly on the rele- vant tasks using weight-sharing (sharing the word embeddings with tasks). ( ) pro- posed the multi-task neural network by modifying the recurrent neural network for text classification tasks. ( <ref type="bibr" target="#b7">Lan et al., 2013</ref>) present a multi-task learn- ing based system which can effectively use syn- thetic data for implicit discourse relation recogni- tion. ( <ref type="bibr" target="#b24">Wu et al., 2016</ref>) use bilingually-constrained synthetic implicit data for implicit discourse rela- tion recognition a multi-task neural network. ( ) propose a convolutional neural net- work embedded multi-task learning system to im- prove the performance of implicit discourse iden- tification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deep learning with Attention</head><p>Recently deep learning with attention has been widely adopted by NLP researchers. (  proposed an attention-based Bi-LSTM for relation classification. ( <ref type="bibr" target="#b23">Wang et al., 2016c</ref>) pro- posed an attention-based LSTM for aspect-level sentiment classification. ( <ref type="bibr" target="#b18">Tan et al., 2016</ref>) pro- posed a attentive LSTMs for Question Answer Matching. ( <ref type="bibr" target="#b19">Wang et al., 2016a)</ref> proposed an in- ner attention based RNN (add attention informa- tion before RNN hidden representation) for An- swer Selection in QA. ( <ref type="bibr" target="#b21">Wang et al., 2016b</ref>) pro- posed multi-level attention CNNs for relation clas- sification. ( <ref type="bibr" target="#b25">Yin et al., 2016)</ref> proposed an attentive convolutional neural network for QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>We present a novel multi-task attention-based neu- ral network model for implicit discourse relation- ship representation and identification. Our method captures both the discourse relationships through interactions between discourse arguments and the complementary knowledge through interactions between annotated and unannotated data. The ex- perimental results showed that our proposed mod- el outperforms the state-of-the-art systems on two benchmark corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 showsFigure 1 :</head><label>11</label><figDesc>Figure 1 shows the traditional LSTM model for representation learning of arguments. First of al</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sigmoid (Gated) interaction shared in multi-task framework (GShare).</figDesc><graphic url="image-37.png" coords="5,204.06,203.00,82.40,51.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>tried various settings of word embeddings trained on the BLLIP corpus with different dimen- sions d W E = [50, 100, 150, 200] by word2vec tool 1 and finally set dimensionality as 50 based on the results on development set. we also ex- plored the hidden state d h = [50, 100, 150, 200] and the size of hidden layer in multi-task frame- work d multi−task = [50, 80, 120, 150]. Finally, for binary classification and four way classifica- tion on PDTB, we chose d h = 50 and d multi−task = 80. For multi-class classification on CoNLL-2016, we chose d h = 100 and d multi−task = 120.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of top level implicit discourse relations in PDTB 2.0 with different weights w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>. Clearly, W inter is representation</head><label></label><figDesc></figDesc><table>Arg Pair 
representation 

Arg-1 
Arg-2 
Arg-1 
Arg-2 

Share 

hidden layer 

softmax 

Main Task 
Aux Task 

W 

R main 
R aux 

′ 
R aux 
′ 
R main 

Loss main 
Loss aux 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>summarizes the statistics of four top level implicit discourse relations in PDTB.</figDesc><table>Relation Train Dev Test 
Comp. 
1942 
197 152 
Cont. 
3342 
295 279 
Exp. 
7004 
671 574 
Temp. 
760 
64 
85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of multiple binary classification on the top level classes in PDTB corpus in terms 
of F 1 (%). 

PDTB (Four way) 
CoNLL-Test (Acc) CoNLL-Blind (Acc) 

STL 
LSTM 
F1: 36.16; Acc: 56.12 
34.45 
35.07 
Bi-LSTM 
F1: 36.54; Acc: 54.30 
34.85 
35.83 
Attention 
F1: 45.57; Acc: 57.55 
37.41 
38.36 

Eshare 
Imp + Exp 
F1: 44.17; Acc: 55.65 
35.56 
37.06 
Imp + BLLIP 
F1: 44.57; Acc: 55.85 
36.66 
38.28 

Wshare 
Imp + Exp 
F1: 45.03; Acc: 56.21 (w=0.3) 
36.24 (w=0.2) 
37.34 (w=0.3) 
Imp + BLLIP F1: 45.80; Acc: 58.95 (w=0.2) 
38.13 (w=0.1) 
39.14 (w=0.4) 

Gshare 
Imp + Exp 
F1: 45.70; Acc: 57.17 
37.84 
38.10 
Imp + BLLIP 
F1: 47.80; Acc: 57.39 
39.40 
40.12 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of multi-class classification on PDTB and CoNLL-2016 in terms of accuracy (Acc) 
(%) and macro-averaged F 1 (%). 

Binary Classification (F1) 
Multi-class Classification (Acc) 
Comp. Cont. Exp. Exp+ Temp 
PDTB (Four way) 
CoNLL-Test(Acc) CoNLL-Blind(Acc) 
(Chen et al., 2016) 
40.17 54.76 
-
80.62 31.32 
-
-
-
(Qin et al., 2016b) 
41.55 57.32 71.50 80.96 35.43 
-
-
-
(Liu and Li, 2016) 
39.86 54.48 70.43 80.86 38.84 F1: 46.29; Acc: 57.57 
-
-
(Wu et al., 2016) 
-
-
-
-
-
F1: 42.50; Acc: -
-
-
(Qin et al., 2016a) 
38.67 54.91 
-
80.66 32.76 
-
-
-
(Liu et al., 2016b) 
37.91 55.88 69.97 
-
37.17 F1: 44.98; Acc: 57.27 
-
-
(Lan et al., 2013) 
31.53 47.52 70.01 
-
29.51 
-
-
-
(Wang and Lan, 2016) 
-
-
-
-
-
-
40.91 
34.20 
(Rutherford and Xue, 2016) 
-
-
-
-
-
-
36.13 
37.67 
Our model 
40.73 58.96 72.47 81.36 38.50 F1: 47.80; Acc: 57.39 
39.40 
40.12 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison with the state-of-the-art systems reported on PDTB and CoNLL-2016, where -
means N.A. 

</table></figure>

			<note place="foot" n="1"> http://www.code.google.com/p/word2vec</note>

			<note place="foot" n="2"> https://en.wikinews.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by grants from Science and Technology Commission of Shanghai Munici-pality (14DZ2260800 and 15ZR1410700), Shang-hai Collaborative Innovation Center of Trustwor-thy Software for Internet of Things (ZF1213) and NSFC (61402175).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparing word representations for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning connective-based word representations for implicit discourse relation identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Implicit discourse relation detection via a deep architecture with gated relevance network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spectral semisupervised discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Simmons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One vector is not enough: Entity-augmented distributed semantics for discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno>arX- iv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations via repeated reading: Neural networks with multi-level attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1224" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno>arX- iv:1603.02776</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Penn Discourse TreeBank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Implicit discourse relation recognition with contextaware character-enhanced embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A stacking gated neural architecture for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving the inference of implicit discourse relations via classifying explicit discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT</title>
		<meeting>the NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Robust non-explicit neural discourse parser in english and chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Attapol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved representation learning for question answer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inner attention based recurrent neural networks for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two end-to-end shallow discourse parsers for english and chinese in conll-2016 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL16 shared task</title>
		<meeting>the CoNLL16 shared task</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kernel based discourse relation recognition with temporal ordering information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="710" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiaoyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilinguallyconstrained synthetic data for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2306" to="2312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple question answering by attentive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1746" to="1756" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shallow convolutional neural network for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2230" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting discourse connectives for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
