<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
							<email>zhanghj@300188.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xiamen Meiya Pico information Co.,Ltd. Xiamen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiji</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4273" to="4283"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4273</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose an addition-subtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT), typically with an attention-based encoder-decoder frame- work ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, has recently be- come the dominant approach to machine transla- tion and already been deployed for online trans- lation services ( . Recurrent neu- ral networks (RNN), e.g., LSTMs <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>) or GRUs ( <ref type="bibr" target="#b8">Chung et al., 2014</ref>), are widely used as the encoder and de- coder for NMT. In order to alleviate the gradient * Corresponding author. vanishing issue found in simple recurrent neural networks (SRNN) <ref type="bibr" target="#b9">(Elman, 1990)</ref>, recurrent units in LSTMs or GRUs normally introduce different gates to create shotcuts for gradient information to pass through.</p><p>Notwithstanding the capability of these gated recurrent networks in learning long-distance de- pendencies, they use remarkably more ma- trix transformations (i.e., more parameters) than SRNN. And with many non-linear functions mod- eling inputs, hidden states and outputs, they are also less transparent than SRNN. These make NMT which is based on these gated RNNs suffer from not only inefficiency in training and infer- ence due to recurrency and heavy computation in recurrent units ( <ref type="bibr" target="#b30">Vaswani et al., 2017</ref>) but also diffi- culty in producing interpretable models ( <ref type="bibr" target="#b15">Lee et al., 2017)</ref>. These also hinder the deployment of NMT models particularly on memory-and computation- limited devices.</p><p>In this paper, our key interest is to simplify re- current units in RNN-based NMT. In doing so, we want to investigate how further we can ad- vance RNN-based NMT in terms of the number of parameters (i.e., memory consumption), run- ning speed and interpretability. This simplification shall preserve the capability of modeling long- distance dependencies in LSTMs/GRUs and the expressive power of recurrent non-linearities in SRNN. The simplification shall also reduce com- putation load and physical memory consumption in recurrent units on the one hand and allow us to take a good look into the inner workings of RNNs on the other hand.</p><p>In order to achieve this goal, we propose an addition-subtraction twin-gated recurrent network (ATR) for NMT. In the recurrent units of ATR, we only keep the very essential weight matrices: one over the input and the other over the his- tory (similar to SRNN). Comparing with previous RNN variants (e.g., LSTM or GRU), we have the smallest number of weight matrices. This will re- duce the computation load of matrix multiplica- tion. ATR also uses gates to bypass the vanishing gradient problem so as to capture long-range de- pendencies. Specifically, we use the addition and subtraction operations between the weighted his- tory and input to estimate an input and forget gate respectively. These add-sub operations not only distinguish the two gates so that we do not need to have different weight matrices for them, but also make the two gates dynamically correlate to each other. Finally, we remove some non-linearities in recurrent units.</p><p>Due to these simplifications, we can easily show that each new state in ATR is an unnormalized weighted sum of previous inputs, similar to re- current additive networks ( <ref type="bibr" target="#b15">Lee et al., 2017</ref>). This property not only allows us to trace each state back to those inputs which contribute more but also es- tablishes unnormalized forward self-attention be- tween the current state and all its previous inputs. The self-attention mechanism has already proved very useful in non-recurrent NMT ( <ref type="bibr" target="#b30">Vaswani et al., 2017)</ref>.</p><p>We build our NMT systems on the proposed ATR with a single-layer encoder and decoder. Experiments on WMT14 English-German and English-French translation tasks show that our model yields competitive results compared with GRU/LSTM-based NMT. When we integrate an orthogonal context-aware encoder (still single layer) into ATR-based NMT, our model (yield- ing 24.97 and 39.06 BLEU on English-German and English-French translation respectively) is even comparable to deep RNN and non-RNN NMT models which are all with multiple en- coder/decoder layers. In-depth analyses demon- strate that ATR is more efficient than LSTM/GRU in terms of NMT training and decoding speed.</p><p>We adapt our model to other language transla- tion and natural language processing tasks, includ- ing NIST Chinese-English translation, natural lan- guage inference and Chinese word segmentation. Our conclusions still hold on all these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most widely used RNN models are LSTM <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref> and GRU ( <ref type="bibr" target="#b8">Chung et al., 2014</ref>), both of which are good at handling gradient vanishing problem, a notorious bottleneck of the simple RNN <ref type="bibr" target="#b9">(Elman, 1990)</ref>. The design of gates in our model follows the gate philosophy in LSTM/GRU.</p><p>Our work is closely related to the recurrent additive network (RAN) proposed by <ref type="bibr" target="#b15">Lee et al. (2017)</ref>. They empirically demonstrate that many non-linearities commonly used in RNN transition dynamics can be removed, and that recurrent hid- den states computed as purely the weighted sum of input vectors can be quite efficient in lan- guage modeling. Our work follows the same spirit of simplifying recurrent units as they do. But our proposed ATR is significantly different from RAN in three aspects. First, ATR is simpler than RAN with even fewer parameters. There are only two weight matrices in ATR while four different weight matrices in the simplest version of RAN (two for each gate in RAN). Second, since the only difference between the input and forget gate in ATR is the addition/subtraction operation between the history and input, the two gates can be learned to be highly correlated as shown in our analysis. Finally, although RAN is verified effective in lan- guage modeling, our experiments show that ATR is better than RAN in machine translation in terms of both speed and translation quality.</p><p>To speed up RNN models, a line of work has attempted to remove recurrent connections. For example, <ref type="bibr" target="#b4">Bradbury et al. (2016)</ref> propose the quasi- recurrent neural network (QRNN) which uses con- volutional layers and a minimalist recurrent pool- ing function to improve parallelism. Very recently, <ref type="bibr" target="#b16">Lei and Zhang (2017)</ref> propose a simple recurrent unit (SRU). With the cuDNN optimization, their RNN model can be trained as fast as CNNs. How- ever, to obtain promising results, QRNN and SRU have to use deep architectures. In practice, 4-layer QRNN encoder and decoder are used to gain trans- lation quality that is comparable to that of single- layer LSTM/GRU NMT. In particular, our one- layer model achieves significantly higher perfor- mance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. <ref type="bibr" target="#b45">Zhou et al. (2016)</ref> introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. <ref type="bibr" target="#b33">Wang et al. (2017a)</ref> propose a linear associate unit to reduce the gradi- ent propagation length along layers in deep NMT. <ref type="bibr" target="#b11">Gehring et al. (2017b)</ref> and <ref type="bibr" target="#b30">Vaswani et al. (2017)</ref> explore purely convolutional and attentional archi-</p><formula xml:id="formula_0">× + × × tanh σ tanh σ σ c t ht ht−1 c t−1 xt × tanh σ ht xt ht−1 σ × 1- × + ht xt ht−1 × + σ- × σ+ Concatenate Copy Neural Network Neural Network Layer Pointwise Operation Vector Transfer (a) LSTM σ tanh σ σ ht ht−1 xt × tanh σ ht xt ht−1 σ × 1- × + ht xt ht−1 × + σ- × σ+ Concatenate Copy Neural Network Neural Network Layer Pointwise Operation Vector Transfer (b) GRU tanh σ xt σ ht xt ht−1 × + σ- × σ+ Concatenate Copy Neural Network Neural Network Layer</formula><p>Pointwise Operation</p><p>Vector Transfer (c) ATR <ref type="figure">Figure 1</ref>: Architecture for LSTM, GRU and ATR. c * indicates the memory cell specific to the LSTM network. x * and h * denote the input and output hidden states respectively.</p><p>tectures as alternatives to RNNs for neural transla- tion. With careful configurations, their deep mod- els achieve state-of-the-art performance on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Addition-Subtraction Twin-Gated Recurrent Network</head><p>Given a sequence x = {x 1 , x 2 ,. . . , x T }, RNN up- dates the hidden state h t recurrently as follows:</p><formula xml:id="formula_1">h t = φ(h t−1 , x t )<label>(1)</label></formula><p>where h t−1 is the previous hidden state, which is considered to store information from all previous inputs, and x t is the current input. The function φ(·) is a non-linear recurrent function, abstracting away from details in recurrent units. GRU can be considered as a simplified version of LSTM. In this paper, theoretically, we use GRU as our benchmark and propose a new recurrent unit to further simplify it. The GRU function is defined as follows (see <ref type="figure">Figure 1b</ref>):</p><formula xml:id="formula_2">z t = σ(W z x t + U z h t−1 ) (2) r t = σ(W r x t + U r h t−1 ) (3) ˜ h t = tanh(W h x t + U h (r t h t−1 )) (4) h t = z t h t−1 + (1 − z t ) ˜ h t (5)</formula><p>where denotes an element-wise multiplication.</p><p>The reset gate r t and update gate z t enable man- ageable information flow from the history and the current input to the new state respectively. Despite the success of these two gates in handling gradient flow, they consume extensive matrix transforma- tions and weight parameters. We argue that many of these matrix transfor- mations are not essential. We therefore propose an addition-subtraction twin-gated recurrent unit (ATR), formulated as follows (see <ref type="figure">Figure 1c</ref>):</p><formula xml:id="formula_3">p t = W h h t−1 , q t = W x x t (6) i t = σ(p t + q t ) (7) f t = σ(p t − q t ) (8) h t = i t q t + f t h t−1 (9)</formula><p>The hidden state h t in ATR is a weighted mixture of both the current input q t and the history h t−1 controlled by an input gate i t and a forget gate f t respectively. Notice that we use the transformed representation q t for the current input rather than the raw vector x t due to the potential mismatch in dimensions between h t and x t . Similar to GRU, we use gates, especially the forget gate, to control the back-propagated gradi- ent flow to make sure gradients will neither vanish nor explode. We also preserve the non-linearities of SRNN in ATR but only in the two gates.</p><p>There are three significant differences of ATR from GRU. Some of these differences are due to the simplifications introduced in ATR. First, we squeeze the number of weight matrices in gate cal- culation from four to two (see Equation (2&amp;3) and (7&amp;8)). In all existing gated RNNs, the inputs to gates are weighted sum of the previous hid- den state and input. In order to distinguish these gates, the weight matrices over the previous hid- den state and the current input should be different for different gates. The number of different weight matrices in gates is therefore 2|#gates| in previ- ous gated RNNs. Different from them, ATR intro- duces different operations (i.e., addition and sub- traction) between the weighted history and input to distinguish the input and forget gate. Therefore, the weight matrices over the previous state/input in the two gates can be the same in ATR. Second, we keep the very essential non-linearities, only in the two gates. In ATR, the role of q t is similar to that of˜hof˜ of˜h t in GRU (see Equation <ref type="formula">(4)</ref>). However, we completely wipe out the recurrent non-linearity of˜hof˜ of˜h t in q t (i.e., q t = W x x t ). <ref type="bibr" target="#b15">Lee et al. (2017)</ref> show that this non-linearity is not necessary in lan- guage modeling. We further empirically demon- strate that it is neither essential in machine trans- lation. Third, in GRU the gates for˜hfor˜ for˜h t and h t−1 are coupled and normalized to 1 while we do not explicitly associate the two gates in ATR. Instead, they can be learned to be correlated in an implicit way, as shown in the next subsection and our em- pirical analyis in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Twin-Gated Mechanism</head><p>Unlike GRU, we use an addition and subtraction operation over the transformed current input q t and history p t to differentiate the two gates in ATR. As the two gates have the same weights for their input components with only a single differ- ence in the operation between the input compo- nents, they act like twins. We term the two gates in ATR as twin gates and the procedure, shown in Equation (7&amp;8), as the twin-gated mechanism. This mechanism endows our model with the fol- lowing two advantages: 1) Both addition and sub- traction operations are completely linear so that fast computation can be expected; and 2) No other weight parameters are introduced for gates so that our model is more memory-compact.</p><p>A practical question for the twin-gated mech- anism is whether twin gates are really capable of dynamically weighting the input and history infor- mation. To this end, we plot the surface of one- dimensional σ(x + y) − σ(x − y) in <ref type="figure" target="#fig_0">Figure 2</ref>. It is clear that both gates are highly non-linearly cor- related, and that there are regions where σ(x + y) is equal to, greater or smaller than σ(x − y). In other words, by adapting the distribution of input and forget gates, the twin-gated mechanism has the potential to automatically seek suitable regions in <ref type="figure" target="#fig_0">Figure 2</ref> to control its preference between the new and past information. We argue that the in- put and forget gates are negatively correlated after training, and empirically show their actual corre- lation in Section 5.1.</p><formula xml:id="formula_4">Model # WM # MT LSTM 8 8 GRU 6 6 RAN 4 4 ATR 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computation Analysis</head><p>Here we provide a systematical comparison of computations in LSTM, GRU, RAN and our ATR with respect to the number of weight matrices and matrix transformations. Notice that all these units are building blocks of RNNs so that the total com- putational complexity and the minimum number of sequential operations required are unchanged, i.e. O(n · d 2 ) and O(n) respectively where n is the sequence length and d is the dimensionality of hidden states. However, the actual number of ma- trix transformations in the unit indeed significantly affects the running speed of RNN in practice.</p><p>We summarize the results in <ref type="table" target="#tab_0">Table 1</ref>. LSTM contains three different gates and a cell state, in- cluding 4 different neural layers with 8 weight ma- trices and transformations. GRU simplifies LSTM by removing a gate, but still involves two gates and a candidate hidden state. It includes 3 differ- ent neural layers with 6 weight matrices and trans- formations. RAN further simplifies GRU by re- moving the non-linearity in the state transition and therefore contains 4 weight matrices in its sim- plest version. Although our ATR also has two gates, however, there are only 2 weight matrices and transformations, accounting for only a third and a quarter of those in GRU and LSTM respec- tively. To the best of our knowledge, ATR has the smallest number of weight transformations in ex- isting gated RNN units. We provide a detailed and empirical analysis on the speed in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interpretability Analysis of Hidden States</head><p>An appealing property of the proposed ATR is its interpretability. This can be demonstrated by rolling out Equation (9) as follows:</p><formula xml:id="formula_5">h t = i t q t + f t h t−1 = i t W t x t + t−1 k=1 i k t−k l=1 f k+l W x x k ≈ t k=1 g k W x x k<label>(10)</label></formula><p>where g k can be considered as an approximate weight assigned to the k-th input. Similar to the RAN model ( <ref type="bibr" target="#b15">Lee et al., 2017)</ref>, the hidden state in ATR is a component-wise weighted sum of the inputs. This not only enables ATR to build up essential dependencies between preceding inputs and the current hidden state, but also allows us to easily detect which previous words have the promising impacts on the current state. This de- sirable property obviously makes ATR highly in- terpretable.</p><p>Additionally, this form of weighted sum is also related to self-attention ( <ref type="bibr" target="#b30">Vaswani et al., 2017)</ref>. It can be considered as a forward unnormalized self- attention where each hidden state attends to all its previous positions. As the self-attention mech- anism has proved very useful in NMT ( <ref type="bibr" target="#b30">Vaswani et al., 2017)</ref>, we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visu- alize the dependencies captured by Equation <ref type="formula" target="#formula_1">(10)</ref> in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by case- sensitive BLEU-4 metric ( <ref type="bibr" target="#b20">Papineni et al., 2002</ref>). Details about each dataset are as follows: , we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set.</p><p>English-French We used the WMT 2014 train- ing data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set.</p><p>The used NMT system is an attention-based encoder-decoder system, which employs a bidi- rectional recurrent network as its encoder and a two-layer hierarchical unidirectional recurrent network as its decoder, companied with an addi- tive attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. We replaced the recurrent unit with our proposed ATR model. More details are given in Ap- pendix A.1.</p><p>We also conducted experiments on Chinese- English translation, natural language inference and Chinese word segmentation. Details and ex- periment results are provided in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We set the maximum length of training instances to 80 words for both English-German and English- French task. We used the byte pair encoding com- pression algorithm <ref type="bibr" target="#b24">(Sennrich et al., 2016</ref>) to re- duce the vocabulary size as well as to deal with the issue of rich morphology. We set the vocabulary size of both source and target languages to 40K for all translation tasks. All out-of-vocabulary words were replaced with a token "unk".</p><p>We used 1000 hidden units for both encoder and decoder. All word embeddings had dimensional- ity 620. We initialized all model parameters ran- domly according to a uniform distribution ranging from -0.08 to 0.08. These tunable parameters were then optimized using Adam algorithm ( <ref type="bibr" target="#b14">Kingma and Ba, 2015</ref>) with the two momentum parame- ters set to 0.9 and 0.999 respectively. Gradient clipping 5.0 was applied to avoid the gradient ex- plosion problem. We trained all models with a learning rate 5e −4 and batch size 80. We decayed the learning rate with a factor of 0.5 between each training epoch. Translations were generated by a beam search algorithm that was based on log- likelihood scores normalized by sentence length. We used a beam size of 10 in all the experiments. We also applied dropout for English-German and English-French tasks on the output layer to avoid over-fitting, and the dropout rate was set to 0.2.</p><p>To train deep NMT models, we adopted the GNMT architecture ( . We kept all the above settings, except the dimensionality</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Architecture Vocab tok BLEU detok BLEU <ref type="bibr" target="#b5">Buck et al. (2014)</ref> WMT14 winner system phrase-based + large LM - - 20.70 Existing deep NMT systems (perhaps different tokenization) <ref type="bibr" target="#b45">Zhou et al. (2016)</ref> LSTM with 16 layers + F-F connections 160K 20.60 - <ref type="bibr" target="#b16">Lei and Zhang (2017)</ref> SRU with 10 layers 50K 20.70 - Antonino and Federico <ref type="formula" target="#formula_1">(2018)</ref>   <ref type="table">Table 2</ref>: Tokenized (tok) and detokenized (detok) case-sensitive BLEU scores on the WMT14 English- German translation task. "unk replace" and "PosUnk" denotes the approach of handling rare words in <ref type="bibr" target="#b13">Jean et al. (2015)</ref> and <ref type="bibr" target="#b18">Luong et al. (2015a)</ref> respectively. "RL" and "WPM" is the reinforcement learning optimization and word piece model used in . "CA" is the context-aware recurrent encoder ( <ref type="bibr" target="#b40">Zhang et al., 2017b</ref>). "LAU" and "F-F" denote the linear associative unit and the fast-forward architecture proposed by <ref type="bibr" target="#b33">Wang et al. (2017a)</ref> and <ref type="bibr" target="#b45">Zhou et al. (2016)</ref> respectively. "aan" denotes the average attention network proposed by .</p><p>of word embedding and hidden state which we set to be 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on English-German Translation</head><p>The translation results are shown in <ref type="table">Table 2</ref>.</p><p>We also provide results of several existing sys- tems that are trained with comparable experimen- tal settings to ours. In particular, our single model yields a detokenized BLEU score of 21.99. In order to show that the proposed model can be orthogonal to previous methods that improve LSTM/GRU-based NMT, we integrate a single- layer context-aware (CA) encoder (Zhang et al., 2017b) into our system. The ATR+CA system fur- ther reaches 22.7 BLEU, outperforming the win- ner system <ref type="bibr" target="#b5">(Buck et al., 2014</ref>) by a substantial im- provement of 2 BLEU points. Enhanced with the deep GNMT architecture, the GNMT+ATR sys- tem yields a gain of 0.89 BLEU points over the RNNSearch+ATR+CA and 1.6 BLEU points over the RNNSearch + ATR. Notice that different from our system which was trained on the parallel cor- pus alone, the winner system used a huge mono- lingual text to enhance its language model.</p><p>Compared with the existing LSTM-based (Lu- ong et al., 2015a) deep NMT system, our shal- low/deep model achieves a gain of 2.41/3.26 to- kenized BLEU points respectively. Under the same training condition, our ATR outperforms RAN by a margin of 0.34 tokenized BLEU points, and achieves competitive results against its GRU/LSTM counterpart. This suggests that although our ATR is much simpler than GRU, LSTM and RAN, it still possesses strong model- ing capacity.</p><p>In comparison to several advanced deep NMT models, such as the Google NMT (8 layers, 24.61 tokenized BLEU) ( ) and the LAU-connected NMT (4 layers, 23.80 tokenized BLEU) ( <ref type="bibr" target="#b33">Wang et al., 2017a</ref>), the performance of our shallow model (23.31) is competitive. Par- ticularly, when replacing LSTM in the Google NMT with our ATR model, the GNMT+ATR sys- tem achieves a BLEU score of 24.16, merely 0.45 BLEU points lower. Notice that although all sys- tems use the same training data of WMT14, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Architecture Vocab tok BLEU detok BLEU Existing end-to-end NMT systems Jean et al. <ref type="formula" target="#formula_1">(2015)</ref> RNNSearch <ref type="formula">(</ref>   <ref type="table">Table 3</ref>: Tokenized (tok) and detokenized (detok) case-sensitive BLEU scores on the WMT14 English- French translation task. "12M data" indicates the same training data as ours, while "36M data" is a significant larger dataset that contains the 12M data.</p><note type="other">et al. (2017a) RNNSearch (GRU) with 4 layers + LAU 30K 35.10 - Gehring et al. (2017a) Deep Convolutional Encoder 20 layers with kernel width 5 30K 35.70 - Vaswani et al. (2017) Transformer with 6 layers + 36M data + base model 32K 38.10 - Gehring et al. (2017b) ConvS2S with 15 layers + 36M data 40K 40.46 - Vaswani et al. (2017) Transformer with 6 layers + 36M data + big model 32K 41.80 - Wu et al. (2016) LSTM with 8 layers + RL WPM + 36M data + ensemble 32K 41.16 - Our end-to-end NMT systems this work RNNSearch + GRU +</note><p>tokenization of these work might be different from ours. However, the overall results can indicate the competitive strength of our model. In addition, SRU (Lei and Zhang, 2017), a recent proposed efficient recurrent unit, obtains a BLEU score of 20.70 with 10 layers, far more behind ATR's. We further ensemble eight likelihood-trained models with different random initializations for the ATR+CA system. The variance in the tok- enized BLEU scores of these models is 0.07. As can be seen from Table 2, the ensemble system achieves a tokenized and detokenized BLEU score of 24.97 and 24.33 respectively, obtaining a gain of 1.66 and 1.63 BLEU points over the single model. The final result of the ensemble system, to the best of our knowledge, is a very promising re- sult that can be reached by single-layer NMT sys- tems on WMT14 English-German translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on English-French Translation</head><p>Unlike the above translation task, the WMT14 English-French translation task provides a signifi- cant larger dataset. The full training data have ap- proximately 36M sentence pairs, from which we only used 12M instances for experiments follow- ing previous work <ref type="bibr" target="#b13">(Jean et al., 2015;</ref><ref type="bibr" target="#b10">Gehring et al., 2017a;</ref><ref type="bibr" target="#b19">Luong et al., 2015b;</ref><ref type="bibr" target="#b33">Wang et al., 2017a</ref>). We show the results in <ref type="table">Table 3</ref>.</p><p>Our shallow model achieves a tokenized BLEU score of 36.89 and 37.88 when it is equipped with the CA encoder, outperforming almost all the listed systems, except the Google NMT ( , the ConvS2S ( <ref type="bibr" target="#b11">Gehring et al., 2017b</ref>) and the Transformer ( <ref type="bibr" target="#b30">Vaswani et al., 2017)</ref>. En- hanced with the deep GNMT architecture, the GNMT+ATR system reaches a BLEU score of 38.59, which beats the base model version of the Transformer by a margin of 0.49 BLEU points. When we use four ensemble models (the variance in the tokenized BLEU scores of these ensemble models is 0.16), the ATR+CA system obtains an- other gain of 0.47 BLEU points, reaching a tok- enized BLEU score of 39.06, which is comparable with several state-of-the-art systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis on Twin-Gated Mechanism</head><p>We provide an illustration of the actual relation be- tween the learned input and forget gate in <ref type="figure" target="#fig_2">Figure  3</ref>. Clearly, these two gates show strong negative correlation. When the input gate opens with high values, the forget gate prefer to be close. Quantita- tively, on the whole test set, the Pearson's r of the input and forget gate is -0.9819, indicating a high correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis on Speed and Model Parameters</head><p>As mentioned in Section 3.2, ATR has much fewer model parameters and matrix transformations. We  provide more details in this section by comparing against the following two NMT systems:</p><p>• DeepRNNSearch (GRU): a deep GRU- equipped RNNSearch model ( ) with 5 layers. We set the dimension of word embedding and hidden state to 620 and 1000 respectively.</p><p>• Transformer: a purely attentional transla- tor ( <ref type="bibr" target="#b30">Vaswani et al., 2017)</ref>. We set the di- mension of word embedding and filter size to 512 and 2048 respectively. The model was trained with a minibatch size of 256.</p><p>We also compare with the GRU and LSTM-based RNNSearch. Without specific mention, all other experimental settings for all these models are the same as for our model. We implement all these models using the Theano library, and test the speed on one GeForce GTX TITAN X GPU card. We show the results on <ref type="table" target="#tab_6">Table 4</ref>. We observe that the Transformer achieves the best training speed, processing 4961 words per second. This is reasonable since the Transformer can be trained in full parallelization. On the con- trary, DeepRNNSearch is the slowest system. As RNN performs sequentially, stacking more lay- ers of RNNs inevitably reduces the training effi- ciency. However, this situation becomes the re- verse when it comes to the decoding procedure. The Transformer merely generates 44 words per second while DeepRNNSearch reaches 70. This is because during decoding, all these beam search-  Our model with the CA structure, using only 63.1M parameters, processes 3993 words per sec- ond during training and generates 186 words per second during decoding, which yields substantial speed improvements over the GRU-and LSTM- equipped RNNSearch. This is due to the light ma- trix computation in recurrent units of ATR. No- tice that the speed increase of ATR over GRU and LSTM does not reach 3x. This is be- cause at each decoding step, there are mainly two types of computation: recurrent unit and softmax layer. The latter consumes the most calculation, which, however, is the same for different models (LSTM/GRU/ATR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis on Dependency Modeling</head><p>As shown in Section 3.3, a hidden state in our ATR can be formulated as a weighted sum of the previ- ous inputs. In this section, we quantitatively ana- lyze the weights g k in Equation (10) induced from Equation (13). Inspired by <ref type="bibr" target="#b15">Lee et al. (2017)</ref>, we visualize the captured dependencies of an exam- ple in <ref type="figure">Figure 4</ref> where we connect each word to the corresponding previous word with the highest weight g k .</p><p>Obviously, our model can discover strong local dependencies.</p><p>For example, the to- ken "unglück@@" and "lichen" should be a Beide unglück@@ lichen Parteien wurden in die nahe gelegenen Krankenhäuser gebracht . (Both) (unfortunate) (parties) (were) (to) (the) (nearby) (located) (hospitals) (brought) .</p><p>Figure 4: Visualization of dependencies on a target (German) sentence (selected from newstest2014). "@@" indicates a separator that splits one token into two pieces of sub-words.</p><p>single word.</p><p>Our model successfully asso- ciates "unglück@@" closely to the generation of "lichen" during decoding. In addition, our model can also detect non-consecutive long- distance dependencies. Particularly, the predic- tion of "Parteien" relies heavily on the token "unglücklichen", which actually entails an amod linguistic dependency relationship. These cap- tured dependencies make our model more inter- pretable than LSTM/GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper has presented a twin-gated recurrent network (ATR) to simplify neural machine trans- lation. There are only two weight matrices and matrix transformations in recurrent units of ATR, making it efficient in physical memory usage and running speed. To avoid the gradient vanishing problem, ATR introduces a twin-gated mechanism to generate an input gate and forget gate through linear addition and subtraction operation respec- tively, without introducing any additional param- eters. The simplifications allow ATR to produce interpretable results.</p><p>Experiments on English-German and English- French translation tasks demonstrate the effective- ness of our model. They also show that ATR can be orthogonal to and applied with methods that improve LSTM/GRU-based NMT, indicated by the promising performance of the ATR+CA system. Further analyses reveal that ATR can be trained more efficiently than GRU. It is also able to transparently model long-distance dependencies.</p><p>We also adapt our ATR to other natural lan- guage processing tasks. Experiments show en- couraging performance of our model on Chinese- English translation, natural language inference and Chinese word segmentation, demonstrating its generality and applicability on various NLP tasks.</p><p>In the future, we will continue to examine the effectiveness of ATR on different neural models for NMT, such as the hierarchical NMT model ( ) as well as the generative NMT model ( ). We are also interested in adapting our ATR to summarization, semantic parsing etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the difference between σ(x + y) (the input gate) and σ(x − y) (the forget gate). Here, we set x, y ∈ [−5, 5].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>English-German To compare with previous re- ported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of correlation between the input and forget gate learned in Equation (13). For each gate, we record its mean value ( 1 d h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of LSTM, GRU, RAN and 
ATR in terms of the number of weight matrices 
(WM) and matrix transformations (MT). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison on the training and decoding 
speed and the number of model parameters of dif-
ferent NMT models on WMT14 English-German 
translation task with beam size 1. #PMs: the num-
ber of model parameters. Train/Test: the number 
of words in one second processed during train-
ing/testing. The number in bracket indicates the 
average decoding time per source sentence (in sec-
onds). 

based systems must generate translation one word 
after another. Therefore the parallelization advan-
tage of the Transformer disappears. In comparison 
to DeepRNNSearch, the Transformer spends extra 
time on performing self-attention over all previous 
hidden states. 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors were supported by National Nat-ural Science Foundation of China (Grants No. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Neural Machine Translation with Weakly-Recurrent Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antonino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01576</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3579" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Recurrent additive networks. CoRR, abs/1705.07393</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training RNNs as Fast as CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional LSTM model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1605.09090</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The first international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
	<note>SIGHAN &apos;03</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Variational recurrent neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05119</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hierarchyto-sequence attentional neural machine translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="632" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1511.06361</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Deep Neural Machine Translation with Linear Associative Unit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep neural machine translation with linear associative unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno>abs/1702.03814</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A gru-gated attention model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1704.08430</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accelerating neural transformer via an average attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A context-aware recurrent encoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incorporating word reordering knowledge into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Asynchronous bidirectional decoding for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongji</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1801.05122</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pro. of EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Modeling past and future for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno>abs/1711.09502</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep recurrent models with fast-forward connections for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
