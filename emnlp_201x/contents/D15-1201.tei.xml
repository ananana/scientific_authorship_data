<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
							<email>majid.yazdani@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="department" key="dep3">Xerox Research Center Europe</orgName>
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghdad</forename><surname>Farahmand</surname></persName>
							<email>meghdad.farahmand@ unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="department" key="dep3">Xerox Research Center Europe</orgName>
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
							<email>james.henderson@ xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="department" key="dep3">Xerox Research Center Europe</orgName>
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">University of Geneva</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation , machine translation and word sense disambiguation. We present methods of non-compositionality detection for En-glish noun compounds using the unsu-pervised learning of a semantic composition function. Compounds which are not well modeled by the learned semantic composition function are considered non-compositional. We explore a range of dis-tributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multiword Expressions (MWEs) are sequences of words that exhibit some kind of idiosyncrasy. This idiosyncrasy can be semantic, statistical, or syn- tactic <ref type="bibr">1</ref> . Ivory tower, speed limit, and at large are examples of semantically, statistically and syn- tactically idiosyncratic MWEs respectively. Note that an MWE can be idiosyncratic at several lev- els. In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional ( <ref type="bibr" target="#b0">Baldwin and Kim, 2010)</ref> and statistically idiosyn- cratic MWEs are commonly referred to as col- locations ( <ref type="bibr" target="#b20">Sag et al., 2002</ref>). Non-compositional MWEs are those whose meaning can not be read- ily inferred from the meaning of their constituents and collocations are those MWEs whose con- stituents co-occur more than expected by chance. Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for ex- ample word sense disambiguation ( <ref type="bibr" target="#b13">McCarthy et al., 2003</ref>) and machine translation <ref type="bibr" target="#b12">(Lin, 1999)</ref>. It may also be more challenging to model non- compositionality than collocational weight as the former has to do with modelling the semantics and the latter can to some extent be modeled by con- ventional statistical measures such as mutual in- formation. Detecting non-compositionality in an automatic fashion has been the aim of much pre- vious research.</p><p>In this paper, we capture non-compositionality of English Noun Compounds (NCs) 2 based on the assumption that the majority of the compounds are compositional, for which a composition func- tion can be learned. This implies that the com- pounds for which a composition function cannot be learned with a relatively low error are non- compositional.</p><p>In previous work on vector-space models of dis- tributional semantics, semantic composition has been commonly assumed to be a trivial predeter- mined function such as addition, multiplication, and their weighted variations <ref type="bibr" target="#b18">(Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b19">Reddy et al., 2011;</ref><ref type="bibr" target="#b21">Salehi et al., 2015)</ref>. Nevertheless there is some work that regards com- position as a more complex function. For in- stance <ref type="bibr" target="#b27">Widdows (2008)</ref> who propose (but doesn't empirically test) the use of Tensor and Convolu- tion products for modelling non-compositionality, <ref type="bibr" target="#b2">Baroni and Zamparelli (2010)</ref> who regard adjec- tives in adjectival-noun compositions as matri- ces that can be learned by linear regression, and  who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outper- form additive and multiplicative functions. In this work, we too assume that composition is ar- guably a complex function. We believe simpli- fied composition functions, such as additive and multiplicative functions and their weighted varia- tions, while having advantages such as being im- pervious to overfitting, can not completely cap- ture semantic composition. Nevertheless mod- elling composition by means of a powerful func- tion can be equally inadequate for our purposes. An overly powerful composition function mem- orizes all compositional and non-compositional compounds, resulting in overfitting and low learn- ing error that hinders discrimination between com- positional and non-compositional compounds. We examine various classes of composition functions, ranging from the least to the most powerful (in terms of learning capacity). We show that com- plex functions clearly do a better job in mod- elling semantic composition and in detecting non- compositionality compared to commonly used ad- ditive and multiplicative functions.</p><p>Compositional compounds are also decompos- able; intuitively, their semantics is the union of the semantics of their components. More for- mally, conditioned on the vector of the com- pound, vectors of the component words should be independently predictable. This principle, to- gether with the assumption that most of the com- pounds are compositional, leads to the conclusion that a model of composition should be able to be auto-reconstructive: the composition function that maps component-words' vectors to their com- pound vector should have an associated decom- position function that independently predicts each of the component-words' vectors from this com- pound vector. An auto-reconstructive model en- ables us to exploit more data in order to learn se- mantic composition and predict compositionality. We show that auto-reconstruction can improve the accuracy of composition functions and improve detecting non-compositionality.</p><p>To further improve non-compositionality de- tection, we propose an EM-like detection algo- rithm based on hidden compositionality annota- tions. The best composition is the one that is the best fit on all the data points except the non- compositional ones. Since we don't use annotated data at training time, we assume annotations to be hidden variables and iteratively alternate between optimizing the composition function and optimiz- ing the hidden compositionality annotations. We show that this iterative algorithm increases the accuracy of non-compositionality detection com- pared to the case when training is done on all ex- amples.</p><p>We run our experiments on the data set of <ref type="bibr" target="#b8">Farahmand et al. (2015)</ref> who provide a set of English NCs which are annotated with non- compositionality judgments.</p><p>We show that quadratic regression significantly outperforms ad- ditive and multiplicative baselines and all other models in modelling semantic composition and identifying the non-compositional NCs. In short, the contributions of our work are: to empirically evaluate various composition functions ranging from simple to overly complex in order to find the most accurate function; to propose, to the best of our knowledge for the first time, a method of iden- tifying non-compositional phrases as phrases for which a composition function cannot be readily learned; to propose learning decomposability as another criterion to detect non-compositionality; and to examine possible ways of improving the ac- curacy of the models by means of EM on hidden compositionality annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To the best of our knowledge, attempts to extract non-compositionality in computational linguistics go back to 1998. <ref type="bibr" target="#b24">Tapanainen et al. (1998)</ref> pro- pose a method to identify non-compositional verb- object collocations based on the semantic asym- metry of verb-object relation. They assume that in a verb-object idiomatic expression, the object is a more interesting element in the sense that if the object appears with one (or only a few) verbs in a large corpus, it presumably has an id-iomatic nature. <ref type="bibr" target="#b12">Lin (1999)</ref> argues that the mu- tual information between the constituents of a non-compositional phrase is significantly differ- ent from that of a phrase created by substitut- ing the constituents of that phrase with their sim- ilar words. Their evaluation reveals a low pre- cision (16 − 39%) and recall (14 − 21%). In any case this method is not able to discrimi- nate non-compositional MWEs from collocational MWEs as they share the same property of non- substitutability (their constituents cannot be re- placed with their synonyms). <ref type="bibr" target="#b1">Baldwin et al. (2003)</ref> present a method that decides about the non-compositionality of English NCs and verb particle constructions by using latent semantic analysis to calculate the similarity between a MWE and its components. They argue that a higher similarity indicates a higher degree of com- positionality. <ref type="bibr" target="#b13">McCarthy et al. (2003)</ref> devise a number of measures based on comparison of the neighbors of phrasal verbs and their correspond- ing simplex verbs. They evaluate these measures by calculating their correlation with human com- positionality judgments on a set of phrasal verbs. They show that some of the measures have signifi- cant correlations with human judgments. Venkata- pathy and Joshi (2005) present a supervised model that benefits from both collocational and con- textual information and ranks the MWE candi- dates based on their non-compositionality. <ref type="bibr" target="#b10">Katz and Giesbrecht (2006)</ref> use distributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distin- guish its idiomatic use from literal use. They fur- ther compare the context of a MWE with the con- text of its components and show that this can be used to decide whether the expression is idiomatic or not. <ref type="bibr" target="#b6">Cook et al. (2007)</ref> is a relatively differ- ent work where the authors propose a syntactic ap- proach to identify semantic non-compositionality of verb-noun MWEs. <ref type="bibr" target="#b14">McCarthy et al. (2007)</ref> use various models of selectional preferences for de- tecting non-compositional verb-object pairs.</p><p>Reddy et al. (2011) employ the additive and multiplicative composition functions presented by <ref type="bibr" target="#b18">Mitchell and Lapata (2008)</ref>  <ref type="bibr">3</ref> and several similar-ity based models to measure the compositionality of MWEs. Similarity based models measure the similarity of a MWE vector and sum/product of its constituents' vectors. Their evaluation (which is carried out on a set of 90 annotated NCs) shows that there is a relatively high correlation (Spear- man ρ of between 0.51 and 0.71) between their models' predictions and human judgments on non- compositionality of English NCs, with weighted additive function outperforming all the other mod- els. <ref type="bibr" target="#b11">Kiela and Clark (2013)</ref> present a model of de- tecting non-compositionality based on the hypoth- esis that the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality. In particular composi- tional phrases are less similar to their neighbors in semantic space. The distributional vectors repre- senting the semantics of words were created using the standard window method and 50,000 most fre- quent context words. They show that their model slightly (+0.014 and +0.007) outperforms their baselines ( <ref type="bibr" target="#b26">Venkatapathy and Joshi, 2005;</ref><ref type="bibr" target="#b14">McCarthy et al., 2007)</ref>.</p><p>All of the models mentioned so far are based on conventional <ref type="bibr">4</ref> or count based vector space repre- sentation of the words. More recent works how- ever are based on representation learning of word embeddings. <ref type="bibr" target="#b2">Baroni and Zamparelli (2010)</ref> regard adjective as matrices and nouns as real-valued vec- tors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters.  suggest that composition function is a matrix that multiplies on the word vectors, and <ref type="bibr" target="#b16">Mikolov et al. (2013b)</ref> present a model of learning non-compositional phrases by calculating a data-driven score for cer- tain frequent phrases (up to size two) and learn them as a whole. <ref type="bibr" target="#b21">Salehi et al. (2015)</ref> borrow the word embeddings from ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref> to model the semantics of words and use sev- eral composition functions from <ref type="bibr" target="#b18">(Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b19">Reddy et al., 2011</ref>) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with con- ventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representation of Words and Compounds</head><p>In order to represent words and compounds we use word embeddings, which are a form of vec- tor space models. Vector space models represent the semantics of words and phrases with real val- ued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks ( <ref type="bibr" target="#b3">Baroni et al., 2014;</ref><ref type="bibr" target="#b5">Collobert et al., 2011;</ref><ref type="bibr" target="#b4">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b28">Yazdani and Popescu-Belis, 2013;</ref><ref type="bibr" target="#b9">Huang et al., 2012;</ref><ref type="bibr" target="#b17">Mikolov et al., 2013c</ref>). They have been success- fully applied to semantic composition ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>) and outperformed the conventional count based contextual models in predicting non- compositionality of MWEs ( <ref type="bibr" target="#b21">Salehi et al., 2015)</ref>. In this work we use word embeddings of <ref type="bibr" target="#b15">Mikolov et al. (2013a)</ref> to represent the seman- tics of words and compounds. We chose an En- glish Wikipedia dump as our corpus. After fil- tering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to- kens using the word2vec 5 bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com- pounds' component words. Compounds' sizes are restricted to two (i.e. bigrams) for the sake of sim- plicity and to respect the evaluation set standards. The compounds and word embeddings are then used as supervised signals to learn a composition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised Models of Composition on Word Embeddings</head><p>After the unsupervised learning of word embed- dings and candidate compound embeddings (see section 3), we use these embeddings as supervised signals in order to train our composition func- tions. The term supervised might be misleading as the models do not have any information about the compositionality of the compounds during the training phase, and in that respect it is unsuper- vised. To describe the models in a formal way, throughout the paper we use the following nota- tions: d represents the size of embeddings, φ(w i ) represents embedding of w i , and˜φand˜ and˜φ(w i −w j ) = f (φ(w i ), φ(w j )) represents the learned embed- ding of bigram w i −w j by the composition func- tion f . The training error of bigram w i −w j by f is e ij = ˜ φ(w i −w j ) − φ(w i , w j ), and is norm 2. The composition functions are described in the following sections.</p><p>Given</p><note type="other">unsupervised embeddings for both words and compounds, a composition model is trained to map the word embeddings to the compound embeddings, with norm 2 error e ij defined above. Then this same error for this same task (norm 2 between predicted and unsupervised compound embeddings) is used to measure non- compositionality. In other words, we learn a com- position function (with several models) and iden- tify non-compositional expressions as those for which the error of this composition function is high.</note><p>We explore various classes of composition functions of word embeddings, ranging from sim- ple to complex, to find the most effective one. We want a composition function that is pow- erful enough to learn composition for composi- tional compounds, but simple enough that it fails to learn composition for non-compositional com- pounds. To this end, we investigate linear pro- jections, polynomial projections, and neural net- works. We try these models with and without spar- sity regularisation, which reduces the number of non-zero parameters while otherwise keeping the complexity of the function that can be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear Projection</head><p>In this model we assume that the embedding of a bigram is a linear projection of its component words' embeddings.</p><formula xml:id="formula_0">f (φ(w i ), φ(w j )) = [φ(w i ), φ(w j )]θ 2d×d</formula><p>To train this function we optimize the least square error, which gives us a multi-variant linear regres- sion.</p><formula xml:id="formula_1">min θ [φ(w i ), φ(w j )]θ 2d×d − φ(w i , w j )</formula><p>As mentioned earlier, a composition function that doesn't overfit the training data and induces a more meaningful error is more suitable for our purpose. One effective way of reducing overfitting and increasing generalization is by keeping only the important parameters of the model, which is done by enforcing sparsity on model parameters. In case of sparse linear projections, only a few el- ements of the projection matrix θ are non-zero. This means that not all dimensions of the latent space has a role in all dimensions of the compound embedding.</p><p>To apply sparsity on θ, we add a norm 1 penalty on it and add that to the least square optimization. This forms a multi-variant lasso regression (Tib- shirani, 2011). <ref type="figure" target="#fig_0">Figure 1</ref> shows the transformation matrices of linear projection and sparse linear projection. The two diagonals of the matrices correspond to the sum of the two embeddings, which we can see are the main component of the sparse function, and play an important role in the non-sparse one. We will see that despite being an important compo- nent of these functions, sum alone is not capable of accurately modelling semantic composition.</p><formula xml:id="formula_2">min θ [φ(w i ), φ(w j )]θ − φ(w i , w j ) + λ|θ|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Polynomial Projection</head><p>Polynomial projection is a non-linear projection that assumes the relation between compound em- bedding and the component words' embeddings should be a polynomial of degree n. This can be viewed as a form of linear regression where first a polynomial transformation is applied to the in- put vector and then a linear projection is fitted. If ψ shows the polynomial transformation then we have:</p><formula xml:id="formula_3">f (φ(w i ), φ(w j )) = ψ([φ(w i ), φ(w j )])θ</formula><p>We couldn't successfully apply any polynomial beyond quadratic transformation without overfit- ting. The case of a quadratic ψ transformation is:</p><formula xml:id="formula_4">ψ(x) = x 2 1 , · · · x 2 n Pure quadratic , x 1 x 2 , · · · x n−1 x n interaction terms , x 1 , · · · x n linear terms</formula><p>Similar to the linear case we can have sparse version of the polynomial regression in which we allow the presence of only a few non-zero ele- ments in the θ matrix. The sparsity regularizer is more important in the case of polynomial re- gression as we have many more parameters. The quadratic model is similar to Recursive Neural Tensor compositionality model of <ref type="bibr" target="#b23">Socher et al. (2013)</ref>. But in our model the tensor is symmet- ric around the diagonal. <ref type="figure">Figure 2</ref> shows the pure quadratic transformation matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neural Networks</head><p>A feed forward neural network is a universal ap- proximator <ref type="bibr" target="#b7">(Cybenko, 1989)</ref>: feed-forward net- work with a single hidden layer can approximate any continuous function, provided it has enough hidden units. Therefore we use neural networks as a powerful class of learning models to learn se- mantic composition. The number of hidden units gives us a measure to control expressiveness of our model.</p><formula xml:id="formula_5">f (φ(w i ), φ(w j )) = σ([φ(w i ), φ(w j )]W ih )W ho</formula><p>Similar to the previous models, we optionally impose sparsity over weight matrices of the neu- ral network to be able to induce more meaningful learning errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>We evaluated the above models on the data set of <ref type="bibr" target="#b8">Farahmand et al. (2015)</ref>. They provide a set of 1042 English NCs with four non-compositionality judgments. The judgments are binary decisions taken by four experts about whether or not a com- pound is non-compositional. We calculate a vote- based non-compositionality score for each of the data set compounds by summing over its non- compositionality judgments. The neural network models are trained using stochastic gradient de- scent. We use the additive and multiplicative mod- els of modelling composition and detecting non- compositionality presented by <ref type="bibr" target="#b21">Salehi et al. (2015)</ref> and <ref type="bibr" target="#b19">(Reddy et al., 2011</ref>) as state of the art base- lines.</p><p>The results are shown in <ref type="table">Table 1</ref>. The sec- ond column shows the correlation between dif- ferent models' predictions and the annotated data in terms of Spearman ρ. The last three columns show the performance of different mod- els in terms of Normalized Discounted Cumu- lative Gain (NDCG), F 1 score and and Preci- sion at 100 (P @100). For these three scores we consider the problem of predicting non- compositional NCs a problem with a binary so- lution where we assume compounds (of the evalu- ation set) with at least two non-compositionality votes are non-compositional. NDCG assigns a higher score to a ranked list of compounds if the non-compositional ones are ranked higher in the list. F 1 column represents the maximum F 1 score on the top-n elements of the ranked list re- turned by the corresponding model for all n in   <ref type="table">Table 1</ref>: Results for each model's ability to predict non-compositionality.</p><p>[1 − size-of-ranked-list]. P @100 shows the pre- cision at the first 100 compounds ranked as non- compositional. The models are listed in the or- der of complexity of the composition function. The addition-based baseline which was explored in a variety of previous work does not seem to be as powerful as the other models. It is outper- formed by almost all learned models. In general, we can see that more complex functions tend to learn compositionality in a more effective way.</p><p>As mentioned earlier, overly powerful learners overfit and do not produce meaningful errors for the detection task. Sparsity seems to address this issue by reducing the number of non-zero param- eters while the function can still keep the complex terms if needed. In general sparse models show improvement over their non-sparse counterparts, specifically for more powerful models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Auto-reconstructive Models</head><p>In this section, we investigate the hypothesis that we can detect non-compositionality better by not only modelling a composition function, but also modelling a decomposition function. For com- positional compounds, given the meaning of the compound, the meaning of the two component words should be conditionally independent. We therefore assume that the decomposition function predicts the component words' vectors indepen- dently. Let us illustrate this assumption by exam- ining the non-compositional compound flag stop. Given the semantics of this compound (a point at which a vehicle in public transportation stops only on prearrangement or signal <ref type="bibr">6</ref> ), we can not readily predict one of its component words without know- ing the other. Now consider the compositional compound hip injury. Given the semantics of this compound it is much easier to predict each of its component words independently.</p><p>In the previous section, the training signals came from the embeddings of the candidate com- pounds and their component words. In this section we extend our model such that it can benefit from more training signals. To this end, we formal- ize the assumption that a compositional compound is also decomposable as an auto-reconsructive model. We thus add this hypothesis to the learn- ing process: a good composition function not only builds the semantics of the compound from the semantics of its component words, but it also al- lows the independent prediction of the semantics of its component words from the compound se- mantics. In the following sections we add this as- sumption to both linear projection (which encom- passes polynomial) and Neural Network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Auto-reconstructive Linear Models</head><p>Let Y M ×d be a matrix whose rows are the pre- computed compound embeddings, and X M ×2d be a matrix whose rows are the concatenation of the embeddings for the words of these compounds. Let A N ×2d be another matrix where every row contains the concatenation of the embeddings for the words of a compound, but this matrix in- cludes many compounds for which we did not pre- compute compound embeddings. We assume that the rows of matrix A include the rows of matrix X. In linear models the auto-reconstructive objective function is as follows:</p><formula xml:id="formula_6">min θ,θ Xθ − Y + λAθθ − A</formula><p>where λ is a meta-parameter for the importance of the auto-reconstruction in the objective. A schematic of this model is shown in <ref type="figure">Figure 3a</ref>.</p><p>We can look at this problem as the following weighted least square problem: First we initialize θ 0 to be the answer of the original multi-variant linear regression, θ 0 =X \ Y where X \ =(X T X) −1 X T is the pseudoinverse of X. Let us assume W is the diagonal matrix with first M elements of the diagonal being 1 and the remaining N being λ. We alternate between the following formulas until the algorithm con- verges. First we approximate the next θ based on the current approximation of θ, then we use this value of θ to calculate the pseudo regressand part of the least square. In the final step we solve the weighted least square for this new regressand ma- trix and continue iterating these stages until the al- gorithm converges.</p><formula xml:id="formula_7">min θ,θ X A θ − Y Aθ T (θ θ T ) −1    1 . . .</formula><formula xml:id="formula_8">θ t = (Aθ t ) \ A (1) X 2 = X A Y 2 = Y Aθ T t−1 (θ t−1 θ T t−1 ) −1 (2) θ t = (X T 2 W X 2 ) −1 (X 2 ) T W Y 2 (3)</formula><p>The above algorithm can also be used in the case of polynomial regression. The only thing that needs to be done is to replace X and A by their polynomial transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Auto-reconstructive Neural Networks</head><p>The auto-reconstructive neural network follows the same idea. The objective function changes to:  <ref type="table">Table 2</ref>: Results comparing the auto- reconstructive models' ability to predict non- compositionality. <ref type="figure">Figure 3b</ref> shows the schematic of this model. We optimize this objective using stochastic gradi- ent descent with early stopping. The results are shown in <ref type="table">Table 2</ref>. We choose the first 300K fre- quent noun-noun compounds from the corpus in order to build matrix A. Each row of A cre- ated by concatenating the component words vec- tors. The results show that the auto-reconstructive models generally improve over their counterparts. As mentioned earlier, the improvement comes from two facts. On the one hand we increase the training signals by implementing the decom- posability hypothesis. On the other hand, the auto-reconstructive model enables us to exploit more data in addition to the candidate compounds. There is almost no improvement in the case of linear model because this model does not have enough learning capacity to benefit from a higher number of training signals.</p><formula xml:id="formula_9">min W ih ,W hi ,W oh σ(XW ih )W ho − Y + λσ(AW ih )W hi − A (4) Composition Spearman ρ NDCG F 1 P @100</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Non-compositionality Detection Using Latent Annotations</head><p>All the models discussed in this paper are unsu- pervised since they don't have any access to la- bels specifying compositionality of compounds. The above models simply assume that most com- pounds are compositional, and therefore train their composition and decomposition functions on all compounds. In this section we incorporate in the models an intrinsic uncertainty about the compo- sitionality annotation of the training set. The best (optimum) composition function is the one that fits well all the compositional com- pounds and does not fit the non-compositional ones. But we assume that we do not have train- ing labels indicating compositionality. To over- come this uncertainty and improve the learning process, we introduce latent compositionality la- bels to the model. We assume each candidate com- pound has a latent annotation, 1 or 0, showing  <ref type="table">Table 3</ref>: Results comparing the latent annotation models' ability to predict non-compositionality.</p><p>whether or not it is compositional. Let us assume a non-compositionality detection system that re- turns B non-compositional candidates that should have their own lexical unit and parameters. The objective of this composition function training is to minimize the error of compositional compounds and not the error of non-compositional ones. In order to implement this objective we use the fol- lowing loss function:</p><formula xml:id="formula_10">min λ ij ,θ ij λ ij e 2 ij s.t λ ij ∈ {0, 1}, ij λ ij = N − B</formula><p>where λ ij represents the hidden compositionality annotation and e ij is again the learning error for the pair w i −w j . We want to find the B points such that annotating them as non-compositional results in the minimum error of this objective. The algo- rithm that alternates between optimizing the com- position learning and the hidden annotations even- tually converges to this solution. If the errors are fixed, the B compounds with the biggest errors are the answers to the non- compositional annotation optimization of that it- eration. Therefore to solve this optimization we follow an EM-like algorithm: First we set all λ ij to 1 and perform the optimization on the compo- sition function. Then we sort the compounds by their error and set the λ ij of the biggest B el- ements to 0, and the rest to 1. In other words we assume the compounds with big error are pre- sumably non-compositional according to what we know until that iteration. We continue alternat- ing between training the composition function and annotating high error points until the algorithm reaches convergence. The results are shown in Ta- ble 3. Models that use latent annotations clearly outperform their counterparts, especially in terms of precision at 100. This is expected since at train- ing time we consider a model that returns B non- compositional compounds and therefore precision at 100 is optimized. The latent annotations do not improve the linear model since the model is simple and there is not much room to improve its learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We proposed a framework to detect non- compositional compounds as the compounds that stand out as outliers in the process of learning compositionality of English noun compounds. We proposed and evaluated a range of functions with a variety of complexities that model semantic com- position. We showed that learners such as poly- nomial projection and neural networks which are distinctly more complex than commonly used ad- ditive and multiplicative functions can model se- mantic composition more effectively. We showed that a function as complex as quadratic projection is a better learner of compositionality than simpler models. We further showed that enforcing sparsity is an effective way of learning a complex compo- sition function while avoiding overfitting and pro- ducing meaningful learning errors. Furthermore, we improved our models by incorporating an auto- reconstructive loss function that enables us to ben- efit from more training signals and cover more data. Finally, we addressed the intrinsic label un- certainty in training data by considering latent an- notations, and showed that it can further improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Linear transformation matrix of compositionality for embeddings of size 50</figDesc><graphic url="image-3.png" coords="5,140.03,233.27,158.74,119.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Auto-reconstructive linear and neural network models</figDesc><graphic url="image-6.png" coords="7,420.66,62.81,113.39,113.68" type="bitmap" /></figure>

			<note place="foot">compositional. We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.</note>

			<note place="foot" n="1"> MWEs can have other less significant kinds of idiosyncrasies. For instance lexical idiosyncrasy as in ad hoc, and pragmatic idiosyncrasy as in good morning (Baldwin and Kim, 2010)</note>

			<note place="foot" n="2"> MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs.</note>

			<note place="foot" n="3"> Mitchell and Lapata (2008) present an analysis of vectorbased additive and multiplicative semantic composition models where each words is represented by its distributional vector. They conclude that multiplicative and combined models do a better job in modelling vector-based semantic composition than other models.</note>

			<note place="foot" n="4"> Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014).</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially funded by Hasler foun-dation project no. 15019, "Deep Neural Network Dependency Parser for Context-aware Represen-tation Learning".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiword expressions. Handbook of Natural Language Processing, second edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su Nam</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical model of multiword expression decomposability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment</title>
		<meeting>the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on a broader perspective on multiword expressions</title>
		<meeting>the workshop on a broader perspective on multiword expressions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multiword expression data set: Annotating non-compositionality and conventionalization for english noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghdad</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Multiword Expressions (MWENAACL 2015). Association for Computational Linguistics</title>
		<meeting>the 11th Workshop on Multiword Expressions (MWENAACL 2015). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving Word Representations via Global Context and Multiple Word Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic identification of non-compositional multiword expressions using latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenie</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE &apos;06</title>
		<meeting>the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting compositionality of multi-word expressions using nearest neighbours in vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1427" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic identification of non-compositional phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting a continuum of compositionality in phrasal verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acqusation and Treatment</title>
		<meeting>the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acqusation and Treatment</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting compositionality of verbobject combinations using selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013). Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical study on compositionality in compound nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiword expressions: A pain in the neck for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Ivan A Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A word embedding approach to predicting the compositionality of multiword expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahar</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Idiomatic object usage and support verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jussi</forename><surname>Piitulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Järvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1289" to="1293" />
		</imprint>
	</monogr>
	<note>ACL &apos;98. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso: a retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Measuring the relative compositionality of verbnoun (v-n) collocations by integrating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic vector products: Some initial investigations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second AAAI Symposium on Quantum Interaction</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computing text semantic relatedness using the contents and links of a hypertext encyclopedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="176" to="202" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
