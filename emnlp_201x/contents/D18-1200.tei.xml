<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Dubossarsky</surname></persName>
							<email>haim.dub@gmail.com, {eitan.grossman,daphna}@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Edmond and Lily Safra Center for Brain Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Grossman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1732" to="1740"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1732</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The point of departure of this article is the claim that sense-specific vectors provide an advantage over normal vectors due to the pol-ysemy that they presumably represent. This claim is based on performance gains observed in gold standard evaluation tests such as word similarity tasks. We demonstrate that this claim, at least as it is instantiated in prior art, is unfounded in two ways. Furthermore, we provide empirical data and an analytic discussion that may account for the previously reported improved performance. First, we show that ground-truth polysemy degrades performance in word similarity tasks. Therefore word similarity tasks are not suitable as an evaluation test for polysemy representation. Second, random assignment of words to senses is shown to improve performance in the same task. This and additional results point to the conclusion that performance gains as reported in previous work may be an artifact of random sense assignment , which is equivalent to sub-sampling and multiple estimation of word vector representations. Theoretical analysis shows that this may on its own be beneficial for the estimation of word similarity, by reducing the bias in the estimation of the cosine distance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Polysemy is a fundamental feature of natural lan- guages, which typically have many polysemic words. Chair, for example, can refer to either a piece of furniture or to a person in charge of a meeting. Therefore both theoretical linguis- tics and computational linguistics seek to establish principled methods of identifying the senses that together constitute the meaning of words.</p><p>It is commonly assumed or claimed that stan- dard word embeddings are unable to capture poly- semy ( <ref type="bibr" target="#b9">Iacobacci et al., 2015)</ref>, which results in sub- optimal performance in gold standard evaluation tests such as word similarity tasks, and potentially hamper performance in downstream tasks. The corollary assumption is that sense-specific repre- sentations will lead to improved performance on these evaluation tests. This assumption is concep- tually attractive, since it makes sense that sense- specific representations are more accurate than global representations which conflate the differ- ent senses of a word into a single representation. For example, in translating 'chair,' it is reasonable that performance should improve if the two senses are represented separately. This view is supported by several studies <ref type="bibr" target="#b8">(Huang et al., 2012;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b2">Chen et al., 2014;</ref><ref type="bibr" target="#b10">Li and Jurafsky, 2015;</ref><ref type="bibr" target="#b9">Iacobacci et al., 2015;</ref><ref type="bibr" target="#b11">Mancini et al., 2017)</ref>, which argue that sense-specific representa- tions lead to improved performance in word simi- larity tasks.</p><p>Ideally, such claims about polysemy should be evaluated using a gold standard evaluation set that is tailored specifically for polysemous words. As this set does not exist, tasks involving word sim- ilarity tests have been used as a proxy (see Sec- tion 2). The underlying hypothesis is that enrich- ing word vector representations with polysemic information should express itself in performance gains in these tasks. However, this hypothesis has never been tested directly, and the ability of word similarity tasks to directly benefit from polysemic information must first be validated if they are to serve as genuine evaluation sets in polysemy re- search. Until then, the validity of any reported positive effects of sense-specific representations on evaluation tests is to be treated with caution.</p><p>In this paper our first aim is to assess the validity of word similarity tasks as proper evaluation tests for polysemic word representations. We use two independent corpora in order to obtain polysemic vectors: (i) a sense-annotated corpus, and (ii) an artificially-induced annotated corpus, constructed by using an established method that we modify for our purposes. Surprisingly, our analyses show that even the most accurate sense-specific word vectors do not improve performance. In fact, peak perfor- mance is achieved when polysemic information is ignored, i.e., when all different annotated senses are collapsed to a single word, as they naturally appear in a text. These counter-intuitive results in- dicate that the word similarity tasks are not a suit- able test to evaluate polysemic representations.</p><p>Although these negative results point to the in- adequacy of the evaluation test, they also stress a necessary critical analysis of the sense-specific vectors. Specifically, they give rise to the follow- ing question: why might previously reported poly- semic representations show superior performance in these inadequate evaluation tests?</p><p>One explanation for the reported effects may lie in an inherent property of sense-specific represen- tations. The procedure of assigning a word oc- currence to a particular sense amounts to a sam- pling procedure. This sampling procedure itself, regardless of its validity (whether its sense assign- ments are correct or not), may be the true source of the reported performance gains. To test this hy- pothesis, we created a control condition in which word occurrences are randomly assigned to dif- ferent senses. Determining that an effect is at- tributable to genuine polysemy can only be estab- lished if a similar effect is lacking or significantly reduced in this control condition.</p><p>We demonstrate that performance gains are in- deed obtained for a corpus with randomly assigned senses. In addition, we modify two models for sense-specific polysemy representation ( <ref type="bibr" target="#b10">Li and Jurafsky, 2015;</ref><ref type="bibr" target="#b11">Mancini et al., 2017)</ref> to randomly assign words to senses, and observe that the effect size remains unchanged between the original and random conditions.</p><p>In support of our empirical findings, we discuss the difficulty of obtaining an unbiased estimator for the cosine distance between two normalized random variable vectors. This distance is the ba- sis of all word similarity tasks that serve to evalu- ate performance, and thus it may provide a partial explanation for the empirical findings, under the assumption that words are better represented as a population of vectors. Specifically, the true source of the reported performance gains may be an arti- fact of a purely statistical benefit that derives from the assignment of words to particular senses, or separate sub-samples, which subsequently reduces the bias of the similarity estimator.</p><p>We thus identify two independent pitfalls in NLP research on polysemy representation. First, the inadequacy of currently-used evaluation tests to properly assess polysemic representations. And second, the essence of polysemic representations, whose reported benefits might not come from pol- ysemic information per se, but rather from other unrelated sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Previous attempts to use polysemic information for enriching word representation used normal unannotated corpora, and therefore disambiguated the different senses of words before exploiting any sense-specific information. Prior art has either taken an automatic approach to detect word senses based on the statistics extracted from texts ( <ref type="bibr" target="#b8">Huang et al., 2012;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b10">Li and Jurafsky, 2015)</ref>, or relied on external lexical resources (e.g., WordNet or BabelNet) which guarantee that the detected senses are mapped to true sense dis- tinctions in natural language ( <ref type="bibr" target="#b2">Chen et al., 2014;</ref><ref type="bibr" target="#b9">Iacobacci et al., 2015;</ref><ref type="bibr" target="#b11">Mancini et al., 2017)</ref>. Im- portantly, both types of models report marked per- formance gains in evaluation tests.</p><p>This kind of approach produces (i) global vec- tors that represent a word's meaning as a single vector (with no subdivision into distinct senses), as well as (ii) sense-specific vectors representing individual senses of words, determined in the dis- ambiguation step, as separate vectors. For exam- ple, such approaches would represent the mean- ing of chair as a single vector, as well as distinct vectors for each of its multiple senses, e.g., "chair (person)" and "chair (furniture)".</p><p>In order to evaluate performance, the vectors created by the models are standardly evaluated using word similarity tasks (the most common are WordSim-353 ( <ref type="bibr" target="#b5">Finkelstein et al., 2001</ref>) and Stanford's Contextual Word Similarities (SCWS) ( <ref type="bibr" target="#b8">Huang et al., 2012)</ref>). These tasks comprise pairs of words and the similarity scores assigned to them by human annotators. For example, the similar- ity between table and chair might be rated as 0.8 (i.e., human annotators found these words to be very similar, but not perfectly so), while the sim- ilarity between table and tree might be rated as 0.3 (not very similar). The embedding models produce similarity scores for each word pair by computing the cosine-distance between the word vectors for each pair. The model's performance is then evaluated as the rank-order similarity in the order of pairs (Spearman correlation) between the human annotators' scores and the scores pro- duced by the model. In line with the assumption discussed above, one would predict that the rank- order similarity produced by the sense-specific vectors should outperform the one produced by the global vectors. In particular, more accurate sense-specific vectors should produce better re- sults in these tasks; conversely, better performance on these tasks is interpreted as indicating that pol- ysemy has been captured more accurately. We di- rectly test these two predictions in this paper.</p><p>Computing word similarity is straightforward when each word is represented as a single vec- tor, but it is less so when the meaning of a pol- ysemic word is represented by multiple sense- specific vectors. This problem of matching the senses relevant for a specific word pair, i.e., match- ing the "person" sense of chair with the correct sense of the word meeting, poses a major hurdle for meaningful comparison, and has been tack- led in three different ways: (i) average over all similarity scores between all the different possi- ble pairs; (ii) weighted average over these scores according to the probability of senses assigned by the disambiguation model; or (iii) selection of the most suitable sense according to the disambigua- tion model, and using only the corresponding sim- ilarity score.</p><p>Intuitively, the third approach should outper- form the others, as it is based on the clearest distinction between the relevant and non-relevant senses. However, this na¨ıvena¨ıve prediction is not sup- ported by previous studies. Rather, the best results are usually obtained for average and weighted av- erage, followed by global (ignoring polysemy), while selection falls far behind the others. This counter-intuitive finding suggests that the ob- served benefit may be less related to sense disam- biguation than previously supposed 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task validation</head><p>Generally, before any task can be used as an eval- uation testbed for polysemy discovery algorithms or polysemous representations, we argue that the task itself should be validated as suitable (or not) for the intended purpose. We propose the fol- lowing task validation methodology: (i) Start by identifying a corpus where polysemic information is known for a significant number of words. (ii) Compute two sets of word representations: A 1 -which computes a single representation for all words in the corpus, and A 2 -which computes multiple representations for each polysemic word in the corpus based on the different known senses of the word. (iii) Evaluate the task using the two representation sets A 1 and A 2 . Only if significant performance gains can be shown when using A 2 as compared to A 1 , the task can be used to evalu- ate polysemy representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Polysemy induction</head><p>A major drawback of the proposed methodology is that such annotated corpora are scarce, and the largest among them are still small (OntoNotes ( <ref type="bibr">Weischedel et al., 2013</ref>) comprises 1.5 million words, cf. unannotated corpora (e.g. Wikipedia) which are about 1000 times larger). We therefore articulate a methodology to generate a task vali- dation test from any corpus and evaluation task, even without prior annotation of polysemy, that is based on the pseudo-words approach ( <ref type="bibr" target="#b6">Gale et al., 1992;</ref><ref type="bibr" target="#b15">Pilehvar and Navigli, 2014</ref>).</p><p>More specifically, we induce polysemy in a nat- ural corpus by pairing words, and collapsing ev- ery pair of words into a single word-form while keeping their "original identity" as polysemy an- notation. For example ring and table may be col- lapsed to a single word with two senses <ref type="table" target="#tab_2">, table 1 and  table 2</ref> respectively. The new corpus is polysemic with respect to the collapsed words, while all other words keep a single sense. This corpus has most of the features of a natural corpus, but unlike most natural corpora (and all large corpora), it contains polysemy annotation. Subsequently, the relevant items in the word similarity tasks are collapsed in the same way, making the items polysemous, and thus the tasks suitable as validation tests.</p><p>With these polysemy-induced corpus and word similarity tasks, we follow the methodology for task validation described above. Only if a model that is based on multiple representations per pol- ysemous word leads to a significant performance gain in the task as compared to a model with sin- gle representation for each word then the task un-der examination should be considered adequate to evaluate the utility of polysemy representation of word senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods</head><p>Word embedding model word2vec-SkipGram model ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) is used to obtain vector representations for words and senses. The model is separately trained over the corpus: first producing sense-specific vectors according to the annotated senses, and next producing global vec- tors by ignoring the annotated senses and col- lapsing all their occurrences to a single word. Throughout the analyses we use an embedding size of 300d, a window size of 5 words from each side of the target word, negative-sampling of 5 words, and an initial learning rate of 0.025.</p><p>Sense-annotated corpus We use OntoNotes ( <ref type="bibr">Weischedel et al., 2013)</ref>, the largest available cor- pus annotated for word senses. This allows us to circumvent the problem of first disambiguating the words' senses, and thus to directly test the util- ity of using polysemic information in word vec- tor representations. The corpus contains 1.5 mil- lion English tokens, comprising about 50k English word types, of which 8675 word types are sense- annotated. Because the annotation is not uniform throughout the corpus (words are not annotated every time they appear), which can bias the anal- ysis described below, we extract a subset of the corpus by removing sentences where polysemous words are not annotated, thus removing 40% of the corpus. Stopwords as well as words occurring less than 10 times are ignored by the word embedding models. All words are lowercased.</p><p>Sense-induced corpus Wikipedia dump (04/2017) is the original corpus from which a pol- ysemic version is induced. We separately used a list of semantically-aware pseudowords <ref type="bibr" target="#b14">(Pilehvar and Navigli, 2013)</ref>, in addition to random pairs from the 6000 most frequent words to collapse them into a single word-form (see Section 3.1). Stopwords and infrequent words (&lt;300 tokens) are ignored by the word embedding models.</p><p>Evaluation Polysemic word representations are evaluated on the two word similarity tasks de- scribed. Crucially, the problem of matching the relevant sense in these tests (described in Sec- tion 2) is tackled by taking the average of the sense-specific representations and comparing it to the global word representations 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Results clearly demonstrate that global represen- tations are significantly superior to sense-specific representations in both evaluation tests and across corpora, as shown in <ref type="table">Table 1</ref>  The critical comparison between the global and average conditions is expected to show bet- ter performance for the latter under the assump- tion that polysemic information improves perfor- mance. This difference (marked as actual gain in <ref type="figure" target="#fig_0">Figure 1</ref>), surprisingly shows an opposite out- come, which means that word similarity tasks fail to demonstrate the value of polysemy representa- tion in improving performance.</p><p>Complementarily, we report performance for the optimal condition. In this condition, word sim- ilarity scores are obtained for a model that was trained on the the original Wikipedia corpus (be- fore polysemy induction), and thus represents an upper bound for the performance of any polysemy model that would be trained on an induced poly- semy version of the same corpus (i.e., the max- imum ideal gain a model can achieve over its global vectors). The fact that performance is high- est for that condition is thus expected, and reas- sures us that the induced polysemy procedure has worked as planned for both kinds of induction.</p><p>Overall, our results points to a failure of poly- semy models to improve performance over global vectors by averaging sense-specific vectors. This worsening in performance of the sense-specific <ref type="bibr">2</ref> Recall that average was reported to be one of the best performing matching methods is previous work. vectors stands in marked contrast to previous stud- ies which reported performance gains (see <ref type="table" target="#tab_2">Table 2</ref> and Reported gains in <ref type="figure" target="#fig_0">Figure 1)</ref>    <ref type="table" target="#tab_2">Tables 1, 2</ref>, show- ing performance gains across conditions. Ideal gain: max. performance gains using fully annotated corpus with perfect sense matching, Actual gain: de-facto gain (global -average), and Reported gains: previous re- ported results. Color (dark and white) marks the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>The main result described above is negative, demonstrating that word similarity tasks are not suitable to evaluate polysemy representation. However, the methodology we proposed for poly- semy induction constitutes a positive contribution, as it can be used to effectively test any task for its utility in the evaluation of polysemy representa- tion while using any corpora. This may aid in find- ing tasks which are more suitable to serve as gold standard evaluation tests for polysemy. Moreover, the use of polysemy induction for these purposes adds yet another type of control to the NLP tool- box; such controls are scarcely implemented in NLP studies (but see <ref type="bibr" target="#b3">Dubossarsky et al. (2017)</ref>). Importantly, the inability of the induced pol- ysemy models to produce positive performance gains as reported in prior art may indicate that these reported gains do not reflect benefits from true polysemy, but rather from an unknown factor that boosts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The statistical signature of polysemy</head><p>In order to further investigate the reason for the lack of performance gains, we analyzed the prop- erties of the sense-specific vectors in the induced polysemy conditions, and compared them to those obtained by previous studies. We looked at the pairwise similarity between the different sense-specific representations of the same word. We start from the observation that pol- ysemy is inherently defined by word senses that are distinguishable from each other. Importantly, we compared the models that did not produce per- formance gains in word similarity tasks to those which did report such gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis and results</head><p>We tested four sets of sense-specific representa- tions: two from our polysemy induction models (see Section 3.2) and two from Li and Jurafsky (2015) <ref type="bibr">3 and Mancini et al. (2017)</ref>  <ref type="bibr">4</ref> which reported performance gains. For each word in each set, the average cosine-distance between its different sense-specific vectors is computed. The distribu- tion of these average distances within a specific set is defined as its "polysemic signature", which is then compared across sets.</p><p>The results shown in <ref type="figure" target="#fig_1">Figure 2</ref> reveal marked dif- ferences in the polysemic signature between the four sets. A high polysemic signature is seen for the two induced polysemy sets, which are the only sets with guaranteed semantically different senses. In these sets, the polysemy model that is based on the random pairing of words has a higher polysemic signature than the one based on semantically-aware pairing. This is well expected, as semantically-aware pseudowords are designed to simulate "true polysemy" in which the differ- ent senses of a word are still semantically related, unlike random pairing which produces a "coarse" distinction more similar to homonymy <ref type="bibr" target="#b15">(Pilehvar and Navigli, 2014</ref>).</p><p>The critical comparison to the two sets of pol- ysemy models that did find performance gains shows that these models exhibit a smaller pol- ysemic signature, as the different senses seem to be more similar to one another. Crucially, even the polysemic signature of <ref type="bibr" target="#b11">Mancini et al. (2017)</ref>, which exhibit an intermediate polysemic signature, shows greater similarity in its senses as compared to the sense-aware polysemy induction model, which presumably represents a more subtle (and ecological) model for polysemy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head><p>The broader implications of these results on our research hypothesis can be understood in the con- text of the findings reported in Section 3.3. The re- sults described in <ref type="figure" target="#fig_0">Figure 1</ref> demonstrate a marked difference between previously reported gains and the actual gain condition which shows a worsen- ing of performance in the same task when poly- semic information is included. The results demon- strated in <ref type="figure" target="#fig_1">Figure 2</ref> can be described as a negative image of those presented in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, the actual gain condition of the induced-polysemy has the largest polysemic signature as compared to the other conditions.</p><p>Together, these results indicate that the con- dition that demonstrates polysemy most clearly shows the poorest performance in the evaluation tests. The converse is also true: the conditions that diverge from the clearest polysemic represen- tation show heightened performance in the evalua- tion tests. A gold standard for polysemy represen- tation should entail that given optimal vector rep- resentations, performance on the evaluation tests would be optimal, and vice versa. Since our results demonstrate that the directions of optimal vector representation and optimal test performance are opposite, we are led to the following conclusions:</p><p>First, polysemy models which provide improve- ment in word similarity tasks may not necessarily be suitable for polysemous vector representations. Second, word similarity tasks are not suitable eval- uation tests for polysemy representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical discussion</head><p>In this section we recall and analyze some proper- ties of the cosine distance, and describe how they may partially account for the empirical observa- tions discussed in this article. The crucial point is to model the contextual representation of words as a distribution over some vector space.</p><p>Let X i denote the random variable which cap- tures the contextual representation of word i. Let</p><formula xml:id="formula_0">{X l i , X l j ∈ R d } L l=1</formula><p>denote a sample of such rep- resentations for words i, j respectively, where L denotes the sample size. d corresponds to the di- mension of the vector space when using word2vec representations, or the number of words in the dic- tionary when using explicit representations (e.g., PPMI) (see Section 3.2). To simplify the analysis, we further assume that X l = 1 ∀l.</p><p>The similarity between two words i, j can be plausibly measured (as customarily done) by the cosine distance between their contextual represen- tations, namely, E [X i X j ]:</p><formula xml:id="formula_1">E [X i X j ] = E [X i ] E [X j ] + cov(X i , X j ) (1)</formula><p>Thus the average distance is not equivalent to the distance between the average representations alone, but has to include an additional bias term -cov(X i , X j ) -which reflects the statistical de- pendence between the two vector representations X i , X j . This term is significant, because the con- textual representation of two words is likely to exhibit strong dependence, especially when the words are more similar. This is where the problem lies. In the process of generating words' representations, we start from a sample of sentences and generate a single rep- resentation. This representation is essentially our estimate of E [X i ] for word i. When multiplying two such representations in order to compute the cosine distance between them, we obtain an esti-</p><formula xml:id="formula_2">mate for E [X i ] E [X j ]</formula><p>, which is not a good esti- mate for E [X i X j ] because of the bias term in (1).</p><p>Ideally, in order to provide an unbiased esti- mate of E [X i X j ], we should divide the sample of sentences into mini-batches, compute the ap- propriate contextual representation for both words i, j from each mini-batch, and then directly esti- mate E [X i X j ] by taking the average multiplica- tion of the corresponding representations in each mini-batch. Interestingly, in the process of gen- erating polysemous representations, whether rely- ing on true polysemy or arbitrary polysemy, we essentially accomplish the same goal: for each word, a mini-batch is replaced by the subset of sentences in which only one of the word's mean- ings is present <ref type="bibr">5</ref> .</p><p>If sense matching (see Section 2) is achieved by way of average or weighted average, it implies that our estimate of word similarity should im- prove with the number of senses used in the analy- sis, especially when the assignment is arbitrary. Of course, any improvement is hampered by the dete- rioration in the quality of the contextual represen- tation computed from the smaller mini-batch sam- ple, and therefore improvement is only expected for a small number of real or artificial "senses".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance gains revisited</head><p>The empirical findings presented so far converge on the conclusion that the performance gains re- ported in prior art may not stem from the utility of polysemic information, as previously claimed, but are the result of an alternative source. In the theoretical discussion we argue that random sense annotation is equivalent to sub-sampling and mul- tiple estimation of contextual vector representa- tions, and that this alone may be beneficial for the task performance of word similarity. It is reason- able to conclude that these repeated sampling pro- cedures may have produced the reported perfor- mance gains. Here we test this hypothesis directly.</p><p>We propose a simple control condition, in which senses are randomly assigned to words in a corpus, and sense-specific vectors are produced in the same way as before. Determining that an ef- fect is reliably attributed to genuine polysemy can only be established if a similar effect is lacking or significantly reduced in this control condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Random sense assignment</head><p>We achieve random sense assignments in 2 ways:</p><p>Sampling from a known distribution. For the entire corpus and vocabulary (100k words, and note that <ref type="bibr" target="#b13">Neelakantan et al. (2014)</ref> and <ref type="bibr" target="#b10">Li and Jurafsky (2015)</ref> also took this entire vocabulary approach), we assigned senses at random from a categorical distribution under two conditions. In the first we used a uniform prior which produced equal sense probabilities for each word, and in the second condition we used a biased prior in which one sense predominates. We also experi- mented with different number of senses per word. We found that the results differed only slightly be- tween the conditions and across the different num- ber of senses. The same regular embedding model was trained as before.</p><p>Sampling from an unknown distribution. To test the hypothesis more directly against the sense distributions used in prior work, we reproduced sense-specific vectors of two models using their code: (1) for Li and Jurafsky (2015) we kept their Chinese-Restaurant-Process probabilistic mecha- nism, where senses are assigned to words based on the similarity of their contexts. We only shuf- fled the elements of the final vector of sense as- signments produced by the model, and (2) for <ref type="bibr" target="#b11">Mancini et al. (2017)</ref> we shuffled between the sense tags of each word in their original sense- annotated Wikipedia. For further comparisons we used the original code unchanged to reproduce an- other set of global and sense-specific vectors for each model separately to serve as a baseline. <ref type="table">Table 3</ref> shows the results of these simulations. Regular embedding with random sense assign- ments shows marked performance gains of the sense-specific vectors over the global vectors. In fact, the effect reported in <ref type="bibr" target="#b13">Neelakantan et al. (2014)</ref> (which report the highest score in the SCWS task, see <ref type="table" target="#tab_2">Table 2</ref>) is replicated almost ex- actly, perhaps due to the fact that they also used a fixed number of senses for each word as we did in this simulation. Furthermore, the reanalysis of previous models lead to almost identical results in the original and random conditions, which means that randomly assigning words to senses does not weaken the effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance boost due to word sampling</head><p>Together, these three control simulations clearly show that an effect of the same magnitude as pre- viously reported in several studies emerges under random sense assignment. Therefore, our find- ings strongly undermine the assumption that the reported effects are in any way related to actual polysemy, and strongly suggests that it is repeated sub-sampling that boosts performance.  <ref type="table">Table 3</ref>: Word similarity scores for random sense as- signments, compared to the original (where available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary and Discussion</head><p>Here we investigate the common-held view that polysemy representation improves performance. Specifically, we question the validity of word sim- ilarity tasks as a suitable evaluation test that would allow drawing such conclusions. To test the claim that resolving the polysemy of words improves performance in these tasks, as was repeatedly re- ported in prior art, we used real-world polysemy in two independent conditions: (i) a human sense- annotated corpus and (ii) a corpus in which poly- semy was induced in a controlled artificial fashion. In both conditions, the performance in word sim- ilarity tasks deteriorated. We claim that this neg- ative finding alone may suffice to determine that word similarity tasks are not suitable tests for eval- uating polysemy representation. However, if the word similarity task is inade- quate to evaluate polysemy, why would it show high gains for polysemic representations as re- ported in many previous studies? To investigate this question we first ask whether polysemic infor- mation per se is required to drive such effects, or whether these effects are artificially caused by the procedures by which polysemous representations are created. To test this, we set out to demonstrate that even representations that bear no genuine pol- ysemic information could nonetheless yield im- proved performance due to a methodological arti- fact. We thus created control conditions, in which we randomly assign word occurrences to senses, and found that randomly-produced sense-specific vectors indeed show marked improvements in per- formance. Since this effect cannot stem from poly- semy (which is lacking in these conditions), it may only be the result of a methodological artifact -the sampling procedure entailed by the assignment of words to senses.</p><p>The existence of a sampling artifact is supported by our theoretical discussion, showing that multi- ple vector sampling can lower the bias of the es- timator of the cosine distance between two vecto- rial random variables. The underlying assumption is that a better model for contextual word repre- sentation should employ a population of vectors. Interestingly, the conclusion that word vectors are better if constructed from multiple representations might apply to word vectors in general, and not just to sense-specific vectors. However, such a claim is outside the scope of this study and re- mains for future research.</p><p>demonstrated on the basis of tasks whose suitabil- ity has been properly validated. Moreover, any ef- fects reported will have to be supported by demon- strating that these effects are absent or strongly reduced in a properly articulated condition that should control for the sampling artifact. Critically, until a reliable evaluation test exists, research on polysemic word representation is se- riously hampered. In fact, a re-evaluation of past models would be in place, as both "positive" and "negative" results that were previously reported are in fact invalid.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of results in Tables 1, 2, showing performance gains across conditions. Ideal gain: max. performance gains using fully annotated corpus with perfect sense matching, Actual gain: de-facto gain (global-average), and Reported gains: previous reported results. Color (dark and white) marks the tasks.</figDesc><graphic url="image-1.png" coords="5,72.00,301.71,226.77,143.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Density distribution of polysemic signatures for the four sets, see text for details.</figDesc><graphic url="image-2.png" coords="6,79.09,62.81,204.09,138.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>GLOBAL 
AVERAGE 

WS-353 
Huang (2012) 
22.8 
71.3 
Neelakantan (2014) 
69.2 
70.9 
Li (2015)* 
61.0 
67.8 
Mancini (2017)* 
49.1 
55.6 
SCWS 
Huang (2012) 
58.6 
62.8 
Neelakantan (2014) 
65.5 
67.2 
Chen (2014) 
64.2 
66.2 
Li (2015) 
64.6 
66.4 
Mancini (2017) * 
57.2 
62.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Previously reported results in word similarity 
tasks. *Reproduced (see section 4.1) where consistent 
comparison on the same tasks is not originally reported. 

</table></figure>

			<note place="foot" n="1"> Iacobacci et al. (2015) and Mancini et al. (2017) only reported results on selection, for which they found performance gains. For consistent comparison with other models we report average (but using their code), noting that it also provides performance gains over global vectors.</note>

			<note place="foot" n="3"> https://github.com/jiweil/mutli-sense-embedding 4 http://lcl.uniroma1.it/sw2v/</note>

			<note place="foot" n="5"> For the purpose of this discussion we ignore sentences in which a word appears more than once.</note>

			<note place="foot">This account is further supported by our polysemic signature analysis, which shows that the similarity between the senses in the polysemic models that produced performance gains is greater than in the models that did not produce this effect (the polysemy-induced models). This finding is in line with the sampling artifact account, as sensespecific vectors which are based on random sense assignments are expected to be more similar compared to sense-specific vectors which are based on true polysemic distinctions. We stress that our analysis does not argue for or against the accuracy of sense-specific vectors in capturing true polysemy (other tests exist for that purpose). Instead, it focuses only on the true source of previously reported performance gains of this kind of representation, and on the validity of word similarity tasks for their evaluation. All in all, we provide converging evidence, both experimental and theoretical, that word similarity tasks do not provide a marker for the utility of polysemic information. Rather, performance gains in word similarity tasks are an artifact of the procedure by which polysemic representations are created. These conclusions join a general skepticism expressed in the literature about the use of word similarity tasks for the evaluation of word vectors (Hill et al., 2015; Avraham and Goldberg, 2016; Batchkarov et al., 2016; Faruqui et al., 2016). Essentially, our findings mean that there is no solid empirical foundation to the claim that polysemic information improves performance in evaluation tests. In fact, they corroborate the general impression that polysemic representation does not improve performance on most downstream tasks (Li and Jurafsky, 2015). It may be the case that sense-specific vectors can or will show heightened performance on evaluation tests, or improve downstream tasks. This, however, will have to be</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Matan Ben-Yosef for his invaluable con-tribution in executing and manipulating the code of <ref type="bibr" target="#b10">Li and Jurafsky (2015)</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving reliability of word similarity evaluation by redesigning annotation task and performance measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A critique of word similarity as a method for evaluating distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Batchkarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Outta control: Laws of semantic change and inherent biases in word representation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Dubossarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1136" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Problems with evaluation of word embeddings using word similarity tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Work on statistical methods for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>William A Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium on Probabilistic Approaches to Natural Language</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sensembed: Learning sense embeddings for word and relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Embedding words and senses together via joint knowledgeenhanced training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="100" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Re</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paving the way to a large-scale pseudosenseannotated dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A large-scale pseudoword-based evaluation framework for state-of-the-art word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="837" to="881" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2013. Ontonotes release 5.0</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Linguistic Data Consortium, Philadelphia</title>
		<imprint>
			<pubPlace>PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
