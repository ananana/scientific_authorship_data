<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-Computable Multi-Layer Neural Network Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<email>jdevlin@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
							<email>chrisq@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
							<email>arulm@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-Computable Multi-Layer Neural Network Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the last several years, neural network models have significantly improved accuracy in a number of NLP tasks. However , one serious drawback that has impeded their adoption in production systems is the slow runtime speed of neu-ral network models compared to alternate models, such as maximum entropy classi-fiers. In Devlin et al. (2014), the authors presented a simple technique for speeding up feed-forward embedding-based neural network models, where the dot product between each word embedding and part of the first hidden layer are pre-computed of-fline. However, this technique cannot be used for hidden layers beyond the first. In this paper, we explore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed &quot;next to&quot; one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at run-time and a significant reduction in perplex-ity when compared to a standard multi-layer network.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network models have become extremely popular in the last several years for a wide va- riety of NLP tasks, including language model- ing <ref type="bibr" target="#b13">(Schwenk, 2007)</ref>, sentiment analysis ( <ref type="bibr" target="#b14">Socher et al., 2013)</ref>, translation modeling ( <ref type="bibr" target="#b3">Devlin et al., 2014)</ref>, and many others <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy <ref type="bibr" target="#b0">(Berger et al., 1996)</ref> or back- off models <ref type="bibr" target="#b7">(Kneser and Ney, 1995)</ref>.</p><p>One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly costly in this scenario, since decoding a single sentence typically requires tens of thou- sands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model.</p><p>One popular technique for improving the run- time speed of NNLMs involves training the net- work to be "approximately normalized," so that the softmax normalizer does not have to be com- puted after training. Two algorithms have been proposed to achieve this: (1) noise-contrastive es- timation (NCE) <ref type="bibr" target="#b11">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b17">Vaswani et al., 2013</ref>) and (2) explicit self-normalization <ref type="bibr" target="#b3">(Devlin et al., 2014)</ref>, which is used in this paper.</p><p>However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multipli- cation. To mitigate this, <ref type="bibr" target="#b3">Devlin et al. (2014)</ref> made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed af- ter training is complete, which allows the matrix- vector multiplication to be replaced by a hand- ful of vector additions. Using these two tech- niques in combination improves the runtime speed of NNLMs by several orders of magnitude with no degradation to accuracy.</p><p>To understand pre-computation, first assume that we are training a NNLM that uses 250- dimensional word embeddings, a four word con- text window, and a 500-dimensional hidden layer. The weight matrix for the first hidden layer is thus 1000 × 500. For each word in the vocabulary and each of the four positions in the context vector, we  can pre-compute the dot product between the 250- dimensional word embedding and the 250 × 500 section of the hidden layer. This results in four 500-dimensional vectors for each word that can be stored in a lookup table. At test time, we can sim- ply sum four vectors to obtain the output of the first hidden layer. This is shown visually in <ref type="figure" target="#fig_0">Fig- ure 1</ref>. Note that this is not an approximation, and the resulting output vector is identical to the orig- inal matrix-vector product. However, the major limitation of the "pre-computation trick" is that it only works with 1-hidden layer architectures, even though more accurate models can nearly always be obtained by training multi-layer networks.</p><p>In this paper, we explore a network architecture where multiple hidden layers are placed "next to" one another instead of "on top of" one another, as is usually done. The output of these lateral lay- ers are combined using an inexpensive element- wise function and fed into the output layer. Cru- cially, then, we can apply the pre-computation trick to each hidden layer independently, allowing for very powerful models that are orders of magni- tude faster at runtime than a standard multi-layer network.</p><p>Mathematically, this can be thought of as a gen- eralization of maxout networks ( <ref type="bibr" target="#b4">Goodfellow et al., 2013)</ref>, where different element-wise combination functions are explored rather than just the max function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lateral Network</head><p>In a standard feed-forward embedding-based neu- ral network, the input tokens are mapped into a continuous vector using an embedding table <ref type="bibr">1</ref> , and this embedding vector is fed into the first hidden layer. The output of each hidden layer is then fed into the next hidden layer. We refer to this as the stacked architecture. For a two layer network, we can represent the output of the final hidden layer as:</p><formula xml:id="formula_0">H = φ(W 2 φ(W 1 E(x)))</formula><p>where x is the input vector, E(x) is the output of the embedding layer, W i is the weight matrix for layer i, and φ is the transfer function such as tanh. Generally, H is then multiplied by an output matrix and a softmax is performed to obtain the output probabilities.</p><p>In the lateral network architecture, the embed- ding layer is fed into two or more "side-by-side" hidden layers, and the outputs of these hidden lay- ers are combined using an element-wise function such as maximum or multiplication. This is repre- sented as:</p><formula xml:id="formula_1">H = C(φ(W 1 E(x)), φ(W 2 E(x)))</formula><p>Where C is a combination function that takes two or more k-dimensional vectors as inputs and produces as k-dimensional vector as output. If C(h 1 , h 2 ) = max(h 1 , h 2 ) then this is equivalent to a maxout network ( <ref type="bibr" target="#b4">Goodfellow et al., 2013)</ref>. To generalize this, we explore three different combi- nation functions: 2</p><formula xml:id="formula_2">C max (h 1 , h 2 ) = max(h 1 , h 2 ) C mul (h 1 , h 2 ) = h 1 * (h 2 + 1) C add (h 1 , h 2 ) = h 1 + h 2</formula><p>The three-or-more hidden layer versions are constructed as expected. <ref type="bibr">3</ref> A visualization is given in <ref type="figure" target="#fig_1">Figure 2</ref>. Crucially, for the lateral architecture, each hidden layer can be pre-computed independently, allowing for very fast n-gram probability lookups at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Modeling Results</head><p>In this section we report results on a large scale language modeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>Our LM training corpus consists of 120M words from the New York Times portion of the English GigaWord data set. This was chosen instead of the commonly used 1M word Penn Tree Bank corpus in order to better represent real world LM training scenarios. We use all data from 2008 and 2009 as training, the first 100k words from June 2010 as validation, and the first 100k words from Decem- ber 2010 as test. The data is segmented and to- kenized using the Stanford Word Segmenter with default settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Network Training</head><p>Training was performed with an in-house toolkit using stochastic gradient descent. The vocab- ulary is limited to 16k words so that the out- put layer can be trained using a basic softmax with self-normalization. All experiments use 250- dimensional word embeddings and a tanh activa- tion function. The weights were initialized in the range [-0.05, 0.05], the batch size was 256, and the initial learning rate was 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">5-gram LM Perplexity</head><p>5-gram results are shown in <ref type="table">Table 1</ref>. The 1-layer NNLM achieves a 13.2 perplexity improvement over the Kneser-Ney smoothed baseline <ref type="bibr" target="#b7">(Kneser and Ney, 1995)</ref>. Consistent with , using additional hidden layers to the stacked (standard) network results in 2.0-3.0 per- plexity improvements on top of the 1-layer model.</p><p>The lateral architecture significantly outper- forms any of the stacked networks, achieving a 6.5 perplexity reduction over the 1-layer model. The multiplicative combination function performs better than the additive and max functions by a small margin, which suggests that it better allows for modeling complex relationships between input words.</p><p>Perhaps most surprisingly, the additive function performs as well as the max function, despite the fact that it provides no additional modeling power compared to a 1-layer network. However, it does allow the model to generalize better than a 1-layer network by explicitly tying together two or three hidden nodes from each node in the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition</head><p>PPL 5-gram KNLM 91.1 1-Layer (k=500) 77.9 1-Layer (k=1000) 77.7 2-Stacked (k=500) 76.3 2-Stacked (k=1000) 76.2 3-Stacked (k=1000) 74.8 2-Lateral Max (k=500) 73.8 2-Lateral Mul ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="72.7">2-Lateral Add 73.7 3-Lateral Max 73.1 3-Lateral Mul 71.1 3-Lateral Add</head><p>72.3 <ref type="table">Table 1</ref>: Perplexity of 5-gram models on the New York Times test set. k is the size of the hidden layer(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Runtime Speed</head><p>The runtime speed of the various models is shown in <ref type="table">Table 2</ref>. These are computed on a single core of a E5-2650 2.6 GHz CPU. Con- sistent with <ref type="bibr" target="#b3">Devlin et al. (2014)</ref>, we see that the baseline model achieves only 230 n-gram lookups per second (LPS) at test time, while the pre-computed, self-normalized 1-layer network achieves 600,000 LPS. Adding a second stacked layer slows this down to 24,000 LPS due to the 500 × 500 matrix-vector multiplication that must be performed. However, the lateral configura- tion achieves 305,000 LPS while obtaining a bet- ter perplexity than the stacked network. In com- parison, the fastest backoff LM implementation, KenLM (Heafield, 2011), achieves 1-2 million lookups per second. In terms of memory usage, it is difficult to fairly compare backoff LMs and NNLMs because neural networks scale linearly with the vocabulary size, while backoff LMs scale linearly with the num- ber of unique n-grams. In this case, the non- precomputed neural network model is 25 MB, and the pre-computed 2-lateral network is 136 MB. <ref type="bibr">4</ref> The KenLM models are 1.1 GB for the Probing model and 317 MB for the Trie model. With a vocabulary of 50k, the 2-lateral network would be 425MB. In general, a pre-computed NNLM is comparable to or smaller than an equivalent back- off LM in terms of model size. <ref type="table">Table 2</ref>: Runtime speed of the 5-gram LM on a single CPU core. "PC" = pre-computation, "SN" = self-normalization, which are used in all but the first two experiments. The batch size is 1 except when specified. 500-dimensional hidden layers are used in all cases. "Float Ops." is the ap- proximate number of floating point operations per lookup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition</head><note type="other">Lookups Per Sec. KenLM Probing 1,923,000 KenLM Trie 950,000 1-Layer (No PC, No SN) 230 1-Layer (No PC) 13,000 1-Layer 600,000 2-Stacked 24,000 2-Stacked (Batch=128) 58,000 2-Lateral Mul 305,000</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">High-Order LM Perplexity</head><p>We also report results on a 10-gram LM trained on the same data, to explore whether the lateral network can achieve an even higher relative gain when a large input context window is available. Results are shown in <ref type="table" target="#tab_0">Table 3</ref>. Although there is a large absolute improvement over the 5-gram LM, the relative improvement between the 1-layer, 3- stacked, and 3-lateral systems are similar to the 5-gram scenario.</p><p>Condition PPL 1-Layer (k=500) 69.8 3-Stacked (k=1000) 65.8 3-Lateral Mul (k=500) 63.4 Gated Recurrent (k=1000) 55.4 As another point of comparison we report re- sults with an gated recurrent network ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>). As is consistent with the literature, the recurrent network significantly outperforms any of the feed-forward models <ref type="bibr" target="#b15">(Sundermeyer et al., 2013)</ref>.</p><p>However, recurrent models have two major downsides. First, they cannot easily be integrated into existing MT/ASR engines without signifi- cantly altering the search algorithm and search  <ref type="table">Table 4</ref>: Results on English-German machine translation test set.</p><p>space, since they require a fully expanded tar- get context. Second, the matrix-vector product between the previous hidden state and the hid- den weight matrix cannot be pre-computed, which makes the models significantly slower than pre- computable feed-forward networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Machine Translation Results</head><p>Although the lateral networks achieve a significant reduction in LM perplexity over the 1-layer net- work, it is not clear how much this will improved performance in a downstream task. To evaluate this, we trained two neural network models for use as additional features in a machine translation (MT) system. The first feature is a 5-gram NNLM, which used 1000 dimensions for the stacked network and 500 for the lateral network. The second feature is a neural network joint model (NNJM), which pre- dicts each target word using 5-gram target context and 7-gram source context. For evaluation, we present both the model perplexity and the BLEU score when using the model as an additional MT feature.</p><p>Results are presented on a large scale English- German speech translation task. The parallel train- ing data consists of 600M words from a variety of sources, including OPUS (Tiedemann, 2012) and a large in-house web crawl. The baseline 4-gram Kneser-Ney smoothed LM is trained on 7B words of German data. The NNLM and NNTMs are trained only on the parallel data. Our MT decoder is a proprietary engine similar to <ref type="bibr">Moses (Koehn et al., 2007)</ref>. The tuning set consists of 4000 utter- ances from conversational and newswire data, and the test set consists of 1500 sentences of collected conversational data.</p><p>Results are show in <ref type="table">Table 4</ref>. We can see that perplexity improvements are similar to what is seen in the English NYT data, and that improve- ments in BLEU over a 1-layer model are small but consistent. There is not a significant difference in BLEU between the 2-Stacked and 2-Lateral con- figuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explored an alternate architec- ture for embedding-based neural network models which allows for a fully pre-computable network with multiple hidden layers. The resulting mod- els achieve better perplexity than a standard multi- layer network and is at least an order of magnitude faster at runtime.</p><p>In future work, we can assess the impact of this model on a wider array of feed-forward embedding-based neural network models, such as the DSSM ( <ref type="bibr" target="#b6">Huang et al., 2013</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The "pre-computation trick." The dot product between each word embedding and each section of the hidden layer can be computed offline.</figDesc><graphic url="image-1.png" coords="2,96.10,62.81,170.08,89.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Network architectures.</figDesc><graphic url="image-2.png" coords="2,81.92,234.74,198.42,98.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Perplexity of 10-gram models on the New York Times test set. The Gated Recurrent model uses the full word history.</figDesc><table></table></figure>

			<note place="foot" n="1"> The embeddings may or may not be trained jointly with the rest of the model. 2 Note that C is an element-wise function, so these represent the operation on a single dimension of the input vectors. 3 The + 1 in C mul is used for all hidden layers after the first. This is used to prevent the value from being very close to 0. 257</note>

			<note place="foot" n="4"> The floats can be quantized to 2 bytes after training without loss of accuracy.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch. The Journal of Machine Learning Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ICASSP-95</title>
	</analytic>
	<monogr>
		<title level="j">In Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Assoc. for Computational Linguistics (ACL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient training strategies for deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loıc</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of feedforward and recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Freiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Acoustics, Speech and Signal Process. (ICASSP)</title>
		<meeting>Conf. Acoustics, Speech and Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
