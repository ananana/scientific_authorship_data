<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Piecewise Latent Variables for Neural Variational Text Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia Ii</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information Sciences &amp; Technology</orgName>
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Operations Research</orgName>
								<orgName type="institution">Universite de Montreal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Piecewise Latent Variables for Neural Variational Text Processing</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="422" to="432"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>iulian [DOT] vlad [DOT] serban [AT] umontreal [DOT] ca ago109 [AT] psu [DOT] edu jpineau [AT] cs [DOT] mcgill [DOT] ca aaron [DOT] courville [AT] umontreal [DOT] ca</keywords>
			</textClass>
			<abstract>
				<p>Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as varia-tional autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However , current models often assume simplis-tic priors on the latent variables-such as the uni-modal Gaussian distribution-which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple , but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The development of the variational autoencoder framework ( <ref type="bibr" target="#b24">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b41">Rezende et al., 2014</ref>) has paved the way for learning large- scale, directed latent variable models. This has led to significant progress in a diverse set of machine learning applications, ranging from computer vi- sion ( <ref type="bibr" target="#b13">Gregor et al., 2015;</ref><ref type="bibr" target="#b26">Larsen et al., 2016</ref>) to natural language processing tasks <ref type="bibr" target="#b35">(Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b34">Miao et al., 2016;</ref><ref type="bibr" target="#b4">Bowman et al., 2015</ref>; * The first two authors contributed equally. <ref type="bibr" target="#b51">Serban et al., 2017b</ref>). It is hoped that this frame- work will enable the learning of generative pro- cesses of real-world data -including text, audio and images -by disentangling and representing the underlying latent factors in the data. How- ever, latent factors in real-world data are often highly complex. For example, topics in newswire text and responses in conversational dialogue of- ten posses latent factors that follow non-linear (non-smooth), multi-modal distributions (i.e. dis- tributions with multiple local maxima).</p><p>Nevertheless, the majority of current models as- sume a simple prior in the form of a multivariate Gaussian distribution in order to maintain mathe- matical and computational tractability. This is of- ten a highly restrictive and unrealistic assumption to impose on the structure of the latent variables. First, it imposes a strong uni-modal structure on the latent variable space; latent variable samples from the generating model (prior distribution) all cluster around a single mean. Second, it forces the latent variables to follow a perfectly symmet- ric distribution with constant kurtosis; this makes it difficult to represent asymmetric or rarely occur- ring factors. Such constraints on the latent vari- ables increase pressure on the down-stream gen- erative model, which in turn is forced to carefully partition the probability mass for each latent factor throughout its intermediate layers. For complex, multi-modal distributions -such as the distribu- tion over topics in a text corpus, or natural lan- guage responses in a dialogue system -the uni- modal Gaussian prior inhibits the model's ability to extract and represent important latent structure in the data. In order to learn more expressive latent variable models, we therefore need more flexible, yet tractable, priors.</p><p>In this paper, we introduce a simple, flexible prior distribution based on the piecewise constant distribution. We derive an analytical, tractable form that is applicable to the variational autoen- coder framework and propose a differentiable parametrization for it. We then evaluate the ef- fectiveness of the distribution when utilized both as a prior and as approximate posterior across variational architectures in two natural language processing tasks: document modeling and natu- ral language generation for dialogue. We show that the piecewise constant distribution is able to capture elements of a target distribution that can- not be captured by simpler priors -such as the uni-modal Gaussian. We demonstrate state-of- the-art results on three document modeling tasks, and show improvements on a dialogue natural lan- guage generation. Finally, we illustrate qualita- tively how the piecewise constant distribution rep- resents multi-modal latent structure in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The idea of using an artificial neural network to approximate an inference model dates back to the early work of Hinton and colleagues <ref type="bibr" target="#b16">(Hinton and Zemel, 1994;</ref><ref type="bibr" target="#b14">Hinton et al., 1995;</ref><ref type="bibr" target="#b9">Dayan and Hinton, 1996)</ref>. Researchers later proposed Markov chain Monte Carlo methods (MCMC) <ref type="bibr" target="#b36">(Neal, 1992)</ref>, which do not scale well and mix slowly, as well as variational approaches which require a tractable, factored distribution to approximate the true posterior distribution ( <ref type="bibr" target="#b20">Jordan et al., 1999</ref>). Others have since proposed using feed-forward in- ference models to initialize the mean-field infer- ence algorithm for training <ref type="bibr">Boltzmann architectures (Salakhutdinov and Larochelle, 2010;</ref><ref type="bibr" target="#b37">Ororbia II et al., 2015)</ref>. Recently, the variational autoencoder framework (VAE) was proposed by <ref type="bibr" target="#b24">Kingma and Welling (2014)</ref> and <ref type="bibr" target="#b41">Rezende et al. (2014)</ref>, closely related to the method proposed by <ref type="bibr" target="#b35">Mnih and Gregor (2014)</ref>. This framework allows the joint training of an inference network and a di- rected generative model, maximizing a variational lower-bound on the data log-likelihood and facil- itating exact sampling of the variational posterior. Our work extends this framework.</p><p>With respect to document modeling, neural ar- chitectures have been shown to outperform well- established topic models such as Latent Dirich- let Allocation (LDA) <ref type="bibr" target="#b17">(Hofmann, 1999;</ref><ref type="bibr" target="#b2">Blei et al., 2003)</ref>. Researchers have successfully proposed several models involving discrete latent vari- ables ( <ref type="bibr" target="#b54">Srivastava et al., 2013;</ref><ref type="bibr" target="#b25">Larochelle and Lauly, 2012;</ref><ref type="bibr" target="#b55">Uria et al., 2014;</ref><ref type="bibr" target="#b27">Lauly et al., 2016;</ref><ref type="bibr" target="#b3">Bornschein and Bengio, 2015;</ref><ref type="bibr" target="#b35">Mnih and Gregor, 2014</ref>). The success of such dis- crete latent variable models -which are able to partition probability mass into separate regions - serves as one of our main motivations for investi- gating models with more flexible continuous latent variables for document modeling. More recently, <ref type="bibr" target="#b34">Miao et al. (2016)</ref> proposed to use continuous la- tent variables for document modeling.</p><p>Researchers have also investigated latent vari- able models for dialogue modeling and dialogue natural language generation ( <ref type="bibr" target="#b1">Bangalore et al., 2008;</ref><ref type="bibr" target="#b8">Crook et al., 2009;</ref><ref type="bibr" target="#b56">Zhai and Williams, 2014)</ref>. The success of discrete latent variable models in this task also motivates our investi- gation of more flexible continuous latent vari- ables.</p><p>Closely related to our proposed ap- proach is the Variational Hierarchical Recur- rent Encoder-Decoder (VHRED, described below) ( <ref type="bibr" target="#b51">Serban et al., 2017b</ref>), a neural architecture with latent multivariate Gaussian variables.</p><p>Researchers have explored more flexible dis- tributions for the latent variables in VAEs, such as autoregressive distributions, hierarchical prob- abilistic models and approximations based on MCMC sampling ( <ref type="bibr" target="#b41">Rezende et al., 2014;</ref><ref type="bibr" target="#b40">Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b23">Kingma et al., 2016;</ref><ref type="bibr" target="#b39">Ranganath et al., 2016;</ref><ref type="bibr" target="#b31">Maaløe et al., 2016;</ref><ref type="bibr" target="#b47">Salimans et al., 2015;</ref><ref type="bibr" target="#b5">Burda et al., 2016;</ref><ref type="bibr" target="#b7">Chen et al., 2017;</ref><ref type="bibr" target="#b44">Ruiz et al., 2016)</ref>. These are all complimentary to our approach; it is possible to combine them with the piecewise constant latent variables. In parallel to our work, multiple research groups have also proposed VAEs with discrete latent variables ( <ref type="bibr" target="#b32">Maddison et al., 2017;</ref><ref type="bibr" target="#b18">Jang et al., 2017;</ref><ref type="bibr" target="#b43">Rolfe, 2017;</ref><ref type="bibr" target="#b19">Johnson et al., 2016)</ref>. This is a promising line of research, however these approaches often require approximations which may be inaccurate when applied to larger scale tasks, such as docu- ment modeling or natural language generation. Fi- nally, discrete latent variables may be inappropri- ate for certain natural language processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Variational Models</head><p>We start by introducing the neural variational learning framework. We focus on modeling dis- crete output variables (e.g. words) in the context of natural language processing applications. How-ever, the framework can easily be adapted to han- dle continuous output variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Variational Learning</head><p>Let w 1 , . . . , w N be a sequence of N tokens (words) conditioned on a continuous latent vari- able z. Further, let c be an additional observed variable which conditions both z and w 1 , . . . , w N . Then, the distribution over words is:</p><formula xml:id="formula_0">P θ (w 1 , . . . , w N |c) = � N � n=1 P θ (w n |w &lt;n , z, c)P θ (z|c)dz,</formula><p>where θ are the model parameters. The model first generates the higher-level, continuous latent vari- able z conditioned on c. Given z and c, it then gen- erates the word sequence w 1 , . . . , w N . For unsu- pervised modeling of documents, the c is excluded and the words are assumed to be independent of each other, when conditioned on z:</p><formula xml:id="formula_1">P θ (w 1 , . . . , w N ) = � N � n=1 P θ (w n |z)P θ (z)dz.</formula><p>Model parameters can be learned using the varia- tional lower-bound ( <ref type="bibr" target="#b24">Kingma and Welling, 2014)</ref>:</p><formula xml:id="formula_2">log P θ (w 1 , . . . , w N |c) ≥ E z∼Qψ(z|w1,...,wN ,c) [log P θ (w n |w &lt;n , z, c)] − KL [Q ψ (z|w 1 , . . . , w N , c)||P θ (z|c)] ,<label>(1)</label></formula><p>where we note that Q ψ (z|w 1 , . . . , w N , c) is the approximation to the intractable, true posterior P θ (z|w 1 , . . . , w N , c). Q is called the encoder, or sometimes the recognition model or inference model, and it is parametrized by ψ. The distri- bution P θ (z|c) is the prior model for z, where the only available information is c. The VAE framework further employs the re-parametrization trick, which allows one to move the derivative of the lower-bound inside the expectation. To ac- complish this, z is parametrized as a transforma- tion of a fixed, parameter-free random distribu- tion z = f θ (�), where � is drawn from a ran- dom distribution. Here, f is a transformation of �, parametrized by θ, such that f θ (�) ∼ P θ (z|c). For example, � might be drawn from a standard Gaussian distribution and f might be defined as f θ (�) = µ + σ�, where µ and σ are in the param- eter set θ. In this case, z is able to represent any Gaussian with mean µ and variance σ 2 .</p><p>Model parameters are learned by maximizing the variational lower-bound in eq. (1) using gra- dient descent, where the expectation is computed using samples from the approximate posterior.</p><p>The majority of work on VAEs propose to parametrize z as multivariate Gaussian distrib- tions. However, this unrealistic assumption may critically hurt the expressiveness of the latent vari- able model. See Appendix A for a detailed dis- cussion. This motivates the proposed piecewise constant latent variable distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Piecewise Constant Distribution</head><p>We propose to learn latent variables by parametriz- ing z using a piecewise constant probability den- sity function (PDF). This should allow z to rep- resent complex aspects of the data distribution in latent variable space, such as non-smooth regions of probability mass and multiple modes.</p><p>Let n ∈ N be the number of piecewise constant components. We assume z is drawn from PDF:</p><formula xml:id="formula_3">P (z) = 1 K n � i=1 1 � i − 1 n ≤z≤ i n � a i ,<label>(2)</label></formula><p>where 1 (x) is the indicator function, which is one when x is true and otherwise zero. The distribu- tion parameters are a i &gt; 0, for i = 1, . . . , n. The normalization constant is:</p><formula xml:id="formula_4">K = n � i=1 K i , where K 0 = 0, K i = a i n , for i = 1, . . . , n.</formula><p>It is straightforward to show that a piecewise con- stant distribution with more than n &gt; 2 pieces is capable of representing a bi-modal distribution. When n &gt; 2, a vector z of piecewise constant variables can represent a probability density with 2 |z| modes. <ref type="figure">Figure 1</ref> illustrates how these variables help model complex, multi-modal distributions.</p><p>In order to compute the variational bound, we need to draw samples from the piecewise constant distribution using its inverse cumulative distribu- tion function (CDF). Further, we need to compute the KL divergence between the prior and posterior. The inverse CDF and KL divergence quantities are both derived in Appendix B. During training we must compute derivatives of the variational bound in eq. (1). These expressions involve derivatives of indicator functions, which have derivatives zero everywhere except for the changing points where the derivative is undefined. However, the proba- bility of sampling the value exactly at its changing <ref type="figure">Figure 1</ref>: Joint density plot of a pair of Gaussian and piecewise constant variables. The horizontal axis corresponds to z 1 , which is a univariate Gaus- sian variable. The vertical axis corresponds to z 2 , which is a piecewise constant variable. point is effectively zero. Thus, we fix these deriva- tives to zero. Similar approximations are used in training networks with rectified linear units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Latent Variable Parametrizations</head><p>In this section, we develop the parametrization of both the Gaussian variable and our proposed piecewise constant latent variable.</p><p>Let x be the current output sequence, which the model must generate (e.g. w 1 , . . . , w N ). Let c be the observed conditioning information. If the task contains additional conditioning information this will be embedded by c. For example, for dialogue natural language generation c represents an em- bedding of the dialogue history, while for docu- ment modeling c = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gaussian Parametrization</head><p>Let µ prior and σ 2,prior be the prior mean and vari- ance, and let µ post and σ 2,post be the approximate posterior mean and variance. For Gaussian la- tent variables, the prior distribution mean and vari- ances are encoded using linear transformations of a hidden state. In particular, the prior distribu- tion covariance is encoded as a diagonal covari- ance matrix using a softplus function: are learnable parameters. For the posterior distribution, previous work has shown it is better to parametrize the posterior distribution as a linear interpolation of the prior distribution mean and variance and a new estimate of the mean and variance based on the observation x ( <ref type="bibr" target="#b12">Fraccaro et al., 2016)</ref>. The interpolation is controlled by a gating mechanism, allowing the model to turn on/off latent dimensions:</p><formula xml:id="formula_5">µ prior = H prior µ Enc(c) + b prior µ , σ 2,prior = diag(log(1 + exp(H prior σ Enc(c) + b prior σ ))),</formula><formula xml:id="formula_6">µ post =(1 − α µ )µ prior + α µ � H post µ Enc(c, x) + b post µ � , σ 2,post =(1 − α σ )σ 2,prior + α σ diag(log(1 + exp(H post σ Enc(c, x) + b post σ ))),</formula><p>where Enc(c, x) is an embedding of both c and x. The matrices H The interpolation mechanism is controlled by α µ and α σ , which are initialized to zero (i.e. initial- ized such that the posterior is equal to the prior).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Piecewise Constant Parametrization</head><p>We parametrize the piecewise prior parameters us- ing an exponential function applied to a linear transformation of the conditioning information: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Variational Text Modeling</head><p>We now introduce two classes of VAEs. The mod- els are extended by incorporating the Gaussian and piecewise latent variable parametrizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document Model</head><p>The neural variational document model (NVDM) model has previously been proposed for document modeling <ref type="bibr" target="#b35">(Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b34">Miao et al., 2016)</ref>, where the latent variables are Gaussian.</p><p>Since the original NVDM uses Gaussian latent variables, we will refer to it as G-NVDM. We pro- pose two novel models building on G-NVDM. The first model we propose uses piecewise constant la- tent variables instead of Gaussian latent variables.</p><p>We refer to this model as P-NVDM. The second model we propose uses a combination of Gaus- sian and piecewise constant latent variables. The models sample the Gaussian and piecewise con- stant latent variables independently and then con- catenates them together into one vector. We refer to this model as H-NVDM.</p><p>Let for the Gaussian means and variances. We initialize the bias parameters to zero in order to start with centered Gaussian and piecewise constant priors. The encoder will adapt these priors as learning progresses, using the gating mechanism to turn on/off latent dimensions.</p><note type="other">V be the vocabulary of document words. Let W represent a document matrix, where row w i is the 1-of-|V | binary encoding of the i'th word in the document. Each model has an encoder com- ponent Enc(W ), which compresses a document vector into a continuous distributed representa- tion upon which the approximate posterior is built. For document modeling, word order information is not taken into account and no additional condi- tioning information is available. Therefore, each model uses a bag-of-words encoder, defined as a multi-layer perceptron (MLP) Enc(c = ∅, x) = Enc(x). Based on preliminary experiments, we choose the encoder to be a two-layered MLP with parametrized rectified linear activation functions (we omit these parameters for simplicity). For the approximate posterior, each model has the param- eter matrix W</note><p>Let z be the vector of latent variables sampled according to the approximate posterior distribu- tion. Given z, the decoder Dec(w, z) outputs a distribution over words in the document:</p><formula xml:id="formula_7">Dec(w, z) = exp (−w T Rz + b w ) � w � exp (−w T Rz + b w � ) ,</formula><p>where R is a parameter matrix and b is a parameter vector corresponding to the bias for each word to be learned. This output probability distribution is combined with the KL divergences to compute the lower-bound in eq. <ref type="bibr">(1)</ref>. See Appendix C. Our baseline model G-NVDM is an improve- ment over the original NVDM proposed by <ref type="bibr" target="#b35">Mnih and Gregor (2014)</ref> and <ref type="bibr" target="#b34">Miao et al. (2016)</ref>. We learn the prior mean and variance, while these were fixed to a standard Gaussian in previous work. This increases the flexibility of the model and makes optimization easier. In addition, we use a gating mechanism for the approximate pos- terior of the Gaussian variables. This gating mech- anism allows the model to turn off latent vari- able (i.e. fix the approximate posterior to equal the prior for specific latent variables) when computing the final posterior parameters. Furthermore, <ref type="bibr" target="#b34">Miao et al. (2016)</ref> alternated between optimizing the ap- proximate posterior parameters and the generative model parameters, while we optimize all parame- ters simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dialogue Model</head><p>The variational hierarchical recurrent encoder- decoder (VHRED) model has previously been pro- posed for dialogue modeling and natural language generation ( <ref type="bibr" target="#b51">Serban et al., 2017b</ref><ref type="bibr" target="#b50">Serban et al., , 2016a</ref>). The model decomposes dialogues using a two-level hi- erarchy: sequences of utterances (e.g. sentences), and sub-sequences of tokens (e.g. words). Let w n be the n'th utterance in a dialogue with N utter- ances. Let w n,m be the m'th word in the n'th utter- ance from vocabulary V given as a 1-of-|V | binary encoding. Let M n be the number of words in the n'th utterance. For each utterance n = 1, . . . , N , the model generates a latent variable z n . Condi- tioned on this latent variable, the model then gen- erates the next utterance:</p><formula xml:id="formula_8">P θ (w 1 , z 1 , . . . , w N , z N ) = N � n=1 P θ (z n |w &lt;n ) × Mn � m=1 P θ (w n,m |w n,&lt;m , w &lt;n , z n ),</formula><p>where θ are the model parameters. VHRED con- sists of three RNN modules: an encoder RNN, a context RNN and a decoder RNN. The en- coder RNN computes an embedding for each ut- terance. This embedding is fed into the context RNN, which computes a hidden state summariz- ing the dialogue context before utterance n: h con n−1 . This state represents the additional conditioning information, which is used to compute the prior distribution over z n :</p><formula xml:id="formula_9">P θ (z n | w &lt;n ) = f prior θ (z n ; h con n−1 ),</formula><p>where f prior is a PDF parametrized by both θ and h con n−1 . A sample is drawn from this distribution: z n ∼ P θ (z n |w &lt;n ). This sample is given as input to the decoder RNN, which then computes the out- put probabilities of the words in the next utterance. The model is trained by maximizing the varia- tional lower-bound, which factorizes into indepen- dent terms for each sub-sequence (utterance):</p><formula xml:id="formula_10">log P θ (w 1 , . . . , w N ) ≥ N � n=1 − KL [Q ψ (z n | w 1 , . . . , w n )||P θ (z n | w &lt;n )] + E Q ψ (zn|w 1 ,...,wn) [log P θ (w n | z n , w &lt;n )] ,</formula><p>where distribution Q ψ is the approximate posterior distribution with parameters ψ, computed simi- larly as the prior distribution but further condi- tioned on the encoder RNN hidden state of the next utterance.</p><p>The original VHRED model ( <ref type="bibr" target="#b51">Serban et al., 2017b</ref>) used Gaussian latent variables. We re- fer to this model as G-VHRED. The first model we propose uses piecewise constant latent vari- ables instead of Gaussian latent variables. We re- fer to this model as P-VHRED. The second model we propose takes advantage of the representation power of both Gaussian and piecewise constant la- tent variables. This model samples both a Gaus- sian latent variable z gaussian n and a piecewise la- tent variable z piecewise n independently conditioned on the context RNN hidden state:</p><formula xml:id="formula_11">P θ (z gaussian n | w &lt;n ) = f prior, gaussian θ (z gaussian n ; h con n−1 ), P θ (z piecewise n | w &lt;n ) = f prior, piecewise θ (z piecewise n ; h con n−1 ),</formula><p>where f prior, gaussian and f prior, piecewise are PDFs parametrized by independent subsets of parame- ters θ. We refer to this model as H-VHRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the proposed models on two types of natural language processing tasks: document modeling and dialogue natural language genera- tion. All models are trained with back-propagation using the variational lower-bound on the log- likelihood or the exact log-likelihood. We use the first-order gradient descent optimizer Adam ( <ref type="bibr" target="#b22">Kingma and Ba, 2015)</ref>   <ref type="table">Table 1</ref>: Test perplexities on three document mod- eling tasks: 20-NewGroup (20-NG), Reuters cor- pus (RCV1) and CADE12 (CADE). Perplexities were calculated using 10 samples to estimate the variational lower-bound. The H-NVDM models perform best across all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Document Modeling</head><p>Tasks We use three different datasets for docu- ment modeling experiments. First, we use the 20 News-Groups (20-NG) dataset ). Second, we use the Reuters corpus (RCV1-V2), using a version that con- tained a selected 5,000 term vocabulary. As in previous work <ref type="bibr" target="#b25">Larochelle and Lauly, 2012)</ref>, we transform the original word frequencies using the equation log(1 + TF), where TF is the original word fre- quency. Third, to test our document models on text from a non-English language, we use the Brazilian Portuguese CADE12 dataset <ref type="bibr" target="#b6">(Cardoso-Cachopo, 2007)</ref>. For all datasets, we track the validation bound on a subset of 100 vectors randomly drawn from each training corpus. Training All models were trained using mini- batches with 100 examples each. A learning rate of 0.002 was used. Model selection and early stop- ping were conducted using the validation lower- bound, estimated using five stochastic samples per validation example. Inference networks used 100 units in each hidden layer for 20-NG and CADE, and 100 for RCV1. We experimented with both 50 and 100 latent random variables for each class of models, and found that 50 latent variables per- formed best on the validation set. For H-NVDM we vary the number of components used in the PDF, investigating the effect that 3 and 5 pieces had on the final quality of the model. The number <ref type="table" target="#tab_3">G-NVDM   H-NVDM-3 H-NVDM-5  environment project  science  project  gov  built  flight  major  high  lab  based  technology  mission  earth  world  launch  include  form  field  science  scale  working  nasa  sun  build  systems  special  gov</ref> technical area <ref type="table">Table 2</ref>: Word query similarity test on 20 News- Groups: for the query 'space", we retrieve the top 10 nearest words in word embedding space based on Euclidean distance. H-NVDM-5 asso- ciates multiple meanings to the query, while G- NVDM only associates the most frequent meaning.</p><p>of hidden units was chosen via preliminary exper- imentation with smaller models. On 20-NG, we use the same set-up as ) and therefore report the perplexities of a topic model <ref type="bibr">(LDA, (Hinton and Salakhutdinov, 2009)</ref>), the document neural auto-regressive esti- mator (docNADE, (Larochelle and Lauly, 2012)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest re- ported perplexity, ( <ref type="bibr" target="#b34">Miao et al., 2016)</ref>).</p><p>Results In <ref type="table">Table 1</ref>, we report the test docu- ment perplexity:</p><formula xml:id="formula_12">exp(− 1 D � n 1</formula><p>Ln log P θ (x n ). We use the variational lower-bound as an approxima- tion based on 10 samples, as was done in <ref type="bibr" target="#b35">(Mnih and Gregor, 2014</ref>). First, we note that the best baseline model (i.e. the NVDM) is more competi- tive when both the prior and posterior models are learnt together (i.e. the G-NVDM), as opposed to the fixed prior of ( <ref type="bibr" target="#b34">Miao et al., 2016)</ref>. Next, we observe that integrating our proposed piecewise variables yields even better results in our docu- ment modeling experiments, substantially improv- ing over the baselines. More importantly, in the 20-NG and Reuters datasets, increasing the num- ber of pieces from 3 to 5 further reduces perplex- ity. Thus, we have achieved a new state-of-the- art perplexity on 20 News-Groups task and -to the best of our knowledge -better perplexities on the CADE12 and RCV1 tasks compared to us- ing a state-of-the-art model like the G-NVDM. We also evaluated the converged models using an non- parametric inference procedure, where a separate approximate posterior is learned for each test ex- ample in order to tighten the variational lower- bound. H-NVDM also performed best in this eval- uation across all three datasets, which confirms that the performance improvement is due to the piecewise components. See appendix for details.</p><p>In <ref type="table">Table 2</ref>, we examine the top ten highest ranked words given the query term "space", using the decoder parameter matrix. The piecewise vari- ables appear to have a significant effect on what is uncovered by the model.In the case of "space", the hybrid with 5 pieces seems to value two senses of the word-one related to "outer space" (e.g., "sun", "world", etc.) and another related to the dimen- sions of depth, height, and width within which things may exist and move (e.g., "area", "form", "scale", etc.). On the other hand, G-NVDM ap- pears to only capture the "outer space" sense of   <ref type="figure" target="#fig_3">Figure 2</ref>, both G-NVDM and H-NVDM-5 learn representations which disentangle the topic clusters on 20-NG. However, G-NVDM appears to have more dis- persed clusters and more outliers (i.e. data points in the periphery) compared to H-NVDM-5. Al- though it is difficult to draw conclusions based on these plots, these findings could potentially be ex- plained by the Gaussian latent variables fitting the latent factors poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dialogue Modeling</head><p>Task We evaluate VHRED on a natural language generation task, where the goal is to generate re- sponses in a dialogue. This is a difficult prob- lem, which has been extensively studied in the recent literature <ref type="bibr" target="#b42">(Ritter et al., 2011;</ref><ref type="bibr" target="#b29">Lowe et al., 2015;</ref><ref type="bibr" target="#b53">Sordoni et al., 2015;</ref><ref type="bibr" target="#b28">Li et al., 2016;</ref><ref type="bibr">Serban et al., 2016a,b)</ref>. Dialogue response generation has recently gained a significant amount of atten- tion from industry, with high-profile projects such as Google SmartReply ( <ref type="bibr" target="#b21">Kannan et al., 2016)</ref> and Microsoft Xiaoice ( <ref type="bibr" target="#b33">Markoff and Mozur, 2015)</ref>. Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system <ref type="bibr" target="#b11">(Farber, 2016)</ref>.</p><p>We evaluate on the technical support response generation task for the Ubuntu operating system. We use the well-known Ubuntu Dialogue Corpus ( <ref type="bibr" target="#b29">Lowe et al., 2015</ref><ref type="bibr" target="#b30">Lowe et al., , 2017</ref>, which consists of about 1/2 million natural language dialogues extracted from the #Ubuntu Internet Relayed Chat (IRC) channel. The technical problems discussed span a wide range of software-related and hardware- related issues. Given a dialogue history -such as a conversation between a user and a technical support assistant -the model must generate the next appropriate response in the dialogue. For ex- ample, when it is the turn of the technical support assistant, the model must generate an appropriate response helping the user resolve their problem.</p><p>We evaluate the models using the activity-and entity-based metrics designed specifically for the Ubuntu domain ( <ref type="bibr" target="#b49">Serban et al., 2017a</ref>). These metrics compare the activities and entities in the model generated responses with those of the ref- erence responses; activities are verbs referring to high-level actions (e.g. download, install, unzip) and entities are nouns referring to technical ob- jects (e.g. Firefox, GNOME). The more activities and entities a model response overlaps with the reference response (e.g. expert response) the more likely the response will lead to a solution.</p><p>Training The models were trained to maxi- mize the log-likelihood of training examples us- ing a learning rate of 0.0002 and mini-batches of size 80. We use a variant of truncated back- propagation. We terminate the training procedure for each model using early stopping, estimated using one stochastic sample per validation exam- ple. We evaluate the models by generating dia- logue responses: conditioned on a dialogue con- text, we fix the model latent variables to their me- dian values and then generate the response using a beam search with size 5. We select model hyper- parameters based on the validation set using the F1 activity metric, as described earlier.</p><p>It is often difficult to train generative models for language with stochastic latent variables <ref type="bibr" target="#b4">(Bowman et al., 2015;</ref><ref type="bibr" target="#b51">Serban et al., 2017b</ref>). For the latent variable models, we therefore experiment with reweighing the KL divergence terms in the variational lower-bound with values 0.25, 0.50, 0.75 and 1.0. In addition to this, we linearly in- crease the KL divergence weights starting from zero to their final value over the first 75000 train- ing batches. Finally, we weaken the decoder RNN by randomly replacing words inputted to the de- coder RNN with the unknown token with 25% probability. These steps are important for effec- tively training the models, and the latter two have been used in previous work by <ref type="bibr" target="#b4">Bowman et al. (2015)</ref> and <ref type="bibr" target="#b51">Serban et al. (2017b)</ref>.</p><p>HRED (Baseline): We compare to the HRED model ( <ref type="bibr" target="#b50">Serban et al., 2016a</ref>): a sequence-to- sequence model, shown to outperform other es-tablished models on this task, such as the LSTM RNN language model ( <ref type="bibr" target="#b49">Serban et al., 2017a</ref>). The HRED model's encoder RNN uses a bidirectional GRU RNN encoder, where the forward and back- ward RNNs each have 1000 hidden units. The context RNN is a GRU encoder with 1000 hidden units, and the decoder RNN is an LSTM decoder with 2000 hidden units. <ref type="bibr">2</ref> The encoder and con- text RNNs both use layer normalization ( <ref type="bibr" target="#b0">Ba et al., 2016)</ref>. <ref type="bibr">3</ref> We also experiment with an additional rectified linear layer applied on the inputs to the decoder RNN. As with other hyper-parameters, we choose whether to include this additional layer based on the validation set performance. HRED, as well as all other models, use a word embedding dimensionality of size 400.</p><p>G-HRED: We compare to G-VHRED, which is VHRED with Gaussian latent variables ( <ref type="bibr" target="#b51">Serban et al., 2017b</ref>). G-VHRED uses the same hyper- parameters for the encoder, context and decoder RNNs as the HRED model. The model has 100 Gaussian latent variables per utterance. P-HRED: The first model we propose is P- VHRED, which is VHRED model with piecewise constant latent variables. We use n = 3 number of pieces for each latent variable. P-VHRED also uses the same hyper parameters for the encoder, context and decoder RNNs as the HRED model. Similar to G-VHRED, P-VHRED has 100 piece- wise constant latent variables per utterance.</p><p>H-HRED: The second model we propose is H- VHRED, which has 100 piecewise constant (with n = 3 pieces per variable) and 100 Gaussian la- tent variables per utterance. H-VHRED also uses the same hyper-parameters for the encoder, con- text and decoder RNNs as HRED.</p><p>Results: The results are given in <ref type="table" target="#tab_3">Table 3</ref>. All latent variable models outperform HRED w.r.t. both activities and entities. This strongly suggests that the high-level concepts represented by the latent variables help generate meaningful, goal- directed responses. Furthermore, each type of latent variable appears to help with a different aspects of the generation task. G-VHRED per- forms best w.r.t. activities (e.g. download, install and so on), which occur frequently in the dataset. This suggests that the Gaussian latent variables learn useful latent representations for frequent ac- tions. On the other hand, H-VHRED performs best w.r.t. entities (e.g. Firefox, GNOME), which are often much rarer and mutually exclusive in the dataset. This suggests that the combination of Gaussian and piecewise latent variables help learn useful representations for entities, which could not be learned by Gaussian latent variables alone. We further conducted a qualitative analysis of the model responses, which supports these conclu- sions. See Appendix G. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we have sought to learn rich and flexible multi-modal representations of latent vari- ables for complex natural language processing tasks. We have proposed the piecewise constant distribution for the variational autoencoder frame- work. We have derived closed-form expressions for the necessary quantities required for in the au- toencoder framework, and proposed an efficient, differentiable implementation of it. We have in- corporated the proposed piecewise constant dis- tribution into two model classes -NVDM and VHRED -and evaluated the proposed models on document modeling and dialogue modeling tasks. We have achieved state-of-the-art results on three document modeling tasks, and have demonstrated substantial improvements on a dialogue modeling task. Overall, the results highlight the benefits of incorporating the flexible, multi-modal piece- wise constant distribution into variational autoen- coders. Future work should explore other natural language processing tasks, where the data is likely to arise from complex, multi-modal latent factors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>σ</head><label></label><figDesc>, α µ , α σ are parameters to be learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Enc(c) + b prior a,i ), i = 1, . . . , n, where matrix H prior a and vector b prior a are learnable. As before, we define the posterior parameters as a function of both c and x: a post i = exp(H post a,i Enc(c, x) + b post a,i ), i = 1, . . . , n, where H post a and b post a are parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>post a and vector b post a for the piece- wise latent variables, and the parameter matrices W post µ , W post σ and vectors b post µ , b post σ for the Gaus- sian means and variances. For the prior, each model has parameter vector b prior a for the piece- wise latent variables, and vectors b prior µ , b prior σ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Latent variable approximate posterior means t-SNE visualization on 20-NG for GNVDM and H-NVDM-5. Colors correspond to the topic labels assigned to each document.</figDesc><graphic url="image-2.png" coords="7,318.03,84.02,182.69,345.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Ubuntu evaluation using F1 metrics w.r.t. 
activities and entities. G-VHRED, P-VHRED and 
H-VHRED all outperform the baseline HRED. 
G-VHRED performs best w.r.t. activities and H-
VHRED performs best w.r.t. entities. 

the word. More examples are in the appendix. 
Finally, we visualized the means of the approx-
imate posterior latent variables on 20-NG through 
a t-SNE projection. As shown in </table></figure>

			<note place="foot" n="1"> Code and scripts are available at https://github. com/ago109/piecewise-nvdm-emnlp-2017 and https://github.com/julianser/ hred-latent-piecewise.</note>

			<note place="foot" n="2"> Since training lasted between 1-3 weeks for each model, we had to fix the number of hidden units during preliminary experiments on the training and validation datasets. 3 We did not apply layer normalization to the decoder RNN, because several of our colleagues have found that this may hurt the performance of generative language models.</note>

			<note place="foot" n="4"> Results on a Twitter dataset are given in the appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors acknowledge NSERC, Canada Re-search Chairs, CIFAR, IBM Research, Nuance Foundation and Microsoft Maluuba for fund-ing.</p><p>Alexander G. Ororbia II was funded by a NACME-Sloan scholarship. The authors thank Hugo Larochelle for sharing the News-Group 20 dataset.</p><p>The authors thank Lau-rent Charlin, Sungjin Ahn, and Ryan Lowe for constructive feedback. This research was en-abled in part by support provided by Calcul Qubec (www.calculquebec.ca) and Com-pute Canada (www.computecanada.ca).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning the structure of task-driven human-human dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Di</forename><surname>Fabbrizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1249" to="1259" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. JAIR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reweighted wakesleep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Importance weighted autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving Methods for Single-label Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardoso-Cachopo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Instituto Superior Tecnico, Universidade Tecnica de Lisboa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PdD Thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised classification of dialogue acts using a dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Granell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="341" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Varieties of helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1385" to="1403" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sample-based non-uniform random variate generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Winter simulation</title>
		<meeting>the 18th conference on Winter simulation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="260" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Amazon&apos;s &apos;Alexa Prize&apos; Will Give College Students Up To $2.5M To Create A Socialbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Fortune</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The&quot; wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2946" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smart Reply: Automated Response Suggestion for Email</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Improving variational inference with inverse autoregressive flow. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4736" to="4744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05962</idno>
		<title level="m">Document neural autoregressive distribution estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured MultiTurn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training End-to-End Dialogue Systems with the Ubuntu Dialogue Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1445" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">For Sympathetic Ear, More Chinese Turn to Smartphone Program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Markoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mozur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>New York Times</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Connectionist learning of belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="113" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06964</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">On the difficulty of training recurrent neural networks. ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical variational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Datadriven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Rolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The generalized reparameterization gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Francisco R Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Aueb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient learning of deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATs</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="693" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generative deep neural networks for dialogue: A short review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, Let&apos;s Discuss: Learning Methods for Dialogue Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling documents with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Discovering latent structure in task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
