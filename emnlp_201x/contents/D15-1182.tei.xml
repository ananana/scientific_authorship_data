<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Posterior calibration and exploratory analysis for natural language processing models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
							<email>kxnguyen@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>College Park College Park</addrLine>
									<postCode>20742</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>O&amp;apos;connor</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Posterior calibration and exploratory analysis for natural language processing models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model&apos;s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies; and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language processing systems are imper- fect. Decades of research have yielded analyzers that mis-identify named entities, mis-attach syn- tactic relations, and mis-recognize noun phrase coreference anywhere from 10-40% of the time. But these systems are accurate enough so that their outputs can be used as soft, if noisy, indicators of language meaning for use in downstream analysis, such as systems that perform question answering, machine translation, event extraction, and narra- tive analysis <ref type="bibr" target="#b25">(McCord et al., 2012;</ref><ref type="bibr" target="#b14">Gimpel and Smith, 2008;</ref><ref type="bibr" target="#b27">Miwa et al., 2010;</ref><ref type="bibr" target="#b0">Bamman et al., 2013)</ref>.</p><p>To understand the performance of an ana- lyzer, researchers and practitioners typically mea- sure the accuracy of individual labels or edges among a single predicted output structure y, such as a most-probable tagging or entity clustering arg max y P (y|x) (conditional on text data x). But a probabilistic model gives a probability distribution over many other output structures that have smaller predicted probabilities; a line of work has sought to control cascading pipeline errors by passing on multiple structures from earlier stages of analysis, by propagating prediction uncertainty through multiple samples ( <ref type="bibr" target="#b12">Finkel et al., 2006</ref>), K-best lists ( <ref type="bibr" target="#b46">Venugopal et al., 2008;</ref><ref type="bibr" target="#b44">Toutanova et al., 2008)</ref>, or explicitly diverse lists ( <ref type="bibr" target="#b16">Gimpel et al., 2013)</ref>; often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decod- ing <ref type="bibr" target="#b18">(Goodman, 1996;</ref><ref type="bibr" target="#b22">Kumar and Byrne, 2004</ref>), or to perform joint inference between early and later stages of NLP analysis (e.g. <ref type="bibr">Singh et al., 2013;</ref><ref type="bibr" target="#b11">Durrett and Klein, 2014)</ref>.</p><p>These approaches should work better when the posterior probabilities of the predicted linguistic structures reflect actual probabilities of the struc- tures or aspects of the structures. For example, say a model is overconfident: it places too much prob- ability mass in the top prediction, and not enough in the rest. Then there will be little benefit to us- ing the lower probability structures, since in the training or inference objectives they will be incor- rectly outweighed by the top prediction (or in a sampling approach, they will be systematically un- dersampled and thus have too-low frequencies). If we only evaluate models based on their top pre- dictions or on downstream tasks, it is difficult to diagnose this issue.</p><p>Instead, we propose to directly evaluate the cal- ibration of a model's posterior prediction distri- bution. A perfectly calibrated model knows how often it's right or wrong; when it predicts an event with 80% confidence, the event empirically turns out to be true 80% of the time. While perfect accuracy for NLP models remains an unsolved challenge, perfect calibration is a more achievable goal, since a model that has imperfect accuracy could, in principle, be perfectly calibrated. In this paper, we develop a method to empirically analyze calibration that is appropriate for NLP models <ref type="bibr">( §3)</ref> and use it to analyze common generative and dis- criminative models for tagging and classification ( §4).</p><p>Furthermore, if a model's probabilities are meaningful, that would justify using its proba- bility distributions for any downstream purpose, including exploratory analysis on unlabeled data. In §6 we introduce a representative corpus explo- ration problem, identifying temporal event trends in international politics, with a method that is de- pendent on coreference resolution. We develop a coreference sampling algorithm ( §5.2) which projects uncertainty into the event extraction, in- ducing a posterior distribution over event frequen- cies. Sometimes the event trends have very high posterior variance (large confidence intervals), 2 reflecting when the NLP system genuinely does not know the correct semantic extraction. This highlights an important use of a calibrated model: being able to tell a user when the model's predic- tions are likely to be incorrect, or at least, not giv- ing a user a false sense of certainty from an erro- neous NLP analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definition of calibration</head><p>Consider a binary probabilistic prediction prob- lem, which consists of binary labels and proba- bilistic predictions for them. Each instance has a ground-truth label y ∈ {0, 1}, which is used for evaluation. The prediction problem is to gener- ate a predicted probability or prediction strength q ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Typically, we use some form of a prob- abilistic model to accomplish this task, where q represents the model's posterior probability 3 of the instance having a positive label (y = 1).</p><p>Let S = {(q 1 , y 1 ), (q 2 , y 2 ), · · · (q N , y N )} be the set of prediction-label pairs produced by the model. Many metrics assess the overall quality of how well the predicted probabilities match the data, such as the familiar cross entropy (negative average log-likelihood),</p><formula xml:id="formula_0">L ( y, q) = 1 N i y i log 1 q i + (1 − y i ) log 1 1 − q i</formula><p>or mean squared error, also known as the Brier score when y is binary <ref type="bibr" target="#b4">(Brier, 1950)</ref>,</p><formula xml:id="formula_1">L 2 ( y, q) = 1 N i (y i − q i ) 2</formula><p>2 We use the terms confidence interval and credible inter- val interchangeably in this work; the latter term is debatably more correct, though less widely familiar.</p><p>3 Whether q comes from a Bayesian posterior or not is ir- relevant to the analysis in this section. All that matters is that predictions are numbers q ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>Both tend to attain better (lower) values when q is near 1 when y = 1, and near 0 when y = 0; and they achieve a perfect value of 0 when all q i = y i . <ref type="bibr">4</ref> Let P(y, q) be the joint empirical distribution over labels and predictions. Under this notation, L 2 = E q,y [y − q] 2 . Consider the factorization P(y, q) = P(y | q) P(q) where P(y | q) denotes the label empirical fre- quency, conditional on a prediction strength <ref type="bibr" target="#b28">(Murphy and Winkler, 1987)</ref>. <ref type="bibr">5</ref> Applying this factor- ization to the Brier score leads to the calibration- refinement decomposition <ref type="bibr" target="#b8">(DeGroot and Fienberg, 1983)</ref>, in terms of expectations with respect to the prediction strength distribution P(q):</p><formula xml:id="formula_2">L 2 = E q [q − p q ] 2 Calibration MSE + E q [p q (1 − p q )] Refinement (1)</formula><p>where we denote p q ≡ P(y = 1 | q) for brevity.</p><p>Here, calibration measures to what extent a model's probabilistic predictions match their cor- responding empirical frequencies. Perfect calibra- tion is achieved when P(y = 1 | q) = q for all q; intuitively, if you aggregate all instances where a model predicted q, they should have y = 1 at q percent of the time. We define the magnitude of miscalibration using root mean squared error: Definition 1 (RMS calibration error).</p><formula xml:id="formula_3">CalibErr = E q [q − P(y = 1 | q)] 2</formula><p>The second term of Eq 1 refers to refinement, which reflects to what extent the model is able to separate different labels (in terms of the con- ditional Gini entropy p q (1 − p q )). If the predic- tion strengths tend to cluster around 0 or 1, the re- finement score tends to be lower. The calibration- refinement breakdown offers a useful perspective on the accuracy of a model posterior. This paper focuses on calibration. There are several other ways to break down squared error, log-likelihood, and other probabilis- tic scoring rules. <ref type="bibr">6</ref> We use the Brier-based calibra- tion error in this work, since unlike cross-entropy Algorithm 1 Estimate calibration error using adaptive binning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>A set of N prediction-label pairs {(q1, y1), (q2, y2), · · · , (qN , yN )}. Output: Calibration error. Parameter: Target bin size β.</p><p>Step 1: Sort pairs by prediction values q k in ascending order.</p><p>Step 2: For each, assign bin label</p><formula xml:id="formula_4">b k = k−1 β + 1.</formula><p>Step 3: Define each bin Bi as the set of indices of pairs that have the same bin label. If the last bin has size less than β, merge it with the second-to-last bin (if one exists). Let {B1, B2, · · · , BT } be the set of bins.</p><p>Step 4: Calculate empirical and predicted probabilities per bin:</p><formula xml:id="formula_5">ˆ pi = 1 |Bi| k∈B i y k andˆqi andˆ andˆqi = 1 |Bi| k∈B i q k</formula><p>Step 5: Calculate the calibration error as the root mean squared error per bin, weighted by bin size in case they are not uniformly sized:</p><formula xml:id="formula_6">CalibErr = 1 N T i=1 |Bi|(ˆ qi − ˆ pi) 2</formula><p>it does not tend toward infinity when near prob- ability 0; we hypothesize this could be an issue since both p and q are subject to estimation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical calibration analysis</head><p>From a test set of labeled data, we can analyze model calibration both in terms of the calibration error, as well as visualizing the calibration curve of label frequency versus predicted strength. How- ever, computing the label frequencies P(y = 1|q) requires an infinite amount of data. Thus approx- imation methods are required to perform calibra- tion analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adaptive binning procedure</head><p>Previous studies that assess calibration in super- vised machine learning models ( <ref type="bibr" target="#b30">Niculescu-Mizil and Caruana, 2005;</ref><ref type="bibr" target="#b1">Bennett, 2000</ref>) calculate la- bel frequencies by dividing the prediction space into deciles or other evenly spaced bins-e.g. q ∈ [0, 0.1), q ∈ [0.1, 0.2), etc.-and then calculat- ing the empirical label frequency in each bin. This procedure may be thought of as using a form of nonparametric regression (specifically, a regres- sogram; Tukey 1961) to estimate the function f (q) = P(y = 1 | q) from observed data points. But models in natural language processing give very skewed distributions of confidence scores q (many are near 0 or 1), so this procedure performs poorly, having much more variable estimates near Algorithm 2 Estimate calibration error's confi- dence interval by sampling.</p><p>Input:</p><formula xml:id="formula_7">A set of N prediction-label pairs {(q1, y1), (q2, y2), · · · , (qN , yN )}.</formula><p>Output: Calibration error with a 95% confidence interval. Parameter: Number of samples, S.</p><p>Step 1: Calculate {ˆp1{ˆp1, ˆ p2, · · · , ˆ pT } from step 4 of Algo- rithm 1.</p><p>Step 2: Draw S samples. For each s = 1..S,</p><p>• For each bin i = 1..T , drawˆpdrawˆ drawˆp</p><formula xml:id="formula_8">(s) i ∼ N ˆ pi, ˆ σ 2 i , wherêwherê σ 2 i = ˆ pi(1 − ˆ pi)/|Bi|. If necessary clip to [0, 1]: ˆ p (s) i := min(1, max(0, ˆ p (s) i ))</formula><p>• Calculate the sample's CalibErr from using the pairs</p><formula xml:id="formula_9">(ˆ qi, ˆ p (s) i ) as per</formula><p>Step 5 of Algorithm 1.</p><p>Step 3: Calculate the 95% confidence interval for the calibra- tion error as:</p><formula xml:id="formula_10">CalibErravg ± 1.96ˆserror96ˆ 96ˆserror</formula><p>where CalibErravg andˆserrorandˆ andˆserror are the mean and the stan- dard deviation, respectively, of the CalibErrs calculated from the samples.</p><p>the middle of the q distribution ( <ref type="figure" target="#fig_0">Figure 1</ref>). We propose adaptive binning as an alterna- tive. Instead of dividing the interval [0, 1] into fixed-width bins, adaptive binning defines the bins such that there are an equal number of points in each, after which the same averaging proce- dure is used. This method naturally gives wider bins to area with fewer data points (areas that re- quire more smoothing), and ensures that these ar- eas have roughly similar standard errors as those near the boundaries, since for a bin with β num- ber of points and empirical frequency p, the stan- dard error is estimated by p(1 − p)/β, which is bounded above by 0.5/ √ β. Algorithm 1 describes the procedure for estimating calibration error us- ing adaptive binning, which can be applied to any probabilistic model that predicts posterior proba- bilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Confidence interval estimation</head><p>Especially when the test set is small, estimating calibration error may be subject to error, due to uncertainty in the label frequency estimates. Since how to estimate confidence bands for nonparamet- ric regression is an unsolved problem <ref type="bibr" target="#b47">(Wasserman, 2006)</ref>, we resort to a simple method based on the binning. We construct a binomial normal approx- imation for the label frequency estimate in each bin, and simulate from it; every simulation across all bins is used to construct a calibration error; these simulated calibration errors are collected to construct a normal approximation for the calibra- tion error estimate. Since we use bin sizes of at least β ≥ 200 in our experiments, the central limit theorem justifies these approximations. We report all calibration errors along with their 95% confi- dence intervals calculated by Algorithm 2. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visualizing calibration</head><p>In order to better understand a model's calibration properties, we plot the pairs</p><formula xml:id="formula_11">(ˆ p 1 , ˆ q 1 ), (ˆ p 2 , ˆ q 2 ), · · · , (ˆ p T , ˆ q T )</formula><p>obtained from the adaptive binning procedure to visualize the calibration curve of the model-this visualization is known as a calibration or reliability plot. It provides finer grained insight into the calibra- tion behavior in different prediction ranges. A perfectly calibrated curve would coincide with the y = x diagonal line. When the curve lies above the diagonal, the model is underconfident (q &lt; p q ); and when it is below the diagonal, the model is overconfident (q &gt; p q ).</p><p>An advantage of plotting a curve estimated from fixed-size bins, instead of fixed-width bins, is that the distribution of the points hints at the refinement aspect of the model's performance. If the points' positions tend to cluster in the bottom-left and top- right corners, that implies the model is making more refined predictions.</p><p>iments, we set the target bin size in Algorithm 1 to be 5,000 and the number of samples in Algo- rithm 2 to be 10,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Naive Bayes and logistic regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Introduction</head><p>Previous work on Naive Bayes has found its prob- abilities to have calibration issues, in part due to its incorrect conditional independence assump- tions ( <ref type="bibr" target="#b30">Niculescu-Mizil and Caruana, 2005;</ref><ref type="bibr" target="#b1">Bennett, 2000;</ref><ref type="bibr" target="#b9">Domingos and Pazzani, 1997</ref>). Since logistic regression has the same log-linear repre- sentational capacity <ref type="bibr" target="#b29">(Ng and Jordan, 2002</ref>) but does not suffer from the independence assump- tions, we select it for comparison, hypothesizing it may have better calibration.</p><p>We analyze a binary classification task of Twit- ter sentiment analysis from emoticons. We col- lect a dataset consisting of tweets identified by the Twitter API as English, collected from 2014 to 2015, with the "emoticon trick" <ref type="bibr" target="#b36">(Read, 2005;</ref><ref type="bibr" target="#b24">Lin and Kolcz, 2012</ref>) to label tweets that contain at least one occurrence of the smiley emoticon ":)" as "happy" (y = 1) and others as y = 0. The smiley emoticons are deleted in positive examples. We sampled three sets of tweets (subsampled from the Decahose/Gardenhose stream of public tweets) with Jan-Apr 2014 for training, May-Dec 2014 for development, and Jan-Apr 2015 for testing. Each set contains 10 5 tweets, split between an equal number of positive and negative instances. We use binary features based on unigrams extracted from the twokenize.py 8 tokenization. We use the scikit-learn maximize the F-1 score on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>Naive Bayes attains a slightly higher F-1 score (NB 73.8% vs. LR 72.9%), but logistic regression has much lower calibration error: less than half as much RMSE (NB 0.105 vs. LR 0.041; <ref type="figure" target="#fig_1">Figure  2</ref>). Both models have a tendency to be undercon- fident in the lower prediction range and overconfi- dent in the higher range, but the tendency is more pronounced for Naive Bayes. To prepare a POS tagging dataset, we ex- tract Wall Street Journal articles from the En- glish CoNLL-2011 coreference shared task dataset from Ontonotes (Pradhan et al., 2011), using the CoNLL-2011 splits for training, development and testing. This results in 11,772 sentences for train- ing, 1,632 for development, and 1,382 for testing, over a set of 47 possible tags.</p><p>We train an HMM with Dirichlet MAP us- ing one pseudocount for every transition and word emission. For the CRF, we use the L 2 - regularized L-BFGS algorithm implemented in CRFsuite <ref type="bibr" target="#b32">(Okazaki, 2007)</ref>. We compare an HMM to a CRF that only uses basic transition (tag-tag) and emission (tag-word) features, so that it does not have an advantage due to more features. In order to compare models with similar task perfor- mance, we train the CRF with only 3000 sentences from the training set, which yields the same accu- racy as the HMM (about 88.7% on the test set). In each case, the model's hyperparameters (the CRF's L 2 regularizer, the HMM's pseudocount) are selected by maximizing accuracy on the devel- opment set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Predicting single-word tags</head><p>In this experiment, we measure miscalibration of the two models on predicting tags of single words. First, for each tag type, we produce a set of 33,306 prediction-label pairs (for every token); we then concatenate them across the tags for calibration analysis. <ref type="figure" target="#fig_3">Figure 3</ref> shows that the two models exhibit distinct calibration patterns. The HMM tends to be very underconfident whereas the CRF is overconfident, and the CRF has a lower (better) overall calibration error.</p><p>We also examine the calibration errors of the individual POS tags <ref type="figure" target="#fig_4">(Figure 4(a)</ref>). We find that CRF is significantly better calibrated than HMM in most but not all categories (39 out of 47). For example, they are about equally calibrated on pre- dicting the NN tag. The calibration gap between the two models also differs among the tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Predicting two-consecutive-word tags</head><p>There is no reason to restrict ourselves to model predictions of single words; these models define marginal distributions over larger textual units. Next we examine the calibration of posterior pre- dictions of tag pairs on two consecutive words in the test set. The same analysis may be impor- tant for, say, phrase extraction or other chunk- ing/parsing tasks. We report results for the top 5 and 100 most fre- quent tag pairs <ref type="figure" target="#fig_4">(Figure 4(b)</ref>). We observe a simi- lar pattern as seen from the experiment on single tags: the CRF is generally better calibrated than the HMM, but the HMM does achieve better cali- bration errors in 29 out of 100 categories.</p><p>These tagging experiments illustrate that, de- pending on the application, different models can exhibit different levels of calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Coreference resolution</head><p>We examine a third model, a probabilistic model for within-document noun phrase coreference, which has an efficient sampling-based inference procedure. In this section we introduce it and ana- lyze its calibration, in preparation for the next sec- tion where we use it for exploratory data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Antecedent selection model</head><p>We use the Berkeley coreference resolution sys- tem <ref type="bibr" target="#b10">(Durrett and Klein, 2013)</ref>, which was origi- nally presented as a CRF; we give it an equivalent a series of independent logistic regressions (see appendix for details). The primary component of this model is a locally-normalized log-linear dis- tribution over clusterings of noun phrases, each cluster denoting an entity. The model takes a fixed input of N mentions (noun phrases), indexed by i in their positional order in the document. It posits that every mention i has a latent antecedent selec- tion decision, a i ∈ {1, . . . , i − 1, NEW}, denoting which previous mention it attaches to, or NEW if it is starting a new entity that has not yet been seen at a previous position in the text. Such a mention- mention attachment indicates coreference, while the final entity clustering includes more links im- plied through transitivity. The model's generative process is:</p><p>Definition 2 (Antencedent coreference model and sampling algorithm).</p><p>• For i = 1..N , sample</p><formula xml:id="formula_12">a i ∼ 1 Z i exp(w T f (i, a i , x))</formula><p>• Calculate the entity clusters as e := CC(a), the connected components of the antecedent graph having edges (i, a i ) for i where a i = NEW.</p><p>Here x denotes all information in the document that is conditioned on for log-linear features f . e = {e 1 , ...e M } denotes the entity clusters, where each element is a set of mentions. There are M en- tity clusters corresponding to the number of con- nected components in a. The model defines a joint distribution over antecedent decisions P (a|x) = i P (a i |x); it also defines a joint distribution over entity clusterings P (e|x), where the probability of an e is the sum of the probabilities of all a vectors that could give rise to it. In a manner similar to a distance-dependent Chinese restaurant process ( <ref type="bibr" target="#b2">Blei and Frazier, 2011)</ref>, it is non-parametric in the sense that the number of clusters M is not fixed in advance. This antecedent-based model admits a very straightforward procedure to draw independent e samples, by stepping through Def. 2: indepen- dently sample each a i then calculate the connected components of the resulting antecedent graph. By construction, this procedure samples from the joint distribution of e (even though we never com- pute the probability of any single clustering e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sampling-based inference</head><p>Unlike approximate sampling approaches, such as Markov chain Monte Carlo methods used in other coreference work to sample e ( <ref type="bibr" target="#b19">Haghighi and Klein, 2007)</ref>, here there are no questions about burn-in or autocorrelation ( <ref type="bibr">Kass et al., 1998</ref>). Every sample is independent and very fast to  compute-only slightly slower than calculating the MAP assignment (due to the exp and normal- ization for each a i ). We implement this algorithm by modifying the publicly available implementa- tion from Durrett and Klein. <ref type="bibr">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Calibration analysis</head><p>We consider the following inference query: for a randomly chosen pair of mentions, are they coref- erent? Even if the model's accuracy is compara- tively low, it may be the case that it is correctly calibrated-if it thinks there should be great vari- ability in entity clusterings, it may be uncertain whether a pair of mentions should belong together.</p><p>Let ij be 1 if the mentions i and j are predicted to be coreferent, and 0 otherwise. Annotated data defines a gold-standard (g) ij value for every pair i, j. Any probability distribution over e defines a marginal Bernoulli distribution for every proposi- tion ij , marginalizing out e:</p><formula xml:id="formula_13">P ( ij = 1 | x) = e 1{(i, j) ∈ e}P (e | x) (2)</formula><p>where (i, j) ∈ e is true iff there is an entity in e that contains both i and j.</p><p>In a traditional coreference evaluation of the best-prediction entity clustering, the model as- signs 1 or 0 to every ij and the pairwise precision and recall can be computed by comparing them to the corresponding</p><formula xml:id="formula_14">(g) ij .</formula><p>Here, we instead compare the q ij ≡ P ( ij = 1 | x, e) prediction strengths against (g) ij empirical frequencies to assess pair- wise calibration, with the same binary calibration analysis tools developed in §3 by aggregating pairs with similar q ij values. Each q ij is computed by averaging over 1,000 samples, simply taking the fraction of samples where the pair (i, j) is coref- erent. f (c, e; x d ) assesses whether an entity e is affiliated with country c and is described as the agent of an attack, based on document text and parses x d ; f returns true iff both: <ref type="bibr">10</ref> • There exists a mention i ∈ e described as country c: either its head word is in w(c) (e.g. "Americans"), or its head word has an nmod or amod modifier in w(c) (e.g. "American forces", "president of the U.S."); and there is only one unique country c among the mentions in the entity.</p><p>• There exists a mention j ∈ e which is the nsubj or agent argument to the verb "attack" (e.g. "they attacked", "the forces attacked", "attacked by them").</p><p>For a given c, we first calculate a binary variable for whether there is at least one entity fulfilling f in a particular document,</p><formula xml:id="formula_15">a(d, c, e d ) = e∈e d f (c, e; x d ) (3)</formula><p>and second, the number of such documents in d(t), the set of New York Times articles published in a given time period t,</p><formula xml:id="formula_16">n(t, c, e d(t) ) = d∈d(t) a(d, c, e d ) (4)</formula><p>These quantities are both random variables, since they depend on e; thus we are interested in the posterior distribution of n, marginalizing out e,</p><formula xml:id="formula_17">P (n(t, c, e d(t) ) | x d(t) )<label>(5)</label></formula><p>If our coreference model was highly certain (only one structure, or a small number of similar struc- tures, had most of the probability mass in the space of all possible structures), each document would have an a posterior near either 0 or 1, and their sum in Eq. 5 would have a narrow distribution. But if the model is uncertain, the distribution will be wider. Because of the transitive closure, the prob- ability of a is potentially more complex than the single antecedent linking probability between two mentions-the affiliation and attack information can propagate through a long coreference chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We tag and parse a 193,403 article subset of the Annotated New York Times LDC corpus <ref type="bibr" target="#b37">(Sandhaus, 2008)</ref>, which includes articles about world <ref type="bibr">10</ref> Syntactic relations are Universal Dependencies (de <ref type="bibr" target="#b7">Marneffe et al., 2014</ref>); more details for the extrac- tion rules are in the appendix. news from the years 1987 to <ref type="bibr">2007 (details in appendix)</ref>. For each article, we run the coreference system to predict 100 samples, and evaluate f on every entity in every sample. <ref type="bibr">11</ref> The quantity of interest is the number of articles mentioning at- tacks in a 3-month period (quarter), for a given country. <ref type="figure" target="#fig_8">Figure 6</ref> illustrates the mean and 95% posterior credible intervals for each quarter. The posterior mean m is calculated as the mean of the samples, and the interval is the normal approxima- tion m ± 1.96 s, where s is the standard deviation among samples for that country and time period.</p><p>Uncertainty information helps us understand whether a difference between data points is real. In the plots of <ref type="figure" target="#fig_8">Figure 6</ref>, if we had used a 1-best coreference resolution, only a single line would be shown, with no assessment of uncertainty. This is problematic in cases when the model genuinely does not know the correct answer. For example, the 1993-1996 period of the USA plot ( <ref type="figure" target="#fig_8">Figure 6</ref>, top) shows the posterior mean fluctuating from 1 to 5 documents; but when credible intervals are taken into consideration, we see that model does not know whether the differences are real, or were caused by coreference noise.</p><p>A similar case is highlighted at the bottom plot of <ref type="figure" target="#fig_8">Figure 6</ref>. Here we compare the event counts for Yugoslavia and NATO, which were engaged in a conflict in 1999. Did the New York Times de- vote more attention to the attacks by one particu- lar side? To a 1-best system, the answer would be yes. But the posterior intervals for the two coun- tries' event counts in mid-1999 heavily overlap, indicating that the coreference system introduces too much uncertainty to obtain a conclusive an- swer for this question. Note that calibration of the coreference model is important for the credible in- tervals to be useful; for example, if the model was badly calibrated by being overconfident (too much probability over a small set of similar structures), these intervals would be too narrow, leading to in- correct interpretations of the event dynamics.</p><p>Visualizing this uncertainty gives richer infor- mation for a potential user of an NLP-based sys- tem, compared to simply drawing a line based on a single 1-best prediction. It preserves the gen- uine uncertainty due to ambiguities the system was unable to resolve. This highlights an alternative use of <ref type="bibr" target="#b12">Finkel et al. (2006)</ref>'s approach of sampling multiple NLP pipeline components, which in that work was used to perform joint inference. Instead of focusing on improving an NLP pipeline, we can pass uncertainty on to exploratory purposes, and try to highlight to a user where the NLP system may be wrong, or where it can only imprecisely specify a quantity of interest.</p><p>Finally, calibration can help error analysis. For a calibrated model, the more uncertain a predic- tion is, the more likely it is to be erroneous. While coreference errors comprise only one part of event extraction errors (alongside issues in parse qual- ity, factivity, semantic roles, etc.), we can look at highly uncertain event predictions to understand the nature of coreference errors relative to our task. We manually analyzed documents with a 50% probability to contain an "attack"ing country- affiliated entity, and found difficult coreference cases.</p><p>In one article from late 1990, an "attack" event for IRQ is extracted from the sentence "But some political leaders said that they feared that Mr. Hus- sein might attack Saudi Arabia". The mention "Mr. Hussein" is classified as IRQ only when it is coreferent with a previous mention "President Saddam Hussein of Iraq"; this occurs only 50% of the time, since in some posterior samples the coreference system split apart these two "Hussein" mentions. This particular document is addition- ally difficult, since it includes the names of more than 10 countries (e.g. United States, Saudi Ara- bia, Egypt), and some of the Hussein mentions are even clustered with presidents of other countries (such as "President Bush"), presumably because they share the "president" title. These types of er- rors are a major issue for a political analysis task; further analysis could assess their prevalence and how to address them in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we argue that the calibration of pos- terior predictions is a desirable property of prob- abilistic NLP models, and that it can be directly evaluated. We also demonstrate a use case of having calibrated uncertainty: its propagation into downstream exploratory analysis.</p><p>Our posterior simulation approach for ex- ploratory and error analysis relates to posterior predictive checking <ref type="bibr" target="#b13">(Gelman et al., 2013)</ref>, which analyzes a posterior to test model assumptions; <ref type="bibr" target="#b26">Mimno and Blei (2011)</ref>  Serbia/Yugo. NATO density estimation. Another important question is: what types of in- ferences are facilitated by correct calibration? In- tuitively, we think that overconfidence will lead to overly narrow confidence intervals; but in what sense are confidence intervals "good" when cal- ibration is perfect? Also, does calibration help joint inference in NLP pipelines? It may also assist calculations that rely on expectations, such as in- ference methods like minimum Bayes risk decod- ing, or learning methods like EM, since calibrated predictions imply that calculated expectations are statistically unbiased (though the implications of this fact may be subtle). Finally, it may be in- teresting to pursue recalibration methods, which readjust a non-calibrated model's predictions to be calibrated; recalibration methods have been de- veloped for binary <ref type="bibr" target="#b34">(Platt, 1999;</ref><ref type="bibr" target="#b30">Niculescu-Mizil and Caruana, 2005</ref>) and multiclass ( <ref type="bibr" target="#b48">Zadrozny and Elkan, 2002</ref>) classification settings, but we are unaware of methods appropriate for the highly structured outputs typical in linguistic analysis. Another approach might be to directly constrain CalibErr = 0 during training, or try to reduce it as a training-time risk minimization or cost objec- tive ( <ref type="bibr" target="#b42">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b15">Gimpel and Smith, 2010;</ref><ref type="bibr" target="#b43">Stoyanov et al., 2011;</ref><ref type="bibr" target="#b6">Brümmer and Doddington, 2013)</ref>.</p><p>Calibration is an interesting and important prop- erty of NLP models. Further work is necessary to address these and many other questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) A skewed distribution of predictions on whether a word has the NN tag ( §4.2.2). Calibration curves produced by equally-spaced binning with bin width equal to 0.02 (b) and 0.1 (c) can have wide confidence intervals. Adaptive binning (with 1000 points in each bin) (d) gives small confidence intervals and also captures the prediction distribution. The confidence intervals are estimated as described in §3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Calibration curve of (a) Naive Bayes and (b) logistic regression on predicting whether a tweet is a "happy" tweet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hidden Markov models (HMM) and linear chain conditional random fields (CRF) are another com- monly used pair of analogous generative and dis- criminative models. They both define a posterior over tag sequences P (y|x), which we apply to part-of-speech tagging. We can analyze these models in the binary cal- ibration framework ( §2-3) by looking at marginal distribution of binary-valued outcomes of parts of the predicted structures. Specifically, we examine calibration of predicted probabilities of individual tokens' tags ( §4.2.2), and of pairs of consecutive tags ( §4.2.3). These quantities are calculated with the forward-backward algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Calibration curves of (a) HMM, and (b) CRF, on predictions over all POS tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Calibration errors of HMM and CRF on predicting (a) single-word tags and (b) two-consecutive-word tags. Lower errors are better. The last two columns in each graph are the average calibration errors over the most common labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For both calibration analysis and exploratory ap- plications, we need to analyze the posterior distri- bution over entity clusterings. This distribution is a complex mathematical object; an attractive ap- proach to analyze it is to draw samples from this distribution, then analyze the samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Coreference calibration curve for predicting whether two mentions belong to the same entity cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Number of documents with an "attack"ing country per 3-month period, and coreference posterior uncertainty for that quantity. The dark line is the posterior mean, and the shaded region is the 95% posterior credible interval. More examples in appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>apply it to a topic model. One avenue of future work is to investigate more effective nonparametric regression methods to better estimate and visualize calibration error, such as Gaussian processes or bootstrapped kernel</figDesc><table>1990 
1995 
2000 
2005 

0 
10 20 30 

USA 

0 
10 20 30 

1995 
1996 
1997 
1998 
1999 
2000 

0 
5 10 15 

</table></figure>

			<note place="foot" n="1"> See the extended version of this paper for software, appendix, and acknowledgments sections: http://brenocon.com/nlpcalib/ http://arxiv.org/abs/1508.05154</note>

			<note place="foot" n="4"> These two loss functions are instances of proper scoring rules (Gneiting and Raftery, 2007; Bröcker, 2009). 5 We alternatively refer to this as label frequency or empirical frequency. The P probabilities can be thought of as frequencies from the hypothetical population the data and predictions are drawn from. P probabilities are, definitionally speaking, completely separate from a probabilistic model that might be used to generate q predictions. 6 They all include a notion of calibration corresponding to a Bregman divergence (Bröcker, 2009); for example, crossentropy can be broken down such that KL divergence is the measure of miscalibration.</note>

			<note place="foot" n="4"> Calibration for classification and tagging models Using the method described in §3, we assess the quality of posterior predictions of several classification and tagging models. In all of our exper7 A major unsolved issue is how to fairly select the bin size. If it is too large, the curve is oversmoothed and calibration looks better than it should be; if it is too small, calibration looks worse than it should be. Bandwidth selection and cross-validation techniques may better address this problem in future work. In the meantime, visualizations of calibration curves help inform the reader of the resolution of a particular analysis-if the bins are far apart, the data is sparse, and the specific details of the curve are not known in those regions.</note>

			<note place="foot" n="8"> https://github.com/myleott/ ark-twokenize-py</note>

			<note place="foot" n="9"> Berkeley Coreference Resolution System, version 1.1: http://nlp.cs.berkeley.edu/projects/ coref.shtml</note>

			<note place="foot" n="11"> We obtained similar results using only 10 samples. We also obtained similar results with a different query function, the total number of entities, across documents, that fulfill f .</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Uncertainty in Entity-based</head><p>Exploratory Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Entity-syntactic event aggregation</head><p>We demonstrate one important use of calibration analysis: to ensure the usefulness of propagating uncertainty from coreference resolution into a sys- tem for exploring unannotated text. Accuracy can- not be calculated since there are no labels; but if the system is calibrated, we postulate that un- certainty information can help users understand the underlying reliability of aggregated extractions and isolate predictions that are more likely to con- tain errors.</p><p>We illustrate with an event analysis application to count the number of "country attack events": for a particular country of the world, how many news articles describe an entity affiliated with that country as the agent of an attack, and how does this number change over time? This is a simpli- fied version of a problem where such systems have been built and used for political science analysis ( <ref type="bibr" target="#b39">Schrodt et al., 1994;</ref><ref type="bibr" target="#b38">Schrodt, 2012;</ref><ref type="bibr" target="#b23">Leetaru and Schrodt, 2013;</ref><ref type="bibr" target="#b3">Boschee et al., 2013;</ref>. A coreference component can im- prove extraction coverage in cases such as "Rus- sian troops were sighted . . . and they attacked . . . "</p><p>We use the coreference system examined in §5 for this analysis. To propagate coreference un- certainty, we re-run event extraction on multiple coreference samples generated from the algorithm described in §5.2, inducing a posterior distribution over the event counts. To isolate the effects of coreference, we use a very simple syntactic depen- dency system to identify affiliations and events. Assume the availability of dependency parses for a document d, a coreference resolution e, and a lexicon of country names, which contains a small set of words w(c) for each country c; for example, w(FRA) = {france, french}. The binary function</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning latent personas of film characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Assessing the calibration of naive Bayes&apos; posterior estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distance dependent Chinese restaurant processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2488" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic extraction of events from open source text for predictive forecasting. Handbook of Computational Approaches to Counterterrorism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reliability, sufficiency, and the decomposition of proper scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Bröcker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">643</biblScope>
			<biblScope unit="page" from="1512" to="1519" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Likelihood-ratio calibration using priorweighted proper scoring rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7981</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal Stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The comparison and evaluation of forecasters. The statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fienberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the optimality of the simple Bayesian classifier under zero-one loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aki</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich sourceside context for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Softmaxmargin CRFs: Training log-linear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A systematic exploration of diversity in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1111" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1100" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strictly proper scoring rules, prediction, and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilmann</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">477</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="doi">doi:10.3115/981863.981887</idno>
		<ptr target="http://www.aclweb.org/anthology/P96-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 34th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Santa Cruz, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-06" />
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
	<note>Parsing algorithms and metrics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised coreference resolution in a nonparametric Bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">848</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monte Carlo in practice: a roundtable discussion</title>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="100" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimum Bayes-risk decoding for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004: Main Proceedings</title>
		<editor>Daniel Marcu Susan Dumais and Salim Roukos</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05-07" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GDELT: Global data on events, location, and tone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalev</forename><surname>Leetaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Schrodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISA Annual Convention</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale machine learning at Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alek</forename><surname>Kolcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="793" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep parsing in Watson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">William</forename><surname>Mccord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branimir</forename><forename type="middle">K</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boguraev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3.4</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D11-1021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
	<note>Bayesian checking for topic models</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating dependency representations for event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<idno>Au- gust 2010. Coling</idno>
		<ptr target="http://www.aclweb.org/anthology/C10-1088" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A general framework for forecast verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">H</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1330" to="1338" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting good probabilities with supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Mizil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to extract international relations from political context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Crfsuite: a fast implementation of conditional random fields (CRFs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<ptr target="http://www.chokkan.org/software/crfsuite/" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/pubs/69187/svmprob.ps.gz" />
	</analytic>
	<monogr>
		<title level="m">Advances in large margin classifiers</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CoNLL-2011 shared task: Modeling unrestricted coreference in Ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using emoticons to reduce dependency in machine learning techniques for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Student Research Workshop</title>
		<meeting>the ACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times Annotated Corpus. Linguistic Data Consortium, LDC2008T19</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Precedents, progress, and prospects in political event data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Schrodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Interactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="546" to="569" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">KEDS-a program for the machine coding of event data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Schrodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><forename type="middle">G</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">L</forename><surname>Weddle</surname></persName>
		</author>
		<idno type="doi">doi:10.1177/089443939401200408</idno>
		<ptr target="http://ssc.sagepub.com/" />
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="561" to="587" />
			<date type="published" when="1994-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Joint inference of entities, relations, and coreference</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 2013 Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P06-2101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ropson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="725" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A global joint model for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Curves as parameters, and touch estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<ptr target="http://projecteuclid.org/euclid.bsmsp/1200512189" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fourth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1961" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="681" to="694" />
		</imprint>
	</monogr>
	<note>Contributions to the Theory of Statistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wider pipelines: Nbest alignments and parses in MT training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">All of nonparametric statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transforming classifier scores into accurate multiclass probability estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
