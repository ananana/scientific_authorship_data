<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Creating Causal Embeddings for Question Answering with Minimal Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Sharp</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hammond</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Creating Causal Embeddings for Question Answering with Minimal Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="138" to="148"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A common model for question answering (QA) is that a good answer is one that is closely related to the question, where re-latedness is often determined using general-purpose lexical models such as word embed-dings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by boot-strapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embed-dings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA), i.e., finding short answers to natural language questions, is one of the most im- portant but challenging tasks on the road towards natural language understanding <ref type="bibr" target="#b8">(Etzioni, 2011)</ref>. A common approach for QA is to prefer answers that are closely related to the question, where relatedness is often determined using lexical semantic models such as word embeddings <ref type="bibr" target="#b37">(Yih et al., 2013;</ref><ref type="bibr" target="#b18">Jansen et al., 2014;</ref><ref type="bibr" target="#b11">Fried et al., 2015)</ref>. While appealing for its robustness to natural language variation, this one- size-fits-all approach does not take into account the wide range of distinct question types that can appear in any given question set, and that are best addressed individually ( <ref type="bibr" target="#b4">Chu-Carroll et al., 2004;</ref><ref type="bibr" target="#b9">Ferrucci et al., 2010;</ref>.</p><p>Given the variety of question types, we suggest that a better approach is to look for answers that are related to the question through the appropriate re- lation, e.g., a causal question should have a cause- effect relation with its answer. If we adopt this view, and continue to work with embeddings as a mechanism for assessing relationship, this raises a key question: how do we train and use task-specific embeddings cost-effectively? Using causality as a use case, we answer this question with a framework for producing causal word embeddings with mini- mal supervision, and a demonstration that such task- specific embeddings significantly benefit causal QA.</p><p>In particular, the contributions of this work are:</p><p>(1) A methodology for generating causal embed- dings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns, e.g., X causes Y. We then train dedi- cated embedding (as well as two other distributional similarity) models over this data. <ref type="bibr" target="#b21">Levy and Goldberg (2014)</ref> have modified the algorithm of <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref> to use an arbitrary, rather than linear, context. Here we make this context task-specific, i.e., the context of a cause is its effect. Further, to mitigate sparsity and noise, our models are bidirec- tional, and noise aware (by incorporating the likeli- hood of noise in the training process).</p><p>(2) The insight that QA benefits from task-specific embeddings. We implement a QA system that uses the above causal embeddings to answer questions and demonstrate that they significantly improve per- formance over a strong baseline. Further, we show that causal embeddings encode complementary in- formation to vanilla embeddings, even when trained from the same knowledge resources.</p><p>(3) An analysis of direct vs. indirect evaluations for task-specific word embeddings. We evaluate our causal models both directly, in terms of measuring their capacity to rank causally-related word pairs over word pairs of other relations, as well as indi- rectly in the downstream causal QA task. In both tasks, our analysis indicates that including causal models significantly improves performance. How- ever, from the direct evaluation, it is difficult to estimate which models will perform best in real- world tasks. Our analysis re-enforces recent obser- vations about the limitations of word similarity eval- uations ( <ref type="bibr" target="#b9">Faruqui et al., 2016)</ref>: we show that they have limited coverage and may align poorly with real-world tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Addressing the need for specialized solving meth- ods in QA, Oh et. al (2013) incorporate a dedicated causal component into their system, and note that it improves the overall performance. However, their model is limited by the need for lexical overlap be- tween a causal construction found in their knowl- edge base and the question itself. Here, we develop a causal QA component that exploits specialized word embeddings to gain robustness to lexical variation. There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering -see inter alia ( <ref type="bibr" target="#b11">Fried et al., 2015;</ref><ref type="bibr" target="#b37">Yih et al., 2013)</ref>. However, <ref type="bibr" target="#b21">Levy and Goldberg (2015)</ref> note that there are lim- itations on the type of semantic knowledge which is encoded in these general-purpose similarity embeddings. Therefore, here we build customized task-specific embeddings for causal QA.</p><p>Customized embeddings have been created for a variety of tasks, including semantic role la- beling ( <ref type="bibr" target="#b10">FitzGerald et al., 2015;</ref><ref type="bibr" target="#b33">Woodsend and Lapata, 2015)</ref>, and binary relation extraction ( <ref type="bibr" target="#b27">Riedel et al., 2013</ref>). Similar to Riedel et al., we train embeddings customized for specific relations, but we bootstrap training data using minimal super- vision (i.e., a small set of patterns) rather than rely- ing on distant supervision and large existing knowl- edge bases. Additionally, while Riedel et al. repre- sent all relations in a general embedding space, here we train a dedicated embedding space for just the causal relations.</p><p>In QA, embeddings have been customized to have question words that are close to either their answer words ( <ref type="bibr" target="#b1">Bordes et al., 2014)</ref>, or to structured knowl- edge base entries ( . While these methods are useful for QA, they do not distinguish between different types of questions, and as such their embeddings are not specific to a given question type.</p><p>Additionally, embeddings have been customized to distinguish functional similarity from relatedness ( <ref type="bibr" target="#b21">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b20">Kiela et al., 2015</ref>). In particular, Levy and Goldberg train their embed- dings by replacing the standard linear context of the target word with context derived from the syntac- tic dependency graph of the sentence. In this work, we make use of this extension to arbitrary context in order to train our embeddings with contexts derived from binary causal relations. We extract cause-effect text pairs such that the cause text becomes the target text and the effect text serves as the context.</p><p>Recently, <ref type="bibr" target="#b9">Faruqui et al.(2016)</ref> discussed issues surrounding the evaluation of similarity word em- beddings, including the lack of correlation be- tween their performance on word-similarity tasks and "downstream" or real-world tasks like QA, text classification, etc. As they advocate, in addition to a direct evaluation of our causal embeddings, we also evaluate them independently in a downstream QA task. We provide the same comparison for two alter- native approaches (an alignment model and a con- volutional neural network model), confirming that the direct evaluation performance can be misleading without the task-specific, downstream evaluation.</p><p>With respect to extracting causal relations from text, <ref type="bibr" target="#b12">Girju et al. (2002)</ref> use modified Hearst pat- terns <ref type="bibr" target="#b13">(Hearst, 1992)</ref> to extract a large number of potential cause-effect tuples, where both causes and effects must be nouns. However, <ref type="bibr" target="#b6">Cole et al. (2005)</ref> show that these nominal-based causal relations ac- count for a relatively small percentage of all causal relations, and for this reason, <ref type="bibr" target="#b34">(Yang and Mao, 2014</ref>) allow for more elaborate argument structures in their causal extraction by identifying verbs, and then fol- lowing the syntactic subtree of the verbal arguments to construct their candidate causes and effects. Ad- ditionally, <ref type="bibr" target="#b7">Do et al. (2011)</ref> observe that nouns as well as verbs can signal causality. We follow these intuitions in developing our causal patterns by using both nouns and verbs to signal potential participants in causal relations, and then allowing for the entire dominated structures to serve as the cause and/or ef- fect arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our focus is on reranking answers to causal ques- tions using using task-specific distributional similar- ity methods. Our approach operates in three steps:</p><p>(1) We start by bootstrapping a large number of cause-effect pairs from free text using a small num- ber of syntactic and surface patterns (Section 4).</p><p>(2) We then use these bootstrapped pairs to build several task-specific embedding (and other distribu- tional similarity) models (Section 5). We evaluate these models directly on a causal-relation identifica- tion task (Section 6).</p><p>(3) Finally, we incorporate these models into a reranking framework for causal QA and demonstrate that the resulting approach performs better than the reranker without these task-specific models, even if trained on the same data (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extracting Cause-Effect Tuples</head><p>Because the success of embedding models depends on large training datasets ( <ref type="bibr" target="#b30">Sharp et al., 2015)</ref>, and such datasets do not exist for open-domain causality, we opted to bootstrap a large number of cause-effect pairs from a small set of patterns. We wrote these patterns using Odin <ref type="bibr">(Valenzuela-Escárcega et al., 2016</ref>), a rule-based information extraction frame- work which has the distinct advantage of being able to operate over multiple representations of content (i.e., surface and syntax). For this work, we make use of rules that operate over both surface sequences as well as dependency syntax in the grammars intro- duced in steps <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref> below.</p><p>Odin operates as a cascade, allowing us to imple- ment a two-stage approach. First, we identify poten- tial participants in causal relations, i.e., the poten- tial causes and effects, which we term causal men- tions (CM). A second grammar then identifies ac- tual causal relations that take these CMs as argu- ments.</p><p>We consider both noun phrases (NP) as well as entire clauses to be potential CMs, since causal pat- terns form around participants that are syntactically more complex than flat NPs. For example, in the sentence The collapse of the housing bubble caused stock prices to fall, both the cause (the collapse of the housing bubble) and effect (stock prices to fall) are more complicated nested structures. Reducing these arguments to non-recursive NPs (e.g., The col- lapse and stock prices) is clearly insufficient to cap- ture the relevant context.</p><p>Formally, we extract our causal relations using the following algorithm:</p><p>(1) Pre-processing: Much of the text we use to ex- tract causal relation tuples comes from the Anno- tated Gigaword ( <ref type="bibr" target="#b24">Napoles et al., 2012)</ref>. This text is already fully annotated and no further process- ing is necessary. We additionally use text from the Simple English Wikipedia 1 , which we processed us- ing the Stanford CoreNLP toolkit ( ) and the dependency parser of Chen and Man- ning (2014).</p><p>(2) CM identification: We extract causal mentions (which are able to serve as arguments in our causal patterns) using a set of rules designed to be robust to the variety that exists in natural language. Namely, to find CMs that are noun phrases, we first find words that are tagged as nouns, then follow outgoing dependency links for modifiers and attached prepo- <ref type="table">Corpus   Extracted Tuples  Annotated Gigaword  798,808  Simple English Wikipedia  16,425  Total  815,233  Table 1</ref>: Number of causal tuples extracted from each corpus. sitional phrases 2 , to a maximum depth of two links. To find CMs that are clauses, we first find words that are tagged as verbs (excluding verbs which them- selves were considered to signal causation 3 ), then again follow outgoing dependency links for modi- fiers and arguments. We used a total of four rules to label CMs.  <ref type="bibr" target="#b19">Khoo et al., 1998</ref>). To minimize the noise in the extracted pairs, we restrict ourselves to a set of 13 rules de- signed to find unambiguously causal patterns, such as CAUSE led to EFFECT, where CAUSE and EF- FECT are CMs. The rules operate by looking for a trigger phrase, e.g., led, and then following the de- pendency paths to and/or from the trigger phrase to see if all required CM arguments exist.</p><p>Applying this causal grammar over Gigaword and Simple English Wikipedia produced 815,233 causal tuples, as summarized in <ref type="table">Table 1</ref>. As bootstrapping methods are typically noisy, we manually evaluated the quality of approximately 250 of these pairs se- lected at random. Of the tuples evaluated, approxi- mately 44% contained some amount of noise. For example, from the sentence Except for Springer's show, which still relies heavily on confrontational topics that lead to fistfights virtually every day..., while ideally we would only extract (confrontational topics → fistfights), instead we extract the tuple (show which still relies heavily on confrontational topics → fistfights virtually every day), which con- tains a large amount of noise: show, relies, heavily, etc. This finding prompted our noise-aware model described at the end of Section 5. <ref type="bibr">2</ref> The outgoing dependency links from the nouns which we followed were: nn, amod, advmod, ccmod, dobj, prep of, prep with, prep for, prep into, prep on, prep to, and prep in. <ref type="bibr">3</ref> The verbs we excluded were: cause, result, lead, create.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Models</head><p>We use the extracted causal tuples to train three dis- tinct distributional similarity models that explicitly capture causality. Causal Embedding Model (cEmbed): The first distributional similarity model we use is based on the skip-gram word-embedding algorithm of <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref>, which has been shown to im- prove a variety of language processing tasks includ- ing QA ( <ref type="bibr" target="#b37">Yih et al., 2013;</ref><ref type="bibr" target="#b11">Fried et al., 2015)</ref>. In par- ticular, we use the variant implemented by <ref type="bibr" target="#b21">Levy and Goldberg (2014)</ref> which modifies the original algo- rithm to use an arbitrary, rather than linear, context. Our novel contribution is to make this context task- specific: intuitively, the context of a cause is its ef- fect. Further, these contexts are generated from tu- ples that are themselves bootstrapped, which mini- mizes the amount of supervision necessary.</p><p>The Levy and Goldberg model trains using single- word pairs, while our CMs could be composed of multiple words. For this reason, we decompose each cause-effect tuple, (CM c , CM e ), such that each word w c ∈ CM c is paired with each word w e ∈ CM e .</p><p>After filtering the extracted cause-effect tuples for stop words and retaining only nouns, verbs, and ad- jectives, we generated over 3.6M (w c , w e ) word- pairs <ref type="bibr">4</ref> from the approximately 800K causal tuples.</p><p>The model learns two embedding vectors for each word, one for when the word serves as a target word and another for when the word serves as a context word. Here, since the relation of interest is inher- ently directional, both sets of embeddings are mean- ingful, and so we make use of both -the target vec- tors encode the effects of given causes, whereas the context vectors capture the causes of the correspond- ing effects. Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA ( <ref type="bibr" target="#b0">Berger et al., 2000;</ref><ref type="bibr">Echihabi and Marcu, 2003;</ref><ref type="bibr" target="#b31">Soricut and Brill, 2006;</ref><ref type="bibr" target="#b28">Riezler et al., 2007;</ref><ref type="bibr" target="#b31">Surdeanu et al., 2011;</ref>, and recent work has shown that they can be successfully trained with less data than embedding models ( <ref type="bibr" target="#b30">Sharp et al., 2015</ref>). To verify these observations in our context, we train an alignment model that "translates" causes (i.e., the "source language") into effects (i.e., the "destination language"), using our cause-effect tu- ples. This is done using IBM Model 1 ( <ref type="bibr" target="#b1">Brown et al., 1993)</ref> and GIZA++ <ref type="bibr" target="#b25">(Och and Ney, 2003)</ref>.</p><p>Causal Convolutional Neural Network Model (cCNN): Each of the previous models have at their root a bag-of-words representation, which is a sim- plification of the causality task. To address this po- tential limitation, we additionally trained a convo- lutional neural network (CNN) which operates over variable-length texts, and maintains distinct embed- dings for causes and effects. The architecture of this approach is shown in <ref type="figure" target="#fig_1">Figure 1</ref>, and consists of two sub-networks (one for cause text and one for effect text), each of which begins by converting the corresponding text into 50-dimensional embed- dings. These are then fed to a convolutional layer, <ref type="bibr">5</ref> which is followed by a max-pooling layer of equal length. Then, these top sub-network layers, which can be thought of as a type of phrasal embedding, are merged by taking their cosine similarity. Finally, this cosine similarity is normalized by feeding it into a dense layer with a single node which has a soft- plus activation. In designing our CNN, we attempted to minimize architectural and hyperparameter tun- ing by taking inspiration from <ref type="bibr" target="#b17">Iyyer et al. (2015)</ref>, preferring simpler architectures. We train the net- work using a binary cross entropy objective function and the Adam optimizer ( <ref type="bibr" target="#b20">Kingma and Ba, 2014)</ref>, us- ing the Keras library (Chollet, 2015) operating over Theano (Theano Development Team, 2016), a pop- ular deep-learning framework. 6 <ref type="bibr">5</ref> The convolutional layer contained 100 filters, had a filter length of 2 (i.e., capturing bigram information), and an inner ReLU activation. <ref type="bibr">6</ref> We also experimented with an equivalent architecture where the sub-networks are implemented using long short- Noise-aware Causal Embedding Model (cEm- bedNoise): We designed a variant of our cEmbed approach to address the potential impact of the noise introduced by our bootstrapping method. While training, we weigh the causal tuples by the likeli- hood that they are truly causal, which we approxi- mate with pointwise mutual information (PMI). For this, we first score the tuples by their causal PMI and then scale these scores by the overall frequency of the tuple <ref type="bibr" target="#b29">(Riloff, 1996)</ref>, to account for the PMI bias toward low-frequency items. That is, the score S of a tuple, t, is computed as:</p><formula xml:id="formula_0">S(t) = log p(t|causal) p(t) * log(f req(t))<label>(1)</label></formula><p>We then discretize these scores into five quantiles, ascribing a linearly decreasing weight during train- ing to datums in lower scoring quantiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Direct Evaluation: Ranking Word Pairs</head><p>We begin the assessment of our models with a direct evaluation to determine whether or not the proposed approaches capture causality better than general- purpose word embeddings and whether their robust- ness improves upon a simple database look-up. For this evaluation, we follow the protocol of <ref type="bibr" target="#b21">Levy and Goldberg (2014)</ref>. In particular, we create a collec- tion of word pairs, half of which are causally re- lated, with the other half consisting of other rela- tions. These pairs are then ranked by our models and several baselines, with the goal of ranking the causal pairs above the others. The embedding models rank the pairs using the cosine similarity between the tar- get vector for the causal word and the context vector of the effect word. The alignment model ranks pairs using the probability P (Effect|Cause) given by IBM Model 1, and the CNN ranks pairs by the value of the output returned by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data</head><p>In order to avoid bias towards our extraction meth- ods, we evaluate our models on an external set of term memory (LSTM) networks <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>, and found that they consistently under-perform this CNN architecture. Our conjecture is that CNNs perform better be- cause LSTMs are more sensitive to overall word order than CNNs, which capture only local contexts, and we have rela- tively little training data, which prevents the LSTMs from gen- eralizing well. We used a total of 1730 nominal pairs, 865 of which were from the Cause-Effect relation (e.g., (dancing → happiness)) and an equal number which were randomly selected from the other eight relations (e.g., (juice → grapefruit), from the Entity-Origin relation). This set was then randomly divided into equally-sized development and test partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>We compared our distributional similarity models against three baselines:</p><p>Vanilla Embeddings Model (vEmbed): a standard word2vec model trained with the skip-gram algo- rithm and a sliding window of 5, using the original texts from which our causal pairs were extracted. <ref type="bibr">7</ref> As with the cEmbed model, SemEval pairs were ranked using the cosine similarity between the vec- tor representations of their arguments.</p><p>Look-up Baseline: a given SemEval pair was ranked by the number of times it appeared in our extracted cause-effect tuples.</p><p>Random: pairs were randomly shuffled. <ref type="figure" target="#fig_2">Figure 2</ref> shows the precision-recall (PR) curve for each of the models and baselines. As expected, the causal models are better able to rank causal pairs than the vanilla embedding baseline (vEmbed), which, in turn, outperforms the random baseline. Our look-up baseline, which ranks pairs by their fre- quency in our causal database, shows a high preci- sion for this task, but has coverage for only 35% of the causal SemEval pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>Some models perform better on the low-recall portion of the curve (e.g., the look-up baseline and cCNN), while the embedding and alignment mod- els have a higher and more consistent performance across the PR curve. We hypothesize that models that better balance precision and recall will perform better in a real-world QA task, which may need to access a given causal relation through a variety of lexical patterns or variations. We empirically vali- date this observation in Section 7.</p><p>The PR curve for the causal embeddings shows an atypical dip at low-recall. To examine this, we analyzed its top-ranked 15% of SemEval pairs, and found that incorrectly ranked pairs were not found in the database of causal tuples. Instead, these incor- rect rankings were largely driven by low frequency words whose embeddings could not be robustly es- timated due to lack of direct evidence. Because this sparsity is partially driven by directionality, we im- plemented a bidirectional embedding model (cEm- bedBi) that (a) trains a second embedding model by reversing the input (effects as targets, causes as contexts), and (b) ranks pairs by the average of the scores returned by these two unidirectional causal embedding models. Specifically, the final bidirec- tional score of the pair, (e 1 , e 2 ), where e 1 is the can-didate cause and e 2 is the candidate effect, is:</p><formula xml:id="formula_1">s bi (e 1 , e 2 ) = 1 2 (s c→e (e 1 , e 2 ) + s e→c (e 2 , e 1 )) (2)</formula><p>where s c→e is the score given by the original causal embeddings, i.e., from cause to effect, and s e→c is the score given by the reversed-input causal embed- dings, i.e., from effect to cause. As <ref type="figure" target="#fig_2">Figure 2</ref> shows, the bidirectional embedding variants consistently outperform their unidirectional counterparts. All in all, the best overall model is cEmbedBiNoise, which is both bidirectional and in- corporates the noise handling approach from Sec- tion 5. This model substantially improves perfor- mance in the low-recall portion of the curve, while also showing strong performance across the curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Indirect Evaluation: QA Task</head><p>The main objective of our work is to investigate the impact of a customized causal embedding model for QA. Following our direct evaluation, which solely evaluated the degree to which our models directly encode causality, here we evaluate each of our pro- posed causal models in terms of their contribution to a downstream real-world QA task.</p><p>Our QA system uses a standard reranking ap- proach ( <ref type="bibr" target="#b18">Jansen et al., 2014</ref>). In this architecture, the candidate answers are initially extracted and ranked using a shallow candidate retrieval (CR) component that uses solely information retrieval techniques, then they are re-ranked using a "learning to rank" approach. In particular, we used SVM rank 8 , a Sup- port Vector Machines classifier adapted for ranking, and re-ranked the candidate answers with a set of features derived from both the initial CR score and the models we have introduced. For our model com- binations (see <ref type="table">Table 2</ref>), the feature set includes the CR score and the features from each of the models in the combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data</head><p>We evaluate on a set of causal questions extracted from the Yahoo! Answers corpus 9 with simple sur- face patterns such as What causes ... and What is the result of ... <ref type="bibr">10</ref> . We extracted a total of 3031 questions, each with at least four candidate answers, and we evaluated performance using five-fold cross- validation, with three folds for training, one for de- velopment, and one for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Models and Features</head><p>We evaluate the contribution of the bidirectional and noise-aware causal embedding models (cEmbedBi, and cEmbedBiNoise) as well as the causal alignment model (cAlign) and the causal CNN (cCNN). These models are compared against three baselines: the vanilla embeddings (vEmbed), the lookup baseline (LU), and additionally a vanilla alignment model (vAlign) which is trained over 65k question-answer pairs from Yahoo! Answers.</p><p>The features <ref type="bibr">11</ref> we use for the various models are:</p><p>Embedding model features: For both our vanilla and causal embedding models, we use the same set of features as <ref type="bibr" target="#b11">Fried et al. (2015)</ref>: the maximum, minimum, and average pairwise cosine similarity between question and answer words, as well as the overall similarity between the composite question and answer vectors. When using the causal embed- dings, since the relation is directed, we first deter- mine whether the question text is the cause or the ef- fect 12 , which in turn determines which embeddings to use for the question text and which to use for the candidate answer texts. For example, in a question such as "What causes X?", since X is the effect, all cosine similarities would be found using the effect vectors for the question words and the cause vectors for the answer candidate words.   Jensen-Shannon distance, proposed more recently by <ref type="bibr" target="#b11">Fried et al. (2015)</ref>, in our vanilla alignment model. However, due to the directionality inherent in causality, they do not apply to our causal model so there we omit them.</p><p>Look-up feature: For the look-up baseline we count the number of times words from the question and answer appear together in our database of ex- tracted causal pairs, once again after determining the directionality of the questions. If the total number of matches is over a threshold <ref type="bibr">13</ref> , we consider the causal relation to be established and give the candidate an- swer a score of 1; or a score of 0, otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Results</head><p>The overall results are summarized in <ref type="table">Table 2</ref>. Lines 1-5 in the table show that each of our baselines per- formed better than CR by itself, except for vAlign, suggesting that the vanilla alignment model does not generate accurate predictions for causal questions.</p><p>The strongest baseline was CR + vEmbed (line 3), the vanilla embeddings trained over Gigaword, at 34.6% P@1. For this reason, we consider this to be the baseline to "beat", and perform statistical sig- nificance of all proposed models with respect to it.</p><p>Individually, the cEmbedBi model is the best per- forming of the causal models. While the perfor- mance of cAlign in the direct evaluation was com- parable to that of cEmbedBi, here it performs far worse (line 6 vs 8), suggesting that the robustness of embeddings is helpful in QA. Notably, despite the strong performance of the cCNN in the low-recall portion of the PR curve in the direct evaluation, here the model performs poorly <ref type="bibr">(line 9)</ref>.</p><p>No individual causal model outperforms the strong vanilla embedding baseline (line 3), likely owing to the reduction in generality inherent to building task-specific QA models. However, com- paring lines 6-9 vs. 10-14 shows that the vanilla and causal models are capturing different and comple- mentary kinds of knowledge (i.e., causality vs. as- sociation through distributional similarity), and are able to be combined to increase overall task perfor- mance (lines 10-12). These results highlight that QA is a complex task, where solving methods need to address the many distinct information needs in question sets, including both causal and direct as- sociation relations. This contrasts with the direct evaluation, which focuses strictly on causality, and where the vanilla embedding baseline performs near chance. This observation highlights one weakness of word similarity tasks: their narrow focus may not directly translate to estimating their utility in real- world NLP applications.</p><p>Adding in the lookup baseline (LU) to the best- performing causal model does not improve perfor- mance (compare lines 10 and 12), suggesting that the bidirectional causal embeddings subsume the contribution of the LU model. cEmbedBi (line 10) also performs better than cEmbedBiNoise (line 11). We conjecture that the "noise" filtered out by cEm- bedBiNoise contains distributional similarity infor- mation, which is useful for the QA task. cEmbedBi vastly outperforms cCNN (line 14), suggesting that strong overall performance across the precision- recall curve better translates to the QA task. We hy- pothesize that the low cCNN performance is caused by insufficient training data, preventing the CNN ar-Error/observation % Q Both chosen and gold are equally good answers 45% Causal max similarity of chosen is higher 35% Vanilla overall similarity of chosen is higher 35% Chosen answer is better than the gold answer 25% The question is very short / lacks content words 15% Other 10% <ref type="table">Table 3</ref>: Results of an error analysis performed on a random sample of 20 incorrectly answered questions showing the source of the error and the percentage of questions that were affected.</p><p>Note that questions can belong to multiple categories. chitecture from generalizing well.</p><p>Our best performing overall model combines both variants of the causal embedding model (cEmbedBi and cEmbedBiNoise), reaching a P@1 of 37.3%, which shows a 7.7% relative improvement over the strong CR + vEmbed baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Error Analysis</head><p>We performed an error analysis to gain more insight into our model as well as the source of the remain- ing errors. For simplicity, we used the combina- tion model CR + vEmbed + cEmbedBi. Examining the model's learned feature weights, we found that the vanilla overall similarity feature had the high- est weight, followed by the causal overall similarity and causal maximum similarity features. This in- dicates that even in causal question answering, the overall topical similarity between question and an- swer is still useful and complementary to the causal similarity features.</p><p>To determine sources of error, we randomly se- lected 20 questions that were incorrectly answered and analyzed them according to the categories shown in <ref type="table">Table 3</ref>. We found that for 70% of the questions, the answer chosen by our system was as good as or better than the gold answer, often the case with community question answering datasets.</p><p>Additionally, while the maximum causal similar- ity feature is useful, it can be misleading due to em- bedding noise, low-frequency words, and even the bag-of-words nature of the model (35% of the incor- rect questions). For example, in the question What are the effects of growing up with an older sibling who is better than you at everything?, the model chose the answer ...You are you and they are them -you will be better and different at other things... largely because of the high causal similarity between (grow → better). While this could arguably be help- ful in another context, here it is irrelevant, suggest- ing that in the future improvement could come from models that better incorporate textual dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a framework for creating customized embeddings tailored to the information need of causal questions. We trained three popular mod- els (embedding, alignment, and CNN) using causal tuples extracted with minimal supervision by boot- strapping cause-effect pairs from free text, and eval- uated their performance both directly (i.e., the de- gree to which they capture causality), and indirectly (i.e., their real-world utility on a high-level question answering task).</p><p>We showed that models that incorporate a knowl- edge of causality perform best for both tasks. Our analysis suggests that the models that perform best in the real-world QA task are those that have consis- tent performance across the precision-recall curve in the direct evaluation. In QA, where the vocabulary is much larger, precision must be balanced with high- recall, and this is best achieved by our causal embed- ding model. Additionally, we showed that vanilla and causal embedding models address different in- formation needs of questions, and can be combined to improve performance.</p><p>Extending this work beyond causality, we hypoth- esize that additional embedding spaces customized to the different information needs of questions would allow for robust performance over a larger variety of questions, and that these customized em- bedding models should be evaluated both directly and indirectly to accurately characterize their per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resources</head><p>All code and resources needed to reproduce this work are available at http://clulab.cs. arizona.edu/data/emnlp2016-causal/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 )</head><label>3</label><figDesc>Causal tuple extraction: After CMs are iden- tified, a grammar scans the text for causal relations that have CMs as arguments. Different patterns have varying probabilities of signaling causation (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the causal convolutional network.</figDesc><graphic url="image-1.png" coords="5,99.00,57.83,170.06,95.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-recall curve showing the ability of each model to rank causal pairs above non-causal pairs. For clarity, we do not plot cEmbedNoise, which performs worse than cEmbedBiNoise. The Look-up model has no data points beyond the 35% recall point. word pairs drawn from the SemEval 2010 Task 8 (Hendrickx et al., 2009), originally a multi-way classification of semantic relations between nominals. We used a total of 1730 nominal pairs, 865 of which were from the Cause-Effect relation (e.g., (dancing → happiness)) and an equal number which were randomly selected from the other eight relations (e.g., (juice → grapefruit), from the Entity-Origin relation). This set was then randomly divided into equally-sized development and test partitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 :</head><label>2</label><figDesc>Performance in the QA evaluation, measured by precision-at-one (P@1). The "Bi" suffix indicates a bidirec- tional model; the "Noise" suffix indicates a model that is noise aware. * indicates that the difference between the correspond- ing model and the CR + vEmbed baseline is statistically sig- nificant (p &lt; 0.05), determined through a one-tailed bootstrap resampling test with 10,000 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://simple.wikipedia.org/wiki/Main_Page. The Simple English version was preferred over the full version due to its simpler sentence structures, which make extracting cause-effect tuples more straightforward.</note>

			<note place="foot" n="4"> For all models proposed in this section we used lemmas rather than words.</note>

			<note place="foot" n="7"> All embedding models analyzed here, including this baseline and our causal variants, produced embedding vectors of 200 dimensions.</note>

			<note place="foot" n="8"> http://www.cs.cornell.edu/people/tj/ svm_light/svm_rank.html 9 Freely available through Yahoo!&apos;s Webscope program (research-data-requests@yahoo-inc.com)</note>

			<note place="foot" n="10"> We lightly filtered these with stop words to remove noncausal questions, such as those based on math problems and the results of sporting events. Our dataset will be freely available, conditioned on users having obtained the Webscope license. 11 Due to the variety of features used, each feature described here is independently normalized to lie between 0.0 and 1.0. 12 We do this through the use of simple regular expressions, e.g., &quot;ˆ [Ww]hat ([a-z]+ ){0,3}cause.+&quot;</note>

			<note place="foot" n="13"> Empirically determined to be 100 matches. Note that using this threshold performed better than simply using the total number of matches.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the Allen Institute for Artificial Intelli-gence for funding this work. Additionally, this work was partially funded by the Defense Advanced Re-search Projects Agency (DARPA) Big Mechanism program under ARO contract W911NF-14-1-0395.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bridging the lexical chasm: Statistical approaches to answer finding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd Annual International ACM SIGIR Conference on Research &amp; Development on Information Retrieval</title>
		<meeting>of the 23rd Annual International ACM SIGIR Conference on Research &amp; Development on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
	</analytic>
	<monogr>
		<title level="m">Question answering with subgraph embeddings</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">D</forename><surname>Manning2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conferenc on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>of the Conferenc on Empirical Methods for Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IBM&apos;s PIQUANT II in TREC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study of the knowledge base requirements for passing an elementary science test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2013 workshop on Automated Knowledge Base Construction (AKBC)</title>
		<meeting>of the 2013 workshop on Automated Knowledge Base Construction (AKBC)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A lightweight tool for automatically extracting causal relationships from text</title>
	</analytic>
	<monogr>
		<title level="m">SoutheastCon, 2006. Proc. of the IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Minimally supervised event causality identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
	<note>Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search needs a shake-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">476</biblScope>
			<biblScope unit="issue">7358</biblScope>
			<biblScope unit="page" from="25" to="26" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Problems with evaluation of word embeddings using word similarity tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Faruqui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02276</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="79" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Building Watson: An overview of the DeepQA project. AI magazine</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic role labeling with neural network factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitzgerald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the 2015 Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Higher-order lexical semantic models for non-factoid answer reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hahn-Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Fried et al.2015</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text mining for causal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">R</forename><surname>Moldovan2002</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="360" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th conference on Computational linguistics (COLING)</title>
		<meeting>of the 14th conference on Computational linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrickx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semeval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2015</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discourse complements lexical semantics for non-factoid answer reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic extraction of cause-effect information from newspaper text without knowledge-based inferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Khoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the 2015 Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Kingma and Ba2014</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">O</forename><surname>Goldberg2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">O</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramat-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Och and Ney2003</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Whyquestion answering using intra-and inter-sentential causal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>of Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Riedel et al.2013</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical machine translation for query expansion in answer retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatically generating extraction patterns from untagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>of the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1044" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spinning straw into gold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<meeting>of the Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Retrieval-Special Issue on Web Information Retrieval</title>
		<editor>M. A. ValenzuelaEscárcega, G. Hahn-Powell, and M. Surdeanu</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="383" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Learning to rank answers to non-factoid questions from web collections</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Odin&apos;s runes: A rule language for information extraction</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>of the 10th International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations for unsupervised semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">K</forename><surname>Lapata2015</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2015 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi level causal relation identification using extended features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">X</forename><surname>Mao2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="7171" to="7181" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-markov phrasebased monolingual alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Yih et al.2013</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
