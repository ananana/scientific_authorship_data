<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coverage Embedding Models for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coverage Embedding Models for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="955" to="960"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has gained pop- ularity in recent years (e.g. ( <ref type="bibr" target="#b8">Jean et al., 2015;</ref><ref type="bibr" target="#b11">Luong et al., 2015;</ref><ref type="bibr" target="#b13">Mi et al., 2016b;</ref>), especially for the attention- based models of . The at- tention at each time step shows which source word the model should focus on to predict the next tar- get word. However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage infor- mation typically for each source word. As a result, this kind of model suffers from issues of repeating or dropping translations.</p><p>The traditional statistical machine translation (SMT) systems (e.g. <ref type="bibr" target="#b8">(Koehn, 2004)</ref>) address the above issues by employing a source side "cover- age vector" for each sentence to indicate explicitly which words have been translated, which parts have not yet. A coverage vector starts with all zeros, meaning no word has been translated. If a source word at position j got translated, the coverage vector sets position j as 1, and they won't use this source word in future translation. This mechanism avoids the repeating or dropping translation problems.</p><p>However, it is not easy to adapt the "coverage vec- tor" to NMT directly, as attentions are soft probabili- ties, not 0 or 1. And SMT approaches handle one-to- many fertilities by using phrases or hiero rules (pre- dict several words in one step), while NMT systems only predict one word at each step.</p><p>In order to alleviate all those issues, we borrow the basic idea of "coverage vector", and introduce a coverage embedding vector for each source word. We keep updating those embedding vectors at each translation step, and use those vectors to track the coverage information.</p><p>Here is a brief description of our approach. At the beginning of translation, we start from a full cover- age embedding vector for each source word. This is different from the "coverage vector" in SMT in following two aspects:</p><p>• each source word has its own coverage embed- ding vector, instead of 0 or 1, a scalar, in SMT, • we start with a full embedding vector for each word, instead of 0 in SMT. After we predict a translation word y t at time step t, we need to update each coverage embedding vec- tor accordingly based on the attentions in the current step. Our motivation is that if we observe a very high attention over x i in this step, there is a high chance that x i and y t are translation equivalent. So the em- bedding vector of x i should come to empty (a zero vector) in a one-to-one translation case, or subtract the embedding of y t for the one-to-many translation case. An empty coverage embedding of a word x i in- dicates this word is translated, and we can not trans- late x i again in future. Empirically, we model this procedure by using neural networks (gated recurrent unit (GRU) ( ) or direct subtraction).</p><p>Large-scale experiments over Chinese-to-English on various test sets show that our method improves the translation quality significantly over the large vo- cabulary NMT system (Section 5).</p><formula xml:id="formula_0">s t1 s t … o t y 1 … … y |V y | … … H t = l X i=1 (↵ ti · h i ) l X i=1 (↵ ti · ! h i ) x 1 x l h 1 h l ! h l ! h 1 … … … x 2 ! h 2 h 2 x 1 x l h 1 h l ! h l ! h 1 … … … h j ! h j x j … … … y ⇤ t1 y ⇤ t s t c t1,x j ↵ t,j = exp(e t,j ) P l i=1 exp(e t,i ) A t,j e t,j e t,l e t,1 ↵ t,1 ↵ t,2</formula><p>↵ t,l <ref type="figure">Figure 1</ref>: The architecture of attention-based NMT.The source sentence is x = (x 1 , ..., x l ) with length l, the translation is y * = (y * 1 , ..., y * m ) with length m. ← − h i and − → h i are bi-directional encoder states. α t,j is the attention probability at time t, position j. H t is the weighted sum of encoding states. s t is a hidden state. o t is an output state. Another one layer neural network projects o t to the target output vocabulary, and conducts softmax to predict the probability distribution over the output vocabulary. The attention model (in right gray box) is a two layer feedforward neural network, A t,j is an intermediate state, then another layer converts it into a real number e t,j , the final attention probability at position j is α t,j . We plug coverage embedding models into NMT model by adding an input c t−1,xj to A t,j (the red dotted line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>As shown in <ref type="figure">Figure 1</ref>, attention-based neural ma- chine translation ( ) is an encoder-decoder network. the encoder employs a bi- directional recurrent neural network to encode the source sentence x = (x 1 , ..., x l ), where l is the sentence length, into a sequence of hidden states</p><formula xml:id="formula_1">h = (h 1 , ..., h l ), each h i is a concatenation of a left- to-right − → h i and a right-to-left ← − h i , h i = ← − h i − → h i = ← − f (x i , ← − h i+1 ) − → f (x i , − → h i−1 )</formula><p>, where ← − f and − → f are two GRUs. Given the encoded h, the decoder predicts the target translation by maximizing the conditional log-probability of the correct translation</p><formula xml:id="formula_2">y * = (y * 1 , ...y * m )</formula><p>, where m is the sentence length. At each time t, the probability of each word y t from a target vocabulary V y is:</p><formula xml:id="formula_3">p(y t |h, y * t−1 ..y * 1 ) = g(s t , y * t−1 ),<label>(1)</label></formula><p>where g is a two layer feed-forward neural network (o t is a intermediate state) over the embedding of the previous word y * t−1 , and the hidden state s t . The s t is computed as:</p><formula xml:id="formula_4">s t = q(s t−1 , y * t−1 , H t )<label>(2)</label></formula><formula xml:id="formula_5">H t = l i=1 (α t,i · ← − h i ) l i=1 (α t,i · − → h i ) ,<label>(3)</label></formula><p>where q is a GRU, H t is a weighted sum of h, the weights, α, are computed with a two layer feed- forward neural network r:</p><formula xml:id="formula_6">α t,i = exp{r(s t−1 , h i , y * t−1 )} l k=1 exp{r(s t−1 , h k , y * t−1 )} (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coverage Embedding Models</head><p>Our basic idea is to introduce a coverage embedding for each source word, and keep updating this em- bedding at each time step. Thus, the coverage em- bedding for a sentence is a matrix, instead of a vec- tor in SMT. As different words have different fertili- ties (one-to-one, one-to-many, or one-to-zero), sim- ilar to word embeddings, each source word has its own coverage embedding vector. For simplicity, the number of coverage embedding vectors is the same as the source word vocabulary size. At the beginning of our translation, our cover- age embedding matrix (c 0,x 1 , c 0,x 2 , ...c 0,x l ) is initial- ized with the coverage embedding vectors of all the source words.</p><p>Then we update them with neural networks (a GRU (Section 3.1.1) or a subtraction (Section 3. until we translation all the source words.</p><formula xml:id="formula_7">1.2)) ,l l 1,l y t1 … … … … y t … … ↵ t1,j ↵ t,j c t2,x 1 c t2,x j c t2,x l c t1,x l c t1,x j c t1,x 1 c t,x 1 c t,x j c t,x l</formula><p>In the middle of translation, some coverage em- beddings should be close to zero, which indicate those words are covered or translated, and can not be translated in future steps. Thus, in the end of transla- tion, the embedding matrix should be close to zero, which means all the words are covered.</p><p>In the following part, we first show two updating methods, then we list the NMT objective that takes into account the embedding models. <ref type="figure" target="#fig_0">Figure 2</ref> shows the updating method with a GRU. Then, at time step t, we feed y t and α t,j to the cov- erage model (shown in <ref type="figure" target="#fig_0">Figure 2</ref>),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Updating Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Updating with a GRU</head><formula xml:id="formula_8">z t,j = σ(W zy y t + W zα α t,j + U z c t−1,x j ) r t,j = σ(W ry y t + W rα α t,j + U r c t−1,x j ) ˜ c t,x j = tanh(W y t + W α α t,j + r t,j • U c t−1,x j ) c t,x j = z t,j • c t−1,x j + (1 − z t,j ) • ˜ c t,x j ,</formula><note type="other">where, z t is the update gate, r t is the reset gate, ˜ c t is the new memory content, and c t is the final memory. The matrix W zy , W zα , U z , W ry , W rα , U r , W y , W α and U are shared across different position j.</note><p>• is a pointwise operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Updating as Subtraction</head><p>Another updating method is to subtract the em- bedding of y t directly from the coverage embedding c t,x j with a weight α t,j as</p><formula xml:id="formula_9">c t,x j = c t−1,x j − α t,j • (W y→c y t ),<label>(5)</label></formula><p>where W y→c is a matrix that coverts word embed- ding of y t to the same size of our coverage embed- ding vector c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objectives</head><p>We integrate our coverage embedding models into the attention NMT ( ) by adding c t−1,x j to the first layer of the attention model (shown in the red dotted line in <ref type="figure">Figure 1</ref>). Hopefully, if y t is partial translation of x j with a probability α t,j , we only remove partial information of c t−1,x j . In this way, we enable coverage embed- ding c 0,x j to encode fertility information of x j .</p><p>As we have mentioned, in the end of translation, we want all the coverage embedding vectors to be close to zero. So we also minimize the absolute val- ues of embedding matrixes as</p><formula xml:id="formula_10">θ * = arg max θ N n=1 m t=1 log p(y * n t |x n , y * n t−1 ..y * n 1 ) − λ l i=1 ||c m,x i || ,<label>(6)</label></formula><p>where λ is the coefficient of our coverage model. As suggested by <ref type="bibr" target="#b12">Mi et al. (2016a)</ref>, we can also use some supervised alignments in our training. Then, we know exactly when each c t,x j should become close to zero after step t. Thus, we redefine Equa- tion 6 as:</p><formula xml:id="formula_11">θ * = arg max θ N n=1 m t=1 log p(y * n t |x n , y * n t−1 ..y * n 1 ) − λ l i=1 ( m j=ax i ||c j,x i ||) ,<label>(7)</label></formula><p>where a x i is the maximum index on the target sen- tence x i can be aligned to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There are several parallel and independent related work ( <ref type="bibr" target="#b14">Tu et al., 2016;</ref><ref type="bibr" target="#b6">Feng et al., 2016;</ref><ref type="bibr" target="#b3">Cohn et al., 2016)</ref>. <ref type="bibr" target="#b14">Tu et al. (2016)</ref> is the most relevant one. In their paper, they also employ a GRU to model the coverage vector. One main difference is that our model introduces a specific coverage embedding vector for each source word, in contrast, their work initializes the word coverage vector with a scalar with a uniform distribution. Another difference lays in the fertility part, <ref type="bibr" target="#b14">Tu et al. (2016)</ref> add an accu- mulate operation and a fertility function to simulate the process of one-to-many alignments. In our ap- proach, we add fertility information directly to cov- erage embeddings, as each source word has its own embedding. The last difference is that our baseline system ( <ref type="bibr" target="#b13">Mi et al., 2016b</ref>) is an extension of the large vocabulary NMT of <ref type="bibr" target="#b8">Jean et al. (2015)</ref> with candi- date list decoding and UNK replacement, a much stronger baseline system. <ref type="bibr" target="#b3">Cohn et al. (2016)</ref> augment the attention model with well-known features in traditional SMT, in- cluding positional bias, Markov conditioning, fertil- ity and agreement over translation directions. This work is orthogonal to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preparation</head><p>We run our experiments on Chinese to English task. We train our machine translation systems on two training sets. The first training corpus consists of approximately 5 million sentences available within the DARPA BOLT Chinese-English task. The sec- ond training corpus adds HK Law, HK Hansard and UN data, the total number of training sentence pairs is 11 million. The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF).</p><p>Our development set is the concatenation of sev- eral tuning sets (GALE Dev, P1R6 Dev, and Dev 12) released under the DARPA GALE program. The de- velopment set is 4491 sentences in total. Our test sets are NIST MT06, MT08 news, and MT08 web.</p><p>For all NMT systems, the full vocabulary sizes for thr two training sets are 300k and 500k respectively. The coverage embedding vector size is 100. In the training procedure, we use AdaDelta <ref type="bibr" target="#b15">(Zeiler, 2012)</ref> to update model parameters with a mini-batch size 80. Following <ref type="bibr" target="#b13">Mi et al. (2016b)</ref>, the output vocab- ulary for each mini-batch or sentence is a sub-set of the full vocabulary. For each source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candi- dates of the word-to-word/phrase translation tables learned from 'fast align' <ref type="bibr" target="#b5">(Dyer et al., 2013</ref>). The maximum length of a source phrase is 4. In the train- ing time, we add the reference in order to make the translation reachable.</p><p>Following <ref type="bibr" target="#b8">Jean et al. (2015)</ref>, We dump the align- ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word. Our traditional SMT system is a hybrid syntax- based tree-to-string model <ref type="bibr" target="#b17">(Zhao and Al-onaizan, 2008</ref>), a simplified version of <ref type="bibr" target="#b10">Liu et al. (2009)</ref> and <ref type="bibr" target="#b2">Cmejrek et al. (2013)</ref>. We parse the Chinese side with Berkeley parser, and align the bilingual sen- tences with GIZA++. Then we extract Hiero and tree-to-string rules on the training set. Our two 5- gram language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07)), respectively. As suggestion by <ref type="bibr" target="#b16">Zhang (2016)</ref>, NMT systems can achieve better re- sults with the help of those monolingual corpora. We tune our system with PRO ( <ref type="bibr" target="#b7">Hopkins and May, 2011)</ref> to minimize (TER-BLEU)/2 on the development set. <ref type="table">Table 1</ref> shows the results of all systems on 5 million training set. The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (TER-BLEU)/2. The large- vocabulary NMT (LVNMT), our baseline, achieves an average (TER-BLEU)/2 score of 15.74, which is about 2 points worse than the hybrid system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation Results</head><p>We test four different settings for our coverage embedding models:</p><p>• U GRU : updating with a GRU;</p><p>• U Sub : updating as a subtraction;</p><p>• U GRU + U Sub : combination of two methods (do not share coverage embedding vectors); • +Obj.: U GRU + U Sub plus an additional objec- tive in Equation 6 1 . U GRU improves the translation quality by 1.3 points on average over LVNMT. And U GRU + U Sub achieves the best average score of 13.14, which is about 2.6 points better than LVNMT. All the im- provements of our coverage embedding models over LVNMT are statistically significant with the sign- test of <ref type="bibr" target="#b4">Collins et al. (2005)</ref>. We believe that we need to explore more hyper-parameters of +Obj. in order to get even better results over U GRU + U Sub .    <ref type="table" target="#tab_1">Table 2</ref> shows the results of 11 million sys- tems, LVNMT achieves an average (TER-BLEU)/2 of 13.27, which is about 2.5 points better than 5 million LVNMT. The result of our U GRU cover- age model gives almost 1 point gain over LVNMT. Those results suggest that the more training data we use, the stronger the baseline system becomes, and the harder to get improvements. In order to get a rea- sonable or strong NMT system, we have to conduct experiments over a large-scale training set. <ref type="table" target="#tab_3">Table 3</ref> shows the F1 scores on the alignment test set (447 hand aligned sentences). The MaxEnt model is trained on 67k hand-aligned data, and achieves an F1 score of 75.96. For NMT systems, we dump alignment matrixes, then, for each target word we only add the highest probability link if it is higher than 0.2. Results show that our best coverage model, U GRU + U Sub , improves the F1 score by 2.2 points over the sorce of LVNMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Alignment Results</head><p>We also check the repetition statistics of NMT outputs. We simply compute the number  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose simple, yet effective, cov- erage embedding models for attention-based NMT. Our model learns a special coverage embedding vec- tor for each source word to start with, and keeps up- dating those coverage embeddings with neural net- works as the translation goes. Experiments on the large-scale Chinese-to-English task show significant improvements over the strong LVNMT system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The coverage embedding model with a GRU at time step t − 1 and t. c 0,1 to c 0,l are initialized with the word coverage embedding matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>single</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Single system results in terms of (TER-BLEU)/2 
on 11 million set. NMT results are on a large vocabulary 
(500k) and with UNK replaced. Due to the time limita-
tion, we only have the results of U GRU system. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Alignment F1 scores of different models. 

phrases (length longer or equal than 4 words) for 
each sentence. On MT06 test set, the 5 million 
LVNMT has 209 repeated phrases, our U GRU sys-
tem reduces it significantly to 79, U GRU +U Sub and 
+Obj. only have 50 and 47 repeated phrases, re-
spectively. The 11 million LVNMT gets 115 re-
peated phrases, and U GRU reduces it further down to 
16. Those trends hold across other test sets. Those 
statistics show that a larger training set or coverage 
embedding models alleviate the repeating problem 
in NMT. 

</table></figure>

			<note place="foot" n="1"> We use two λs for UGRU and U Sub separately, and we test λGRU = 1 × 10 −4 and λ Sub = 1 × 10 −2 in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank reviewers for their useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cmejrek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="545" to="555" />
		</imprint>
	</monogr>
	<note>Seattle. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incorporating Structural Alignment Biases into an Attentional Neural Translation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D V</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kucerova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pharaoh: a beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards zero unknown word in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2016</title>
		<meeting>IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint decoding with multiple translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
	<note>ACL &apos;09</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Portugal, September. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>August</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Coveragebased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalizing local and non-local word-reordering patterns for syntaxbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
