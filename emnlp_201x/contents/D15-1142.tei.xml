<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Chinese Word Segmentation based on Bilingual Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Chinese Word Segmentation based on Bilingual Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a bilingual semi-supervised Chinese word segmentation (CWS) method that leverages the natural segmenting information of English sentences. The proposed method involves learning three levels of features, namely, character-level, phrase-level and sentence-level, provided by multiple sub-models. We use a sub-model of conditional random fields (CRF) to learn mono-lingual grammars, a sub-model based on character-based alignment to obtain explicit segmenting knowledge, and another sub-model based on transliteration similarity to detect out-of-vocabulary (OOV) words. Moreover, we propose a sub-model leveraging neural network to ensure the proper treatment of the semantic gap and a phrase-based translation sub-model to score the translation probability of the Chi-nese segmentation and its corresponding English sentences. A cascaded log-linear model is employed to combine these features to segment bilingual unlabeled data, the results of which are used to justify the original supervised CWS model. The evaluation shows that our method results in superior results compared with those of the state-of-the-art monolingual and bilingual semi-supervised models that have been reported in the literature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese word segmentation (CWS) is generally accepted to be a necessary first step in most Chi- nese NLP tasks because Chinese sentences are written in continuous sequences of characters with no explicit delimiters (e.g., the spaces in English). Many studies have been conducted in this area, re- sulting in extensive investigation of the problem of CWS using machine learning techniques in recent years. However, the reliability of CWS that can be achieved using machine learning techniques re- lies heavily on the availability of a large amount of high-quality, manually segmented data. Because hand-labeling individual words and word bound- aries is very difficult ( <ref type="bibr" target="#b9">Jiao et al., 2006</ref>), producing segmented Chinese texts is very time-consuming and expensive. Although a number of manual- ly segmented datasets have been constructed by various organizations, it is not feasible to com- bine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards. Thus, it is difficult to build a large-scale manually segmented corpus, and the resulting lack of such a corpus is detrimental to further enhancement of the accuracy of CWS.</p><p>To address the scarcity of manually segment- ed corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years. These approaches attempt to ei- ther learn the predicted label distribution ( <ref type="bibr" target="#b9">Jiao et al., 2006</ref>) or extract mutual information <ref type="bibr" target="#b4">((Liang et al., 2005</ref>); (Sun and Xu, 2011); ( <ref type="bibr" target="#b25">Zeng et al., 2013a)</ref>) from large-scale monolingual unlabeled data to update the baseline model (from manual- ly segmented corpora). In addition to these tech- niques, several co-training approaches ( <ref type="bibr" target="#b26">Zeng et al., 2013b</ref>) using character-based and word-based models have also been employed. However, be- cause monolingual unlabeled data contain limit- ed natural segmenting information, in most semi- supervised methods, the objective function tend- s to be optimized based on the personal experi- ence and knowledge of the researchers. This prac- tice means that these methods can typically yield high performance in certain specialized domain- s, but they lack generalizability. In contrast with these methods, we propose to leverage bilingual unlabeled data, i.e., a Chinese-English corpus with sentence alignment. Because English sentences are naturally segmented, extracting information from a bilingual corpus is a much more objective task. As the example presented in <ref type="figure" target="#fig_0">Fig 1 shows</ref>, the English sentences that correspond to Chinese text can easily help guide better segmentation, and thus, the learning of segmenting information from bilingual data is a very promising approach.</p><p>In this paper, to obtain high-quality segment- ing information from bilingual unlabeled data, we leverage multilevel features using the following steps: first, we integrate character-level features calculated using a conditional random field (CRF) model, which is used to capture the monolingual grammars. Then, we employ a statistical align- er to perform character-based alignment. Given the results of this character-based alignment, we apply several phrase-level features to extract ex- plicit and implicit segmenting information: (1) we use two types of English-Chinese co-occurrence features (one-to-many and many-to-many) to learn the explicit segmenting information of the English sentences, (2) we use the transliteration similarity feature to detect out-of-vocabulary (OOV) words using a phrase-based translation model, and (3) we employ a neural network to calculate the seman- tic gap between the Chinese and English words to ensure that the Chinese segmentation follows the semantic meanings of the corresponding En- glish sentences as closely as possible. Finally, we employ another phrase-based translation model to perform a sentence-level calculation of the trans- lation probability of the Chinese segmentation and its corresponding English sentences. After obtain- ing these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in <ref type="figure">Fig  2.</ref> Finally, we segment the bilingual unlabeled da- ta using the proposed model and use the segmen- tation of those data to justify the original super- <ref type="figure">Figure 2</ref>: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a stan- dard manually segmented corpus.</p><p>In fact, several semi-supervised CWS methods have previously been proposed that leverage bilin- gual unlabeled data <ref type="bibr" target="#b21">((Xu et al., 2008)</ref>; <ref type="bibr" target="#b1">(Chang et al., 2008)</ref>; <ref type="bibr" target="#b5">(Ma and Way, 2009)</ref>; <ref type="bibr" target="#b2">(Chung et al., 2009)</ref>; <ref type="bibr" target="#b19">(Xi et al., 2012)</ref>). However, most were de- veloped for statistical machine translation (SMT), causing them to focus on decreasing the perplex- ity of the bilingual data and the word alignmen- t process rather than on achieving more accurate segmentation. These methods achieve significan- t improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard gram- mars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and bilin- gual segmenting information, which allows our semi-supervised CWS model to be very efficient at other NLP tasks and endows it with higher gen- eralizability.</p><p>Our evaluation also shows that our method sig- nificantly outperforms the state-of-the-art mono- lingual and bilingual semi-supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>First, we review related work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Monolingual Supervised and Semi-supervised CWS Methods</head><p>Considerable efforts have been made in the NLP community in the study of Chinese word segmen- tation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by <ref type="bibr" target="#b22">(Xue et al., 2003)</ref>. Most previous systems have addressed this task using linear statistical models with carefully de- signed features <ref type="bibr" target="#b14">((Peng et al., 2004)</ref>; <ref type="bibr" target="#b0">(Asahara et al., 2005)</ref>; <ref type="bibr" target="#b23">(Zhang and Clark, 2007)</ref>; <ref type="bibr" target="#b24">(Zhao et al., 2010)</ref>). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very time- consuming and expensive to produce. Thus, the s- cale of available manually labeled data has placed considerable limitations on the further enhance- ment of supervised CWS methods.</p><p>To address this problem, a number of semi- supervised CWS approaches have been intensive- ly investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmenta- tion results by interpolating statistics-based fea- tures derived from unlabeled data into a CRF mod- el. ( <ref type="bibr" target="#b25">Zeng et al., 2013a</ref>) introduced a graph-based semi-supervised joint model of Chinese word seg- mentation and part-of-speech tagging and regular- ized the learning of a linear CRF model based on the label distributions derived from unlabeled da- ta. However, because monolingual unlabeled data lack natural segmenting information, most previ- ous semi-supervised CWS methods have required certain assumptions to be made regarding their ob- jective functions based on the researchers' person- al experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmenta- tion that is present in English sentences and can therefore extract linguistic knowledge without any manual assumptions or bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bilingual Semi-supervised CWS Methods</head><p>Some previous work <ref type="bibr" target="#b21">((Xu et al., 2008)</ref>; <ref type="bibr" target="#b1">(Chang et al., 2008)</ref>; <ref type="bibr" target="#b5">(Ma and Way, 2009)</ref>; <ref type="bibr" target="#b2">(Chung et al., 2009)</ref>; <ref type="bibr" target="#b19">(Xi et al., 2012)</ref>) has been performed on leveraging bilingual unlabeled data to achieve bet- ter segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of indi- vidual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmen- tation ( <ref type="bibr" target="#b20">Xu et al., 2004</ref>) or to form a labeled dataset for training a sequence labeling model ( <ref type="bibr" target="#b14">Peng et al., 2004</ref>). ( <ref type="bibr" target="#b27">Zeng et al., 2014</ref>) also used such map- pings to bias a supervised segmentation model to- ward a better solution for SMT. However, because most of these approaches focus on SMT perfor- mance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy. Thus, they sometimes ignore the standard grammars during segmentation in favor of satisfying the needs of SMT, thereby causing these methods to be rather unsuitable for other NLP tasks. By contrast, we propose to use various types of features to capture syntactic and semantic information and a cascaded log-linear model to maintain balance between the monolingual grammars and the bilingual knowl- edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilevel Features</head><p>In this section, we describe the three levels of fea- tures used in our approach. We propose to use character-level features to capture monolingual grammars and phrase-level and sentence-level fea- tures to obtain bilingual segmenting information. Moreover, we describe a cascaded log-linear mod- el by proposing both inner and outer log-linear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Character-level Feature</head><p>The conditional random field (CRF) ( <ref type="bibr" target="#b3">Lafferty et al., 2001</ref>) model was first used for CWS tasks by <ref type="bibr" target="#b22">(Xue et al., 2003</ref>) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.</p><p>In this paper, we score the character-level fea- ture in the same manner defined by <ref type="bibr" target="#b22">(Xue et al., 2003)</ref>. For the jth character c j in the sentence c J 1 = c 1 ...c J , the score can be calculated as fol- lows:</p><formula xml:id="formula_0">fCRF (j) = k λ k f k (yj−1, yj, c J 1 , j) (1)</formula><p>where f k (y j−1 , y j , c J 1 , j) is a feature function and λ k is a learned weight that corresponds to the feature f k . j represents the index of the character in the sentence. y j−1 and y j represent the tags of the previous and current characters, respectively.</p><p>We do not introduce the CRF-based CWS mod- el in detail here, but more information can be ob- tained from ( <ref type="bibr" target="#b3">Lafferty et al., 2001</ref>) and (Xue et al., 2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phrase-level Features</head><p>In this section, we first describe English-Chinese character-based alignment. Then, we propose sev- eral phrase-level features to obtain explicit and im- plicit segmenting information from the character- based alignment. Finally, we describe the in- ner log-linear model that is used to combine the character-level and phrase-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">English-Chinese Character-based Alignment</head><p>To avoid introducing omissions and mistakes in- to the linguistic information in the initial segmen- tations of the bilingual data, we perform a statis- tical character-based alignment: First, every Chi- nese character in the bitexts is separated by white spaces so that individual characters are recognized as unique /words0 or alignment targets. Then, they are associated with English words using a s- tatistical word aligner. By representing the English and Chinese sen- tences as e I 1 = e 1 e 2 ...e I and c J 1 = c 1 c 2 ...c J , re- spectively, where e i and c j represent single ele- ments of the sentences, we define their alignment as a K 1 , of which each element is a span a k =&lt; s, t &gt; and represents the alignment of the English word e s with the Chinese character c t . Then, the corpus of unlabeled bilingual data can be repre- sented as the set of sentence tuples &lt;e I 1 , c J 1 , a K 1 &gt; To obtain the character-based alignment, we employ an open-source toolkit Pialign 1 <ref type="bibr" target="#b7">((Neubig et al., 2011);</ref><ref type="bibr" target="#b8">(Neubig et al., 2012)</ref>) which us- es Bayesian learning and inversion transduction grammars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Features Obtained from the Character-based Alignment</head><p>Given the English-Chinese character-based align- ment a K 1 , we extract several phrase-level features to optimize the segmentation. For the jth char- acter in c J 1 , we assume that one of the segmen- tations of the substring c j 1 can be represented as</p><formula xml:id="formula_1">w N +1 1 = w 1 w 2 w 3 ...w N +1 = c j 1 1 c j 2 j 1 +1 ...c j j N +1</formula><p>. Then, we calculate the scores of each Chinese word w n = c jn jm (j m = j n−1 + 1) in w N +1 1 us- ing the following features.</p><p>English-Chinese One-to-Many Alignment To evaluate the probability that a sequence of Chinese characters c jn jm = c jm c jm+1 ...c jn should be combined into a word w n based on the corre- sponding English sentence, we integrate the fea- ture of English-Chinese one-to-many alignmen- t (one English word is aligned with multiple Chi- nese characters). First, for any English word e i in e I 1 , the phrase tuple &lt; e i , c jn jm &gt; can be defined as an aligned One-to-Many phrase tuple if it satisfies the following conditions:</p><formula xml:id="formula_2">(1) &lt; i, j m &gt; ∈ a K 1 , &lt; i, j n &gt; ∈ a K 1 (2) ∀j / ∈ [j m , j n ], &lt; i, j &gt; / ∈ a K 1 1 http://www.phontron.com/pialign/ (3) ∀i = i §∀j ∈ [j m , j n ], &lt; i , j &gt; / ∈ a K 1</formula><p>Then, for any phrase tuple &lt; e i , c jn jm &gt; that sat- isfies these conditions, the span &lt; i, j m , j n &gt; is defined as a One-to-Many span and as a member of the set A One .</p><p>Thus, for each span &lt; i, j m , j n &gt;, the One-to- Many score can be calculated as follows:</p><formula xml:id="formula_3">s(&lt; i, jm, jn &gt;) = t(c jn jm |ei) if &lt; i, jm, jn &gt;∈ AOne 0 else<label>(2)</label></formula><p>where t(c jn jm |e i ) represents the translation proba- bility of the phrase tuple c jn jm |e i . Finally, the score for the feature of English- Chinese One-to-Many alignment for w n = c jn jm is derived as follows:</p><formula xml:id="formula_4">fOne−to−Many(n) = argmax i∈[1,I] s(&lt; i, jm, jn &gt;) (3)</formula><p>English-Chinese Many-to-Many Alignment The second phrase-level feature, called English- Chinese Many-to-Many Alignment (multiple En- glish words are aligned with multiple Chinese characters), is used to evaluate the probability that a space should be inserted between c n and c n+1 . Similar to One-to-Many alignment, for any se- quence of English words e i 2 i 1 and the Chinese word w n = c jn jm , the phrase tuple &lt; e i 2</p><formula xml:id="formula_5">i 1 , c jn j 1</formula><p>&gt; is de- fined as an aligned Many-to-Many phrase tuple if it satisfies the following conditions:</p><p>(1) j 1 ≤ j m , and j 1 is the beginning character of a word in w n</p><formula xml:id="formula_6">1 (1) &lt; i 1 , j 1 &gt;∈ a K 1 , &lt; i 2 , j m &gt;∈ a K 1 (2) ∀j / ∈ [j 1 , j m ], ∀i ∈ [i 1 , i 2 ], &lt; i , j &gt; / ∈ a K 1 (3) ∀j ∈ [j 1 , j m ], ∀i / ∈ [i 1 , i 2 ], &lt; i , j &gt; / ∈ a K 1</formula><p>Then, for any phrase tuple &lt; e i 2 i 1</p><p>, c jn jm &gt; that sat- isfies these conditions, the span &lt; i 1 , i 2 , j 1 , j n &gt; is defined as a Many-to-Many span and as a mem- ber of the set A M any .</p><p>Thus, for each span &lt; i 1 , i 2 , j 1 , j n &gt;, the Many-to-Many score can be calculated as follows: </p><formula xml:id="formula_7">s(&lt; i1, i2, j1, jn &gt;) =      t(c jn j 1 |e i 2 i 1 ) if &lt; i1, i2, j1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&gt;.</head><p>Finally, the score for the feature of English- Chinese Many-to-Many alignment for w n = c jn jm is derived as follows:</p><p>fMany−to−Many(n) = argmax</p><formula xml:id="formula_8">i 1 ∈[1,I]i 2 ∈[i 1 ,I]j 1 ≤jm s(i1, i2, j1, jn)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transliteration Feature</head><p>To account for named entities (NEs), which suf- fer from sparsity and thus make it difficult to cal- culate the probabilities discussed above, we intro- duce a transliteration feature to evaluate the simi- larities between the pronunciations of Chinese and English words because many NEs are translated via transliteration. To perform this task, we first introduce an initial NE dictionary and convert each dictionary item-for example, we convert "Ow d/Alice" into "ai l i s i/a l i c e" -by transform- ing the Chinese word into its pronunciation (rep- resented by the function F py (·)) and splitting the English word into its constituent letters (represent- ed by the function F let (·)). Then, we train two phrase-based translation models (Chinese-English and English-Chinese) on the data obtained from the converted NE dictionary.</p><p>Specifically, we apply two standard log-linear phrase-based SMT models. The GIZA++ align- er is adopted to obtain word alignments <ref type="bibr" target="#b12">(Och and Ney, 2000</ref>) from the converted NE dictio- nary. The heuristic strategy of grow-diag-final-and ( <ref type="bibr" target="#b10">Koehn et al., 2003</ref>) is used to combine the bidi- rectional alignments to extract phrase translations and to reorder tables. A 5-gram language mod- el with Kneser-Ney smoothing is trained using S- RILM ( <ref type="bibr" target="#b15">Stolcke et al., 2002</ref>) on the target language. Moses ( <ref type="bibr" target="#b11">Koehn et al., 2007</ref>) is used as a decoder. Minimum error rate training (MERT) ( <ref type="bibr" target="#b13">Och et al., 2003</ref>) is applied to tune the feature parameters on the development dataset.</p><p>Given these two phrase-based translation mod- els, we calculate each span &lt; i, j m , j n &gt; in A One for the Chinese word w n using the following for- mula:</p><p>Str(&lt; i, jm, jn &gt;) = S ch−en (&lt; i, jm, jn &gt;) +S en−ch (&lt; i, jm, jn &gt;)</p><p>where S ch−en (&lt;i, j m , j n &gt;) = D Lev (F let e i , P T ch−en (F py (c jn jm ))) means that the pronuncia- tion conversion in the Chinese-English direction is performed as follows: First, the English word e i is split into its constituent letters; Second, the sequence of Chinese characters c jn jm is converted into its pronunciation; Third, this pronunciation is input into the Chinese-English phrase-based trans- lation model, and the corresponding translation re- sult is obtained; And finally, the Levenshtein dis- tance between the English letters and the transla- tion result is returned.</p><p>S en−ch (&lt;i, j m , j n &gt;) can be calculated in ex- actly the same way.</p><p>We set any span that does not belong to A One to zero, and the transliteration feature score of a word w n = c jn jm is derived as follows:</p><formula xml:id="formula_10">f transliteration (n) = argmax i∈[1,I]</formula><p>Str(&lt; i, jm, jn &gt;) (7)</p><p>English-Chinese semantic gap feature To guarantee that the semantic meanings of the Chinese segmentation match those of the corre- sponding English sentences as closely as possible, we propose to use a feature based on the English- Chinese semantic gap to ensure the retention of se- mantic meaning during the segmentation process.</p><p>First, we pre-train word embeddings using the open-source toolkit Word2Vec ( <ref type="bibr" target="#b6">Mikolov et al., 2013</ref>) on the Chinese (segmented using character- level features only) and English sentences sepa- rately, thereby obtaining the vocabularies V ch and V en and their corresponding embedding matrixes L ch ∈ R n×|V ch | and L en ∈ R n×|Ven| . Given a Chi- nese word w n with an index i in the vocabulary, it is then straightforward to retrieve the word's vec- tor representation via simple multiplication with a binary vector d that is equal to zero at all positions except that with index i:</p><formula xml:id="formula_11">Xi = L ch di ∈ R n<label>(8)</label></formula><p>Because the word embeddings for the two lan- guages (L ch and L en ) are learned separately and located in different vector spaces, we suppose that a transformation exists between these two seman- tic embedding spaces. Thus, we collect all the One-to-Many phrase tuples &lt; e 1 , c j 2 j 1 &gt; that sat- isfy e 1 ∈ V en and c j 2 j 1 ∈ V ch from the entire corpus of bilingual data. Then, we insert the word embed- ding tuple of each One-to-Many phrase tuple into the set A embed . Let us consider a word embedding tuple &lt; p s , p t &gt; in A embed as an example. We define a bidirectional semantic distance using the parameter θ as follows:</p><p>Esem(ps, pt; θ) = Esem(ps|pt, θ) + Esem(pt|ps, θ) <ref type="formula">(9)</ref> Here, E sem (p s |p t , θ) = E sem (p t , f (W ch en p s + b ch en )) represents the transformation of p s and is performed as follows: We first multiply a parame- ter matrix W ch en by p s , and after adding a bias term b ch en , we apply an element-wise activation function f = tanh(·). Finally, we calculate their Euclidean distance:</p><formula xml:id="formula_12">Esem(ps|pt, θ) = 1 2 ||pt − f (W ch en ps + b ch en )|| 2<label>(10)</label></formula><p>E sem (p t |p s , θ) can be calculated in exactly the same way.</p><p>Given the definition of the semantic distance of each word-embedding tuple in A embed , we wish to minimize the following objective function:</p><formula xml:id="formula_13">J = &lt;ps,pt&gt;∈A embed E sem (p s , p t ; θ)<label>(11)</label></formula><p>We apply the Stochastic Gradient Descent (S- GD) algorithm to optimize each parameter and ul- timately obtain the optimized parameters θ * .</p><p>Using θ * , we can calculate the semantic gap for any possible span for w n , such as &lt; i, j m , j n &gt;, as follows:</p><formula xml:id="formula_14">Sgap(&lt; i, jm, jn &gt;) =          1 Esem(p s |p t ,θ * )</formula><p>if ei ∈ Ven c jn jm ∈ V ch &lt; i, jm, jn &gt;∈ AOne 0 else</p><p>where p s and p t are the word vector representation of e i and c jn jm , respectively. Thus, the semantic gap feature score of the word w n = c jn jm is derived as follows:</p><p>f sem (w n ) = argmax i∈ <ref type="bibr">[1,I]</ref> S gap (&lt; i, j m , j n &gt;) (13)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Normalization and the Inner Log-Linear Model</head><p>Because the output scores of each sub-model de- scribed above are not probabilistic and they vary by orders of magnitude, we must first normalize the output scores of each sub-model. After nor- malization, the scores have means and standard deviations of zero. We represent the normalization function by N orm(·). Thus, for the substring c j 1 (j ∈ [1, J)) in c J 1 of the sentence tuple &lt;e I 1 , c J 1 , a K 1 &gt;, assuming that one of its candidate segmentations is w N +1</p><formula xml:id="formula_16">1 = w 1 w 2 w 3 ...w N +1 = c j 1 1 c j 2 j 1 +1 ...c j j N +1</formula><p>, the feature score of the inner log-linear model is derived as follows:</p><formula xml:id="formula_17">finner = j ∈[1,j] N orm(fCRF (j ))+ λ1 n∈[1,N +1] ( k N orm(f k (n)))<label>(14)</label></formula><p>where f k (n) represents the phrase-level features. Then, we tune the weight λ 1 from 0 to 1 in equal increments of 0.1 to optimize its value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence-level Features</head><p>In this section, we describe the sentence-level fea- tures calculated using the phrase-based translation model and the outer log-linear model that is used to combine the sentence-level features with the features in the inner log-linear model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Features Obtained from the Phrase-based Translation Model</head><p>Let us consider the last character c J in c J 1 and as- sume that its candidate segmentation (according to the inner log-linear model only) is w N +1 1 = w 1 w 2 w 3 ...w N +1 . We now add a sentence-level feature to incorporate into the inner log-linear model. This sentence-level feature is obtained us- ing a phrase-based translation model. We segmen- t the Chinese sentences from the bilingual unla- beled data using character-level features only and train a phrase-based translation model on the bilin- gual data that is similar to the phrase-based trans- lation model used for the transliteration features.</p><p>Unlike the usage of the phrase-based translation model in the case of the transliteration features, here, we input both the source and target sentences and achieve the output of translation probability. Thus, we perform a force decoding for the sen- tence tuple &lt;w N +1 1 , e I 1 &gt; and obtain the set of de- coding paths P(w N +1 1 ), where each element acts as a decoding path that can translate w N +1 1 into e I</p><p>1 . Finally, we define the sentence-level feature score of &lt;w N +1 1 , e I 1 &gt; as follows:</p><formula xml:id="formula_18">fsent(w N +1 1 ) = argmax p(w N +1 1 )∈P (w N +1 1 ) Ftrans(p(w N +1 1 ))<label>(15)</label></formula><p>where F trans (·) returns the translation score of the given decoding path based on the phrase-based translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">The Outer Log-Linear Model</head><p>Finally, we normalize the sentence-level features in a manner similar to that described previously and construct the outer log-linear model by com- bining the inner log-linear model and the sentence- level features as follows:</p><formula xml:id="formula_19">f outer = f inner + λ 2 N orm(f sent (w N +1 1 )) (16)</formula><p>Then, we also tune the weight λ 2 from 0 to 1 in equal increments of 0.1 to optimize its value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Decoder</head><p>A traditional viterbi beam search procedure is ap- plied in the decoder to seek the segmented se- quence with the highest score. Given a sentence tuple &lt; e I 1 , c J 1 , a K 1 &gt;, the decoding procedure will proceed in a left-right fashion using a dynamic programming approach. At each position j in the sequence c J 1 , we maintain a vector of size N to s- tore the top N candidate segmentations of subse- quence c j 1 which are scored using the inner log- linear model (j ∈ [1, J)) or the outer log-linear model (j = J). Finally, we return the best seg- mentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Justifying the Original CWS Model</head><p>We justify the original CWS model (the CRF- based model trained on manually segmented data) using the new CRF model trained on the segmen- tation of unlabeled bilingual data. To avoid over- weakening the influence of the small-scale manu- ally segmented data, we again utilized a log-linear model to balance their weights. The formula can be described as follows:</p><formula xml:id="formula_20">fnew mono = k 1 λ k 1 f k 1 (yj−1, yj, c J 1 , j) +θ3 k 2 λ k 2 f k 2 (yj−1, yj, c J 1 , j) (17)</formula><p>where θ 3 represents the weights of the second CR- F model, which are set via minimum error rate training using the developing dataset, and λ k i (i =1, 2) represents the learned weights of the fea- tures of the CRF models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Datasets</head><p>In this paper, we conduct our experiments on the corpus of People's daily of 1998 (from January to June) as the standard (manually segmented) train- ing corpus, the corpus of Bakeoff-2 CWS evalua- tion as the developing and testing dataset. As the corpus of Bakeoff-2 is made up of several sets pro- vided by different organizations, we only select t- wo sets whose segmenting standards are similar to the training corpus. For each set, we take 3000 sentences as the developing dataset and the others as the testing dataset. The statistics of every set and the standard training corpus are shown in <ref type="table">Ta</ref> Moreover, the bilingual unlabeled data is formed by a large in-house Chinese-English par- allel corpus <ref type="bibr" target="#b18">(Tian et al., 2014</ref>). There are in total 2,215,000 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In our evaluation, the F-score was used as the ac- curacy measure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percent- age of gold-standard output words that are correct- ly segmented by the decoder. The balanced F-s- core is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as "SLBD" (segmenter leveraging bilingual data).</p><p>Initially, we evaluated state-of-the-art super- vised CWS methods, i.e., those of ( <ref type="bibr" target="#b14">Peng et al., 2004</ref>  segmenting the bilingual unlabeled dataset using character-level features only, the inner log-linear model (which includes character-level and phrase- level features) and the outer log-linear model (the full SLBD approach). After applying these three segmentations using the different sub-models, we trained the new CRF models on the results of the three segmentations to justify the original CWS model. The evaluation results for the supervised CWS methods and the sub-models are presented in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>It can be seen that we achieved significant im- provement in performance when we combined the character-level and phrase-level features in the in- ner log-linear model, demonstrating that the pro- posed phrase-level features can be used to effi- ciently obtain bilingual segmenting information. Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effec- tively re-rank the candidate segmentations pro- duced by the inner log-linear model. Next, we compared the SLBD method with sev- eral state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (S&amp;X); (Zeng et al., 2013b) (Zeng). To ensure a fair comparison, we performed the evaluation in two steps. First, we input the entire bilingual unlabeled dataset into the SLBD method and input only the Chinese sen- tences from the bilingual unlabeled dataset into the other semi-supervised methods. Then, because the available monolingual unlabeled dataset was much larger than the bilingual unlabeled dataset in natu- ral, we used the XIN CMN portion of Chinese Gi- gaword 2.0 as an additional unlabeled dataset for the monolingual semi-supervised methods. which contains 204 million words, more than ten times   the number of words in the bilingual unlabeled dataset. The testing data was the set of AS only.</p><p>The evaluation is summarized in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>The results demonstrate that either leveraging the same unlabeled data or providing a much larg- er unlabeled dataset for the monolingual semi- supervised methods, the SLBD method can sig- nificantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.  <ref type="table" target="#tab_6">Table 4</ref> indi- cate that SLBD demonstrates much stronger per- formance, primarily because these other methods were developed with a focus on SMT, which caus- es them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater gen- eralizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a cascaded log-linear model to involve learning three levels of bilin- gual linguistic features to semi-supervisedly learn a new CWS model. Different from other mono- lingual and bilingual semi-supervised approach- es, we employ various types of features to cap- ture both monolingual grammars and bilingual segmenting information, which allows our mod- el to be very efficient at other NLP tasks and en- dows it with higher generalizability. The evalu- ation shows that our method significantly outper- forms the state-of-the-art monolingual and bilin- gual semi-supervised approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The examples of different segmentation on the same Chinese sentences guided by the English sentences</figDesc><graphic url="image-1.png" coords="2,89.34,62.81,183.60,91.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Finally</head><label></label><figDesc>, we evaluated SLBD in comparison with other bilingual semi-supervised methods, includ- ing (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014). The results presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Word segmentation performance of SLB-
D and supervised CWS methods[%] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Word segmentation performance of SLB-
D and other monolingual semi-supervised CWS 
methods[%] 

methods 
AS 
PKU 
F 
OOV 
F 
OOV 
Xu 
92.8 70.5 92.1 
66 
Ma 
93.1 
73 
92.6 71.1 
Xi 
90.2 
63 
90.9 67.2 
Zeng2014 93.5 
76 
93.2 73.3 
SLBD 
96.7 80.8 97.1 
85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Word segmentation performance of SLB-
D and other bilingual semi-supervised CWS meth-
ods[%] 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combination of machine learning methods for optimum chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Fukuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chooiling</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotaro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahashi</forename><surname>Tsuzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Fourth SIGHAN Workshop</title>
		<meeting>The Fourth SIGHAN Workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="134" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing Chinese word segmentation for machine translation performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised Tokenization for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="718" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th International Conf. on Machine Learning</title>
		<meeting>18th International Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">Master.s thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilingually motivated domain-adapted word segmentation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="549" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Unsupervised Model for Joint Phrase Alignment and Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine Translation without Words through Substring Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2012</title>
		<meeting>ACL 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised conditional random fields for improved sequence segmentation and labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Josef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">UM-Corpus: a large EnglishChinese parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation. ELRA</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation. ELRA<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhancing statistical machine translation with character alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do we need Chinese word segmentation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third SIGHAN Workshop on Chinese Language Learning</title>
		<meeting>the Third SIGHAN Workshop on Chinese Language Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="122" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A unified character-based tagging framework for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<idno>5:1-5:32</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph-based SemiSupervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-regularizing characterbased and word-based models for semi-supervised Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1360" to="1369" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
