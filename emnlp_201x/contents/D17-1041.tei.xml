<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotated Word Vector Representations and their Interpretability</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyeong</forename><surname>Bak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rotated Word Vector Representations and their Interpretability</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="401" to="411"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Vector representation of words improves performance in various NLP tasks, but the high-dimensional word vectors are very difficult to interpret. We apply several rotation algorithms to the vector representation of words to improve the interpretabil-ity. Unlike previous approaches that induce sparsity, the rotated vectors are in-terpretable while preserving the expressive performance of the original vectors. Furthermore , any pre-built word vector representation can be rotated for improved in-terpretability. We apply rotation to skip-grams and glove and compare the expressive power and interpretability with the original vectors and the sparse overcom-plete vectors. The results show that the rotated vectors outperform the original and the sparse overcomplete vectors for inter-pretability and expressiveness tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vector representations of words contain rich se- mantic and syntactic information and thus improve the performance of numerous natural language processing tasks. The vectors also play a basic role as an embedding layer in deep learning models for NLP, affecting the expressive performance of the model ( <ref type="bibr" target="#b17">Iyyer et al., 2014;</ref><ref type="bibr" target="#b39">Tai et al., 2015;</ref><ref type="bibr" target="#b44">Yang et al., 2016)</ref>. However, the many dimensions com- prising the vector representation are not amenable to interpretation.</p><p>Previous research on vector representation of words has proposed improving interpretability while keeping the expressive performance by in- ducing sparsity in word vector dimensions <ref type="bibr" target="#b29">(Murphy et al., 2012;</ref><ref type="bibr" target="#b11">Fyshe et al., 2014</ref>). Recent re- search has proposed to build sparse vector repre- sentations from a large corpus and added the non- negativity constraint using improved projected gradient ( <ref type="bibr" target="#b25">Luo et al., 2015</ref>), while ( <ref type="bibr" target="#b38">Sun et al., 2016</ref>) learns l1-regularised vectors. But, these models cannot be learned over pre-trained word vectors based on skip-gram ( <ref type="bibr" target="#b28">Mikolov et al., 2013</ref>) or glove ( <ref type="bibr" target="#b34">Pennington et al., 2014</ref>) which are widely used. <ref type="bibr">Faruqui et al.</ref> proposes an alternative approach to stand-alone models by forming sparse representa- tions based on the pre-trained models. To do this, they use overcomplete vectors, which are much higher in dimensionality than the original vectors.</p><p>Unlike these sparsity-inducing approaches, we construct an interpretable word vector representa- tion by using the pre-trained word vectors as in- put and using a basis rotation algorithm from the Exploratory Factor Analysis (EFA) literature used in developing psychological scales <ref type="bibr" target="#b33">(Osborne and Costello, 2009)</ref>. Like the word vector representa- tion, every single item in the scale is represented as a numeric vector in the latent factor space. The set of item vectors are represented in a factor loading matrix, and the matrix is rotated such that the fac- tors (i.e., dimensions) become interpretable. The rotation achieves a Simple Structure <ref type="bibr" target="#b40">(Thurstone, 1947)</ref> through minimizing the row and the column complexity of the matrix <ref type="bibr" target="#b8">(Crawford and Ferguson, 1970)</ref>. We elaborate on this process in the next section. As in EFA, we rotate the word vector rep- resentation matrix to obtain dimension-wise inter- pretability while retaining the number of dimen- sions the same. For example, <ref type="figure" target="#fig_1">Figure 1</ref> shows the rotated skip-gram vectors for two groups of words. These words are top five words of two dimensions from rotated Word2Vec.</p><p>Our main contribution is applying the matrix rotation algorithm from psychometric analysis to word vector representation models to improve the interpretability of the vector. This approach gives an answer to the question why and how word vec- tor representations work well by revealing a hid- den structure of the original word vectors. That is, it is meaningful to transform the hard-to-interpret dimensions of the pre-built word vectors, which are widely used, to more interpretable vectors. We also show that the rotated vectors retain their effec- tiveness with respect to downstream tasks without re-building the vector representations. Our method can be applied to any type of word vectors as a post-processing method such that it does not require a large corpus to be trained. In addition, it does not require additional number of dimensions so it does not increase the complexity of the model. Furthermore, we explore the charac- teristics of the rotated word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Factor Rotation</head><p>We take the rotation algorithm from the ex- ploratory factor analysis (EFA) conducted to ver- ify the construct validity of the psychological scale in development. For example, when validating a scale measuring respondents' latent factors, such as "Engineering problem solving" and "Interest in engineering", items should be similar within a fac- tor, and distinguished between factors. As shown in <ref type="table">Table 1</ref>, EFA projects every item into the latent factor space as an unrotated factor loading matrix. However, since it is unclear what the factor means, factor rotation is applied to the matrix that pro- duces the rotated factor loading matrix which en- hances the interpretability of the dimensions (Os- borne, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rotating Factors</head><p>The rotation algorithm transforms factor loading matrix to the simple structure which is much eas- ier to interpret <ref type="bibr" target="#b40">(Thurstone, 1947</ref>  <ref type="table">Table 1</ref>: An example of the factor rotation process to verify the construct validity of the psychological scale and its intended latent factor (left) in development. Items and loadings are from <ref type="bibr" target="#b32">(Osborne, 2015)</ref>.</p><p>which minimizes the cost function f (Î›), also known as the rotation criterion. The function min- imizes the complexity of the matrix, to make the rotated matrix have a few large values in a row or a column. Minimizing the complexity allows non-binary values in the vector, and thus a more complex so- lution that the perfect simple structure. This is a more realistic solution since a solution with binary vectors may be misleading in representing the fac- tor of interest <ref type="bibr" target="#b45">(Yates, 1988;</ref><ref type="bibr" target="#b1">Browne, 2001</ref>). More details are described in the next subsection.</p><p>The intuition behind this approach is that induc- ing interpretability by factor rotation reforms the word embedding matrix to have a simple struc- ture by linear transformation. It encourages each word vector (row) and dimension (column) to have a few large values, leading to more interpretable dimensions as shown in Fig 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Crawford-Ferguson Rotation Family</head><p>The rotation criterion introduced in Crawford and Ferguson is a family of complexity functions as follows:</p><formula xml:id="formula_0">f (Î›) = (1 âˆ’ Îº) Î£ p i=1 Î£ m j=1 Î£ m l =j,l=1 Î» 2 ij Î» 2 il + Îº Î£ m j=1 Î£ p i=1 Î£ m l =i,l=1 Î» 2 ij Î» 2 lj (2)</formula><p>where Î» ij is an element of Î›. The first term rep- resents the row (item) complexity, and the sec- ond term represents the column (factor) complex- ity. The ratio between the two is adjusted by the parameter Îº (0 â‰¤ k â‰¤ 1). The criterion is a generalized version of the widely used criteria, the orthomax family <ref type="bibr" target="#b12">(Harman, 1960)</ref> which in- cludes quartimax <ref type="bibr" target="#b6">(Carroll, 1953;</ref><ref type="bibr" target="#b10">Ferguson, 1954;</ref><ref type="bibr" target="#b30">Neuhaus and Wrigley, 1954)</ref>, varimax <ref type="bibr" target="#b21">(Kaiser, 1958)</ref>, and direct quartimin <ref type="bibr" target="#b5">(Carroll, 1960)</ref>. It effectively reflects the simple structure as well <ref type="bibr" target="#b1">(Browne, 2001)</ref>. In this work, we apply the fol-  <ref type="bibr" target="#b36">and Schmitt, 2010)</ref>, we use 4 criterion referred to as CF-Quartimax, CF-Varimax, CF-Parsimax, CF-FacParsim. We omit 'CF-' for simplicity and do not separate the name of the kappa condition whether it is orthogonal or oblique. FacParsim stands for factor parsimony.</p><formula xml:id="formula_1">Quartimax Varimax Parsimax FacParsim Îº 0 1 p m âˆ’ 1 p + m âˆ’ 2 1</formula><p>lowing representative Îº values in <ref type="table" target="#tab_1">Table 2</ref> ( <ref type="bibr" target="#b36">Sass and Schmitt, 2010)</ref>. In addition, the constraints for the rotation ma- trix T can be applied in general. We can catego- rize the rotation as orthogonal and oblique based on the constraint. Orthogonal rotation assumes the correlation between the rotated dimensions is zero. Hence, the matrix should be an orthogonal matrix that with m(m âˆ’ 1)/2 constraints, satisfies:</p><formula xml:id="formula_2">T T = I (3)</formula><p>Oblique rotations allow the correlation between dimension to be non-zero, resulting in m con- straints satisfying:</p><formula xml:id="formula_3">diag(T âˆ’1 T âˆ’1 ) = I (4)</formula><p>The solution for the input matrix is computed by using the gradient projection algorithm <ref type="bibr" target="#b18">(Jennrich, 2001</ref><ref type="bibr" target="#b19">(Jennrich, , 2002</ref>). The algorithm minimizes equation 2 while satisfying the constraints of the rotation ma- trix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head><p>We choose the Wikipedia English articles 1 to train the word vector models. The corpus contains 5.3M articles, 83M sentences and 1,676M tokens. For preprocessing, we leave only the alphanumeric to- kens and apply lowercase to all words. Then we remove the words with frequency less than 50, and the size of the remaining vocabulary is 306,491.</p><p>We train skip-gram 2 (Mikolov et al., 2013) and glove <ref type="bibr">3 (Pennington et al., 2014</ref>) based on the cor- pus by using existing implementations. We set the window size to 5 for both skip-gram and glove. We set the number of negative samples to 5 and the number of dimensions to 300. We use the default values for the other hyperparameters. The size of the resulting word vector matrix is <ref type="bibr">(306,</ref><ref type="bibr">491,</ref><ref type="bibr">300)</ref>.</p><p>We compare our model with two baseline mod- els: sparse overcomplete vector representations (SOV) and the non-negative version of the SOV. We set the hyperparameters of these models as Î» = .5, Ï„ = 10 âˆ’5 , K = 3000 for SG, and Î» = 1.0, Ï„ = 10 âˆ’5 , K = 3000 for Glove <ref type="bibr" target="#b9">(Faruqui et al., 2015</ref>). We excluded methods as baselines that construct interpretable word vectors using huge training corpora because our method works with pre-trained vectors.</p><p>We apply four rotation algorithms for each or- thogonal and oblique rotation, listed in <ref type="table" target="#tab_1">Table 2</ref>. Since we have two original word vector represen- tations, we have 16 (4 x 2 x 2) rotated vectors in total. We implement the algorithm through Tensor- Flow ( <ref type="bibr" target="#b0">Abadi et al., 2016)</ref>, and it is publicly avail- able on GitHub 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interpretability</head><p>In this section, we show how the rotation of word vectors results in improved dimension-wise inter- pretability using the word intrusion task. ( <ref type="bibr" target="#b29">Murphy et al., 2012;</ref><ref type="bibr" target="#b9">Faruqui et al., 2015;</ref><ref type="bibr" target="#b38">Sun et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Intrusion</head><p>Word intrusion task seeks to measure the semantic coherence of a set of words. For example, consider a set of words consists of ('daughter', 'wife', 'sis- ter', 'mother', 'son') and add an 'intruder' word ('bigram') to the set. Since the words except in- truder has similar meanings to each other, we can easily pick out the intruder to conclude that the five words are sharing coherent meanings.</p><p>We apply this task to measure interpretability of every word vector dimensions. If we choose the words with the highest embedding values for each of the dimensions (top words for that dimen- sion) and add an random (intruder) word and see whether the intruder can be easily identified, then we can conclude the dimension is semantically co- herent. In this way, we can measure the extent of interpretability of a dimension in vector represen- tations by this task. Note that we pick top words for a dimension by looking only for the value of that dimension, ignoring values in the other di- mensions.</p><p>Specifically, we first choose the top five words in each dimension, and then we choose an intruder word based on two criteria: 1) it is in the lower half of that dimension, and 2) it is in the top 10% in some other dimension. Also, we follow the set- tings of the measure (k = 5, top 10%) from pre- vious works. We see similar results when we run experiments with larger k. ( <ref type="bibr" target="#b29">Murphy et al., 2012;</ref><ref type="bibr" target="#b38">Sun et al., 2016)</ref> In the standard word intrusion task, human eval- uators pick out the intruder words, and the results report the accuracy of the evaluators ( <ref type="bibr" target="#b7">Chang et al., 2009)</ref>. But this approach would be impractical to use for all experimental conditions with 300 di- mensions and the baselines, so we use the follow- ing distance ratio (DR) metric as an alternative ap- proach in <ref type="bibr" target="#b38">(Sun et al., 2016</ref>) with slight modifica- tions. Another advantage of our metric is that it can be used to quantify the distance between the intruder and the non-intruder words. We define the overall metric as the average of the ratio between D a inter and D a intra over d dimensions as</p><formula xml:id="formula_4">DR overall = 1 d Î£ d a=1 D a inter Î£ d a=1 D a intra (5)</formula><p>where D a intra is the average distance of every pair among the top k words in dimension a</p><formula xml:id="formula_5">D a intra = Î£ w i Î£ w j dist(w i , w j ) k(k âˆ’ 1) ,<label>(6)</label></formula><p>and D a inter is the average distance between the in- truder word and each of the top k words in dimen- sion a</p><formula xml:id="formula_6">D a inter = Î£ w i dist(w i , w intruder ) k .<label>(7)</label></formula><p>We define dist(w j , w k ) as the cosine distance be- tween w j and w k . We set k = 5 and repeat this three times for each dimension a and use the aver- age to compute DR overall .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glove</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>1.258 1.095 SOV 1.089 1.050 SOV (non-neg)</p><p>1.081 1.074 Quartimax (orthogonal) 1.479 1.248 Varimax (orthogonal)</p><p>1.477 1.289 Parsimax (orthogonal)</p><p>1.596 1.261 FacParsim (orthogonal) 1.300 1.102 Quartimax (oblique)</p><p>1.385 1.225 Varimax (oblique)</p><p>1.398 1.222 Parsimax (oblique)</p><p>1.386 1.174 FacParsim (oblique)</p><p>1.145 1.081 <ref type="table">Table 3</ref>: Overall distance ratio (DR overall ) of the original, sparse overcomplete vectors, and the rotated (orthogonal and oblique) vector repre- sentations. Rotated vectors show improved inter- pretability over SOV and the original. <ref type="table">Table 3</ref> shows the results of word intrusion in terms of the distance ratio metric. Overall, the re- sults of the rotated vector representations show improvements over SOV and the original word vector representations. For skip-grams, orthogonal parsimax shows the best result while for Glove, or- thogonal varimax outperforms the others. Among oblique rotation, varimax and quartimax show bet- ter performance than factor parsimony. In general, interpretability varies with different values of Îº. It increases when Îº is close to zero and decreases when Îº is close to one, putting more weight on the column complexity. Also, orthogo- nal rotation shows better performance than oblique rotation when Îº is controlled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Examples</head><p>We present the top words of five dimensions for skip-gram and rotated skip-gram (parsimax- orthogonal) in <ref type="table">Table 4</ref>. The dimensions shown are randomly selected for both conditions. Overall, the top words in each dimension of skip-gram do not clearly show a common topic among them. Only a few dimensions out of 300 are interpretable, such as the second row in the ta- ble which is related to numbers. The overall dis- tance ratio of the original vectors is slightly higher than one.</p><p>For the rotated word vectors, the top words show clear semantic coherence. The first row shows words about social network services, the <ref type="table">Table 4</ref>: 5 top words for the original and the ro- tated skip-gram word representations. The rotated vectors show common semantic or syntactic co- herence while the original vectors do not.</p><p>second row is about biology, the third row is about geographical locations in the US, and the fourth is about paintings. As the last row shows, some of these dimensions represent syntactic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expressive Performance</head><p>We evaluate the expressive power of word vec- tor representations on the following tasks and re- port Spearman's correlation coefficient for the first task, and accuracy for the other tasks. <ref type="table">Table 5</ref> shows the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation</head><p>We briefly describe the seven benchmark tasks: word similarity and semantic/syntactic analogy, and four classification tasks. For the classification tasks, we average the word vectors in each training sentence or phrase to use them as features. SVM and random forest classifier are trained to predict the target values, and hyperparameters are tuned on the validation set.</p><p>Word Similarity (Simil.) SimLex-999 ( <ref type="bibr" target="#b15">Hill et al., 2016)</ref> presented to evaluate the similarity of word pairs, rather than relatedness. We compute the cosine similarity between the given word pairs, and report the Spearman's correlation coefficient as a measure of consistency between the similar- ity and human ratings.</p><p>Semantic and Syntactic Analogies (Analg. sem, syn). The second and third tasks are word analogy tasks proposed by <ref type="bibr" target="#b28">(Mikolov et al., 2013)</ref>. The semantic task includes 8,869 questions (sem) and the syntactic task includes 10,675 questions (syn).</p><p>Sentiment Analysis (Sent.) The first classifica- tion task is sentiment classification on the movie reviews ( <ref type="bibr" target="#b37">Socher et al., 2013</ref>  <ref type="table">Table 5</ref>: Evaluation results of the original skip-gram, sparse overcomplete vectors (SOV), and the rotated (orthogonal and oblique) word vectors on various tasks. The left three columns show tasks based on cosine similarity, and the right four columns show classification tasks using average word vectors as features. Overall, the rotated word vectors show higher or comparable performance to that of the SOV and the original. We observe a similar pattern in Glove as well.</p><p>6,920, 872, 1,821 sentences for training, develop- ment, and test, respectively. The goal of this task is to predict positive or negative sentiment of the reviews. Question Classification (Ques.) Next, we use TREC dataset to classify categories of the ques- tions ( <ref type="bibr" target="#b9">Faruqui et al., 2015)</ref>. We divide the dataset into 4,952, 500, 500 for training, development, and test. The dataset has six types of questions includ- ing about person, location, etc.</p><p>Topic Classification (Topics: Sp.) Next, we ob- tain the 20 newsgroup dataset to classify Sports (baseball vs. hockey) topics <ref type="bibr" target="#b46">(Yogatama and Smith, 2014;</ref><ref type="bibr" target="#b9">Faruqui et al., 2015</ref>). The dataset consists of 958, 239, 796 for training, development, and test.</p><p>NP bracketing (NP brckt.) The final task is classifying noun phrases in terms of bracketing ( <ref type="bibr" target="#b23">Lazaridou et al., 2013;</ref><ref type="bibr" target="#b9">Faruqui et al., 2015)</ref>. Each phrase consists of three words, and the task is to predict the correct bracketing to match the similar words. We compute the average of NPs and per- form ten-fold cross-validation over 2,227 phrases. The classifiers are trained and the hyperparameters are tuned for every fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Word Similarity and Analogies We observe im- proved performance of oblique rotation of word vectors compared to the original and the SOV in word similarity and semantic analogy tasks. In the syntactic analogy, orthogonal rotation shows the same performance as the original. Note that the orthogonal rotations preserve the cosine-based ex- pressive performances because the cosine similar- ity between any two vectors does not change after the orthogonal rotation.</p><p>Classification Tasks The SOV models show slightly higher performance except the question classification task. However, we can observe the rotated word vectors have improved performance over the original vectors. We observe a similar pat- tern in Glove as well. In conclusion, the rotated representations preserve the expressive power of the original word vectors, and it is quite close to that of the sparse representation with 10 times larger dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Understanding Rotated Word Vectors</head><p>In this section, we perform several experiments to understand the characteristics of the rotated word vector representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Directionality</head><p>One conventional approach to make the word vec- tors to be more interpretable is by forcing the rep- resentation to have non-negative values <ref type="bibr" target="#b9">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b25">Luo et al., 2015)</ref>. However, the dimen- sions in the rotated vectors are not non-negative, spread in both directions. Hence, we investigate the relationship between the directionality (posi- tive / negative) and interpretability.</p><formula xml:id="formula_7">(A) (B) (C) (D) (E) (F) (G) Desc (Hi) Asc (Lo) Cor (Hi, Lo) Cor (abs, DR)</formula><p>Cor (abs, intra) Cor (abs, inter) DR (abs) Quartimax (orthogonal) 1.479 1.507 -.452*** .843*** -.835*** .204*** 2.045 Varimax (orthogonal)</p><p>1.477 1.478 -.431*** .847*** -.840*** .205*** 2.004 Parsimax (orthogonal)</p><p>1  <ref type="table">Table 6</ref>: Overall distance ratio based on the top words extracted from the values in word vectors sorted by descending order (Hi) and ascending order (Lo). Cor(Hi, Lo) is correlation between two distance ratios based on both directions. Next three columns present correlation between the absolute word vector values of the top words and distance ratios. The last columns shows selective distance ratio measure. The results implies generally both direction is interpretable, one direction is more interpretable than the other within a dimension, and larger absolute value in a dimension means higher interpretability. (* p &lt; .05, ** p &lt; .01, *** p &lt; .001)</p><p>Overall Interpretability of both directions The first two columns (A) and (B) in <ref type="table">table 6</ref> show the overall distance ratio computed over the top words extracted by descending order and ascend- ing order, respectively. In other words, (A) refers to the top words having the highest positive values in each dimension, while (B) uses the lowest neg- ative values. Note that we used descending order in word intrusion task in the previous section.</p><p>Interestingly, the overall distance ratios in both directions are comparable to each other. On av- erage, both sides of a dimension are more inter- pretable than the unrotated vector representations except the oblique factor parsimony rotation.</p><p>Interpretability of both directions within a dimension Next, we compare the interpretability of both directions within a dimension. We first de- fine the distance ratio of an individual dimension a as follows:</p><formula xml:id="formula_8">DR a = D a inter D a intra<label>(8)</label></formula><p>We compute the ratio by using top words ex- tracted from positive and negative directions for every dimension, and compute Spearman's corre- lation of the distance ratio pairs. <ref type="table">Table 6</ref> column (C) shows the results. All of the rotation condi- tions except the oblique factor parsimony shows significant (p &lt; .05) negative correlation, mean- ing that both directions are hard to be highly inter- pretable within a dimension simultaneously.  <ref type="table">Table 7</ref>: Examples of top words in both directions. The words are extracted from a part of the orthog- onal parsimax rotated skip-gram word vectors.</p><p>Case Study We present the top words in both directions for some dimensions of orthogonal par- simax rotated word vectors. As shown in table 7, some dimensions show a relationship between the opposite directions that they consist of consecu- tively used words, such as "rely on", "depends upon", "which includes", "that contains", "many years", "weeks ago". However, other dimensions show that one direction is relatively more inter- pretable than the other direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Selecting the Direction</head><p>Next, it is natural to question whether the larger absolute value in word vectors means higher inter- pretability, regardless of its directionality. We ver- ify the relation between them by investigating the size of the absolute value in a dimension and the individual distance ratios. <ref type="table">Table 6</ref> column (D) presents Spearman's correlation between individ- ual distance ratio and the mean absolute vector value of top words for that dimension. The fifth column (E) also shows the correlation between the intra-distance among the top words and the mean absolute value, and the sixth column (F) is the rela- tionship of the inter-distance among the top words and the intruder and the mean absolute value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to distance ratio</head><p>Correlation coefficients show that the larger mean absolute value means higher interpretability for that dimension. In detail, there exists tenden- cies that larger mean absolute value of dimension reduces the intra-distances among the top words while increasing the inter-distances among the top words and the intruder.</p><p>Overall, we summarize our findings as follows: 1) generally both directions are somewhat inter- pretable, 2) one direction is usually more inter- pretable than the other within a dimension, and 3) a larger absolute value in a dimension means higher interpretability of the dimension.</p><p>Selective Distance Ratio We can select a more interpretable direction for each dimension through inspecting the mean absolute value of the top words in both directions. If we choose a direction that has a larger mean absolute value among the top words, each dimension should be easier to in- terpret. <ref type="table">Table 6</ref> column (G) presents this distance ratio computed on the rotated vectors, resulting in in- creased distance ratio values. We name this ratio as the overall selective distance ratio. This mea- sure could be effectively used when vector repre- sentation is interpretable in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of Îº</head><p>We explore the effect on performance of the ratio between the row and the column complexity of the rotation criteria. As shown in section 4, choosing an appropriate Îº is important for interpretability.</p><p>We set the Îº value from zero to one and the numbers divided on a log scale. We run the word similarity task and the word intrusion to evaluate the performance. We present Spearman's correla- tion and the selective overall distance ratio. <ref type="figure">Figure 2</ref> shows that the performance of the sim- ilarity task tends not to change regardless of Îº, however, the selective distance ratio starts to de- crease when Îº &gt; .01. Considering the ratio be- tween the number of rows and columns of the  Figure 2: Spearman's correlation of the word simi- larity and the selective distance ratio of word intru- sion changes over Îºs, computed over oblique ro- tated (a) skip-gram and (b) glove vectors. Dashed line is original performance for each task. Word similarity does not change regardless of Îºs, while the distance ratio falls when Îº is larger than 1e-4.</p><p>word vector matrix, giving too much weight to the column complexity results in degraded inter- pretability.</p><p>In our experiments, Îº values of the quartimax, varimax, and parsimax rotation are computed as 0, 3e-06, 1e-04 respectively. Based on the results, our selection of kappas have shown interpretabil- ity improvement effectively, compared to factor parsimony (Îº = 1). We observe these tendencies in orthogonal rotations as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of the Number of Dimensions</head><p>To investigate the effect of the number of di- mensions to interpretability of dimensions, we also measure the overall distance ratio (DR overall ) on 50, 100 and 200 dimensions of unrotated skip-gram and parsimax (orthogonal) and varimax (oblique) rotated word vectors. <ref type="figure" target="#fig_2">Figure 3</ref> shows the results. For all settings, the rotated vectors orthogonal (parsimax) and oblique (varimax) show higher DR overall score than the original skip-gram vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Since distributed representations play an impor- tant role in various NLP tasks, they are applied to semantics <ref type="bibr" target="#b14">(Herbelot and Vecchi, 2015;</ref><ref type="bibr" target="#b35">Qiu et al., 2015;</ref><ref type="bibr" target="#b43">Woodsend and Lapata, 2015)</ref>, with incor- porating external information to them <ref type="bibr" target="#b41">(Tian et al., 2016;</ref><ref type="bibr" target="#b31">Nguyen et al., 2016</ref>). In addition, finding in- terpretable regularities from the representations is often conducted through non-negative and sparse coding ( <ref type="bibr" target="#b29">Murphy et al., 2012;</ref><ref type="bibr" target="#b9">Faruqui et al., 2015;</ref><ref type="bibr" target="#b25">Luo et al., 2015;</ref><ref type="bibr" target="#b22">Kober et al., 2016)</ref>, and regular- ization ( <ref type="bibr" target="#b38">Sun et al., 2016)</ref>. Instead, our approach is using rotation, showing better results in terms of interpretability. Meanwhile, various rotation methods are pro- posed such as CF-family <ref type="bibr" target="#b8">(Crawford and</ref><ref type="bibr">Ferguson, 1970), Infomax (McKeon, 1968)</ref>, Minimum Entropy <ref type="bibr" target="#b20">(Jennrich, 2006</ref>), Geomin <ref type="bibr" target="#b45">(Yates, 1988)</ref>, procrustues <ref type="bibr" target="#b16">(Hurley and Cattell, 1962)</ref>, and pro- max rotation criteria. <ref type="bibr" target="#b13">(Hendrickson and White, 1964)</ref>. Incorporating prior knowledge about ro- tated matrix is possible through target rotations <ref type="bibr" target="#b12">(Harman, 1960;</ref><ref type="bibr">Browne, 1972a,b)</ref> are proposed as well. There are various ways to rotated dimen- sions, we select a CF-family that covers frequently used rotation methods in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Discussions</head><p>In this paper, we applied the rotation algorithm to improve interpretability of distributed represen- tation of words. We applied quartimax, varimax, parsimax and factor parsimony rotation by using the Crawford-Ferguson rotation criteria, then we constructed the rotated word vector representa- tions. We evaluated the expressive performance and interpretability for the rotated word vectors by word similarity, analogy, classification, and word intrusion task. The results show that the ro- tated word vector representations are highly inter- pretable with preserving expressive performance.</p><p>In addition, we explored the characteristics of the rotated word vectors: we observed 1) increased interpretability in both directions and 2) the posi- tive relation between absolute value of the dimen- sion and interpretability. Based on these observa- tions, we proposed the selective distance ratio to measure and maximize the interpretability when the vector representation has interpretable mean- ing in both directions. We expect that the rotation algorithm can be easily applied to other word vec- tor representations.</p><p>Our results imply that a rotated word vector can be used to understand what the word vectors are comprised of. Since a lexicon can be decomposed into morphemes, a word can have multiple mean- ing as a polysemy, contain information of syntac- tic structure in its meaning <ref type="bibr" target="#b4">(Carpenter et al., 1995;</ref><ref type="bibr" target="#b26">MacDonald et al., 1994;</ref><ref type="bibr" target="#b42">Trueswell et al., 1994)</ref>, or it can be divided into a variety of sub-components. Hence, we can investigate the lexical semantics of words by exploring the dimensions for which a word has higher values.</p><p>In addition, there are practical implications of interpreting the dimensions as well. Based on the meanings, we can remove irrelevant dimensions for a specific task of interest, in order to secure more efficient storage of the vectors and decrease the complexity of downstream NLP models. We will examine the issues in future work.</p><p>We plan to explore following issues. First, we apply target rotation <ref type="bibr" target="#b12">(Harman, 1960;</ref><ref type="bibr">Browne, 1972a,b)</ref> to incorporate prior knowledge when constructing the rotated word vector representa- tions. Second, we will investigate the interpretabil- ity of hidden structures of neural networks for NLP tasks such as ( <ref type="bibr" target="#b44">Yang et al., 2016;</ref><ref type="bibr" target="#b24">Li et al., 2016)</ref>, when the rotated word vectors are used as an embedding layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Figure 1: Overview of rotating word vectors dimensions. We plot (a) unrotated and (b) rotated skipgram word vectors in 2-D projected embedding space using PCA (left), and visualization of the vectors in original 300 dimensional space (right). Colors of words indicates the meaning of countries (Red) and positions (Blue). As in (b), after the dimensions are rotated, interpretability for each dimensions is improved having meaning of countries and positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall distance ratio (DR overall ) over word vector dimensions. The rotated vectors (parsimax-orthogonal and varimax-oblique) show higher DR overall score than the original skip-gram vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Representative Îº values used. As (Sass 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>). This dataset contains</figDesc><table># 
dims 
Simil. 
Analg. 
(sem) 
Analg. 
(syn) 
Sent. Ques. 
Topics 
(Sp.) 
NP 
brckt. 
Skip-Gram 
300 
.374 
.668 
.652 .741 
.920 
.960 
.812 
SOV 
3000 
.390 
.640 
.594 .751 
.910 
.955 
.836 
SOV (non-neg) 
3000 
.384 
.566 
.480 .761 
.918 
.960 
.829 
Quartimax (orthogonal) 
300 
.374 
.668 
.652 .744 
.922 
.956 
.822 
Varimax (orthogonal) 
300 
.374 
.668 
.652 .744 
.922 
.956 
.822 
Parsimax (orthogonal) 
300 
.374 
.668 
.652 .744 
.922 
.956 
.819 
FacParsim (orthogonal) 
300 
.374 
.668 
.652 .744 
.922 
.956 
.822 
Quartimax (oblique) 
300 
.422 
.673 
.624 .755 
.932 
.955 
.820 
Varimax (oblique) 
300 
.422 
.673 
.624 .755 
.932 
.955 
.820 
Parsimax (oblique) 
300 
.421 
.671 
.623 .752 
.932 
.956 
.826 
FacParsim (oblique) 
300 
.417 
.660 
.620 .751 
.928 
.952 
.820 

</table></figure>

			<note place="foot" n="1"> https://dumps.wikimedia.org/enwiki/20170120/</note>

			<note place="foot" n="2"> https://radimrehurek.com/gensim 3 https://nlp.stanford.edu/projects/glove 4 https://github.com/SungjoonPark/factor rotation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) <ref type="bibr">(No. 2016R1A2B4016048)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">MartÃ­n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An overview of analytic rotation in exploratory factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael W Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="150" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oblique rotation to a partially specified target</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Orthogonal rotation to a partially specified target</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="120" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language comprehension: Sentence and discourse processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Adam</forename><surname>Miyake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="120" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ibm 704 program for generalized analytic rotation solution in factor analysis. Unpublished manuscript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="page">324</biblScope>
			<pubPlace>Harvard University, 9</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analytical solution for approximating simple structure in factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carroll</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general rotation criterion and its use in orthogonal rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George A</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="332" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The concept of parsimony in factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George A Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interpretable semantic vectors from a joint model of brain-and text-based meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Partha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Modern factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Promax: A quick method for rotation to oblique simple structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">E</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Owen White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British journal of statistical psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a shared world: mapping distributional to modeltheoretic semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">AurÃ©lie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The procrustes program: Producing direct rotation to test a hypothesized factor structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond B</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems Research and Behavioral Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="258" to="262" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Political ideology detection using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Enns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple general procedure for orthogonal rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert I Jennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="306" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple general method for oblique rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert I Jennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rotation to simple loadings using component loss functions: The oblique case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert I Jennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The varimax criterion for analytic rotation in factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="200" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving sparse word representations with distributional inference for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fish transporters and miracle homes: How compositional distributional semantics can help np parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative topic embedding: a continuous representation of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online learning of interpretable word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Bo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The lexical nature of syntactic ambiguity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maryellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">676</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rotation for maximum association between factors and tests. Unpublished manuscript, Biometric Laboratory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mckeon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
		<respStmt>
			<orgName>George Washington University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning effective and interpretable semantic models using non-negative sparse embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of COLING</title>
		<meeting>eddings of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The quartimax method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wrigley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Kim Anh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What is rotating in exploratory factor analysis. Practical Assessment, Research &amp; Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason W</forename><surname>Osborne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">B</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pan-Pacific Management Review</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="146" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Syntactic dependencies and distributed word representations for chinese analogy detection and mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparative investigation of rotation criteria within exploratory factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas A</forename><surname>Sass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="103" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparse word embeddings using l1 regularized online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multiple factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louis Leon Thurstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning semantically and additively compositional distributional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John C Trueswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">M</forename><surname>Michael K Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of memory and language</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed representations for unsupervised semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multivariate exploratory data analysis: A perspective on exploratory factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Yates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Suny Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
