<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transition-Based Dependency Parsing with Heuristic Backtracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
							<email>jacobbuckman@cmu.edu, miguel.ballesteros@upf.edu, cdyer@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">♦ NLP Group</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pompeu Fabra University</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transition-Based Dependency Parsing with Heuristic Backtracking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2313" to="2318"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel approach to the decoding problem in transition-based parsing: heuris-tic backtracking. This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search. This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions. When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transition-based parsing, one of the most prominent dependency parsing techniques, constructs a depen- dency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure. Transition-based parsing has been shown to be both fast and accurate; the number of transi- tions required to fully parse the sentence is linear relative to the number of words in the sentence.</p><p>In recent years, the field has seen dramatic im- provements in the ability to correctly predict tran- sitions. Recent models include the greedy Stack- LSTM model of <ref type="bibr" target="#b5">Dyer et al. (2015)</ref> and the globally normalized feed-forward networks of <ref type="bibr" target="#b0">Andor et al. (2016)</ref>. These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important compo- nent of high-accuracy parsers.</p><p>One common search technique is beam search.</p><p>( <ref type="bibr" target="#b13">Zhang and Clark, 2008;</ref><ref type="bibr" target="#b14">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b2">Bohnet and Nivre, 2012;</ref><ref type="bibr" target="#b15">Zhou et al., 2015;</ref><ref type="bibr" target="#b10">Weiss et al., 2015;</ref><ref type="bibr" target="#b12">Yazdani and Henderson, 2015)</ref> In beam- search, a fixed number of candidate transition se- quences are generated, and the highest-scoring se- quence is chosen as the answer. One downside to beam search is that it often results in a significant amount of wasted predictions. A constant number of beams are explored at all points throughout the sentence, leading to some unnecessary exploration towards the beginning of the sentence, and poten- tially insufficient exploration towards the end.</p><p>One way that this problem can be mitigated is by using a dynamically-sized beam <ref type="bibr" target="#b7">(Mejia-Lavalle and Ramos, 2013)</ref>. When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distri- bution of scores of available beams. Common meth- ods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam.</p><p>Another approach to solving this issue is given by <ref type="bibr" target="#b3">Choi and McCallum (2013)</ref>. They introduced se- lectional branching, which involves performing an initial greedy parse, and then using confidence esti- mates on each prediction to spawn additional beams. Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up.</p><p>In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function <ref type="bibr" target="#b9">(Pearl, 1984)</ref>: a function which estimates the future cost of taking a particular decision. When paired with a good heuristic, heuristic backtracking maintains the property of reducing wasted predic- tions, but allows us to more fully explore the space of possible transition sequences (as compared to se- lectional branching). In this paper, we use a heuristic based on the confidence of transition predictions.</p><p>We also introduce a new optimization: heuristic backtracking with cutoff. Since heuristic backtrack- ing produces results incrementally, it is possible to stop the search early if we have found an answer that we believe to be the gold parse, saving time propor- tional to the number of backtracks remaining.</p><p>We compare the performance of these various decoding algorithms with the Stack-LSTM parser <ref type="bibr" target="#b5">(Dyer et al., 2015)</ref>, and achieve slightly higher ac- curacy than beam search, in significantly less time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transition-Based Parsing With</head><p>Stack-LSTM</p><p>Our starting point is the model described by <ref type="bibr" target="#b5">Dyer et al. (2015)</ref>. <ref type="bibr">1</ref> The parser implements the arc-standard algorithm <ref type="bibr" target="#b8">(Nivre, 2004)</ref> and it therefore makes use of a stack and a buffer. In ( <ref type="bibr" target="#b5">Dyer et al., 2015)</ref>, the stack and the buffer are encoded with Stack-LSTMs, and a third sequence with the history of actions taken by the parser is encoded with another Stack-LSTM. The three encoded sequences form the parser state p t defined as follows,</p><formula xml:id="formula_0">p t = max {0, W[s t ; b t ; a t ] + d} ,<label>(1)</label></formula><p>where W is a learned parameter matrix, b t , s t and a t are the stack LSTM encoding of buffer, stack and the history of actions, and d is a bias term. The out- put p t (after a component-wise rectified linear unit (ReLU) nonlinearity <ref type="bibr" target="#b6">(Glorot et al., 2011)</ref>) is then used to compute the probability of the parser action at time t as:</p><formula xml:id="formula_1">p(z t | p t ) = exp g zt p t + q zt z ∈A(S,B) exp g z p t + q z ,<label>(2)</label></formula><p>where g z is a column vector representing the (out- put) embedding of the parser action z, and q z is a bias term for action z. The set A(S, B) represents <ref type="bibr">1</ref> We refer to the original work for details.</p><p>the valid transition actions that may be taken in the current state. The objective function is:</p><formula xml:id="formula_2">L θ (w, z) = |z| t=1 log p(z t | p t )<label>(3)</label></formula><p>where z refers to parse transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Heuristic Backtracking</head><p>Using the Stack-LSTM parsing model of <ref type="bibr" target="#b5">Dyer et al. (2015)</ref> to predict each decision greedily yields very high accuracy; however, it can only explore one path, and it therefore can be improved by conduct- ing a larger search over the space of possible parses.</p><p>To do this, we introduce a new algorithm, heuristic backtracking. We also introduce a novel cutoff ap- proach to further increase speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decoding Strategy</head><p>We model the space of possible parses as a tree, where each node represents a certain parse state (with complete values for stack, buffer, and action history). Transitions connect nodes of the tree, and leaves of the tree represent final states. During the first iteration, we start at the root of the tree, and greedily parse until we reach a leaf. That is, for each node, we use the Stack-LSTM model to calculate scores for each transition (as described in Section 2), and then execute the highest-scoring transition, generating a child node upon which we repeat the procedure. Additionally, we save an or- dered list of the transition scores, and calculate the confidence of the node (as described in Section 3.2).</p><p>When we reach the leaf node, we backtrack to the location that is most likely to fix a mistake. To find this, we look at all explored nodes that still have at least one unexplored child, and choose the node with the lowest heuristic confidence (see Section 3.2). We rewind our stack, buffer, and action history to that state, and execute the highest-scoring transition from that node that has not yet been explored. At this point, we are again in a fully-unexplored node, and can greedily parse just as before until we reach another leaf.</p><p>Once we have generated b leaves, we score them all and return the transition sequence leading up to the highest-scoring leaf as the answer. Just as in pre- vious studies ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>), we use the</p><formula xml:id="formula_3">n 1 1 n 1 2 n 2 2 n 3 2 n 4 2 n 1 3 n 2 3 n 3 3 n 4 3 n 1 4 n 2 4 n 3 4 n 4 4 n 1 l n 2 l n 3 l n 4 l . . . . . . . . .</formula><p>. . .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Beam Search</head><formula xml:id="formula_4">n 1 1 n 1 2 n 2 2 n 3 2 n 4 2 n 1 3 n 2 3 n 1 4 n 2 4 n 3 4 n 1 l n 2 l n 3 l n 4 l . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Calculating Error Likelihood</head><p>Let n indicate a node, which consists of a state, a buffer, and an action history. We may refer to a specific node as n j i , which means it has i actions in its action history and it is part of the history of the jth leaf (and possibly subsequent leaves). Let the function T (n) represent a sorted vector contain- ing all possible transitions from n, and S(n) rep- resent a sorted vector containing the scores of all of these transitions, in terms of log probabilities of each score. We can index the scores in order of value, so T 1 (n) is the highest-scoring transition and S 1 (n) is its score, T 2 (n) is the second-highest-scoring tran- sition, etc. Here, let u n indicate the ranking of the transition leading to the first unexplored child of a node n. Also, let V (n) represent the total score of all nodes in the history of n, i.e. the sum of all the scores of individual transitions that allowed us to get to n.</p><p>To calculate the confidence of an individual node, <ref type="bibr" target="#b3">Choi and McCallum (2013)</ref> simply found the score margin, or difference in probability between the top- scoring transition and the second-highest scoring transition: C(n) = S 1 (n) − S 2 (n). In selectional branching, the only states for which the confidence was relevant were the states in the first greedy parse, i.e. states n 1 i for all i. For heuristic backtracking, we wish to generalize this to any state n j i for all i and j.</p><p>We do this in the following way:</p><formula xml:id="formula_5">H(n j i ) = (V (n 1 i ) − V (n j i )) + (S (u n j i )−1 (n j i ) + S (u n j i ) (n j i ))<label>(4)</label></formula><p>Intuitively, this formula means that the node that will be explored first is the node that will yield a parse that scores as close to the greedy choice as possible.</p><p>The first term ensures that it has a history of good choices, and the second term ensures that the new child node being explored will be nearly as good as the prior child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Number of Predictions</head><p>As discussed earlier, we use number of predictions made by the model as a proxy for the speed; exe- cution speed may vary based on system and algo- rithmic implementation, but prediction count gives a good estimate of the overall work done by the al- gorithm.</p><p>Consider a sentence of length l, which requires at most 2l transitions with the greedy decoder <ref type="bibr" target="#b8">(Nivre, 2004</ref>). The number of predictions required for heuristic backtracking for b leaves is guaranteed to be less than or equal to a beam search with b beams.</p><p>When doing a beam search, the first transition will require 1 prediction, and then every subsequent tran- sition will require 1 prediction per beam, or b predic- tions. This results in a total of b(2l − 1) + 1 predic- tions.</p><p>When doing heuristic backtracking, the first greedy search will require 2l predictions. Every subsequent prediction will require a number of pre- dictions dependent on the target of the backtrack: backtracking to n j i will require 2l − (i + 1) pre- dictions. Note that 0 &lt; i &lt; 2l. Thus, each back- track will require at maximum 2l − 1 predictions. Therefore, the maximum total amount of predictions is 2l + (b − 1)(2l − 1) = b(2l − 1) + 1.</p><p>However, note that on average, there are signifi- cantly fewer. Assuming that all parts of a sentence have approximately equal score distributions, the av- erage backtrack will be where i = l, and reduce pre- dictions by 50%.</p><p>An intuitive understanding of this difference can be gained by viewing the graphs of various decoding methods in <ref type="figure" target="#fig_2">Figure 1</ref>. Beam search has many nodes which never yield children that reach an end-state; dynamic beam search has fewer, but still several. Se- lectional branching has none, but suffers from the re- striction that every parse candidate can be no more than one decision away from the greedy parse. With heuristic backtracking, there is no such restriction, but yet every node explored is directly useful for generating a candidate parse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Early Cutoff</head><p>Another inefficiency inherent to beam search is the fact that all b beams are always fully explored. Since the beams are calculated in parallel, this is in- evitable. However, with heuristic backtracking, the beams are calculated incrementally; this gives us the opportunity to cut off our search at any point. In or- der to leverage this into more efficient parsing, we constructed a second Stack-LSTM model, which we call the cutoff model. The cutoff model uses a sin- gle Stack-LSTM 2 that takes as input the sequence of parser states (see Eq 1), and outputs a boolean vari- able predicting whether the entire parse is correct or incorrect.</p><p>To train the cutoff model, we used stochastic gra- dient descent over the training set. For each training example, we first parse it greedily using the Stack- LSTM parser. Then, for as long as the parse has at least one mistake, we pass it to the cutoff model as a negative training example. Once the parse is com- pletely correct, we pass it to the cutoff model as a positive training example. The loss function that we 2 2 layers and 300 dimensions. use is:</p><formula xml:id="formula_6">L θ = − log p(t | s)<label>(5)</label></formula><p>where s is the LSTM encoded vector and t is the truth (parse correct/incorrect). When decoding using early cutoff, we follow the exact same procedure as for normal heuristic back- tracking, but after every candidate parse is gener- ated, we use it as input to our cutoff model. When our cutoff model returns our selection as correct, we stop backtracking and return it as the answer. If we make b attempts without finding a correct parse, we follow the same procedure as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>To test the effectiveness of heuristic backtrack- ing, we compare it with other decoding tech- niques: greedy, beam search, 3 , dynamic beam search <ref type="bibr" target="#b7">(Mejia-Lavalle and Ramos, 2013)</ref>, and selec- tional branching <ref type="bibr" target="#b3">(Choi and McCallum, 2013)</ref>. We then try heuristic backtracking (see Section 3.1), and heuristic backtracking with cutoff (see Section 3.4). Note that beam search was not used for early-update training ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>). We use the same greedy training strategy for all models, and we only change the decoding strategy.</p><p>We tested the performance of these algorithms on the English SD and Chinese CTB. 4 A single model was trained using the techniques described in Sec- tion 2, and used as the transition model for all decod- ing algorithms. Each decoding technique was tested with varying numbers of beams; as b increased, both the predictions per sentence and accuracy trended upwards. The results are summarized in <ref type="table">Table 1</ref>. <ref type="bibr">5</ref> Note that we report results for only the highest- accuracy b (in the development set) for each.</p><p>We also report the results of the cutoff model in <ref type="table">Table 2</ref>. The same greedily-trained model as above was used to generate candidate parses and confi- dence estimates for each transition, and then the cut- off model was trained to use these confidence esti- mates to discriminate between correctly-parsed and incorrectly-parsed sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In <ref type="table">Table 1</ref> we see that in both English and Chi- nese, the best heuristic backtracking performs ap- proximately as well as the best beam search, while making less than half the predictions. This supports our hypothesis that heuristic backtracking can per- form at the same level as beam search, but with in- creased efficiency. Dynamic beam search also performed as well as full beam search, despite demonstrating a re- duction in predictions on par with that of heuris- tic backtracking. Since the implementation of dy- namic beam search is very straightforward for sys- tems which have already implemented beam search, we believe this will prove to be a useful finding.</p><p>Heuristic backtracking with cutoff outperformed greedy decoding, and reduced transitions by an addi- tional 50%. However, it increased accuracy slightly less than full heuristic backtracking. We believe this difference could be mitigated with an improved cut- off model; as can be seen in <ref type="table">Table 2</ref>, the cutoff model was only able to discriminate between correct and incorrect parses around 75% of the time. Also, note that while predictions per sentence were low, the overall runtime was increased due to running the cutoff LSTM multiple times per sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Cutoff Accuracy</head><p>English 72.43% Chinese 75.18% <ref type="table">Table 2</ref>: Test-set accuracy of cutoff model on En- glish and Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Heuristic backtracking is most similar to the work of <ref type="bibr" target="#b3">Choi and McCallum (2013)</ref>, but is distinguished from theirs by allowing new beams to be initialized from any point in the parse, rather than only from points in the initial greedy parse. Heuristic back- tracking also bears similarity to greedy-best-first- search <ref type="bibr" target="#b9">(Pearl, 1984)</ref>, but is unique in that it guaran- tees that b candidate solutions will be found within b(2l − 1) + 1 predictions. Our work also relates to beam-search parsers <ref type="bibr" target="#b13">(Zhang and Clark, 2008</ref>, inter alia).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have introduced a novel decoding algorithm, called heuristic backtracking, and presented evi- dence that it performs at the same level as beam search for decoding, while being significantly more efficient. We have demonstrated this for both En- glish and Chinese, using a parser with strong re- sults with a greedy decoder. We expect that heuris- tic backtracking could be applied to any other transition-based parser with similar benefits. We plan on experimenting with various heuristics and cutoff models, such as adapting the attention- based models of <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref> to act as a guide for both the heuristic search and cutoff.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of various decoding algorithms</figDesc></figure>

			<note place="foot" n="3"> Greedy and beam-search were already explored by Dyer et al. (2015) 4 Using the exact same settings as Dyer et al. (2015) with pretrained embeddings and part-of-speech tags. 5 The development sets are used to set the model parameters; results on the development sets are similar to the ones obtained in the test sets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Miguel Ballesteros was supported by the Euro-pean Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<idno>abs/1603.06042</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<pubPlace>Slav Petrov, and Michael Collins</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with selectional branching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</title>
		<meeting>the 42Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beam search with dynamic pruning for artificial intelligence hard problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mejia-Lavalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar Geovani Pereyra</forename><surname>Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Mechatronics, Electronics and Automotive Engineering</title>
		<meeting>the 2013 International Conference on Mechatronics, Electronics and Automotive Engineering</meeting>
		<imprint>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Heuristics: Intelligent Search Strategies for Computer Problem Solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental recurrent neural network dependency parser with search-based discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="142" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing using beamsearch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural probabilistic structured-prediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
