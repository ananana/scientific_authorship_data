<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part-of-speech Taggers for Low-resource Languages using CCA Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
							<email>bsnyder@cs.wisc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
							<email>{ybkim, ruhi.sarikaya}@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Part-of-speech Taggers for Low-resource Languages using CCA Features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we address the challenge of creating accurate and robust part-of-speech taggers for low-resource languages. We propose a method that leverages existing parallel data between the target language and a large set of resource-rich languages without ancillary resources such as tag dictionaries. Crucially, we use CCA to induce latent word representations that incorporate cross-genre distri-butional cues, as well as projected tags from a full array of resource-rich languages. We develop a probability-based confidence model to identify words with highly likely tag projections and use these words to train a multi-class SVM using the CCA features. Our method yields average performance of 85% accuracy for languages with almost no resources, outperforming a state-of-the-art partially-observed CRF model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We address the challenge of creating accurate and robust part-of-speech taggers for low-resource languages. We aim to apply our methods to the hundreds, and potentially thousands, of languages with meager electronic resources. We do not as- sume the existence of a tag dictionary, or any other sort of prior knowledge of the target language. In- stead, we base our methods entirely on the exis- tence of parallel data between the target language and a set of resource-rich languages.</p><p>Fortunately, such parallel data exists for just about every written language, in the form of Bible translations. Around 2,500 languages have at least partial Bible translations, and somewhere between 500 and 1,000 languages have complete transla- tions. We have collected such electronic Bible translations for 650 languages. <ref type="figure" target="#fig_0">Figure 1</ref> breaks down the number of languages in our collection according to their token count. The majority of our languages have at least 200,000 tokens of Bible translations.</p><p>While previous studies <ref type="bibr" target="#b33">(Täckström et al., 2013;</ref><ref type="bibr" target="#b9">Ganchev and Das, 2013)</ref> have addressed this gen- eral setting, they have typically assumed the exis- tence of a partial tag dictionary as well as large quantities of non-parallel data in the target lan- guage. These assumptions are quite reasonable for the dozen most popular languages in the world, but are inadequate for the creation of a truly world- wide repository of NLP tools and linguistic data.</p><p>In fact, we argue that such ancillary sources of information are not really necessary once we take into account the vastly multilingual nature of our parallel data. Annotations projected from individ- ual resource-rich languages are often noisy and unreliable, due to systematic differences between the languages in question, as well as word align- ment errors. We can thus think of these languages as very lazy and unreliable annotators of our tar- get language. Despite their incompetence, as the number of such annotators increases, their com- bined efforts converge upon the truth, as idiosyn- cratic biases and random noise are washed away.</p><p>Our assumption throughout will be that we have in our possession a single multilingual corpus (the Bible) consisting of about 200,000 tokens for several hundred languages languages, as well as reasonably accurate POS taggers for about ten "resource-rich" languages. We will tag the Bible data for the resource-rich languages, word-align them to one another, and also word-align them to the remaining several hundred target languages.</p><p>Of course, our goal is not to produce a tagger restricted to the Biblical lexicon. We therefore assume a small unannotated monolingual sample of the target language in an entirely unrelated genre (e.g. newswire). We use this sample trans- ductively to adapt our learned taggers from the Biblical genre. In our experiments, we use the CoNLL 2006 and 2007 shared-task test data for this purpose. Of course tagged data does not exist for truly resource-poor languages, so we evaluate our methodology on the resource-rich languages. Each such language takes a turn playing the role of the target language for testing purposes.</p><p>The goal of the paper is to introduce a gen- eral "recipe" for successful cross-lingual induction of accurate taggers using meager resources. We faced three major technical challenges:</p><p>• First, word alignments across languages are incomplete, and often do not preserve part- of-speech due to language differences.</p><p>• Second, when using multiple resource-rich languages, we need to resolve conflicting projections.</p><p>• Third, the parallel data at our disposal is of an idiosyncratic genre (the Bible), and we wish to induce a general-purpose tagger.</p><p>To address these challenges, we forgo the typi- cal sequence-based learning technique of HMM's and CRF's and instead adopt an instance-learning approach using latent distributional features. To induce these features, we introduced a new method using Canonical Correlation Analysis (CCA) to generalize the aligned information to new words. This method views each word position as consist- ing of three fundamental views: (1) the token view (word context), (2) the type view, and (3) the pro- jected tags in the local vicinity. We perform a CCA to induce latent continuous vector represen- tations of each view that maximizes their correla- tions to one another. On the test data, a simple multi-class classifier then suffices to predict accu- rate tags, even for novel words. This approach out- perform a state-of-the-art baseline <ref type="bibr">(Täckström et al., 2013</ref>) to achieve average tag accuracy of 85% on newswire text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multilingual Projection</head><p>The idea of projecting annotated resources across languages using parallel data was first proposed by <ref type="bibr" target="#b36">Yarowsky et al. (2001)</ref>. This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filter- ing methods to mitigate the effects of cross-lin- gual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token ba- sis ( <ref type="bibr" target="#b6">Durrett et al., 2012)</ref>. Current state-of-the-art results for indirectly su- pervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary ( <ref type="bibr" target="#b23">Li et al., 2012;</ref><ref type="bibr" target="#b33">Täckström et al., 2013;</ref><ref type="bibr" target="#b9">Ganchev and Das, 2013</ref>). As we argued above, the only widely available source of infor- mation for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results.</p><p>Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise.</p><p>Fossum and <ref type="bibr" target="#b8">Abney (2005)</ref> found that using two source languages project-sources gave better re- sults than simply using more data from one lan- guage.  also found advan- tages to using multiple language sources for pro- jecting parsing constraints. In more of an unsu-pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages <ref type="bibr" target="#b29">(Naseem et al., 2009</ref>).</p><p>In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the un- supervised induction of linguistic structure, even in the absence of parallel text <ref type="bibr" target="#b17">(Kim et al., 2011;</ref><ref type="bibr" target="#b14">Kim and Snyder, 2012;</ref><ref type="bibr" target="#b15">Kim and Snyder, 2013a;</ref><ref type="bibr" target="#b16">Kim and Snyder, 2013b</ref>). We have showed that cross-lingual supervised learning leads to signif- icant performance gains over monolingual mod- els. We point out that the previous tasks have con- sidered as word-level structural analyses and our present case as a sentence-level analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Alignment</head><p>Most of the papers surveyed above rely on auto- matic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve pro- jection accuracy, the question of word alignment performance becomes crucial. Our hypothesis is that multiple language projections are beneficial not only in weeding out random errors and id- iosyncratic variations, but also in improving the linguistic consistency of the alignments them- selves. Instead of simply aligning each source lan- guage to the target language in isolation, we will instead use a confidence model to synthesize in- formation from multiple sources.</p><p>While there are not many well-known papers that have explored word alignment on a multilin- gual scale <ref type="bibr">1</ref> , there have been related efforts to sym- metrize bilingual alignment models, using a vari- ety of techniques ranging from modifications of EM ( <ref type="bibr" target="#b24">Liang et al., 2006</ref>), posterior-regularized ob- jective function ( <ref type="bibr" target="#b10">Ganchev et al., 2010)</ref>, and by considering relaxations of the hard combinato- rial assignment problem (DeNero and Macherey, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Canonical Correlation Analysis (CCA)</head><p>Our method for generalizing the projections to unseen words and contexts is based on Canoni- cal Correlation Analysis (CCA), a dimensionality reduction technique first introduced by <ref type="bibr" target="#b12">Hotelling (1936)</ref>. The key idea is to consider two groups of random variables with corresponding observa- tions and to find linear subspaces with highest cor- relation between the two views. This can be seen as a kind of supervised version of Principal Com- ponents Analysis (PCA), where each view is pro- viding supervision for the other. In fact, it can be shown that CCA directly generalizes both multi- ple linear regression and Fisher's Latent Discrimi- native Analysis (LDA) <ref type="bibr" target="#b11">(Glahn, 1968)</ref>.</p><p>From a learning theory perspective, CCA is in- teresting in that it allows us to prove regret-based learning bounds that depend on the "intrinsic" di- mensionality of the problem rather than the ap- parent dimensionality <ref type="bibr" target="#b13">(Kakade and Foster, 2007)</ref>. This seems especially relevant to natural language processing scenarios, where the ambient dimen- sion is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic in- formation. In fact, CCA has recently been adapted to learning latent word representations in an inter- esting way: by dividing each word position into a token view (which only sees surrounding con- text) and a type view (which only sees the word itself) and performing a CCA between these two views ( <ref type="bibr" target="#b5">Dhillon et al., 2012;</ref><ref type="bibr" target="#b31">Stratos et al., 2014;</ref><ref type="bibr" target="#b32">Stratos et al., 2015;</ref><ref type="bibr" target="#b21">Kim et al., 2015c</ref>). CCA is also used to induce label rep- resentations ( <ref type="bibr" target="#b22">Kim et al., 2015d</ref>) and lexicon repre- sentations ( <ref type="bibr" target="#b20">Kim et al., 2015b)</ref>.</p><p>Our technique will extend this idea by addition- ally considering a third projected tag view. Cru- cially, it is this view which pushes the latent repre- sentations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tag projection from resource-rich languages</head><p>In this section, we describe two methods for incor- porating transferred tags from resource-rich lan- guages: sequence-based learning <ref type="bibr" target="#b33">(Täckström et al., 2013;</ref><ref type="bibr" target="#b19">Kim et al., 2015a</ref>) and instance-based learning. In the former, the transferred tags are used to train a partially-observed CRF (PO-CRF) by maximizing the probability of a constrained lat- tice. In contrast, instance-based learning views each word token as an independent classifica- tion task, but uses latent distributional information gleaned from surrounding words as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A sequence learning example of partially observed CRF (PO-CRF)</head><p>A first-order CRF parametrized by θ ∈ R d de- fines a conditional probability of a label sequence y = y 1 . . . y n given an observation sequence x = x 1 . . . x n as follows:</p><formula xml:id="formula_0">p θ (y|x) = exp(θ Φ(x, y)) y ∈Y(x) exp(θ Φ(x, y ))</formula><p>where Y(x) is the set of all possible label se- quences for x and Φ(x, y) ∈ R d is a global fea- ture function that decomposes into local feature functions Φ(x, y) = n j=1 φ(x, j, y j−1 , y j ) by the first-order Markovian assumption. Given fully labeled sequences {(</p><formula xml:id="formula_1">x (i) , y (i) )} N i=1</formula><p>, the standard training method is to find θ that maximizes the log likelihood of the label sequences under the model with l 2 -regularization:</p><formula xml:id="formula_2">θ * = arg max θ∈R d N i=1 log p θ (y (i) |x (i) ) − λ 2 ||θ|| 2</formula><p>We used an l 2 penalty weight λ of 1. Unfortu- nately, in our setting, we do not have fully labeled sequences. Instead, for each token x j in sequence x 1 . . . x n we have the following two sources of la- bel information:</p><p>• A set of allowed label types Y(x j ). (Label dictionary, type constraints)</p><p>• Labels˜yLabels˜ Labels˜y j transferred from resource rich languages. (transferred labels, token con- straints)</p><p>Following previous work of <ref type="bibr">Täckström et al. (2013)</ref>, we first define a constrained lattice</p><formula xml:id="formula_3">Y(x, ˜ y) = Y(x 1 , ˜ y 1 ) × . . . × Y(x n , ˜ y n )</formula><p>where at each position j a set of allowed label types is given as:</p><formula xml:id="formula_4">Y(x j , ˜ y j ) = {˜y{˜y j } if˜yif˜ if˜y j is given Y(x j ) otherwise</formula><p>And then we can define a conditional probabil- ity over label lattices for a given observation se- quence x:</p><formula xml:id="formula_5">p θ (Y(x, ˜ y)|x) = y∈Y(x,˜ y) p θ (y|x)</formula><p>Given a label dictionary Y(x j ) for every token type x j and training sequences {(</p><formula xml:id="formula_6">x (i) , ˜ y (i) )} N i=1</formula><p>where˜ywhere˜ where˜y (i) is transferred labels for x (i) and, the new training method is to find θ that maximizes the log likelihood of the label lattices:</p><formula xml:id="formula_7">θ * = arg max θ∈R d N i=1 log p θ (Y(x (i) , ˜ y (i) )|x (i) ) − λ 2 ||θ|| 2</formula><p>Since this objective is non-convex, we find a local optimum with a gradient-based algorithm.</p><p>The gradient of this objective at each example (x (i) , ˜ y (i) ) takes an intuitive form:</p><formula xml:id="formula_8">∂ ∂θ log p θ (Y(x (i) , ˜ y (i) )|x (i) ) − λ 2 ||θ|| 2 = y∈Y(x (i) ,˜ y) p θ (y|x (i) )Φ(x (i) , y) − y∈Y(x (i) ) p θ (y|x (i) )Φ(x (i) , y) − λθ</formula><p>This is the same as the standard CRF train- ing except the first term where the gold features Φ(x (i) , y (i) ) are replaced by the expected value of features in the constrained lattice Y(x (i) , ˜ y). An important distinction in our setting is that our token and type constraints are generated by only using the transferred tags whereas <ref type="bibr">Täckström et al. (2013)</ref> generate type constraints induced from Wiktionary. Our setting is more realistic for at least two reasons; 1) Wiktionary is not always available. 2) transferable information is not lim- ited, but Wiktionary is (e.g., semantic role and named entity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-lingual instance-based learning</head><p>The proposed method for cross-lingual instance- based learning has three steps:</p><p>1. Select training tokens based on the confi- dence of the projected tag information.</p><p>2. Induce distributional features over these words that incorporate all projected tags.</p><p>3. Train a multi-class classifier with these in- duced features to make local predictions for individual tokens.</p><p>We will describe each step below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Selecting training words</head><p>Since transferred tags are not always reliable, all words in the parallel data are not necessary help- ful in training. Since this method trains on words instead of sequences, it is easy to discard words which have unreliable or highly conflicting pro- jections from different resource-rich languages.</p><p>To select our set of training tokens, we define a simple probability-based confidence model, illus- trated in <ref type="figure" target="#fig_1">Figure 2</ref>. Suppose we have L resource- rich languages with alignments to the word in question. If the true tag is y, we assume that the projected tag for language will be identical to y with probability 1 − , where is a language- specific corruption probability. With probability , the projection will instead be chosen randomly (uniformly).</p><p>To make this explicit, we introduce a corruption indicator variable z with:</p><formula xml:id="formula_9">P (z = 1) =</formula><p>Given z , the probability of the projected tag y is given by:</p><formula xml:id="formula_10">P (y |y, z ) =      1 if z = 0 and y = y , 1 m if z = 1, 0 otherwise.</formula><p>where m is the total number of possible tags. We can now compute a conditional distribution over the unknown tag y, marginalizing out the unknown corruption variables for each language:</p><formula xml:id="formula_11">p(y|y 1 , . . . , y n ) = n =1 m + (1 − )δ(y, y ) 1 m n−1 y ∈Y n =1 m + (1 − )δ(y , y )</formula><p>where Y is all possible tags. For simplicity, we simply set all to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Inducing distributional features</head><p>In this section we discuss our approach for deriv- ing latent distributional features. Canonical Cor- relation Analysis (CCA) is a general method for inducing new representations for a pair of vari- ables X and Y <ref type="bibr" target="#b12">(Hotelling, 1936)</ref>. To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words ( <ref type="bibr" target="#b5">Dhillon et al., 2012;</ref><ref type="bibr" target="#b21">Kim et al., 2015c</ref>). When they are defined as one-hot encod- ings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is</p><formula xml:id="formula_12">Ω w,c = count(w, c) count(w)count(c)</formula><p>where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = c count(w, c), and count(c) = w count(w, c). The resulting word representation is given by U X where U is a matrix of the scaled left singu- lar vectors of Ω (See <ref type="figure" target="#fig_2">Figure 3)</ref>. In our work, we use a slightly modified version of this definition by taking square-root of each count:</p><formula xml:id="formula_13">√ Ω w,c = count(w, c) 1/2 count(w) 1/2 count(c) 1/2</formula><p>This has an effect of stabilizing the variance of each term in the matrix, leading to a more effi- cient estimator. The square-root transformation also transforms the distribution of the count data to look more Gaussian <ref type="bibr" target="#b1">(Bartlett, 1936)</ref>: since an interpretation of CCA is a latent-variable with normal distributions ( <ref type="bibr" target="#b0">Bach and Jordan, 2005</ref>), it makes the data more suitable for CCA. It has been observed in past works (e.g., <ref type="bibr" target="#b5">Dhillon et al. (2012)</ref>) to significantly improve the quality of the resulting representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Induction Algorithm</head><p>We now describe our algorithm for inducing la- tent distributional features both on the multilin- gual parallel corpus, as well as the monolingual, newswire test data. This algorithm is described in detail in <ref type="figure">Figure 4</ref>. The key idea is to per- form two CCA steps. The first step incorporates word-distributional information over both the mul- tilingual corpus (the Bible) as well as the exter- nal domain monolingual corpus (CONLL data) <ref type="bibr">2</ref> . This provides us with word representations that are general, and not overly specific to any single genre. However, it does not incorporate any pro- jected tag information. We truncate this first SVD to the first 100 dimensions <ref type="bibr">3</ref> .</p><p>After this CCA step is performed, we then re- place the words in the multilingual Bible data with their latent representations. We then perform a second CCA between these word representations and vectors representing the projected tags from all resource-rich languages. This step effectively adapts the first latent representation to the infor- mation contained in the tag projections. We trun- cate this second SVD to the first 50 dimensions.</p><p>We now have word embeddings that can be ap- plied to any corpus, and are designed to maximize correlation both with typical surrounding word context, as well as typical projected tag context. These embeddings serve as our primary feature vectors for training the POS classifier (described in the next section). We concatenate this primary feature vector with the embeddings of the previous and subsequent words, in order to provide context- sensitive POS predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Multi-class classifier</head><p>To train our POS tagger, we use a linear multi- class SVM <ref type="bibr" target="#b2">(Crammer and Singer, 2002</ref>). It has a parameter w y ∈ R d for every tag y ∈ T and defines a linear score function s(x, j, y) := w y Φ(x, j). Given any sentence x and a position j, it predicts arg max y∈T s(x, j, y) as the tag of x j . We use the implementation of <ref type="bibr" target="#b7">Fan et al. (2008)</ref> with the default hyperparameter configurations for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Setup</head><p>There are more than 4,000 living languages in the world, and one of the most prevalently translated books is the Bible. We now describe the Bible dataset we collected.</p><formula xml:id="formula_14">CCA-PROJ-SPARSE Input: samples (x (1) , y (1) ) . . . (x (n) , y (n) ) ∈ {0, 1} d × {0, 1} d , dimension k Output: projections A ∈ R d×k and B ∈ R d ×k • Calculate B ∈ R d×d , u ∈ R d , and v ∈ R d : Bi,j = n l=1 [[x (l) i = 1]][[y (l) j = 1]] ui = n l=1 [[x (l) i = 1]] vi = n l=1 [[y (l) i = 1]] • Definê Ω = diag(u) −1/2 Bdiag(v) −1/2 .</formula><p>• Calculate rank-k SVDˆΩSVDˆ SVDˆΩ. Let U ∈ R d×k (V ∈ R d ×k ) be a matrix of the left (right) singular vector corresponding to the largest k singular values.</p><p>•  We first collect 893 bible volumes span- ning several hundred languages that are freely available from three resources (www.bible.is, www.crosswire.org, www.biblegateway.com) and changed to UTF-8 format. The distribution of to- ken in each bible in the unit of a language is in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><formula xml:id="formula_15">Let A = diag(u) −1/2 U and B = diag(v) −1/2 V .</formula><p>Note that the Bible scripts are not exactly trans- lated by sentences but by verses. We thus assume that each verse in a chapter has the same meaning if the number of verses is exactly same in a same chapter. We also assume that the whole chapters have the same meaning if the number of chap- ters in a book are exactly the same. In the same manner, we also assume the volumes that have the same number of chapters are the same. That is, their volume size should be as similar as possible • N "labeled" tokens in the Bible domain: word w (i) ∈ V, corresponding context C(w (i) ) ⊂ V and (projected) tag set P (i) ⊂ T for i = 1 . . . N</p><p>• N tokens in data in the test domain: word</p><formula xml:id="formula_16">v (i) ∈ V and corresponding context C(v (i) ) ⊂ V for i = 1 . . . N • CCA dimensions k1, k2</formula><p>Output: embedding e(w) ∈ R k 2 for each word w ∈ V ∪ V 1. Combine the observed tokens and their context from the Bible and data in the test domain:</p><formula xml:id="formula_17">W1 := w : w ∈ (w (i) ) N i=1 ∪ (v (i) ) N i=1 C1 := C(w) : w ∈ (w (i) ) N i=1 ∪ (v (i) ) N i=1</formula><p>2. Perform rank-k1 CCA-PROJ-SPARSE on (W1, C1) to derive a word projection matrix ΦW 1 and a context projection matrix ΦC 1 .</p><p>3. Project all word examples in the Bible domain using ΦW 1 . Denote these projected words and the corre- sponding projected tag sets from all resource-rich lan- guages by with the respect to the number of verses, chapters, and books. Based upon these assumptions, we choose the best translation in a language based on a compar- ison to a reference Bible, the Modern King James Version (MKJV) in English. We choose the trans- lation for each language that best matches this ref- erence version in terms of chapter and verse num- bering.</p><formula xml:id="formula_18">W2 := ΦW 1 (w (i) ) : i = 1 . . . N</formula><p>There are other factors considered if there are more than one candidates satisfying this matching. We focus on the contents of the bible such as the publication time. For instance, 1599 Geneva Bible in English contains old vocabulary with different spelling systems, causing unexpected errors when tagged by POS annotation tools. Also, some of volumes such as Amplified Bible (AMP) contains extraneous comments on verses themselves, caus- ing errors for word alignments.</p><p>After the choice of the best volume, we finally select the 10 resource rich languages <ref type="bibr">4</ref> . The two criteria to select resource rich languages are hav- ing i) the matched bible scripts both on the Old and New testament and ii) reliable parts-of-speech an- notation tools. If these two requirements are satis- fied, we can freely add more languages as resource rich languages in the future research. We use Hun- pos tagger for CS, DA, DE, EN, and PT, Treetag- ger for BG, ES, IT, and NL, and Meltparser for FR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test Data</head><p>We use CoNLL parts-of-speech tagged data (selected resource-rich languages), plus Basque (EU), Hungarian (HU) and Turkish (TR)) as our test data. It consists of 5,000-6,000 hand-labeled tokens. The accuracy of each supervised tagger on this data is about 94% on average. Since there is no French tagged CoNLL data, we exclude French on testing but still use it in Training. The accuracy of each supervised tagger on this data is shown in <ref type="table">Table 1</ref>.</p><p>The tag definitions used in CoNLL data are not exactly matched the ones used in the taggers when converted to universal POS tags. For instance in Spanish, we initially follow mapping of  for CoNLL data. The 'dp' tag for words sus, su, mi are mapped to DET but they are mapped to PRON in the bible data because of the Treetagger definitions. Whenever we find this kind of issues, we analyze them and choose the one of mappings for compatibility. For the 'dp' tag, we choose to map PRON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Alignments</head><p>We perform two kinds of alignments in our data sets; (i) the verse alignment and (ii) the word alignment. When the tagged bible volumes are prepared, we align verses across all resource rich languages. For verse alignments, we pre-process to remove extraneous information such as in- line reference (e.g. <ref type="bibr">[REV 4:16]</ref>) and HTML tags. These alignments between two languages occurred only when volumes have the exact same number of chapters and verses. For instance, Mark must have 16 chapters and the first chapter of the Mark must have 45 verses in our criteria. The cor- rect number of chapters and verses are pre-defined on MKJV volume, and the number of matched verses on each volume is greater than 30,500.</p><p>After performing verse alignments, we then per- form word alignments. The quality of tags in resource poor languages is highly dependent on the quality of word alignments because parts-of- speech tags will be projected through this align- ment path. First, we use GIZA++ for initial one- to-many alignments and we symmetrize by taking their intersection. This ensures that the resulting alignments are of high quality.  In all experiments, we hold out the tags of the test language. EU, HU and TR used projected tags from 10 resource-rich languages, 9 resource-rich languages are used for the remaining languages. In our first experiment, we consider the state-of- the-art PO-CRF baseline. This model trains a par- tially observed CRF based on a single projected tag for each token. We experiment with different methods of choosing the projected tags. The re- sults are shown in <ref type="table" target="#tab_2">Table 2</ref>. The majority method is to choose the most common tag from the projected tags of the current token. We then experiment with taking the union of all projected tags (i.e. only constraining the lattice based on unanimity of the resource-rich languages). Finally, we considered choosing the high confidence tags, based on our confidence model. The confident tags are defined by a method described in Section 3.2.1 If this ratio is greater than 0.9, we assume that this token has high confidence. As the results indicate, this final method yielded the best tagging performance on the CONLL test data, achieving average accuracy of 82%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>majority union confident</head><p>In the remaining experiments we will adopt the confidence-based selection criterion for both the baseline as well as our method.  In order to isolate the errors due to projection mismatch versus domain variation, we first test both models on the Bible data itself. To do so, we assume that the tags produced by the test- language's supervised tagger are in fact the ground truth. This experiment allows us to compare to tag projection models using (1) PO-CRF and (2) CCA+SVM. Results are given in <ref type="table" target="#tab_4">Table 3</ref>. Unsur- prisingly, PO-CRF performs better on the multi- lingual corpus than on the CONLL data, due to the beneficial constraint of the projected tags. Per- haps interestingly, the CCA+SVM method, which is a simple instance-based classifier using cleverly constructed features, outperforms the sequence la- beller, achieving accuracy of nearly 86% <ref type="bibr">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PO-CRF CCA+SVM</head><p>In our third experiment we use CoNLL test data and compare the PO-CRF models with different settings. See <ref type="table">Table 4</ref>. This experiment is to show the effects of suffix and Brown cluster features on PO-CRF to relieve the unseen words issue. We also show that the more projecting languages are 1 lang (EN) (A) 9/10 langs (W) 9/10 langs (no S/C) 9/10 langs (  <ref type="table">Table 4</ref>: Accuracy of the PO-CRF models on CoNLL data. A, W, no S/C means: all, word, all but no suffix and cluster features are used, respectively.</p><p>included the better the results gets.</p><p>For the features, we used word identity, suffixes of up to length 3, Brown cluster and three indi- cators of (1) capitalization for the first character, (2) containing a hyphen or (3) a digit. Especially, Brown clusters was induced from more than 2 mil- lion line documents, making the setting unrealistic for resource-poor language.</p><p>With just the word features, the averaged per- formance is 0.6993 and other indicator features increase the performance to 0.7768. Also note that the suffix and Brown cluster features increase the performance from 0.7768 to 0.8314. As re- ported, PO-CRF mitigates the adverse effects of the unseen word issues and almost meets the per- formance in the previous experiment (0.8375) of <ref type="bibr">Täckström et al. (2013)</ref> by using these features.</p><p>In fourth and final experiment, we used the same features for PO-CRF, with Brown clusters induced on a realistically obtainable sized (3k) corpus for a low resource language. We compare directly to our CCA+SVM model (which does not use Brown clustering features at all). We achieved 0.7983 on PO-CRF with all features and our cor- responding model on CCA achieved about 0.8474, shown in <ref type="table" target="#tab_7">Table 5</ref>. As reported, our model outper- forms the PO-CRF with the realistic settings for resource poor languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We addressed the challenge of POS tagging low- resource languages. Our key idea is to use a mas- sively multilingual corpus. Instead of relying on a single resource-rich language, we leverage the full  array of currently available POS taggers. This re- moves alignment-mismatch noise and identifies a subset of words with highly confident tags. We then use a CCA procedure to induce latent fea- ture representations across domains, incorporating word contexts as well as projected tags. We then train an SVM to predict tags.</p><p>Experimentally, we show that this procedure yields accuracy of about 85% for languages with nearly no resources available, beating a state-of- the-art partially observed CRF formulation. In the near future, this technique will enable us to re- lease a suite of POS taggers for hundreds of low- resource languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The breakdown of languages by the number of tokens in their available Bible translations. The horizontal axis gives the number of tokens, and the vertical axis gives the number of languages in each token range.</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,218.26,146.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of the confidence model. Unobserved variable y denotes the true target-language tag for a token. Each of the L resource-rich languages displays a project of y, as y , with an indicator variable z determining the fidelity of the projection.</figDesc><graphic url="image-2.png" coords="5,104.74,62.81,152.79,157.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Algorithm for deriving CCA projections from samples of two variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 4: Algorithm for deriving word vectors for the (unannotated) test data that use the projected tags in the Bible data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Baseline model CONLL performance de-
pending on criterion for selecting tag projection. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Performance on multilingual Bible data</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performances on our test data, CoNLL 
document. 

</table></figure>

			<note place="foot" n="1"> Mayer and Cysouw (2012) used multilingual word alignment to compare languages</note>

			<note place="foot" n="2"> For context words, we use 5 words before and after the word occurrence. 3 Embedding dimension was empirically determined by the singular values.</note>

			<note place="foot" n="4"> Bulgarian (BG), Czech(CS), Danish (DA), English(EN), German (DE), French (FR), Spanish (ES), Italian (IT), Dutch (NL), and Portuguese (PT)</note>

			<note place="foot" n="5"> Note that some previous researches (Liang et al., 2008; Wisniewski et al., 2014; Moore, 2014) also pointed out that POS tagging does not necessarily require a sequence model for strong performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The square root transformation in analysis of variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mso</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
	<note>Supplement to the</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the learnability and design of output codes for multiclass problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="201" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with bilingual graphbased projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modelbased aligner combination using dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two Step CCA: A new spectral method for estimating vector models of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine learning</title>
		<meeting>the 29th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntactic transfer using a bilingual lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing-IJCNLP 2005</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="862" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crosslingual discriminative learning of sequence models with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Association for Computational Linguistics</title>
		<meeting>EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Canonical correlation and its relationship to discriminant analysis and multiple regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harry R Glahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the atmospheric sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiview regression via canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="82" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal grapheme-to-phoneme prediction over latin alphabets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="332" to="343" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal data set selection: An application to graphemeto-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised consonant-vowel prediction over hundreds of languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal morphological analysis using structured nearest neighbor prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training a korean srl system with rich morphological features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heemoon</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Seop</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="637" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised slot tagging with partially labeled sequences from web search click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compact lexicon selection with spectral methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
		<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pre-training of hidden-unit crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="192" to="198" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New transfer learning techniques for disparate label sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wiki-ly supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
	<note>Proceedings of Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structure compilation: trading structure for features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language comparison through sparse multilingual word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cysouw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH</title>
		<meeting>the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast high-accuracy part-ofspeech tagging by independent classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert C Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1165" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging: Two unsupervised approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="385" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1104.2086</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Kyum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Uncertainty in Artificial Intelligence</title>
		<meeting>the Association for Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Model-based word embeddings from decompositions of count matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1282" to="1291" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Souhir Gahbiche-Braham, and François Yvon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pécheux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-lingual part-of-speech tagging through ambiguous learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="1779" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inducing multilingual text analysis tools via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Human language technology research</title>
		<meeting>the first international conference on Human language technology research</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
