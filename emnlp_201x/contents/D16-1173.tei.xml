<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Neural Networks with Massive Learned Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Neural Networks with Massive Learned Knowledge</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1670" to="1679"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and in-terpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved inter-pretability. The principled framework can also be applied to posterior regularization for regulating other statistical models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved re- markable success in a large variety of application domains ( <ref type="bibr" target="#b16">Krizhevsky et al., 2012;</ref>; <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). However, the power- ful end-to-end learning comes with limitations, in- cluding the requirement on massive amount of la- beled data, uninterpretability of prediction results, and difficulty of incorporating human intentions and domain knowledge.</p><p>To alleviate these drawbacks, recent work has fo- cused on training DNNs with extra domain-specific features <ref type="bibr" target="#b5">(Collobert et al., 2011</ref>), combining ora- cle similarity constraints ( <ref type="bibr" target="#b14">Karaletsos et al., 2016)</ref>, modeling output correlations ( <ref type="bibr" target="#b6">Deng et al., 2014)</ref>, and others. Recently, <ref type="bibr" target="#b12">Hu et al. (2016)</ref> proposed a general distillation framework that transfers knowl- edge expressed as first-order logic (FOL) rules into neural networks, where FOL constraints are inte- grated via posterior regularization ( <ref type="bibr">Ganchev et al., 2010)</ref>. Despite the intuitiveness of FOL rules and the impressive performance in various tasks, the approach, as with the previous posterior constraint methods ( <ref type="bibr">Ganchev et al., 2010;</ref><ref type="bibr" target="#b21">Liang et al., 2009;</ref>), has been limited to simple a pri- ori fixed constraints with manually selected weights, lacking the ability of inducing and adapting abstract knowledge from data. This issue is further exacer- bated in the context of regulating DNNs that map raw data directly into the label space, leaving a huge semantic gap in between, and making it unfeasible to express rich human knowledge built on the inter- mediate abstract concepts.</p><p>In this paper, we introduce a generalized frame- work which enables a learning procedure for knowl- edge representations and their weights jointly with the regulated DNN models. This greatly extends the applicability to massive structures in diverse forms, such as structured models and soft logic rules, fa- cilitating practitioners to incorporate rich domain expertise and fuzzy constraints. Specifically, we propose a mutual distillation method that iteratively transfers information between DNN and structured knowledge, resulting in effective integration of the representation learning capacity of DNN and the generalization power of structured knowledge. Our method does not require additional supervision be- yond raw data-labels for knowledge learning.</p><p>We present an instantiation of our method in the task of sentence sentiment analysis. We aug-ment a base convolutional network with linguis- tic knowledge that encourages coherent sentiment transitions across the clauses in terms of discourse relations. All uncertain modules, such as clause relation and polarity identification, are automati- cally learned from data, freeing practitioners from exhaustive specification. We further improve the model by integrating thousands of soft word polar- ity and negation rules, with their confidence directly induced from the data.</p><p>Trained with only sentence level supervisions, our model substantially outperforms plain neural net- works learned from both sentence and clause labels. Our method also shows enhanced generalization on limited data size, and improved interpretability of predictions.</p><p>Our work enjoys general versatility on diverse types of structured knowledge and neural architec- tures. The principled knowledge and weight learn- ing approach can also be applied to the posterior constraint frameworks ( <ref type="bibr">Ganchev et al., 2010;</ref><ref type="bibr" target="#b21">Liang et al., 2009</ref>) for regulating other statistical models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep Networks with Structured Knowledge Combining the powerful deep neural models with structured knowledge has been of increasing interest to enhance generalization and improve interpretabil- ity ( <ref type="bibr" target="#b19">Li et al., 2015;</ref><ref type="bibr" target="#b6">Deng et al., 2014;</ref><ref type="bibr" target="#b13">Johnson et al., 2016)</ref>. Recently, <ref type="bibr" target="#b12">Hu et al. (2016)</ref> proposed to trans- fer logical knowledge information into neural net- works with diverse architectures (e.g., convolutional networks and recurrent networks). They devel- oped an iterative distillation framework that trains the neural network to emulate the predictions of a "teacher" model which is iteratively constructed by imposing posterior constraints on the network. The framework has shown to be effective in regulating different neural models. However, the method has required fixed constraints and manually specified weights, making it unsuitable to incorporate large amount of fuzzy human intuitions where adaptation to data is necessary to obtain meaningful knowledge representations.</p><p>The limitation is in fact shared with the general- purpose posterior regularization methods ( <ref type="bibr">Ganchev et al., 2010;</ref><ref type="bibr" target="#b21">Liang et al., 2009;</ref>).</p><p>Though attempts have been made to learn the con- straint weights from additional supervisions ( <ref type="bibr" target="#b22">Mei et al., 2014</ref>) or for tractability purposes <ref type="bibr" target="#b27">(Steinhardt and Liang, 2015)</ref>, learning and optimizing knowledge expressions jointly with the regulated models from data is still unsolved, and critically restricting the application scope.</p><p>Sentiment Analysis Sentence level sentiment classification is to identify the sentiment polarity (e.g., positive or negative) of a sentence <ref type="bibr" target="#b25">(Pang and Lee, 2008)</ref>. Recently, a number of neural models have been developed and achieved new levels of per- formance <ref type="bibr" target="#b15">(Kim, 2014;</ref><ref type="bibr" target="#b26">Socher et al., 2013;</ref><ref type="bibr" target="#b18">Lei et al., 2015</ref>). Despite the impressive success, most of the existing neural network approaches require large amount of labeled data while encoding very lim- ited linguistic knowledge, making them inefficient to handle sophisticated linguistic phenomena, such as contrastive transitions and negations <ref type="bibr" target="#b4">(Choi and Cardie, 2008;</ref><ref type="bibr" target="#b3">Bhatia et al., 2015)</ref>. <ref type="bibr" target="#b12">Hu et al. (2016)</ref> combines a neural network with a logic rule that captures contrastive sense by ob- serving the word "but" in a sentence. However, such simple deterministic rules suffer from limited gener- ality and robustness. This paper develops a new sen- timent neural model that combines a large diverse set of linguistic knowledge through our enhanced framework. Our method efficiently captures com- plex linguistic patterns from limited data, and yields highly interpretable predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mutual Distillation</head><p>This section introduces the proposed framework that enables joint learning of knowledge components and their weights with the neural network models. In particular, we generalize the one-sided distillation method of (Hu et al., 2016) (section 3.1), and pro- pose to mutually transfer information between the neural network and the structured constraints for ef- fective knowledge learning (section 3.2), and opti- mize the weights by considering jointly all compo- nents (section 3.3).</p><p>We consider input variable x ∈ X and target variable y ∈ Y. For clarity we focus on classifi- cation where y is a one-hot encoding of the class labels, though our method also applies to other con- texts. Let (X, Y ) denote a set of instances of (x, y).</p><p>A neural network defines a conditional probability p θ (y|x) parameterized by θ. We will omit the sub- script θ when there is no ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Learning with Knowledge Distillation</head><p>We first review the iterative distillation method ( <ref type="bibr" target="#b12">Hu et al., 2016</ref>) that transfers structured knowledge into neural networks. Consider constraint functions f l ∈ X × Y → R, indexed by l, that encode the knowledge and we want to satisfy (i.e., maximize by optimizing the predictions y) with confidence weights λ l ∈ R. Given the current state of the neural network parameters θ at each iteration, a structure- enriched teacher network q is obtained by solving</p><formula xml:id="formula_0">min q∈P KL(q(Y )p θ (Y |X)) − C l λ l Eq[f l (X, Y )], (1)</formula><p>where P denotes the appropriate distribution space; and C is the regularization parameter. Problem <ref type="formula">(1)</ref> is convex and has a closed-form solution</p><formula xml:id="formula_1">q * (Y ) ∝ p θ (Y |X) exp C l λ l f l (X, Y ) ,<label>(2)</label></formula><p>whose normalization term can be calculated ef- ficiently according to how the constraints factor- ize ( <ref type="bibr" target="#b12">Hu et al., 2016</ref>). The neural network p θ at it- eration t is then updated with a distillation objec- tive ( <ref type="bibr" target="#b10">Hinton et al., 2015</ref>) that balances between im- itating soft predictions of teacher q and predicting true hard labels:</p><formula xml:id="formula_2">θ (t+1) = arg min θ∈Θ 1 N N n=1 (1 − π)(y n , σ θ (x n )) + π(s (t) n , σ θ (x n )),<label>(3)</label></formula><p>where denotes the loss function (e.g., cross en- tropy loss for classification);</p><formula xml:id="formula_3">σ θ (x) is the softmax output of p θ on x; s (t)</formula><p>n is the soft prediction vec- tor of q on training point x n at iteration t; N is the training size; and π is the imitation parameter calibrating the relative importance of the two objec- tives. The training procedure iterates between Eq.(2) and Eq.(3), resulting in the richly structured teacher model q and the knowledge distilled student network p. While q generally provides better accuracy, p is more lightweight and applicable to many different contexts ( <ref type="bibr" target="#b12">Hu et al., 2016;</ref><ref type="bibr" target="#b20">Liang et al., 2008</ref>).</p><p>In ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref>, the constraint f l (X, Y ) has been limited to be of the form r l (X, Y ) − 1, where r l is an FOL function yielding truth values in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and is required to be fully-specified a priori and fixed throughout the training. Besides, the constraint weight λ l has to be manually selected. This severely deviates from the characters of human knowledge which is usually abstract, fuzzy, built on high-level concepts (e.g., discourse relations, visual attributes) as opposed to low-level observations (e.g., word sequences, image pixels), and thus incomplete in the sense of end-to-end learning that maps raw in- put directly into target space of interest. This ne- cessitates expressing structured knowledge allowing some modules unknown and induced automatically from observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Learning</head><p>To substantially extend the scope of knowledge used in the framework, we introduce learnable modules φ in the knowledge expression denoted as f φ . The module φ is general, and can be, e.g., free parame- ters of structured metrics, or dependency structures over semantic units. We assume f φ can be optimized in terms of φ against a given objective (e.g., through gradient descent for parameter updating). We aim to learn the knowledge by determining φ from data.</p><p>For clarity we consider one knowledge constraint and omit the index l. We further assume the con- straint factorizes over data instances. Note that our method can straightforwardly be applied to the case of multiple constraints and constraints span- ning multiple instances. As any meaningful knowl- edge is expected to be consistent with the ob- servations, a straightforward way is then to di- rectly optimize against the training data: φ * = arg max φ 1 N n f φ (x n , y n ), and insert the result- ing f φ * in Eq.(1) for subsequent steps. However, such a pipelined method fails to establish interac- tions between the knowledge and network learning, and can lead to a sub-optimal system, as shown in our experiments.</p><p>To address this, we inspect the posterior regular- ization objective in Eq.(1), and write it in an anal- ogous form to the variational free energy of some model evidence. Specifically, let log h φ (X, Y ) Cλf φ (X, Y ), then the objective can be written as</p><formula xml:id="formula_4">− Y q(Y ) log p(Y |X)h φ (X, Y ) q(Y ) .<label>(4)</label></formula><p>Intuitively, we can view the output distribution of the neural network p(Y |X) as a prior distribution over the labels, while considering h φ (X, Y ) as defining a "likelihood" metric w.r.t the observations, making the objective analogous to a (negative) variational lower bound of the respective "model". This natu- rally inspires an EM-type algorithm <ref type="bibr" target="#b23">(Neal and Hinton, 1998</ref>) to optimize relevant parameters and im- prove the "evidence": the E-step optimizes over q, yielding Eq. <ref type="formula" target="#formula_1">(2)</ref>; and the M-step optimizes over φ. Further incorporating the true training labels with balancing parameter π , we obtain the update for φ:</p><formula xml:id="formula_5">φ (t+1) = arg max φ∈Φ 1 N N n=1 (1 − π )h φ (x n , y n ) + π E q (t) (y) [h φ (x n , y)]<label>(5)</label></formula><p>The update rule resembles the distillation objective for learning parameters θ in Eq. <ref type="formula" target="#formula_2">(3)</ref>. Indeed, the ex- pectation term in Eq. <ref type="formula" target="#formula_5">(5)</ref> in effect optimizes h φ on examples labeled by q(y), i.e., forcing the knowl- edge function to mimic the predictions of the teacher model and distill encoded information. Thus, be- sides transferring from structured knowledge to a neural model by Eq. <ref type="formula" target="#formula_2">(3)</ref>, we now further bridge from the neural network to the knowledge constraints for joint learning and better integrating the best of both worlds. We call our framework with the symmet- ric objectives as mutual distillation. In fact, we can view Eq.(4) as a single joint objective and we are alternating optimization of θ and φ, resulting in the update rules in Eq. <ref type="formula" target="#formula_2">(3)</ref> and Eq.(5) with the supervised loss terms included, respectively (and with the loss function in Eq.(3) being cross-entropy loss). Additionally, the resemblance of the two objec- tives indicates that we can readily translate the suc- cessful neural learning method to knowledge learn- ing. For instance, the expectation term in Eq.(5), as the second loss term in Eq.(3), can be evaluated on rich unlabeled data in addition to labeled exam- ples, enabling semi-supervised learning which has shown to be useful ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref>. Empirical stud- ies show superiority of the proposed method over several potential alternatives (section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Weight Learning</head><p>Besides optimizing the knowledge representations, we also aim to automate the selection of constraint weights by learning from data. This would enable us to incorporate massive amount of noisy knowl- edge, without the need to worry about the confidence which is usually unfeasible to set manually.</p><p>As the constraint weights serve to balance be- tween the different components of the whole frame- work, we learn the weights by optimizing the regu- larized joint model q (see Eq. <ref type="formula" target="#formula_1">(2)</ref>):</p><formula xml:id="formula_6">λ (t+1) = arg max λ≥0 1 N N n=1 q λ (y n )<label>(6)</label></formula><p>This is also validated in the view of regularized Bayes ( ) where q is a generalized posterior function by regularizing the standard pos- terior p (see Eq. <ref type="formula">(1)</ref>). Although here, we omit the Bayesian treatment of the weights λ and instead optimize them directly to find the posterior. It is straightforward to impose priors over λ to encode preferences. In practice, Eq. (6) can be carried out through gradient descent. The training procedure of the proposed mutual distillation is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Mutual Distillation</head><p>Input:</p><formula xml:id="formula_7">Training data D = {(x n , y n )} N n=1 , Initial knowledge constraints F = {f φ,l } L l=1</formula><p>, Initial neural network p θ , Parameters: π, π' -imitation parameters C -regularization parameters 1: Initialize neural network parameters θ 2: Initialize knowledge parameters φ and weights λ 3: while not converged do Update f l (l = 1, . . . , L) with distillation objec- tive Eq. <ref type="formula" target="#formula_5">(5)</ref> 8: end while Output: Learned network p, knowledge modules F, and the joint teacher network q</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sentiment Classification</head><p>This section provides a concrete instance of our general framework in the task of sentence sentiment analysis. We augment a base convolutional network with a large diverse set of linguistic knowledge, in- cluding 1) sentiment transition structure for coher- ent multi-level prediction, 2) conjunction word rules  for improving discourse relation identification, and 3) word polarity rules for tackling negations. These knowledge structures are fulfilled with neural net- work modules that are learned jointly within our framework. The resulting model efficiently captures sophisticated linguistic patterns from limited data, and produces interpretable predictions. <ref type="figure" target="#fig_1">Figure 1</ref> shows an overview of our model. We assume binary sentiment labels (i.e., positive-1 and negative-0). The left part of the figure is the base neural network for sentence classification. Since our framework is agnostic to the neural architecture, we can use any off-the-shelf neural models such as con- volutional network and recurrent network. Here we choose the simple yet effective convolutional net- work proposed in <ref type="bibr" target="#b15">(Kim, 2014</ref>). The network takes as input the word embedding vectors of a given sen- tence, and extracts feature maps with a convolutional layer followed by max-over-time pooling. A final fully-connected layer with softmax activation trans- forms the extracted features into a prediction vector.</p><p>We next introduce the three types of domain knowledge, which leverage rich fine-grained level structures, from clauses to words, to guide sentence level prediction. The clause segmentation of sen- tences is obtained using the public Stanford parser 1 .</p><p>Sentiment transition by discourse relation Dis- course structures characterize how the clauses (i.e., <ref type="bibr">1</ref> http://nlp.stanford.edu/software/openie.html discourse units) of a sentence are connected with each other and thereby provide clues for coher- ent sentence and clause labeling. Instead of us- ing standard general-purpose discourse relation sys- tem, we define three types of relations between ad- jacent clauses (denoted as c i and c i+1 ) specific to sentiment change, namely, consistent (c i and c i+1 have the same polarity), contrastive (c i+1 opposes c i and is the main part), and concessive (c i+1 op- poses c i and is secondary). The relations also indi- cate the connections between clauses and the whole sentence. For instance, a contrastive relation typi- cally indicates c i+1 has the same polarity with the full sentence (we reasonably assume a sentence has contrastive sense in at most one position). To en- code these dependencies we define sentiment tran- sition matrices conditioned on discourse relation r and sentence polarity y, denoted as M r,y . For in- stance, given r = contrastive and y = 0, we expect the sentiment change between two adjacent clauses to follow</p><formula xml:id="formula_8">M r=contrastive,y=0 = 0 0 1 0 ,<label>(7)</label></formula><p>i.e., transiting from positive polarity of c i to negative of c i+1 . We list all transition matrices in supplement.</p><p>We now design a constraint on sentence predic- tions leveraging the above knowledge. Using the identification modules presented shortly, we first get the discourse relation probabilities p r i,i+1 as well as the sentiment polarity probabilities p c i and p c i+1 of adjacent clauses (c i , c i+1 ). For a given sentence la- bel y s , we then compute the expected transition ma- trix at each position by ¯ M i,ys = E p r i,i+1</p><p>[M r,ys ]. The value of the constraint function on y = y s is then defined as the probability of the most likely clause polarity configuration according to the clause pre- dictions p c · and the averaged transitions ¯ M ·,ys :</p><formula xml:id="formula_9">f st (x, y s ) = max a∈{0,1} m i p r i,ai · ¯ M i,ys,aiai+1 , (8)</formula><p>where a is the polarity configuration and m is the number of clauses. We use the Viterbi algorithm for efficient computation. We need the clause relation and polarity proba- bilities p r and p c , which are unfeasible to identify from raw text with only simple deterministic rules. We apply a convolutional network for each module, with similar network architectures to the base net- work (we describe details in the supplement). For ef- ficiency, we tie the convolutional parameters across all the networks, while leaving the parameters of the fully-connected layers to be learned individually.</p><p>Conjunction word rules We enhance the dis- course relation neural network with robust clues from explicit discourse connectives (e.g., "but", "and", etc.) that occur in the sentence. In particular, we collect a set of conjunction words (listed in the supplement) and specify a rule constraint for each of them. For instance, the conjunction "and" results in the following constraint function:</p><formula xml:id="formula_10">f rel (c i , c i+1 , r) = (1 and (c i , c i+1 ) ⇒ r = consistent) ,</formula><p>where 1 and (c i , c i+1 ) is an indicator function that takes 1 if the two clauses are connected by "and", and 0 otherwise. Note that these rules are soft, with the confidence weights learned from data. We use the regularized joint model over the base discourse network for predicting the relations.</p><p>Negation and word polarity rules Negations re- verse the polarity of relevant statements. Identifying negation sense has been a challenging problem for accurate sentiment prediction. We address this by incorporating rich lexicon rules at the clause level. That is, if a polarity-carrying word (e.g., "good") occurs in the scope of a negator (e.g., "not"), then the sentiment prediction of the clause is encouraged to be the opposite polarity. We specify one separate rule for each polarity-carrying word from public lex- icons (see the supplement), e.g.,</p><formula xml:id="formula_11">f lex (c i , y c ) = 1 good (c i ) ⇒ y c = negative ,<label>(9)</label></formula><p>where 1 good (c i ) is an indicator function that takes 1 if word "good" occurs in a negation scope in the clause text, and 0 otherwise. This results in over 3,000 rules, and our automated weight optimization frees us from manually selecting the weights ex- haustively. We define the negation scope to be the 4 words following a negator <ref type="bibr" target="#b4">(Choi and Cardie, 2008)</ref>. Though polarities of single words can be brit- tle features for determining the sentiment of a long statement due to complex semantic compositions, they are more robust and effective at the level of clauses which are generally short and simple. More- over, inaccurate rules will be downplayed through the weight learning procedure.</p><p>We have presented our neural sentiment model. We tackle several long-standing challenges by di- rectly incorporating linguistic knowledge. Compar- ing to previous work that designs various neural ar- chitectures and relies on substantial annotations for specific issues <ref type="bibr" target="#b26">(Socher et al., 2013;</ref><ref type="bibr" target="#b3">Bhatia et al., 2015)</ref>, our knowledge framework is more straight- forward, interpretable, and general, while still pre- serving the power of neural methods.</p><p>Notably, even with several additional compo- nents to be learned for knowledge representation, our method does not require extra supervision sig- nals beyond the raw sentence-labels, making our framework generally applicable to many different tasks ( <ref type="bibr" target="#b24">Neelakantan et al., 2016)</ref>.</p><p>The sentiment transition knowledge is expressed in the form of structured model with features ex- tracted using neural networks. Though apparently similar to recent deep structured models such as neural-CRFs ( <ref type="bibr" target="#b8">Durrett and Klein, 2015;</ref><ref type="bibr" target="#b0">Ammar et al., 2014;</ref><ref type="bibr" target="#b7">Do et al., 2010)</ref>, ours is different since we parsimoniously extract features that are neces- sary for precise and efficient knowledge expression, as opposed to neural-CRFs that learn as rich repre- sentations as possible for final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our method on the widely-used sen- timent classification benchmarks. Our knowledge Model Accuracy (%) sentences 1 CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 86.6 2 CNN+REL q: 87.8; p: 87.1 3 CNN+REL+LEX q: 88.0; p: 87.2 sentences 4 MC-CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 86.8 5 Tensor-CNN ( <ref type="bibr" target="#b18">Lei et al., 2015)</ref> 87.0 6 CNN+But-q ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref> 87.1 +phrases 7 CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 87.2 8 Tree-LSTM ( <ref type="bibr" target="#b28">Tai et al., 2015)</ref> 88.0 9 MC-CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 88.1 10 CNN+But-q ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref> 89.2 11 MVCNN (Yin and Schutze, 2015) 89.4 <ref type="table">Table 1</ref>: Classification performance on SST2. The top and second blocks use only sentence-level annotations for training, while the bottom block uses both sentence-and phrases-level annotations. We report the accuracy of both the regularized teacher model q and the student model p after distillation. enriched model significantly outperforms plain neu- ral networks. We obtain even higher improvements with limited data sizes. Comparison with extensive other potential knowledge learning methods shows the effectiveness of our framework. Our model also shows improved interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Datasets Two classification benchmarks are used: 1) Stanford Sentiment Treebank-2 (SST2) <ref type="bibr" target="#b26">(Socher et al., 2013</ref>) is a binary classification dataset that consists of 6920/872/1821 moview review sentences in the train/dev/test sets, respectively. Besides sentence-level annotations, the dataset also provides exhaustive gold-standard labels at fine-grained lev- els, from clauses to phrases. The resulting full train- ing set includes 76,961 labeled instances. We train our model using only the sentence-level annotations, and compare to baselines learned from either train- ing set. 2) Customer Reviews (CR) (Hu and Liu, 2004) consists of 3,775 product reviews with pos- itive and negative polarities. Following previous work we use 10-fold cross-validation.</p><p>Model configurations We evaluate two variants of our model: CNN+REL leverages the knowledge of sentiment transition and discourse conjunctions, and CNN+REL+LEX additionally incorporates the negation lexicon rules.</p><p>Throughout the experiments we set the regulariza- tion parameter to C = 10. The imitation parameters π and π decay as π (t) = π (t) = 0.9 t where t is training size 10% 30% 50% 100%</p><p>accu <ref type="formula">(</ref> the iteration number <ref type="bibr" target="#b2">(Bengio et al., 2015;</ref><ref type="bibr" target="#b12">Hu et al., 2016)</ref>. For the base neural network, we choose the "non-static" version from <ref type="bibr" target="#b15">(Kim, 2014</ref>) and use the same configurations. <ref type="table">Table 1</ref> shows the classification performance on the SST2 dataset. From rows 1-3 we see that our pro- posed sentiment model that integrates the diverse set of knowledge (section 4) significantly outper- forms the base CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref>. The improve- ment of the student network p validates the effec- tiveness of the iterative mutual distillation process. Consistent with the observations in ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref>, the regularized teacher model q provides further per- formance boost, though it imposes additional com- putational overhead for explicit knowledge repre- sentations. Note that our models are trained with only sentence-level annotations. Compared with the baselines trained in the same setting (rows 4-6), our model with the full knowledge, CNN+REL+LEX, performs the best. CNN+But-q (row 6) is the base CNN augmented with a logic rule that identifies con- trastive sense through explicit occurrence of word "but" (section 3.1) ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref>. Our enhanced framework enables richer knowledge and achieves much better performance. Our method further outperforms the base CNN that is additionally trained with dense phrase-level annotations (row 7), showing improved generaliza- tion of the knowledge-enhanced model from limited data. <ref type="figure" target="#fig_2">Figure 2</ref> further studies the performance with varying training sizes. We can clearly observe that the incorporated knowledge tends to offer higher im- provement with less training data. This property can be particularly desirable in applications of structured predictions where manual annotations are expensive while rich human knowledge is available. 1 CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 84.1±0.2 2 CNN+REL q: 85.0±0.2; p: 84.7±0.2 3 CNN+REL+LEX q: 85.3±0.3; p: 85.0±0.2 4 MC-CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 85.0 5 Bi-RNN ( <ref type="bibr" target="#b17">Lai et al., 2015)</ref> 82   <ref type="table" target="#tab_2">Table 2</ref> shows model performance on the CR dataset. Our model again surpasses the base net- work and several other competitive neural methods by a large margin. Though falling behind AdaSent (row 7) which has a more specialized and complex architecture than standard convolutional networks, the proposed framework indeed is general enough to apply on top of it for further enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification Results</head><p>To further evaluate the proposed mutual distilla- tion framework for learning knowledge, we compare to an extensive set of other possible knowledge op- timization approaches. <ref type="table">Table 3</ref> shows the results. In row 2, the "opt-joint" method optimizes the reg- ularized joint model of Eq.(2) directly in terms of both the neural network and knowledge parameters. Row 3, "opt-knwl-pipeline", is an approach that first optimizes the standalone knowledge component and then inserts it into the previous framework of ( <ref type="bibr" target="#b12">Hu et al., 2016</ref>) as a fixed constraint. Without interaction between the knowledge and neural network learn- ing, the pipelined method yields inferior results. Fi- nally, rows 4-5 display a method that adapts the knowledge component at each iteration by optimiz- ing the joint model q in terms of the knowledge pa- rameters. We report the accuracy of both the student network p (row 4) and the joint teacher network q (row 5), and compare with our method in row 6 and 7, respectively. We can see that both models per- forms poorly, achieving the accuracy of only 68.6% for the knowledge component, similar to the accu- racy achieved by the "opt-joint" method.</p><p>In contrast, our mutual distillation framework of- fers the best performance. <ref type="table">Table 3</ref> shows that the knowledge component as a standalone classi- fier does not achieve high accuracy (the numbers in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy (%)</p><p>1 CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 86.6 2 opt-joint 86.9 (68.8) 3 opt-knwl-pipeline 86.7 (70.4) 4 opt-joint-iterative-p 86.9 5 opt-joint-iterative-q 87.6 (68.6) 6 mutual-p 87.2 7 mutual-q 88.0 (72.5)  enough, good, strong, engaging, great awful, loses, fake doubt, bad parentheses). As discussed in section 4, this is be- cause of the parsimonious formulation for the pre- cise knowledge expression, while leaving the ex- pressive base NN to extract rich representations. The enhanced performance of the combination indicates complementary effects of the two parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head><p>Our model not only provides better classification performance, but also shows improved interpretabil- ity due to the learned structured knowledge repre- sentation. <ref type="figure">Figure 3</ref> illustrates an example sentence from test set. We see that the clause sentiments as well as the discourse relation are correctly captured. The negation rule of "not ... funny" (Eq.(9)) also helps to identify the right polarity. <ref type="table" target="#tab_4">Table 4</ref> lists the top-5 positive and negative words that are most confident for the negation rules, pro- viding insights into the linguistic norms in the movie review context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have developed a framework that learns structured knowledge and its weights for reg- ulating deep neural networks through mutual distil- lation. We instantiated our framework for the senti- ment classification task. Using massive learned lin- guistic knowledge, our neural model provides sub- stantial improvements over many of the existing ap- proaches, especially in the limited data setting. In the future work, we plan to apply our framework to other text and vision applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4: Sample a minibatch (X, Y ) ⊂ D 5: Build the teacher model q with Eq.(2) and Eq.(6) 6: Update p θ with distillation objective Eq.(3) 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our sentiment classification model. The left part is the base convolutional network over sentences, and the right part is the knowledge component over clauses. Blue arrows denote neural feed-forwards; red arrows denote knowledge incorporation steps; and the orange dashed arrows denote the distillation processes. The convolutional parameters are shared across all the networks.</figDesc><graphic url="image-5.pbm" coords="5,152.24,112.45,85.14,94.76" type="bitmap" mask="true" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance with varying sizes of training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 :Figure 3 :</head><label>33</label><figDesc>Figure 3: An example sentence and the results of the learned knowledge modules applied on it. Red denotes positive, and blue denotes negative. The snippet "not ... funny" triggers the negation rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classification performance on the CR dataset. We 

report the average accuracy±one standard deviation with 10-

fold CV. The top block compares the base CNN (row 1) with 

the knowledge-enhanced CNNs by our framework. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The top 5 positive (left) and negative (right) words 

with the largest weights of the negation rules. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able comments. This work is supported by NSF IIS1218282, NSF IIS1447676, Air Force FA8721-05-C-0003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional random field autoencoders for unsupervised structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from rst discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning with compositional semantics as structural inference for subsentential sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trinh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Arti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan Klein ; Joao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural CRF parsing. Kuzman Ganchev</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">B</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian representation learning with oracle constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename><surname>Tech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eudard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure compilation: trading structure for features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from measurements in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust RegBayes: Selectively incorporating first-order logic domain knowledge into Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shike</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="253" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning with relaxed supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Percy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2809" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-aware learning for sentence-level sentiment analysis with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="325" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multichannel variable-size convolution for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CONLL</title>
		<meeting>of CONLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1799" to="1847" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
