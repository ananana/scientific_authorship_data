<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<orgName type="institution">A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><surname>Cheung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†φ Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4492" to="4502"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4492</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning a matching function between two text sequences is a long standing problem in NLP research. This task enables many potential applications such as question answering and paraphrase identification. This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. CSRAN is a deep architecture, involving stacked (multi-layered) recurrent encoders. Stacked/Deep architec-tures are traditionally difficult to train, due to the inherent weaknesses such as difficulty with feature propagation and vanishing gradients. CSRAN incorporates two novel components to take advantage of the stacked architecture. Firstly, it introduces a new bidirec-tional alignment mechanism that learns affinity weights by fusing sequence pairs across stacked hierarchies. Secondly, it leverages a multi-level attention refinement component between stacked recurrent layers. The key intuition is that, by leveraging information across all network hierarchies, we can not only improve gradient flow but also improve overall performance. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Determining the semantic affinity between two text sequences is a long standing research prob- lem in natural language processing research. This is understandable, given that technical innovations in this domain would naturally bring benefits to a diverse plethora of applications ranging from para- phrase detection to standard document retrieval. This work focuses on short textual sequences, fo- cusing on a myriad of applications such as natu- ral language inference, question answering, reply * Denotes equal contribution. prediction and paraphrase detection. This paper presents a new deep matching model for universal text matching.</p><p>Neural networks are dominant state-of-the-art approaches for many of these matching problems ( <ref type="bibr" target="#b4">Gong et al., 2017;</ref><ref type="bibr" target="#b31">Shen et al., 2017;</ref><ref type="bibr" target="#b48">Wang et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2017)</ref>. Fundamentally, neu- ral networks operate via a concept of feature hi- erarchy, in which hierarchical representations are constructed as sequences propagate across the net- work. In the context of matching, representations are often (1) encoded, (2) matched, and then (3) aggregated for prediction. Each key step often comprises several layers, which consequently adds to the overall depth of the network.</p><p>Unfortunately, it is a well established fact that deep networks are difficult to train. This is at- tributed to not only vanishing/exploding gradi- ents but also an instrinsic difficulty pertaining to feature propagation. To this end, commonly adopted solutions include Residual connections (  and/or Highway layers <ref type="bibr" target="#b33">(Srivastava et al., 2015)</ref>. The key idea in these approaches is to introduce additional (skip/residual) connec- tions, propagating shallower layers to deeper lay- ers via shortcuts. To the best of our knowledge, these techniques are generally applied to single se- quences and therefore the notion of pairwise resid- ual connections have not been explored.</p><p>This paper presents Co-Stack Residual Affin- ity Networks (CSRAN), a stacked multi-layered recurrent architecture for general purpose text matching. Our model proposes a new co-stacking mechanism that computes bidirectional affinity scores by leveraging all feature hierarchies be- tween text sequence pairs. More concretely, word- by-word affinity scores are not computed just from the final encoded representations but across all the entire feature hierarchy.</p><p>There are several benefits to our co-stacking mechanism. Firstly, co-stacking acts as a form of residual connector, alleviating the instrinsic is- sues with network depth. Secondly, there are more extensive matching interfaces between text sequences as the affinity matrix is not computed by just one representation but multiple represen- tations instead. Naturally, increasing the oppor- tunities for interactions between sequences is an intuitive method for improving performance. Additionally, our model incorporates a Multi- level Attention Refinement (MAR) architecture in order to fully leverage the stacked recurrent archi- tecture. The MAR architecture is a multi-layered adaptation and extension of the CAFE model ( <ref type="bibr" target="#b38">Tay et al., 2017c)</ref>, in which attention is computed, compressed and then re-fed into the input se- quence. In our approach, we use CAFE blocks to repeatedly refine representations at each level of the stacked recurrent encoder.</p><p>The overall outcome of the above-mentioned ar- chitectural synergies is a highly competitive model that establishes state-of-the-art performance on six well-known text matching datasets such as SNLI and TrecQA. The overall contributions of this work are summarized as follows:</p><p>• We propose a new deep stacked recurrent ar- chitecture for matching text sequences. Our model is based on a new co-stacking mech- anism which learns to align by exploiting matching across feature hierarchies. This can be interpreted as a new way to incorpo- rate shortcut connections within neural mod- els for sequence matching. Additionally, we also propose a multi-level attention refine- ment scheme to leverage our stacked recur- rent model.</p><p>• While stacked architectures can potentially lead to considerable improvements in per- formance, our experiments show that in the absence of our proposed CSRA (Co-stack Residual Affinity) mechanism, stacking may conversely lead to performance degradation. As such, this demonstrates that our proposed techniques are essential for harnessing the potential of deep architectures.</p><p>• We conduct extensive experiments on four text matching tasks across six well-studied datasets, i.e., Natural Language Inference (SNLI ( <ref type="bibr" target="#b0">Bowman et al., 2015</ref>), SciTail ( <ref type="bibr" target="#b11">Khot et al., 2018)</ref>), Paraphrase Identification  <ref type="bibr">Quora, TwitterURL (Lan et al., 2017)</ref>), An- swer Sentence Selection ( <ref type="bibr" target="#b45">Wang et al., 2007)</ref> and Utterance-Response Matching (Ubuntu ( <ref type="bibr" target="#b15">Lowe et al., 2015)</ref>). Our model achieves state-of-the-art performance on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Co-Stack Residual Affinity Networks</head><p>In this section, we introduce our proposed model architecture for general/universal text matching.</p><p>The key idea of this architecture is to leverage deep stacked layers, while mitigating the inherent weaknesses of going deep. As such, our network is in similar spirit to highway networks, residual net- works and DenseNets, albeit tailored specifically for pairwise architectures. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates a high-level overview of our proposed model archi- tecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Encoder</head><p>The inputs to our model are standard sequences of words A and B which represent sequence a and sequence b respectively. In the context of different applications, a and b take different roles such as premise/hypothesis or question/answer. Both se- quences are converted into word representations (via pretrained word embeddings) and character- based representations. Character embeddings are trainable parameters and a final character-based word representation of d dimensions is learned by passing all characters into a Bidirectional LSTM encoder. This is standard, following many works such as ( <ref type="bibr" target="#b48">Wang et al., 2017)</ref>. Word embeddings and character-based word representations are then concatenated to form the final word representa- tion. Next, the word representation is passed through a (optional and tuned as a hyperparame- ter) 2-layered highway network of d dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stacked Recurrent Encoders</head><p>Next, word representations are passed into a stacked recurrent encoder layer. Specifically, we use Bidirectional LSTM encoders at this layer. Let k be the number of layers of the stacked recurrent encoder layer.</p><formula xml:id="formula_0">h i t = BiLSTM i (h t−1 ) ∀t ∈ [1, 2 · · · ] (1)</formula><p>where BiLSTM i represents the i-th BiLSTM layer and h i t represents the t-th hidden state of the i-th BiLSTM layer. is the sequence length. Note that the parameters are shared for both a and b.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-level Attention Refinement (MAR)</head><p>Inspired by CAFE (Tay et al., 2017c) (Compare- Align-Factorized Encoders), a top performing model on the SNLI benchmark, we utilize CAFE blocks between the BiLSTM layers. Each CAFE block returns six features, which are generated by a factorization operation using factorization ma- chines (FM). While the authors in ( <ref type="bibr" target="#b38">Tay et al., 2017c</ref>) simply use this operation in a single layer, we utilize this in a multi-layered fashion which we found to have worked well. This constitutes our multi-level attention refinement mechanism. More concretely, we apply the CAFE operation to the outputs of each BiLSTM layer, allowing the next BiLSTM layer to process the 'augmented' repre- sentations. The next layer retains its dimension- ality by projecting the augmented representation back to its original size using the BiLSTM en- coder. This can be interpreted as repeatedly refin- ing representations via attention. As such, adding CAFE blocks is a very natural fit to the stacked recurrent architecture. </p><formula xml:id="formula_1">M (x) = w 0 + n i=1 w i x i + n i=1 n j=i+1 v i , v j x i x j (3)</formula><p>where v ∈ R d×k , w 0 ∈ R and w i ∈ R d . The output M (x) is a scalar. Intuitively, this layer tries to learn pairwise interactions between every x i and x j using factorized (vector) parameters v.</p><p>Factorization machines model low-rank struc- ture within the matching vector, producing a scalar feature. This enables efficient propagation of these matching features to the next layer. The output of each CAFE block is the original input to the CAFE module, augmented with the output of the factor- ization machines. As such, if the input sequence is of d dimensions, then the output is d + 3 di- mensions. Additionally, intra-attention is applied in similar fashion as above to generate three more features for each sequence. As a result, the output dimensions for each word becomes d + 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Co-Stack Residual Affinity (CSRA)</head><p>This layer is the cornerstone of our proposed ap- proach and is represented as the middle segment of <ref type="figure" target="#fig_1">Figure 1</ref> (the colorful matrices).</p><p>Co-Stacking Co-stacking refers to the fusion of a and b across multiple hierarchies. Recall that the affinity score between two words is typically com- puted by s ij = a b. We extend this to a residual formulation. More concretely, the affinity score between both words is now computed as the max- imum influence it has over all layers.</p><formula xml:id="formula_2">s ij = max p q a pi b qj (4)</formula><p>where a pi is the i-th word for the p-th stacked layer for a and b qj is the j-th word for the q-th stacked layer for b. The choice of the maximum oper- ator is intuitive and is strongly motivated by the fact that we would like to give a high affinity for each word pair that shows a strong match at any of different hierarchical stages of learning repre- sentations. Note that this layer can be interpreted as constructing a matching tensor based on multi- hierarchical information and selecting the most in- formative match across all representation hierar- chies.</p><p>Bidirectional Alignment In order to learn (bidi- rectionally) attentive representations, we first con- catenate all stacked outputs to form a × kd vec- tor. Next, we apply the following operations to A ∈ R a×kd and B ∈ R b ×kd .</p><p>¯ A = S B and ¯ B = AS</p><p>where ¯ A ∈ R b ×kd , ¯ B ∈ R a×kd are the attentive (aligned) representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Matching and Aggregation Layer</head><p>Next, we match the attentive (aligned) represen- tations using the subtraction, element-wise multi- plication and concatenation of each aligned word. Subsequently, we pass this matching vector into a k layered BiLSTM layer.</p><formula xml:id="formula_4">a i = BiLSTM k ([ ¯ b i − a i , ¯ b i a i , ¯ b i , a i ]) (6) b i = BiLSTM k ([¯ a i − b i , ¯ a i b i , ¯ a i , b i ])<label>(7)</label></formula><p>The final feature representation is learned via the summation across the temporal dimension as fol- lows:</p><formula xml:id="formula_5">z = [ a i=1 a i ; b i=1 b i ]<label>(8)</label></formula><p>where <ref type="bibr">[.; .]</ref> is the concatenation operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Output and Prediction Layer</head><p>Our model predicts using the feature vector z for every given sequence pair. At this layer, we utilize standard fully connected layers. The number of output layers is typically 2-3 and is a tuned hy- perparameter. Softmax is applied onto the final layer. The final layer is application specific, e.g., k classes for classification tasks and a two-class softmax for pointwise ranking. For all datasets, we optimize the cross entropy loss.   <ref type="bibr" target="#b31">Shen et al., 2017)</ref> and the recent MCAN model, i.e., Multi-Cast Attention Networks ( <ref type="bibr" target="#b40">Tay et al., 2018c)</ref>. A leaderboard is maintained at https: //aclweb.org/aclwiki/Question_ Answering_(State_of_the_art).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Task</head><note type="other">|C| Pairs SNLI Premise-Hypothesis 3 570K Scitail Premise-Hypothesis 2 27K Quora Question-Question 2 400K Twitter Tweet-Tweet 2 51K TrecQA Question-Answer R 56K Ubuntu Utterance-Response R 1M</note><p>Ubuntu ( <ref type="bibr" target="#b15">Lowe et al., 2015</ref>) is a dataset for Utterance-Response Matching and comprises 1- million utterance-response pairs. This dataset is based on the Ubuntu dialogue corpus. The goal is to predict the response to a message. We use the same setup as ( <ref type="bibr" target="#b50">Wu et al., 2016</ref> Metrics For all datasets, we follow the evalua- tion procedure from all the original papers. The metric for SNLI, SciTail and Quora is the accuracy metric. The metric for the TwitterURL dataset is the F1 score. The metric for TrecQA is the Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) metric. The metric for Ubuntu is the Recall@K for k = 1, 2, 5 (given 9 negative sam- ples) and the binary classification accuracy score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>All baselines are reported from the respective pa- pers. All models are trained with the Adam opti- mizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with learning rates tuned amongst {0.001, 0.0003, 0.0004}. Batch size is tuned amongst {32, 64, 128, 256}. The dimensions of the BiLSTM encoders are tuned amongst {64, 100, 200, 300} and the number of hidden dimensions of the prediction layers are tuned amongst {100, 200, 300, 600}. The num- ber of stacked recurrent layers is tuned from <ref type="bibr">[2,</ref><ref type="bibr">5]</ref> and the number of aggregation BiLSTM layers is tuned amongst {1, 2}. The number of predic- tion layers is tuned from <ref type="bibr">[1,</ref><ref type="bibr">3]</ref>. Parameters are initialized using glorot uniform <ref type="bibr" target="#b3">(Glorot and Bengio, 2010</ref>). All unspecified activation functions are ReLU activations. Word embeddings are ini- tialized with GloVe ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>) and fixed during training. We implement our model in Tensorflow ( <ref type="bibr">Abadi et al., 2015)</ref> and use the CUDNN implementation for all BiLSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>Overall, our proposed CSRAN architecture achieves state-of-the-art performance on all six well-established datasets.</p><p>On SNLI <ref type="table" target="#tab_2">(Table 2)</ref>, CSRAN achieves the best 2 single model performance to date on the well- established dataset. This demonstrates the effec- tiveness of CSRAN, taking into consideration of the inherent competitiveness of this well-known benchmark. On SciTail <ref type="table" target="#tab_3">(Table 3)</ref>, CSRAN simi- larly achieves the best performance to date on this dataset, outperforming the existing CAFE model by +3.4% absolute accuracy.</p><p>On Quora <ref type="table" target="#tab_4">(Table 4)</ref>, CSRAN also achieves the best single model score, outperforming strong baselines such as BiMPM (+1.1%) and DIIN (+0.2%). Moreover, there is also considerable performance improvement on the TwitterURL dataset <ref type="table" target="#tab_5">(Table 5)</ref> as CSRAN outperforms the exist- ing state-of-the-art Subword + LM model (+8%) and Deep Pairwise Word (+9.1%).</p><p>On TrecQA <ref type="table" target="#tab_6">(Table 6</ref>), CSRAN achieves the best performance on this dataset. CSRAN outper- forms the existing state-of-the-art model, IWAN (+3.2%/ + 4.6%). CSRAN also outperforms strong competitive baselines such as BiMPM (+5.2%/+3.6%) and MPCNN (+5.3%/+5.8%). Finally, on Ubuntu <ref type="table" target="#tab_7">(Table 7)</ref>, CSRAN also outper- forms many competitive models such as CNTN, APLSTM and KEHNN. Performance improve- ment over all metrics are ≈ 9% − 10% compared to the existing state-of-the-art.</p><p>Overall, CSRAN achieves state-of-the-art per- formance on six well-studied datasets. On sev- eral datasets, our achieved performance is not only the highest reported score but also outperforms the existing state-of-the-art models by a considerable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc BiMPM ( <ref type="bibr" target="#b48">Wang et al., 2017)</ref> 87.5 ESIM ( <ref type="bibr" target="#b1">Chen et al., 2017)</ref> 88.0 DIIN ( <ref type="bibr" target="#b4">Gong et al., 2017)</ref> 88.0 DR-BiLSTM ( <ref type="bibr" target="#b2">Ghaeini et al., 2018)</ref> 88.5 CAFE ( <ref type="bibr" target="#b38">Tay et al., 2017c)</ref> 88.5 CSRAN 88.7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc DecompAtt (Parikh et al., 2016) 72.3 ESIM ( <ref type="bibr" target="#b1">Chen et al., 2017)</ref> 70.6 DGEM ( <ref type="bibr" target="#b11">Khot et al., 2018)</ref> 77.3 CAFE ( <ref type="table" target="#tab_1">Tay et al., 2017c)</ref> 83.3 CSRAN 86.7       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Training Efficency</head><p>With many BiLSTM layers, it is natural to be skeptical about the training efficiency of our model. However, since we use the CUDNN im- plementation of the BiLSTM model, the runtime is actually very manageable. On SNLI, with a batch size of 128, our model with 3 stacked re- current layers and 2 aggregation BiLSTM layers runs at ≈17 minutes per epoch and converges in less than 20 epochs. On SciTail, our model runs at ≈ 2 minutes per epoch with a batch size of 32. This is benchmarked on a TitanXP GPU. While our model is targetted at performance and not effi- ciency, this section serves as a reassurance that our model is not computationally prohibitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>In order to study the effectiveness of the key com- ponents in our proposed architecture, we conduct an extensive ablation study. <ref type="table" target="#tab_9">Table 8</ref> reports the results on several ablation baselines. There are three key ablation baselines as follows: <ref type="formula">(1)</ref>   MAR and CSRA are critical components in our model, i.e., removing any of them would result in a drop in performance. Secondly, we observe that the relative utility of CSRA and MAR de- pends on the dataset. Removing MAR sigificantly reduces performance on SciTail and TrecQA. On the other hand, removing CSRA degrades the per- formance more than MAR on SNLI. Finally, it is good to note that, while performance degrada- tion on SNLI development set may not seem sig- nificant, the w/o MAR and CSRA ablation perfor- mance baseline achieved only 87.7% accuracy on the test set, compared to 88.7% of the original model. This is equivalent to dropping from state- of-the-art to the 5th ranked model. Overall, we are able to conclude that the CSRA and MAR make meaningful improvements to our model architec- ture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effect of Stack Depth</head><p>In this section, our goals are twofold -(1) study- ing the effect of stack depth on model performance and (2) determining if the proposed CSRAN model indeed helps with enabling deeper stack depths. In order to do so, we compute the devel- opment set performance of two models. The first is the full CSRAN architecture and the second is a baseline stacked model architecture. Note that the bidirectional alignment layer and remainder of the model architecture (highway layers, etc.) re- main completely identical to CSRAN to make this study as fair as possible.  <ref type="figure" target="#fig_7">Figure 2</ref> illustrates the model performance with varying stack depth. As expected, the performance of the stacked model declines when increasing the stack depth. On the other hand, the performance of CSRAN improves by adding additional layers. The largest gain is when jumping from 2 layers to 3 layers. The subsequent performance improve- ment from 3-5 layers is marginal. From this study, the takeaway is that standard stacked architectures are insufficient. As such, our proposed CSRA mechanism can aid in enabling deeper models which can result in stronger model performance <ref type="bibr">3</ref> .</p><p>Next, we study the general effect of stack depth (number of layers) on model performance. <ref type="figure" target="#fig_8">Fig- ure 3</ref> reports the model performance (dev accu- racy) of our CSRAN architecture on Quora and SNLI datasets. We observe that a stacked archi- tecture with 3 layers is significantly better than a single-layered architecture. The optimal devel- opment score is 3-4 layers for SNLI and 3 layers for Quora. However, we observe the performance of Quora declines after 3 layers (notably it is still higher than an unstacked model). However, the performance on SNLI remains relatively stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Learning to matching text sequences is a core and fundamental research problem in NLP and Infor- mation Retrieval. A wide range of NLP appli- cations fall under this paradigm such as natural language inference <ref type="bibr" target="#b0">(Bowman et al., 2015;</ref><ref type="bibr" target="#b11">Khot et al., 2018)</ref>, paraphrase identification ( <ref type="bibr" target="#b14">Lan and Xu, 2018)</ref>, question answering (Severyn and Mos- chitti, 2015), document search ( <ref type="bibr" target="#b32">Shen et al., 2014;</ref>, social media search ( <ref type="bibr" target="#b25">Rao et al., 2018)</ref> and entity linking ( . As such, universal text matching algorithms are gen- erally very attractive, in lieu of the prospects of potentially benefitting an entire suite of NLP ap- plications.</p><p>Neural networks have been the prominent choice for text matching. Earlier works are mainly concerned with learning a matching function be- tween RNN/CNN encoded representations <ref type="bibr" target="#b30">(Severyn and Moschitti, 2015;</ref><ref type="bibr" target="#b53">Yu et al., 2014;</ref><ref type="bibr" target="#b22">Qiu and Huang, 2015;</ref><ref type="bibr" target="#b37">Tay et al., 2017b</ref><ref type="bibr" target="#b39">Tay et al., , 2018b</ref>. Models such as Recursive Neural Networks have also been explored ( <ref type="bibr">Wan et al., 2016b</ref>). Sub- sequently, attention-based models were adopted <ref type="bibr" target="#b27">(Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b18">Parikh et al., 2016)</ref>, demonstrating superior per- formance relative to their non-attentive counter- parts.</p><p>Today, the dominant state-of-the-art approaches for text matching are mostly based on neural mod- els configured with bidirectional attention layers ( <ref type="bibr" target="#b31">Shen et al., 2017;</ref><ref type="bibr" target="#b38">Tay et al., 2017c</ref>). Bidirec- tional attention comes in various flavours which can be known as soft alignment <ref type="bibr" target="#b31">(Shen et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2017)</ref>, decomposable attention ( <ref type="bibr" target="#b18">Parikh et al., 2016)</ref>, attentive pooling (dos <ref type="bibr" target="#b28">Santos et al., 2016)</ref> and even complex-valued attention <ref type="bibr" target="#b36">(Tay et al., 2018a</ref>). The key idea is to jointly soft align text sequences such that they can be compared at the index level. To this end, various comparison functions have been utilized, ranging from feed- forward neural networks ( <ref type="bibr" target="#b18">Parikh et al., 2016</ref>) to factorization machines ( <ref type="bibr" target="#b38">Tay et al., 2017c)</ref>. No- tably, these attention (and bi-attention) mecha- nisms are also widely adopted (or originated) from many related sub-fields of NLP such as machine translation ( <ref type="bibr">Bahdanau et al., 2014</ref>) and reading comprehension ( <ref type="bibr" target="#b51">Xiong et al., 2016;</ref><ref type="bibr" target="#b29">Seo et al., 2016;</ref><ref type="bibr" target="#b47">Wang and Jiang, 2016b</ref>).</p><p>Many text matching neural models are heav- ily grounded in the compare-aggregate architec- ture ( <ref type="bibr" target="#b46">Wang and Jiang, 2016a)</ref>. In these models, matching and comparisons occur between text se- quences, aggregating features for making the final prediction. Recent state-of-the-art models such as BiMPM ( <ref type="bibr" target="#b48">Wang et al., 2017)</ref> and <ref type="bibr">DIIN (Gong et al., 2017</ref>) are representative of such architec- tural paradigm, utilizing an attention-based match- ing scheme and then a CNN or LSTM-based fea- ture aggregator. Earlier works ( <ref type="bibr" target="#b42">Wan et al., 2016a;</ref><ref type="bibr" target="#b6">He et al., 2015;</ref>) exploit a similar paradigm, albeit without the usage of attention.</p><p>Across many NLP and machine learning appli- cations, utilizing stacked architectures is a com- mon way to enhance representation capability of the encoder <ref type="bibr" target="#b34">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b5">Graves et al., 2013;</ref><ref type="bibr" target="#b17">Nie and Bansal, 2017)</ref>, leading to performance improvement. Deep net- works suffer from inherent difficulty in feature propagation and/or vanishing/exploding gradients. As a result, residual strategies have often been em- ployed ( <ref type="bibr" target="#b33">Srivastava et al., 2015;</ref><ref type="bibr" target="#b9">Huang et al., 2017)</ref>. However, to the best of our knowledge, this work presents a new way of resid- ual connections, leveraging on the fact that pair- wise formulation of the text matching task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a deep stacked recurrent architec- ture for general-purpose text sequence match- ing. We proposed a new co-stack residual affin-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed Co-Stack Residual Affinity Network (CSRAN) architecture. Each color coded matrix represents the interactions between two layers of sequence A and sequence B. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>CAFE Blocks This section describes the oper- ation of each CAFE block. The key idea behind CAFE blocks is to align a and b, and compress alignment vectors such as b − a (subtraction), b a (element-wise multiplication) and [b ; a] (concatenation) into scalar features. These scalar features are concatenated to the original input em- bedding, which can be pased into another BiL- STM layer for refining representations. Firstly, a, b are modeled aligned via E ij = F (a) F (b) and then aligned via: A = E B and B = AE (2) Given aligned pairs (A , B) and (B , A), we gen- erate three matching vectors for the concatenation ([a i ; b i ]), element-wise multiplication (a i b i ) and subtraction vectors (a i − b i ) of each pair. After which, we apply a factorization machine (Rendle, 2010) M (x) on each matching vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>). Baselines in- clude CNTN (Qiu and Huang, 2015), APLSTM (dos Santos et al., 2016), MV-LSTM (Wan et al., 2016a) and KEHNN (Wu et al., 2016). Results are reported from (Wu et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Wang et al., 2017) 0.802/0.899 CA (Bian et al., 2017) 0.821/0.899 IWAN (Shen et al., 2017) 0.822/0.889 MCAN (Tay et al., 2018c) 0.838/0.904 CSRAN 0.854/0.935</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relative effect of stack depth on CSRAN and the baseline Stacked Model on SciTail dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of stack depth on CSRAN performance on Quora and SNLI datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistics of datasets used in our experiment. 
|C| denotes the number of classes and R denotes a rank-
ing formulation. Twitter stands for the TwitterURL 
dataset. 

Science Entailment (SciTail) (Khot et al., 
2018) is a new entailment classification dataset 
that was constructed from science questions and 
answers. This dataset involves two-way classifi-
cation (entail or non-entail). We compare with 
DecompAtt (Parikh et al., 2016), ESIM, DGEM 
(Khot et al., 2018) and CAFE. 

Quora Duplicate Detection is a well-studied 
paraphrase identification dataset 1 . We use the 
splits provided by (Wang et al., 2017). The task 
is to determine if two questions are paraphrases 
of each other. This task is formulated as a binary 
classication problem. We compare with L.D.C 
(Wang et al., 2016b), BiMPM, the DecompAtt im-
plementation by (Tomar et al., 2017) (word and 
char level) and DIIN. 

TwitterURL (Lan et al., 2017) is another dataset 
for paraphrase identification. It was constructed 
using Tweets referring to news articles. This task 
is also a binary classification problem. We com-
pare with (1) MultiP (Xu et al., 2014), a strong 
baseline, (2) the implementation of (He and Lin, 
2016) by (Lan et al., 2017) and (3) the Subword + 
LM model from (Lan and Xu, 2018). 

TrecQA (Wang et al., 2007) is a well-studied 
dataset for answer sentence selection task (or 
question-answer matching). The goal is to rank 
answers given a question. This task is formu-
lated as a pointwise learning-to-rank problem. 
Baselines include HyperQA (Tay et al., 2017a), 
Ranking-based Multi-Perspective CNN (He et al., 
2015) implementation by (Rao et al., 2016), 
BiMPM, the compare-aggregate (Wang and 
Jiang, 2016a) model extension by (Bian et al., 
2017) (we denote this model as CA), IWAN 

1 https://data.quora.com/ 
First-Quora-Dataset-Release-Question-Pairs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on single model SNLI 
dataset. 

2 For fair comparison, we do not compare with (1) mod-
els that use external contextualized word embeddings, e.g., 
CoVe (McCann et al., 2017) / ELMo (Peters et al., 2018) / 
generative pretraining (Radford et al.) and (2) ensemble sys-
tems. As either (1) and/or (2) would also intuitively boost the 
performance of the base CSRAN model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results on SciTail dataset. 

Model 
Acc 
L.D.C (Wang et al., 2016b) 
87.5 
Word DecompAtt (Tomar et al., 2017) 87.5 
BiMPM (Wang et al., 2017) 
88.1 
Char DecompAtt (Tomar et al., 2017) 88.4 
DIIN (Gong et al., 2017) 
89.0 
CSRAN 
89.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results on Quora Duplicate De-
tection dataset. 

Model 
F1 
MultiP (Xu et al., 2014) 
0.536 
DeepPairwiseWord (He and Lin, 2016) 0.749 
Subword + LM (Lan and Xu, 2018) 
0.760 
CSRAN 
0.840 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Experimental results on TwitterURL para-
phrase dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : Experimental results on TrecQA dataset.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Experimental results on the Ubuntu dataset 
for utterance-response matching. Baseline results are 
reported from (Wu et al., 2016). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Ablation study (development score) of our key 
model components on three datasets. 

</table></figure>

			<note place="foot" n="3"> Experimental Evaluation In this section, we introduce our experimental setup, baselines and results. 3.1 Datasets and Competitor Baselines We use six public benchmark datasets for evaluating our proposed approach. This section briefly introduces each dataset, along with several state-ofthe-art approaches that we compare against. Table 1 provides a summary of the datasets used in our experiments. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) is a well-known dataset for entailment classification (or natural language inference). The task is to determine if two sequences entail/contradict or are neutral to each other. This task is a three-way classification problem. On this dataset, we compare with several state-of-the-art models such as BiMPM (Wang et al., 2017), ESIM (Chen et al., 2017), DIIN (Gong et al., 2017), DRBiLSTM (Ghaeini et al., 2018) and CAFE (Tay et al., 2017c).</note>

			<note place="foot" n="3"> The best result on Scitail was obtained with 5 layers. Moreover, the difference in test performance between stacked and single-layered model was considerably high (+2.5%) even though dev performance increased by +1%.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1721" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Xiaoli Z Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05577</idno>
		<title level="m">Dr-bilstm: Dependent reading bidirectional lstm for natural language inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1709.04348</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pacrr: A position-aware neural ir model for relevance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00391</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The importance of subword embeddings in sentence pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcutstacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-08" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neupl: Attention-based semantic matching and pair-linking for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Minh C Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1667" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for communitybased question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="1305" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation for answer selection with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-24" />
			<biblScope unit="page" from="1913" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-perspective relevance matching with hierarchical convnets for social media search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08159</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08-09" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inter-weighted alignment network for sentence pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gehui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Enabling efficient question answer retrieval via hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
		<idno>abs/1707.07847</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hermitian co-attention networks for text matching in asymmetrical domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4425" to="4431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to rank question answer pairs with holographic dual LSTM architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-711" />
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross temporal recurrent networks for ranking question answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-cast attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD 2018, KDD &apos;18</title>
		<meeting>KDD 2018, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thyago</forename><surname>Gaurav Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04565</idno>
		<title level="m">Neural paraphrase identification of questions with noisy pretraining</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2835" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04378</idno>
		<title level="m">Liang Pang, and Xueqi Cheng. 2016b. Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inner attention based recurrent neural networks for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? A quasisynchronous grammar for QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-28" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
	<note>EMNLP-CoNLL</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1611.01747</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07019</idno>
		<title level="m">Sentence similarity learning by lexical decomposition and composition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Knowledge enhanced hybrid neural network for text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04684</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extracting lexically divergent paraphrases from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>William B Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="435" to="448" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<idno>abs/1412.1632</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Highway long short-term memory rnns for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
