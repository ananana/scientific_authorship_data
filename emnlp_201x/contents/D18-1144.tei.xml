<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuji</forename><surname>Kano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Taniguchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ying</forename><surname>Chen</surname></persName>
							<email>{yanying, chen}@fxpal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Fuji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1139" to="1145"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We leverage a popularity measure in social media as a distant label for extractive summa-rization of online conversations. In social media , users can vote, share, or bookmark a post they prefer. The number of these actions is regarded as a measure of popularity. However, popularity is not determined solely by content of a post, e.g., a text or an image it contains, but is highly based on its contexts, e.g., timing , and authority. We propose Disjunctive model that computes the contribution of content and context separately. For evaluation, we build a dataset where the informativeness of comments is annotated. We evaluate the results with ranking metrics, and show that our model outperforms the baseline models which directly use popularity as a measure of infor-mativeness.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online conversations are increasingly significant for communication, e.g., Slack 1 for work com- munication and Reddit 2 for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations ( <ref type="bibr" target="#b0">Bhatia et al., 2014;</ref><ref type="bibr" target="#b2">Carenini et al., 2007;</ref><ref type="bibr" target="#b16">Mehdad et al., 2013</ref><ref type="bibr" target="#b18">Oya et al., 2014</ref>). State-of-the-art models in both abstractive <ref type="bibr" target="#b19">(Rush et al., 2015</ref>) and extractive <ref type="bibr" target="#b4">(Cheng and Lapata, 2016</ref>) summarization tasks are based on neural networks, but these models re- quire large amounts of training data. In previous research, these data were created automatically by retrieving headlines and highlights of news articles edited by news editors. However, these method- ologies cannot be applied to the summarization of online conversations because of a lack of summary annotations.</p><p>1 https://slack.com 2 https://www.reddit.com Distant labels have been used to train mod- els, thereby reducing the need for manual label- ing; some of these labels were also applied to the summarization task. Categories of news arti- cles ( <ref type="bibr" target="#b10">Isonuma et al., 2017</ref>) and ratings of online reviews <ref type="bibr" target="#b22">(Xiong and Litman, 2014</ref>) were used as distant labels in extractive summarization. How- ever, these have been used as supplementary la- bels to enhance conventional summarization mod- els, whereas we present labels which a model can solely be trained with.</p><p>We leverage a measure of popularity as a dis- tant label. In social media, users can vote, share, or bookmark a post they prefer, and the number of these actions are regarded as indications of popu- larity. We assume that measures of popularity re- flects the informativeness, the index required for a summary ( <ref type="bibr" target="#b6">Erkan and Radev, 2004)</ref>, and validate whether popularity can be used as a distant label for extractive summarization. However, popular- ity is not solely determined by content, e.g., a text or an image, but is highly affected by contexts, e.g., timing, and authority ( <ref type="bibr" target="#b3">Cheng et al., 2017;</ref><ref type="bibr" target="#b1">Burghardt et al., 2017;</ref><ref type="bibr" target="#b21">Suh et al., 2010;</ref><ref type="bibr" target="#b9">Hessel et al., 2017;</ref><ref type="bibr" target="#b11">Jaech et al., 2015)</ref>. Therefore, to uti- lize popularity as an indicator of informativeness, we need to exclude the effect of context.</p><p>To exclude the effect, we propose Disjunctive model. This model computes two scalar values; one from a content feature and the other from a context feature. These two values are then mul- tiplied to predict the popularity. The scalar val- ues can be interpreted as the contribution of con- tent and context to the prediction. We assume that the contribution of content to indicate informative- ness.</p><p>For evaluation, we build a test dataset where comments are annotated for informativeness. We measure informativeness as an index indicative of the best sentences to extract as a summary. We select Reddit as a data source, where the karma score, a measure of popularity in Reddit, is known to be affected by contexts. Our test task is to ex- tract informative posts. Because informativeness of each post is annotated via crowdsourcing, the extracts can be ranked, but the appropriate number is unknown. Therefore, we employ ranking met- rics in the evaluation. Our experiment only use karma scores for training to verify that they reflect informativeness. The results show that our model outperforms baseline models that directly adopt karma scores as an indicator of informativeness. Furthermore, our model focus on a local feature of a single post, whereas conventional centrality- based models ( <ref type="bibr" target="#b6">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b17">Mihalcea and Tarau, 2004</ref>) use a global context of posts, and the complementary hybrid of the both models out- perform both the centrality-based models and our models.</p><p>The contributions of this paper are three fold. 1) Propose a model that harnesses a popularity mea- sure as a distant label for extractive summariza- tion. 2) Create a dataset of online conversations in which the informativeness of contents are an- notated to verify that popularity does not correlate with informativeness because of the effect of con- text. 3) Demonstrate that our model, when com- bined with a centrality-based model, outperforms baseline models in predicting the informativeness of posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous research of summarizing online conver- sations can be categorized into graph-based meth- ods ( <ref type="bibr" target="#b16">Mehdad et al., 2013</ref><ref type="bibr" target="#b20">Shang et al., 2018)</ref>, template-based methods ( <ref type="bibr" target="#b18">Oya et al., 2014)</ref>, and methods which use dialogue acts as a fea- ture ( <ref type="bibr" target="#b0">Bhatia et al., 2014;</ref><ref type="bibr" target="#b2">Carenini et al., 2007)</ref>. In previous research, few or no training data was adopted because of a lack of labeled data. Our model harnesses a vast amount of data from social media.</p><p>Many researchers used user-contributed labels from social media as distant labels. <ref type="bibr" target="#b22">Xiong (2014)</ref> used review scores on a movie-rating site for a summarization task. For a sentiment analysis on Twitter 3 , Davidov(2010) used hashtags, and Gui- bon (2017) used emoji. In our study, we leverage a popularity measure for a summarization task.</p><p>Factor analysis quantifies the contribution of 3 https://twitter.com each feature to the result, using a linear model. For example, Suh (2010) analyzed factors contributing to popularity in Twitter. Our model assumes a lin- ear relationship between context and content, and thus enables to utilize the contribution of content as an indicator of informativeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>In this study, we work with Reddit threads. A thread is a set of comments, and the first posted comment is called a submission. Comments can be made in response to submissions as well as comments under the submissions, resulting in a thread being tree-structured. Submissions and comments can be upvoted or downvoted by read- ers, and karma scores are computed as upvotes mi- nus downvotes. Karma scores follow Zipf's law ( <ref type="bibr" target="#b3">Cheng et al., 2017</ref>). Therefore, we smooth the karma scores as follows:</p><formula xml:id="formula_0">f (k) = log(k + 1) (k ≥ 0) 0 (k &lt; 0)</formula><p>where k represents the karma score.</p><p>Reddit is organized into subreddits by topic. Posts from the subreddits AskMen, AskWomen, and AskReddit with 420,598, 247,012, and 644,034 comments, respectively, are collected and split into training and validation sets with a 4:1 ratio. The validation sets are used for early- stopping. All comments were posted from June, 1, 2016 through June, 1, 2017.</p><p>Manual Annotation We crowdsource the anno- tation of comments in terms of informativeness to utilize them as test data. Annotators are asked to choose 3 informative comments from 10. We de- fine 10 comments as a subset; each comment is a reply to a submission. For submissions with more than 10 replies, posts with the top 10 karma scores are selected. For each subreddit, 130 subsets were annotated, for a total of 1,300 comments. Because 10 annotators vote for 3 different comments each, the number of votes for a comment ranged from 0 to 10. These numbers we refer to as the annotated score. The comments in each subset are shuffled to invalidate the effect of the order in which anno- tators read the comments.</p><p>Liu and <ref type="bibr" target="#b13">Liu (2008)</ref> observed that the best sum- mary differs among annotators, especially when summarizing conversations, consequently result- ing in low Kappa scores. In their study, the Kappa statistics for six different annotators varied from  0.11 to 0.35. In our study, the Fleiss Kappa coef- ficients ( <ref type="bibr" target="#b7">Fleiss and Cohen, 1973)</ref> of the annotated data are 0.252 for AskMen, 0.191 for AskWomen, and 0.213 for AskReddit.</p><p>The correlation coefficients of the karma scores versus the annotated scores are low: 0.063 for AskMen, 0.081 for AskReddit, and 0.107 for AskWomen. <ref type="table" target="#tab_0">Table 1</ref> shows some examples of posts with low karma scores and high annotated scores, and vice versa. It shows that there are in- formative posts with low karma scores, and non- informative posts with high karma scores. This implies that it is necessary to exclude the effect of context to leverage karma scores as distant labels for summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Model</head><p>To exclude the effect of context from the popu- larity, we propose Disjunctive model <ref type="figure" target="#fig_0">(Figure 1</ref>). This model computes two scalar values, a content score and a context score from a content feature and a context feature, respectively by multiply- ing parameter vectors. The model is trained to predict popularity by multiplying the two scores and adding a context bias, which is also computed from a context feature. After training, the two scores represent the contribution of the content and context to popularity. We assume that the con- tent score indicates informativeness. While train- ing, the popularity score is used to predict the pop- ularity, which is represented by the karma scores in our study. During evaluation, the content score is used for prediction of informativeness. The con- text score is constrained to be positive; otherwise, it can be either positive or negative, making it dif- ficult to assume that a content score represents in- formativeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Context Feature Extractor</head><p>We use a multi-layer perceptron (MLP) to extract the features of the context of comments. Our study discusses six attributes of context: the karma score of a submission, the karma score of the previous comment, the depth in a thread, the relative time since the previous comment, the rank of the rela- tive time among all replies to a previous comment, and the number of replies to a previous comment. The number of layers is set to 3, and the dimen- sions of each layer are 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Content Feature Extractor</head><p>We use two content extractors: long short-term memory (LSTM) as a basic language model, and a factored neural network (FNN) ( <ref type="bibr" target="#b3">Cheng et al., 2017</ref>) as a model that achieved state-of-the-art re- sults in karma score prediction tasks. FNN, which is a language model, sequentially predicts the next words in a comment and its reply using an atten- tion mechanism. As in the previous research, we pretrain this model using the same data used in the training, and fine-tune its parameters on the karma </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We train the summarization model using karma scores as distant labels and evaluate the predic- tion of informativeness with the annotated dataset. As explained in Sec 3, the informativeness of each post is annotated via crowdsourcing and it is diffi- cult to determine the appropriate number of posts needed to create a summary. Therefore, we em- ploy ranking metrics for evaluation. In each sub- set, where subsets were defined in Sec 3, we rank each comment from 1 to 10 in terms of predicted scores and annotated scores. Ranks of tied scores are set randomly. To avoid randomness from af- fecting the result, we evaluate 100 times and com- pute an average as a result. We use three metrics: Spearman's Rho (Sρ), precision@3 (prec3), and Mean Reciprocal Rank (MRR) <ref type="bibr" target="#b14">(Mcfee and Lanckriet, 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>Experiments are conducted by using mean- squared error as the loss function and Adam as the optimizer <ref type="bibr" target="#b12">(Kingma and Ba, 2014</ref>). We re- place words that appear fewer than five times with &lt;unk&gt;. There are 63,093 unique terms for AskMen, 53,589 for AskWomen, and 80,426 for AskReddit. The maximum length of each com- ment is clipped to 50. The mini-batch size is 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Model</head><p>We experiment with four baseline models. Two are supervised models as shown in <ref type="figure" target="#fig_1">Figure 2</ref>: the Concat model concatenates content and context features, and the Text model uses only content features. The other two are centrality-based un- supervised models: <ref type="bibr">LexRank (Erkan and Radev, 2004</ref>) and TextRank ( <ref type="bibr" target="#b17">Mihalcea and Tarau, 2004)</ref>. The unsupervised models only use content fea- tures. Disjunctive model use different scores for prediction in the training and in the test, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>; however, Concat and Text models use a popularity score, the predicted value of a karma score, both in the training and in the test. This is because there is no substitute for the content score in these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hybrid Model</head><p>The supervised models in our study, including the Disjunctive models, compute features from a sin- gle post only. To also harness the global infor- mation encoded in all posts in a subset, we build a Hybrid model which multiplies the scores from the Disjunctive model and the TextRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results of the experiments described in Sec 5 are shown in <ref type="table">Table 2</ref>. The suffixes Disjunctive, Concat, and Text denote the supervised models de- scribed in Sec 4 and Sec 5. The prefixes LSTM and FNN indicate the models we use for content feature extractors. Among the supervised models, our Disjunctive models outperform both LSTM and FNN-based baseline models. In contrast, the results of the Concat models are poor. Unsuper- vised models which use the global feature of posts in a subset perform well. The FNNDisjunctive model combined with TextRank outperforms both the supervised models and the unsupervised mod- els. To confirm that multiplication performs better in our task, we also experimented with Additive models, which simply add the context score and the content score instead of multiplying. Although better performing than the Text model, the perfor- mance was not as good as the Disjunctive model (Not shown in <ref type="table">Table)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Here we discuss the comparison of the results shown in <ref type="table">Table 2</ref>, and how our Disjunctive model separate the effect of content and context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Model vs Concat Model vs Disjunctive</head><p>Model The Text model performs worse than our model. A possible reason is that karma scores can <ref type="table">Table 2</ref>: Result of ranking annotated scores. The best results among the supervised models are underlined, and the best results among all the models are bolded. The names of our models are also bolded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content</head><p>Model <ref type="table">AskMen  AskReddit  AskWomen  Type  Sρ  MRR prec3  Sρ  MRR prec3  Sρ  MRR prec3</ref> Super. Local </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separation of Content and Context</head><p>The visu- alization in <ref type="figure" target="#fig_2">Figure 3</ref> shows that our model can predict informativeness whereas the Concat model cannot. From each subset explained in Sec 3, we extract the post with the highest predicted score by the LSTMConcat and the LSTMDisjunctive. The karma scores and annotated scores of the extracted posts are plotted as blue and red dots, respectively. The karma scores are smoothed by the equation described in Sec 3. There are 130 subsets, for a total of 260 dots plotted. The Concat model ex- tracts posts with low annotated scores but high karma scores, whereas the Disjunctive model ex- tracts posts with high annotated scores regardless of the karma scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed Disjunctive model that harnesses popularity as distant labels for use in extractive summarization. Our model was shown capable of separating the effects of content and context in a popularity measure and predicting the informa- tiveness of content. To evaluate this, we built a Reddit dataset where informative comments were annotated. Our model, combined with a centrality- based model, outperformed the baseline models on the task of ranking posts to correspond to the rank of the annotated scores in three ranking met- rics. Our models currently use only a single post as a feature. In the future, we will develop a model which uses a series of posts as a feature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Description of the Disjunctive model. and represent addition and multiplication, respectively. The components of each model we use for training and testing are shown. Black and white squares represent scalars and functions, respectively.</figDesc><graphic url="image-1.png" coords="3,81.55,241.56,196.44,154.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Description of baseline models. (a) Concat model. (b) Text model. score prediction task. A single-layer LSTM and FNN are used and the last hidden layers are used as the content feature. The dimensions of the hidden layers are set to 64, and the dimensions of the word embedding are set to 256.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,218.27,141.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Karma scores and annotated scores of posts extracted by the LSTMConcat model (blue dots on the left) and the LSTMDisjunctive model (red dots shifted to the right for visibility) from the AskWomen dataset. Histograms of the karma scores and the annotated scores are also shown above and on the right. be different even with similar text content because of different context, and this confuses the models that only use content features. Our model, by contrast, can avoid this problem because it considers the effect of context. The Text model outperform the Concat model because context is a strong factor in predicting karma scores. If a model can use both content and context (as the Concat model does), it might overfit to context and ignore content. This does not happen in our model because it does not use context in test. Hybrid Model vs Disjunctive Model The good performance results of the TextRank model indicate that global features of the posts are informative for ranking. While the supervised models just</figDesc><graphic url="image-3.png" coords="5,76.09,245.15,207.35,212.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Examples of posts with low karma scores and high annotated scores, and vice versa.</head><label>1</label><figDesc></figDesc><table>karma 
score 

annotated 
score 

post 

0 
8 
Martin Shkreli was streaming League of legends and my brother messaged him to see if he 
could get an invite to the group. They have played a few times since. 
1 
10 
Same thing goes for being bitten by a dog, instead of instinctively pulling away...force your 
arm/hand down their throat. Super effective. 
1 
10 
1 Make sure you have solid internet. 2 Find work from home, they actually exist book keeping, 
software testing, data entry, etc. 3 Work from home while earning a modest wage you wont get 
rich on those jobs, but it will certainly pay the bills. 
360 
0 
Ill try this the next time my toddler bites me. 
253 
0 
Im an English major who wants to go into marketing. Wat do 
228 
0 
Sadly people buy the first thing that the see and this is what is in season. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Summarizing online forum discussions-can dialog acts of individual messages help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Biyani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2127" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The myopia of crowds: Cognitive load and collective evaluation of answers on stack exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">F</forename><surname>Alsina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Rand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">173610</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Summarizing email conversations with clue words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW &apos;07</title>
		<meeting>the 16th International Conference on World Wide Web, WWW &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A factored neural network model for characterizing online discussions in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2296" to="2306" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced sentiment learning using twitter hashtags and smileys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="241" to="249" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="613" to="619" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">From emojis to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><surname>Guibon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magalie</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parice</forename><surname>Bellot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1529708</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cats and captions vs. creators and the clock: Comparing multimodal content to context in predicting relative popularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, WWW &apos;17</title>
		<meeting>the 26th International Conference on World Wide Web, WWW &apos;17<address><addrLine>Republic and Canton of Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="927" to="936" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extractive summarization using multi-task learning with document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Isonuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2101" to="2110" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Talking to the crowd: What do people react to in online discussions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Correlation between rouge and human evaluation of extractive meeting summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-08: HLT, Short Papers</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual International Conference on Machine Learning (ICML</title>
		<meeting>the 27th annual International Conference on Machine Learning (ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive summarization of spoken and written conversations based on phrasal queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1220" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization with entailment and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation</title>
		<meeting>the 14th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-04and the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>EMNLP-04and the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A template-based abstractive meeting summarization: Leveraging summary and source text relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuro</forename><surname>Oya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Natural Language Generation Conference</title>
		<meeting>the 8th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised abstractive meeting summarization with multisentence compression and budgeted submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Lorré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="664" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Want to be retweeted? large scale analytics on factors impacting retweet in twitter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongwon</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pirolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE Second International Conference on Social Computing, SOCIALCOM &apos;10</title>
		<meeting>the 2010 IEEE Second International Conference on Social Computing, SOCIALCOM &apos;10<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical analysis of exploiting review helpfulness for extractive summarization of online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
