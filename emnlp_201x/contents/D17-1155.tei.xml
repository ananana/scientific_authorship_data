<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Weighting for Neural Machine Translation Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
							<email>lemaoliu@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
							<email>khchen@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<email>eiichiro.sumita}@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Weighting for Neural Machine Translation Domain Adaptation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1482" to="1488"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-of- domain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks <ref type="bibr">(Axelrod et al., 2011;</ref><ref type="bibr">Luong and Manning, 2015)</ref>. Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores <ref type="bibr" target="#b0">(Moore and Lewis, 2010;</ref><ref type="bibr">Axelrod et al., 2011;</ref><ref type="bibr">Duh et al., 2013;</ref><ref type="bibr">Hoang and Sima'an, 2014a,b;</ref><ref type="bibr">Durrani et al., 2015;</ref><ref type="bibr">Chen et al., 2016)</ref>.</p><p>ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance <ref type="bibr" target="#b4">(Sennrich, 2012;</ref><ref type="bibr" target="#b7">Sennrich et al., 2013;</ref><ref type="bibr">Durrani et al., 2015</ref><ref type="bibr">Durrani et al., , 2016</ref><ref type="bibr">Imamura and Sumita, 2016)</ref>.</p><p>iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks <ref type="bibr">(Jiang and Zhai, 2007)</ref>, such as POS tagging, entity type classification and especially PBSMT ( <ref type="bibr">Matsoukas et al., 2009;</ref><ref type="bibr" target="#b8">Shah et al., 2010;</ref><ref type="bibr">Foster et al., 2010;</ref><ref type="bibr" target="#b3">Rousseau et al., 2011;</ref><ref type="bibr" target="#b12">Zhou et al., 2015;</ref><ref type="bibr" target="#b10">Wang et al., 2016;</ref><ref type="bibr">Imamura and Sumita, 2016)</ref>. They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight.</p><p>For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used <ref type="bibr">(Chen et al., 2016;</ref><ref type="bibr" target="#b9">Wang et al., 2017)</ref>. Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled ( <ref type="bibr">Jean et al., 2015)</ref>. ii) an NMT further training (fine-tuning) method ( <ref type="bibr">Luong and Manning, 2015)</ref>. The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, <ref type="bibr">Chu et al. (2017)</ref> make an empirical comparison of NMT further training ( <ref type="bibr">Luong and Manning, 2015)</ref> and domain control ( <ref type="bibr">Kobus et al., 2016</ref>), which applied word-level domain features to word embedding layer. This approach provides natural baselines for comparison.</p><p>To the best of our knowledge, there is no existing work concerning instance weighting in NMT. The main challenge is that NMT is not a liner model or combination of linear models, where the instance weight can be integrated into directly. To overcome this difficulty, we try to integrate the instance weight into NMT objective function.</p><p>Two technologies, i.e., sentence weighting and domain weighting, are proposed to apply instance weighting to NMT. In addition, we also propose a dynamic weight learning strategy to tune the proposed domain weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NMT Background</head><p>An attention based NMT is a neural network that directly models the conditional probability p(y|x) of translating a source sentence, x = {x 1 , ..., x n }, to a target sentence, y = {y 1 , ..., y m } ( <ref type="bibr">Luong et al., 2015)</ref>:</p><formula xml:id="formula_0">p(y|x) = m j=1 sof tmax(g(y j |y j−1 , s j , c j )), (1)</formula><p>with g being the transformation function that outputs a vocabulary-sized vector, s j being the RNN hidden unit and c j being the weighted sum of source annotations H x . The NMT training objective (maximize) is formulated as,</p><formula xml:id="formula_1">J = (x,y)∈D log p(y|x),<label>(2)</label></formula><p>where D is the parallel training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Instance weighting for NMT</head><p>In this paper, we integrate the instance weight into the NMT objective function. Our main hypothesis is that the in-domain data should have a higher weight in the NMT objective function than the out- of-domain ones.</p><p>The training corpus D can be divided into in- domain one D in and the out-of-domain one D out . So, the Eq. (2) can be rewritten as,</p><formula xml:id="formula_2">J = ( x,y∈D in log p(y|x) + x ,y ∈Dout log p(y |x )), (3)</formula><p>where x, y is a parallel sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Weighting</head><p>A general method is to give each sentence a weight. As Axelrod et al. (2011) mentioned, there are some pseudo in-domain data in out-of-domain data, which are close to in-domain data. We can apply their bilingual cross-entropy method to score each x i , y i as a weight λ i , the higher the better, 1</p><formula xml:id="formula_3">λ i = δ(H out (x i ) − H in (x i ) +H out (y i ) − H in (y i )).<label>(4)</label></formula><p>Take H in (x i ) as example, it indicates the cross-entropy between sentence x i and in-domain language model <ref type="bibr">(Axelrod et al., 2011</ref>). Min-max normalization δ (Priddy and <ref type="bibr" target="#b2">Keller, 2005</ref>) is used to normalize each λ i into range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>,</p><formula xml:id="formula_4">δ(λ i ) = λ i − λ min λ max − λ min .<label>(5)</label></formula><p>The λ for in-domain data will set as one directly.</p><p>The updated objective function by sentence weighting (J sw ) can be rewritten as,</p><formula xml:id="formula_5">J sw = x i ,y i ∈D λ i log p(y i |x i ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain Weighting</head><p>An alternative way is to modify the weight of each domain in objective function. For we design a weight parameter λ in for in-domain data. The updated objective function by domain weighting (J dw ) can be estimated as,</p><formula xml:id="formula_6">J dw = λ in (x,y)∈D in logp(y|x) + (x ,y )∈Dout logp(y |x ). (7)</formula><p>3.2.1 Batch weighting A straightforward domain weighting implementation is to modify the ratio between in-domain and out-of-domain data in each NMT mini-batch. That is, we can increase the in-domain weight by increasing the number of in-domain sentences included in a mini-batch. The updated in-domain data ratio R in in each NMT mini-batch can be calculated as,</p><formula xml:id="formula_7">R in = | ˆ D in | | ˆ D in | + | ˆ D out | = λ in λ in + 1 ,<label>(8)</label></formula><p>where | ˆ D in | and | ˆ D out | are the sentence number from in and out-of-domain data in each mini- batch, respectively.</p><p>Take the IWSLT EN-DE corpus in <ref type="table">Table 1</ref> as example, the original ratio R in between in- domain data and all of the data is around 1:20. That is, for a 80-sized mini-batch, it would include around four sentence from in-domain data and 76 from out-of-domain data on average. For batch weighting, we can set the ratio R in as 1:2 manually. That is, we load 40 in-domain and 40 out-of-domain sentences into each mini-batch.</p><p>In practice, we create two data iterators, one for in-domain and one for out-of-domain. Both of the in and out-of-domain data will be randomly shuffled and then loaded into corresponding data iterators before each epoch. For each mini- batch, the data from these two data iterators are determined by the ratio R in . Because the size of out-of-domain data is much larger than the in- domain one, the in-domain data will be loaded and trained for several epochs, while the out-of- domain data is only trained for one epoch at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dynamic Weight Tuning</head><p>For the batch weight tuning, one way is to fix the weights for several systems and select the best-performed system on the development data. Besides this, we also tried to learn the batch weighting dynamically. That is, the initial in- domain data ration in mini-batch is set as 0%. We increased 10% ratio of in-domain data in the mini- batch if the training cost does not decrease for ten- time evaluations (the training cost is evaluated on development data set every 1K batches training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>The proposed methods were evaluated by adapting WMT corpora to IWSLT (mainly contains TED talks) corpora. 2 Statistics on data sets were shown in <ref type="table">Table 1</ref>.</p><p>• IWSLT 2015 English (EN) to German (DE) training corpus ( <ref type="bibr">Cettolo et al., 2015</ref>) was used as in-domain training data. Out- of-domain corpora contained WMT 2014 English-German corpora. This adaptation corpora settings were the same as those used in ( <ref type="bibr">Luong and Manning, 2015</ref>).</p><p>• IWSLT 2014 English (EN) to French (FR) training corpus ( <ref type="bibr">Cettolo et al., 2014</ref>) was used as in-domain training data. Out- of-domain corpora contained WMT 2015 English-French corpora. This adaptation corpora settings were nearly the same as those used in ( <ref type="bibr" target="#b10">Wang et al., 2016</ref>  <ref type="table">Table 1</ref>: Statistics on data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NMT Systems</head><p>We implemented the proposed method in Nematus 3 (Sennrich et al., 2017) and online available 4 , which is one of the state-of-the-art NMT frameworks. The default settings of Nematus were applied to all NMT systems (both baselines and the proposed methods): the word embedding dimension was 620 and the size of a hidden layer was 1000, the batch size was 80, the maximum sequence length were 50, and the beam size for decoding was 10. The 30K-sized vocabulary, which was created by using both in and out-of-domain data, were applied to all of the systems. Default dropout was applied. Each NMT model was trained for 500K batches by using ADADELTA optimizer <ref type="bibr" target="#b11">(Zeiler, 2012)</ref>. Training was conducted on a single Tesla P100 GPU, taking 7-10 days. We observed that all of the systems converged before 500K batches training.</p><p>For the coding cost of duplicating data, we only add two data iterators as mentioned in 3.2.1. For the training cost, using batch weighting can a accelerate the model converge on development data in our experiments, because the development data are also in-domain data.</p><p>Overall, the overhead cost is not too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>In <ref type="table" target="#tab_2">Tables 2 and 3</ref>, SMT indicates standard PBSMT ( <ref type="bibr">Koehn et al., 2007</ref>) models were trained by corresponding corpora (in, out, and in+out). The in, out and in + out indicate that the in-domain, out-of-domain and their mixture were used as the NMT training corpora.</p><p>For related NMT domain adaptation baselines, "ensemble" indicates in and out models were ensembled in decoding and "sampler" indicates that we sampled duplicated in-domain data into training data, to make the ratio between in/out be 1:1 manually. Actually, if the mini-batch size was as large as the whole corpus, the sampling method, and batch weighting method would be the same. Batch weighting method makes the data more balanced in each single mini-batch. However, the mini-batch size is limited, so these two methods are different.</p><p>We also compared Axelrod et al. (2011)'s sentence selection and <ref type="bibr">Kobus et al. (2016)</ref>'s domain control method, which added a word feature (in or out) to each word in the training corpora. For all of the baselines, we tried our best to re-implemented their methods. The translation performance was measured by the case-insensitive BLEU ( <ref type="bibr" target="#b1">Papineni et al., 2002</ref>), with the paired bootstrap re-sampling test <ref type="bibr">(Koehn, 2004)</ref>    <ref type="table" target="#tab_4">Tables 3) indicate</ref> whether the proposed methods were significantly better than the best performed baselines in bold ("++": better at significance level α = 0.01, "+": α = 0.05).</p><p>In <ref type="table" target="#tab_2">Tables 2 and 3</ref>  • Adding out-of-domain to in-domain data, or directly using out-of-domain data, degraded NMT performance.</p><p>• The proposed instance weighting methods substantially improved NMT performance (in) up to 2.7-6.7 BLEU points, and outperformed the best existing baselines up to 1.6-3.6 BLEU points.</p><p>• Among the proposed methods, batch weighting performed the best, although it was the simplest one. The reason may be: a) the batch weighting method directly balanced the in-domain data ratio in each mini-batch, to overcome the in-domain data sparse problem. b) The batch weight can be tuned on development data, in comparison with sentence weighting method, whose weights were learned and fixed before NMT training.</p><p>• The dynamic weight tuning strategy outperformed the fixed weight tuning strategy by 0.6-2.4 BLEU points. <ref type="figure" target="#fig_0">Figure 1</ref> showed the batch weight tuning experiments on development data of IWSLT EN- DE, where the horizontal axis indicates the in- domain ratio R in in Eq. (8). "Fix" indicates several systems were trained with fixed weights and the best-performed system would be selected. "Dynamic" indicates that only one system was trained and the domain weight was learned dynamically as mentioned in Section 3.2.2. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the fixed weight learning reached the highest BLEU on dev at around 50% and dynamic learning at around 60%. If we keep training the dynamic learning after 100% in- domain data were used, the performance would trend to become similar to only using in-domain data from the beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Weights Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Further Training</head><p>Further training ( <ref type="bibr">Luong and Manning, 2015)</ref> can be viewed as a special case of the proposed batch weighting method. That is, it trained NMT model by using 0% in-domain data at first and then using 100% in-domain data. In comparison, our batch weighting kept some ratio of out-of- domain data during the whole training process. In addition, further training can work together with batch weighting. That is, NMT was trained with 0% in-domain data at first and then with batch weighing method for further training (Luong + bw in <ref type="table" target="#tab_6">Table 4</ref>). . R in was tuned on development data. As mentioned in Section 3.2.2, "bw + dynamic tuning" indicates that this batch weighting was learned dynamically.</p><p>IWSLT   <ref type="bibr">Luong and Manning, 2015</ref>) is the baseline for significance test. <ref type="table" target="#tab_6">Table 4</ref> shows that batch weighting worked synergistically with Luong's further training method, and slightly improved NMT performance. The "bw + dynamic tuning" method outperformed both of them. We observed that the original further training overfitted quickly after around one epoch training. Keeping some out-of-domain data would prevent further training from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we proposed two straightforward instance weighting methods with a dynamic weight learning strategy for NMT domain adaptation. Empirical results on IWSLT EN- DE/FR tasks showed that the proposed methods can substantially improve NMT performances and outperform state-of-the-art NMT adaptation methods.</p><p>The current sentence weighting method is a simple implementation of the existing PBSMT adaptation methods. In the future, we will try to study a specific sentence weighting method for NMT domain adaptation. <ref type="bibr">Chenhui Chu, Raj Dabre, and Sadao Kurohashi.</ref> 2017. An empirical comparison of simple domain adaptation methods for neural machine translation. arXiv preprint arXiv:1701.03214. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Batch weight tuning on IWSLT EN-DE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>IWSLT EN-DE 
Sentences 
Tokens 
TED training (in-domain) 
207.1K 
3.2M 
WMT training (out-of-domain) 
4.5M 119.9M 
TED tst2012 (development) 
1.7K 
29.2K 
TED tst2013 (test) 
0.9K 
19.6K 
TED tst2014 (test) 
1.3K 
23.8K 
IWSLT EN-FR 
Sentences 
Tokens 
TED training (in-domain) 
178.1K 
3.5M 
WMT training (out-of-domain) 
17.8M 450.0M 
TED dev2010 (development) 
0.9K 
20.1K 
TED tst2010 (test) 
1.6K 
31.9K 
TED tst2011 (test) 
0.8K 
21.4K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>5 .</head><label>5</label><figDesc></figDesc><table>IWSLT EN-DE 
tst2012 
tst2013 
tst2014 
SMT (in) 
20.70 
21.01 
18.50 
SMT (out) 
18.82 
18.12 
16.85 
SMT (in + out) 
20.04 
20.23 
17.08 
in 
23.07 
25.40 
21.45 
out 
18.87 
21.23 
17.07 
in + out 
21.31 
23.54 
19.41 
ensemble (in + out) 
24.34 
25.83 
22.50 
sampler 
23.37 
25.22 
21.91 
Kobus et al. (2016) 
23.23 
25.70 
22.03 
Axelrod et al. (2011) 
23.87 
25.52 
22.41 
sentence weighting 
23.46 
26.26+ 
22.51 
domain weighting 
23.55 
25.47 
21.45 
batch weighting (bw) 25.33++ 27.45++ 23.68++ 
bw + dynamic tuning 26.03++ 28.58++ 24.12++ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>IWSLT EN-DE results. The marks (the same in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : IWSLT EN-FR results.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Further training (</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The original cross-entropy is the lower the better, and we swap the in and out order.</note>

			<note place="foot" n="2"> In practice, we also also evaluated on the Chinese-toEnglish NIST task. Due to limited time and space, we only showed the IWSLT task.</note>

			<note place="foot" n="3"> https://github.com/EdinburghNLP/ nematus 4 https://github.com/wangruinlp/nmt_ instance_weighting The batch weighting part was partially motivated by Nematus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the Dr. Andrew Finch, Dr. Atsushi Fujita and three anonymous reviewers for their insightful comments and suggestions. This work is partially supported by the program "Promotion of Global Communications Plan:</p><p>Research, Development, and Social Demonstration of Multilingual Speech Translation Technology" of MIC, Japan. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">L</forename><surname>Priddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Keller</surname></persName>
		</author>
		<title level="m">Artificial Neural Networks: An Introduction (SPIE Tutorial Texts in Optical Engineering</title>
		<imprint>
			<publisher>SPIEInternational Society for Optical Engineering</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Liums systems for the iwslt 2011 speech translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<meeting><address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perplexity minimization for translation model domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="539" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marcin Junczys-Dowmunt</title>
		<imprint>
			<publisher>Samuel Läubli</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-domain translation model framework for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Translation model adaptation by resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="392" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentence embedding for neural machine translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connecting phrase based statistical machine translation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3135" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for SMT using sentence weight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</title>
		<meeting><address><addrLine>Guangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
