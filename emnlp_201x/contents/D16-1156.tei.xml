<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resolving Language and Vision Ambiguities Together: Joint Segmentation &amp; Prepositional Attachment Resolution in Captioned Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Christie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Laddha</surname></persName>
							<email>ankit1991laddha@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kochersberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Resolving Language and Vision Ambiguities Together: Joint Segmentation &amp; Prepositional Attachment Resolution in Captioned Scenes</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1493" to="1503"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence &quot;I shot an elephant in my pajamas&quot;, looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and preposi-tional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly out-performs the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Perception and intelligence problems are hard. Whether we are interested in understanding an im- Ambiguity:</p><p>(dog next to woman) on couch vs dog next to (woman on couch)</p><p>Figure 1: Overview of our approach. We propose a model for simultaneous 2D semantic segmentation and preposi- tional phrase attachment resolution by reasoning about sentence parses. The language and vision modules each produce M diverse hypotheses, and the goal is to select a pair of consistent hypotheses. In this example the am- biguity to be resolved from the image caption is whether the dog is standing on or next to the couch. Both modules benefit by selecting a pair of compatible hypotheses.</p><p>age or a sentence, our algorithms must operate un- der tremendous levels of ambiguity. When a hu- man reads the sentence "I eat sushi with tuna", it is clear that the preposition phrase "with tuna" mod- ifies "sushi" and not the act of eating, but this may be ambiguous to a machine. This problem of deter- mining whether a prepositional phrase ("with tuna") modifies a noun phrase ("sushi") or verb phrase ("eating") is formally known as Prepositional Phrase Attachment Resolution (PPAR) ( <ref type="bibr" target="#b30">Ratnaparkhi et al., 1994)</ref>. Consider the captioned scene shown in Fig-ure 1. The caption "A dog is standing next to a woman on a couch" exhibits a PP attachment am- biguity -"(dog next to woman) on couch" vs "dog next to (woman on couch)". It is clear that having access to image segmentations can help resolve this ambiguity, and having access to the correct PP at- tachment can help image segmentation. There are two main roadblocks that keep us from writing a single unified model (say a graphical model) to perform both tasks: (1) Inaccurate Mod- els -empirical studies <ref type="bibr" target="#b23">(Meltzer et al., 2005</ref><ref type="bibr" target="#b32">, Szeliski et al., 2008</ref><ref type="bibr" target="#b18">, Kappes et al., 2013</ref>) have repeatedly found that models are often inaccurate and miscali- brated -their "most-likely" beliefs are placed on so- lutions far from the ground-truth. (2) Search Space Explosion -jointly reasoning about multiple modal- ities is difficult due to the combinatorial explosion of search space ({exponentially-many segmentations} × {exponentially-many sentence-parses}).</p><p>Proposed Approach and Contributions. In this paper, we address the problem of simultaneous ob- ject segmentation (also called semantic segmenta- tion) and PPAR in captioned scenes. To the best of our knowledge this is the first paper to do so.</p><p>Our main thesis is that a set of diverse plausible hypotheses can serve as a concise interpretable sum- mary of uncertainty in vision and language 'mod- ules' (What does the semantic segmentation mod- ule see in the world? What does the PPAR mod- ule describe?) and form the basis for tractable joint reasoning (How do we reconcile what the semantic segmentation module sees in the world with how the PPAR module describes it?).</p><p>Given our two modules with M hypotheses each, how can we integrate beliefs across the segmenta- tion and sentence parse modules to pick the best pair of hypotheses? Our key focus is consistency -correct hypotheses from different modules will be correct in a consistent way, but incorrect hypotheses will be incorrect in incompatible ways. Specifically, we develop a MEDIATOR model that scores pairs for consistency and searches over all M 2 pairs to pick the highest scoring one. We demonstrate our ap- proach on three datasets -ABSTRACT-50S <ref type="bibr" target="#b33">(Vedantam et al., 2014</ref>), PASCAL-50S, and PASCAL- Context-50S ( <ref type="bibr" target="#b25">Mottaghi et al., 2014</ref>). We show that our vision+language approach significantly outper- forms the Stanford Parser <ref type="bibr" target="#b8">(De Marneffe et al., 2006)</ref> by 20.66% (36.42% relative) for ABSTRACT-50S, 17.91% (28.69% relative) for PASCAL-50S, and by 12.83% (25.28% relative) for PASCAL-Context- 50S. We also make small but consistent improve- ments over DeepLab-CRF <ref type="bibr" target="#b7">(Chen et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most works at the intersection of vision and NLP tend to be 'pipeline' systems, where vision tasks take 1-best inputs from NLP (e.g., sentence pars- ings) without trying to improve NLP performance and vice-versa. For instance, <ref type="bibr" target="#b11">Fidler et al. (2013)</ref> use prepositions to improve object segmentation and scene classification, but only consider the most- likely parse of the sentence and do not resolve ambi- guities in text. Analogously, <ref type="bibr" target="#b36">Yatskar et al. (2014)</ref> in- vestigate the role of object, attribute, and action clas- sification annotations for generating human-like de- scriptions. While they achieve impressive results at generating descriptions, they assume perfect vision modules to generate sentences. Our work uses cur- rent (still imperfect) vision and NLP modules to rea- son about images and provided captions, and simul- taneously improve both vision and language mod- ules. Similar to our philosophy, an earlier work by <ref type="bibr" target="#b3">Barnard and Johnson (2005)</ref> used images to help disambiguate word senses (e.g. piggy banks vs snow banks). In a more recent work, <ref type="bibr" target="#b12">Gella et al. (2016)</ref> studied the problem of reasoning about an image and a verb, where they attempt to pick the correct sense of the verb that describes the action depicted in the image. <ref type="bibr" target="#b6">Berzak et al. (2015)</ref> resolve linguistic ambi- guities in sentences coupled with videos that repre- sent different interpretations of the sentences. Per- haps the work closest to us is <ref type="bibr" target="#b19">Kong et al. (2014)</ref>, who leverage information from an RGBD image and its sentential description to improve 3D seman- tic parsing and resolve ambiguities related to co- reference resolution in the sentences (e.g., what "it" refers to). We focus on a different kind of ambiguity -the Prepositional Phrase (PP) attachment resolu- tion. In the classification of parsing ambiguities, co- reference resolution is considered a discourse am- biguity ( <ref type="bibr" target="#b26">Poesio and Artstein, 2005</ref>) (arising out of two different words across sentences for the same object), while PP attachment is considered a syntac- tic ambiguity (arising out of multiple valid sentence structures) and is typically considered much more difficult to resolve <ref type="bibr" target="#b2">(Bach, 2016</ref><ref type="bibr">, Davis, 2016</ref>.</p><p>A number of recent works have studied problems at the intersection of vision and language, such as Visual Question Answering ( <ref type="bibr" target="#b1">Antol et al., 2015</ref><ref type="bibr" target="#b13">, Geman et al., 2014</ref><ref type="bibr" target="#b22">, Malinowski et al., 2015</ref>, Vi- sual Madlibs ( <ref type="bibr" target="#b37">Yu et al., 2015)</ref>, and image caption- ing ( <ref type="bibr" target="#b34">Vinyals et al., 2015</ref><ref type="bibr" target="#b10">, Fang et al., 2015</ref>. Our work falls in this domain with a key difference that we produce both vision and NLP outputs.</p><p>Our work also has similarities with works on 'spatial relation learning' <ref type="bibr">Fritz, 2014, Lan et al., 2012)</ref>, i.e. learning a visual rep- resentation for noun-preposition-noun triplets ("car on road"). While our approach can certainly utilize such spatial relation classifiers if available, the focus of our work is different. Our goal is to improve se- mantic segmentation and PPAR by jointly reranking segmentation-parsing solution pairs. Our approach implicitly learns spatial relationships for preposi- tions ("on", "above") but these are simply emergent latent representations that help our reranker pick out the most consistent pair of solutions.</p><p>Our work utilizes a line of work ( <ref type="bibr" target="#b5">, Batra, 2012</ref><ref type="bibr" target="#b27">, Prasad et al., 2014</ref>) on pro- ducing diverse plausible solutions from probabilis- tic models, which has been successfully applied to a number of problem domains (Guzman- <ref type="bibr" target="#b15">Rivera et al., 2013</ref><ref type="bibr" target="#b35">, Yadollahpour et al., 2013</ref><ref type="bibr" target="#b14">, Gimpel et al., 2013</ref><ref type="bibr" target="#b28">, Premachandran et al., 2014</ref><ref type="bibr" target="#b31">, Sun et al., 2015</ref><ref type="bibr" target="#b0">, Ahmed et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In order to emphasize the generality of our approach, and to show that our approach is compatible with a wide class of implementations of semantic segmen- tation and PPAR modules, we present our approach with the modules abstracted as "black boxes" that satisfy a few general requirements and minimal as- sumptions. In Section 4, we describe each of the modules in detail, making concrete their respective features, and other details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">What is a Module?</head><p>The goal of a module is to take input variables x ∈ X (images or sentences), and predict out- put variables y ∈ Y (semantic segmentation) and z ∈ Z (prepositional attachment expressed in sen- tence parse). The two requirements on a module are that it needs to be able to produce scores S(y|x) for potential solutions and a list of plausible hypotheses Y = {y 1 , y 2 , . . . , y M }.</p><p>Multiple Hypotheses. In order to be useful, the set Y of hypotheses must provide an accurate sum- mary of the score landscape. Thus, the hypotheses should be plausible (i.e., high-scoring) and mutu- ally non-redundant (i.e., diverse). Our approach (de- scribed next) is applicable to any choice of diverse hypothesis generators. In our experiments, we use the k-best algorithm of Huang and <ref type="bibr" target="#b17">Chiang (2005)</ref> for the sentence parsing module and the DivMBest algorithm ( ) for the semantic seg- mentation module. Once we instantiate the modules in Section 4, we describe the diverse solution gener- ation in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Reasoning Across Multiple Modules</head><p>We now show how to intergrate information from both segmentation and PPAR modules. Recall that our key focus is consistency -correct hypotheses from different modules will be correct in a consis- tent way, but incorrect hypotheses will be incorrect in incompatible ways. Thus, our goal is to search for a pair (semantic segmentation, sentence parsing) that is mutually consistent.</p><p>Let Y = {y 1 , . . . , y M } denote the M seman- tic segmentation hypotheses and Z = {z 1 , . . . , z M } denote the M PPAR hypotheses. MEDIATOR Model. We develop a "mediator" model that identifies high-scoring hypotheses across modules in agreement with each other. Concretely, we can express the MEDIATOR model as a fac- tor graph where each node corresponds to a mod- ule (semantic segmentation and PPAR). Working with such a factor graph is typically completely in- tractable because each node y, z has exponentially- many states (image segmentations, sentence pars- ing). As illustrated in <ref type="figure">Figure 2</ref>, in this factor-graph view, the hypothesis sets Y, Z can be considered 'delta-approximations' for reducing the size of the output spaces.</p><p>Unary factors S(·) capture the score/likelihood of each hypothesis provided by the corresponding module for the image/sentence at hand. Pairwise factors C(·, ·) represent consistency factors. Impor- <ref type="figure">Figure 2</ref>: Illustrative inter-module factor graph. Each node takes exponentially-many or infinitely-many states and we use a 'delta approximation' to limit support.</p><formula xml:id="formula_0">Delta Approximation Semantic Segmentation Sentence Parsing Delta Approximation Score(z) z Score(y) y S(y i ) S(z j ) y Score(z) z y Score(y) C(y i , z j ) z</formula><p>tantly, since we have restricted each module vari- ables to just M states, we are free to capture ar- bitrary domain-specific high-order relationships for consistency, without any optimization concerns. In fact, as we describe in our experiments, these con- sistency factors may be designed to exploit domain knowledge in fairly sophisticated ways.</p><p>Consistency Inference. We perform exhaustive inference over all possible tuples.</p><formula xml:id="formula_1">argmax i,j∈{1,...,M } M(y i , z j ) = S(y i ) + S(z j ) + C(y i , z j ) .<label>(1)</label></formula><p>Notice that the search space with M hypotheses each is M 2 . In our experiments, we allow each mod- ule to take a different value for M , and typically use around 10 solutions for each module, leading to a mere 100 pairs, which is easily enumerable. We found that even with such a small set, at least one of the solutions in the set tends to be highly accurate, meaning that the hypothesis sets have relatively high recall. This shows the power of using a small set of diverse hypotheses. For a large M , we can exploit a number of standard ideas from the graphical models literature (e.g. dual decomposition or belief propaga- tion). In fact, this is one reason we show the factor in <ref type="figure">Figure 2</ref>; there is a natural decomposition of the problem into modules. Training MEDIATOR. We can express the ME- DIATOR score as M(y i , z j ) = w φ(x, y i , z j ), as a linear function of score and consistency features</p><formula xml:id="formula_2">φ(x, y i , z j ) = [φ S (y i ); φ S (z j ); φ C (y i , z j )]</formula><p>, where φ S (·) are the single-module (semantic segmentation and PPAR module) score features, and φ C (·, ·) are the inter-module consistency features. We describe these features in detail in the experiments. We learn these consistency weights w from a dataset anno- tated with ground-truth for the two modules y, z. Let {y * , z * } denote the oracle pair, composed of the most accurate solutions in the hypothesis sets. We learn the MEDIATOR parameters in a discrimina- tive learning fashion by solving the following Struc- tured SVM problem:</p><formula xml:id="formula_3">min w,ξij 1 2 w w + C ij ξ ij (2a) s.t. w φ(x, y * , z * ) Score of oracle tuple − w φ(x, y i , z j )</formula><p>Score of any other tuple</p><formula xml:id="formula_4">≥ 1 Margin − ξ ij L(y i , z j ) Slack scaled by loss ∀i, j ∈ {1, . . . , M }. (2b)</formula><p>Intuitively, we can see that the constraint (2b) tries to maximize the (soft) margin between the score of the oracle pair and all other pairs in the hypothe- sis sets. Importantly, the slack (or violation in the margin) is scaled by the loss of the tuple. Thus, if there are other good pairs not too much worse than the oracle, the margin for such tuples will not be tightly enforced. On the other hand, the mar- gin between the oracle and bad tuples will be very strictly enforced.</p><p>This learning procedure requires us to define the loss function L(y i , z j ), i.e., the cost of predicting a tuple (semantic segmentation, sentence parsing). We use a weighted average of individual losses:</p><formula xml:id="formula_5">L(y i , z j ) = α(y gt , y i ) + (1 − α)(z gt , z j ) (3)</formula><p>The standard measure for evaluating semantic seg- mentation is average Jaccard Index (or Intersection- over-Union) <ref type="bibr" target="#b9">(Everingham et al., 2010)</ref>, while for evaluating sentence parses w.r.t. their prepositional phrase attachment, we use the fraction of preposi- tions correctly attached. In our experiments, we re- port results with such a convex combination of mod- ule loss functions (for different values of α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now describe the setup of our experiments, pro- vide implementation details of the modules, and de- scribe the consistency features.</p><p>Datasets. Access to rich annotated image + caption datasets is crucial for performing quanti- tative evaluations. Since this is the first paper to study the problem of joint segmentation and PPAR, no standard datasets for this task exist so we had to curate our own annotations for PPAR on three image caption datasets -ABSTRACT- 50S ( <ref type="bibr" target="#b33">Vedantam et al., 2014</ref>), PASCAL-50S ( <ref type="bibr" target="#b33">Vedantam et al., 2014</ref>) (expands the UIUC PASCAL sentence dataset ( <ref type="bibr" target="#b29">Rashtchian et al., 2010</ref>) from 5 captions per image to 50), and PASCAL-Context- 50S ( <ref type="bibr" target="#b25">Mottaghi et al., 2014</ref>) (which uses the PAS- CAL Context image annotations and the same sen- tences as PASCAL-50S). Our annotations are pub- licly available on the authors' webpages. To cu- rate the PASCAL-Context-50S PPAR annotations, we first select all sentences that have preposition phrase attachment ambiguities. We then plotted the distribution of prepositions in these sentences. The top 7 prepositions are used, as there is a large drop in the frequencies beyond these. The 7 prepositions are: "on", "with", "next to", "in front of", "by", "near", and "down". We then further sampled sen- tences to ensure uniform distribution across prepo- sitions. We perform a similar filtering for PASCAL- 50S and ABSTRACT-50S (using the top-6 preposi- tions for ABSTRACT-50S). Details are in the sup- plement. We consider a preposition ambiguous if there are at least two parsings where one of the two objects in the preposition dependency is the same across the two parsings while the other object is dif- ferent (e.g. (dog on couch) and (woman on couch)). To summarize the statistics of all three datasets:</p><p>1. ABSTRACT-50S ( <ref type="bibr" target="#b33">Vedantam et al., 2014</ref>): 25,000 sentences (50 per image) with 500 images from abstract scenes made from cli- part. Filtering for captions containing the top-6 prepositions resulted in 399 sentences describ- ing 201 unique images. These 6 prepositions are: "with", 'next to", "on top of", "in front of", "behind", and "under". Overall, there are 502 total prepositions, 406 ambiguous preposi- tions, 80.88% ambiguity rate and 60 sentences with multiple ambiguous prepositions. 2. PASCAL-50S ( <ref type="bibr" target="#b33">Vedantam et al., 2014</ref>): 50,000 sentences (50 per image) for the images in the UIUC PASCAL sentence dataset ( <ref type="bibr" target="#b29">Rashtchian et al., 2010)</ref>. Filtering for the top-7 preposi- tions resulted in a total of 30 unique images, and 100 image-caption pairs, where ground- truth PPAR were carefully annotated by two vision + NLP graduate students. Overall, there are 213 total prepositions, 147 ambigu- ous prepositions, 69.01% ambiguity rate and 35 sentences with multiple ambiguous prepo- sitions. 3. PASCAL-Context-50S ( <ref type="bibr" target="#b25">Mottaghi et al., 2014</ref>): We use images and captions from PASCAL-50S, but with PASCAL Context segmentation annotations (60 categories in- stead of 21). This makes the vision task more challenging. Filtering this dataset for the top-7 prepositions resulted in a total of 966 unique images and 1,822 image-caption pairs. Ground truth annotations for the PPAR were collected using Amazon Mechanical Turk. Workers were shown an image and a prepositional attachment (extracted from the corresponding parsing of the caption) as a phrase ("woman on couch"), and asked if it was correct. A screenshot of our interface is available in the supplement. Overall, there are 2,540 total prepositions, 2,147 ambiguous prepositions, 84.53% ambiguity rate and 283 sentences with multiple ambiguous prepositions. Setup. Single Module: We first show that visual features help PPAR by using the ABSTRACT-50S dataset, which contains clipart scenes where the ex- tent and position of all the objects in the scene is known. This allows us to consider a scenario with a perfect vision system.</p><p>Multiple Modules: In this experiment we use imperfect language and vision modules, and show improvements on the PASCAL-50S and PASCAL- Context-50S datasets.</p><p>Module 1: Semantic Segmentation (SS) y. We use DeepLab-CRF ( <ref type="bibr" target="#b7">Chen et al., 2015)</ref> and Di- vMBest ( ) to produce M diverse segmentations of the images. To evaluate we use image-level class-averaged Jaccard Index.</p><p>Module 2: PP Attachment Resolution (PPAR) z. We use a recent version (v3.3.1; released 2014) of the PCFG Stanford parser module <ref type="bibr">(De Marneffe et al., 2006, Huang and</ref><ref type="bibr" target="#b17">Chiang, 2005</ref>) to pro- duce M parsings of the sentence. In addition to the parse trees, the module can also output depen- dencies, which make syntactical relationships more explicit. Dependencies come in the form depen- dency type(word 1 , word 2 ), such as the preposition dependency prep on(woman-8, couch-11) (the num- ber indicates the word position in sentence). To eval- uate, we count the percentage of preposition attach- ments that the parse gets correct. Baselines:</p><p>• INDEP. In our experiments, we compare our proposed approach (MEDIATOR) to the highest scoring solution predicted independently from each module. For semantic segmentation this is the output of DeepLab-CRF ( <ref type="bibr" target="#b7">Chen et al., 2015)</ref> and for the PPAR module this is the 1-best out- put of the Stanford Parser <ref type="bibr">(De Marneffe et al., 2006, Huang and</ref><ref type="bibr" target="#b17">Chiang, 2005</ref>). Since our hy- pothesis lists are generated by greedy M-Best algorithms, this corresponds to predicting the (y 1 , z 1 ) tuple. This comparison establishes the importance of joint reasoning. To the best of our knowledge, there is no existing (or even natural) joint model to compare to.</p><p>• DOMAIN ADAPTATION. We learn a reranker on the parses. Note that domain adaptation is only needed for PPAR since the Stanford parser is trained on Penn Treebank (Wall Street Jour- nal text) and not on text about images (such as image captions). Such domain adaptation is not necessary for semantic segmentation. This is a competitive single-module baseline. Specifi- cally, we use the same parse-based features as our approach, and learn a reranker over the M z parse trees (M z = 10). Our approach (MEDIATOR) significantly outper- forms both baselines. The improvements over IN- DEP show that joint reasoning produces more ac- curate results than any module (vision or language) operating in isolation. The improvements over DO- MAIN ADAPTATION establish the source of im- provements is indeed vision, and not the reranking step. Simply adapting the parse from its original training domain (Wall Street Journal) to our domain (image captions) is not enough.</p><p>Ablative Study. Ours-CASCADE: This ablation studies the importance of multiple hypothesis. For each module (say y), we feed the single-best out- put of the other module z 1 as input. Each module learns its own weight w using exactly the same con- sistency features and learning algorithm as MEDI- ATOR and predicts one of the plausible hypothesesˆy hypothesesˆ hypothesesˆy CASCADE = argmax y∈Y w φ(x, y, z 1 ). This ab- lation of our system is similar to <ref type="bibr" target="#b16">(Heitz et al., 2008)</ref> and helps us in disentangling the benefits of multiple hypothesis and joint reasoning.</p><p>Finally, we note that Ours-CASCADE can be viewed as special cases of MEDIATOR. Let MEDI- ATOR-(M y , M z ) denote our approach run with M y hypotheses for the first module and M z for the sec- ond. Then INDEP corresponds to MEDIATOR-(1, 1) and CASCADE corresponds to predicting the y so- lution from MEDIATOR-(M y , 1) and the z solution from MEDIATOR-(1, M z ). To get an upper-bound on our approach, we report oracle, the accuracy of the most accurate tuple in 10 × 10 tuples.</p><p>In the main paper, our results are presented where MEDIATOR was trained with equally weighted loss (α = 0.5), but we provide additional results for varying values of α in the supplement.</p><p>MEDIATOR and Consistency Features. Recall that we have two types of features -(1) score fea- tures φ S (y i ) and φ S (z j ), which try to capture how likely solutions y i and z j are respectively, and (2) consistency features φ C (y i , z j ), which capture how consistent the PP attachments in z j are with the segmentation in y i . For each (object 1 , preposi- tion, object 2 ) in z j , we compute 6 features between object 1 and object 2 segmentations in y i . Since the humans writing the captions may use multiple syn- onymous words (e.g. dog, puppy) for the same vi- sual entity, we use word2vec ( <ref type="bibr" target="#b24">Mikolov et al., 2013)</ref> similarities to map the nouns in the sentences to the corresponding dataset categories.</p><p>• Semantic Segmentation Score Features (φ S (y i )) (2-dim): We use ranks and solution scores from DeepLab-CRF ( <ref type="bibr" target="#b7">Chen et al., 2015</ref>).</p><p>• PPAR Score Features (φ S (z i )) (9-dim): We use ranks and the log probability of parses from <ref type="bibr" target="#b8">(De Marneffe et al., 2006</ref>), and 7 binary indicators for PASCAL (6 for ABSTRACT- 50S) denoting which prepositions are present in the parse. <ref type="figure">Figure 3</ref>: Example on PASCAL-50S ("A dog is stand- ing next to a woman on a couch."). The ambiguity in this sentence "(dog next to woman) on couch" vs "dog next to (woman on couch)". We calculate the horizontal and ver- tical distances between the segmentation centers of "per- son" and "couch" and between the segmentation centers of "dog" and "couch". We see that the "dog" is much fur- ther below the couch (53.91) than the woman (2.65). So, if the MEDIATOR model learned that "on" means the first object is above the second object, we would expect it to choose the "person on couch" preposition parsing.</p><p>• Inter-Module Consistency Features (56- dim): For each of the 7 prepositions, 8 features are calculated: -One feature is the Euclidean distance between the center of the segmentation masks of the two objects connected by the preposition. These two objects in the segmentation correspond to the categories with which the soft similarity of the two objects in the sentence is highest among all PASCAL categories. -Four features capture max{0, (normalized -directional-distance)}, where directional- distance measures above/below/left/right displacements between the two objects in the segmentation, and normalization in- volves dividing by height/width. -One feature is the ratio of sizes between object 1 and object 2 in the segmentation. -Two features capture the word2vec sim- ilarity between the two objects in PPAR (say 'puppy' and 'kitty') with their most similar PASCAL category (say 'dog' and 'cat'), where these features are 0 if the cat- egories are not present in segmentation. A visual illustration for some of these features for PASCAL can be seen in <ref type="figure">Figure 3</ref>. In the case where an object parsed from z j is not present in the segmentation y i , the distance features are set to 0. The ratio of areas fea- tures (area of smaller object / area of larger ob- ject) are also set to 0 assuming that the smaller object is missing. In the case where an ob- ject has two or more connected components in the segmentation, the distances are computed w.r.t. the centroid of the segmentation and the area is computed as the number of pixels in the union of the instance segmentation masks. We also calculate 20 features for PASCAL-50S and 59 features for PASCAL-Context-50S that capture that consistency between y i and z j , in terms of presence/absence of PASCAL cate- gories. For each noun in PPAR we compute its word2vec similarity with all PASCAL cat- egories. For each of the PASCAL categories, the feature is the sum of similarities (with the PASCAL category) over all nouns if the cate- gory is present in segmentation, and is -1 times the sum of similarities over all nouns otherwise. This feature set was not used for ABSTRACT- 50S, since these features were intended to help improve the accuracy of the semantic segmen- tation module. For ABSTRACT-50S, we only use the 5 distance features, resulting in a 30- dim feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-Module Results</head><p>We performed a 10-fold cross-validation on the ABSTRACT-50S dataset to pick M (=10) and the weight on the hinge-loss for MEDIATOR (C). The results are presented in <ref type="table">Table 1</ref>. Our approach sig- nificantly outpeforms 1-best outputs of the Stan- ford Parser <ref type="bibr" target="#b8">(De Marneffe et al., 2006</ref>) by 20.66% (36.42% relative). This shows a need for diverse hy- potheses and reasoning about visual features when picking a sentence parse. oracle denotes the best achievable performance using these 10 hypotheses.  <ref type="table">Table 1</ref>: Results on our subset of ABSTRACT-50S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multiple-Module Results</head><p>We performed 10-fold cross-val for our results of PASCAL-50S and PASCAL-Context-50S, with 8   <ref type="table">Table 2</ref>: Results on our subset of the PASCAL-50S and PASCAL-Context-50S datasets. We are able to significantly outperform the Stanford Parser and make small improvements over DeepLab-CRF for PASCAL-50S.</p><p>train folds, 1 val fold, and 1 test fold, where the val fold was used to pick M y , M z , and C. <ref type="figure" target="#fig_1">Fig- ure 4</ref> shows the average combined accuracy on val, which was found to be maximal at M y = 5, M z = 3 for PASCAL-50S, and M y = 1, M z = 10 for PASCAL-Context-50S, which are used at test time. We present our results in <ref type="table">Table 2</ref>. Our approach significantly outperforms the Stanford Parser <ref type="bibr" target="#b8">(De Marneffe et al., 2006</ref>) by 17.91% (28.69% relative) for PASCAL-50S, and 12.83% (25.28% relative) for PASCAL-Context-50S. We also make small improvements over DeepLab- CRF ( <ref type="bibr" target="#b7">Chen et al., 2015</ref>) in the case of PASCAL-50S. To measure statistical significance of our results, we performed paired t-tests between MEDIATOR and INDEP. For both modules (and average), the null hypothesis (that the accuracies of our approach and baseline come from the same distribution) can be successfully rejected at p-value 0.05. For sake of completeness, we also compared MEDIATOR with our ablated system (CASCADE) and found statisti- cally significant differences only in PPAR.</p><p>These results demonstrate a need for each mod- ule to produce a diverse set of plausible hypothe- ses for our MEDIATOR model to reason about. In the case of PASCAL-Context-50S, MEDIATOR per- forms identical to CASCADE since M y is chosen as 1 (which is the CASCADE setting) in cross- validation. Recall that MEDIATOR is a larger model class than CASCADE (in fact, CASCADE is a special case of MEDIATOR with M y = 1). It is interesting to see that the large model class does not hurt, and MEDIATOR gracefully reduces to a smaller capac- ity model (CASCADE) if the amount of data is not enough to warrant the extra capacity. We hypothe- size that in the presence of more training data, cross- validation may pick a different setting of M y and M z , resulting in full utilization of the model capac- ity. Also note that our domain adaptation baseline achieved an accuracy higher than MAP/Stanford- Parser, but significantly lower than our approach for both PASCAL-50S and PASCAL-Context-50S. We also performed this for our single-module experi- ment and picked M z (=10) with cross-validation, We can see that our model has implicitly learned spatial ar- rangements unlike other spatial relation learning (SRL) works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-50S PASCAL-Context-50S</head><p>Feature set Instance-Level Jaccard Index PPAR Acc. PPAR Acc.  which resulted in an accuracy of 57.23%. Again, this is higher than MAP/Stanford-Parser (56.73%), but significantly lower than our approach (77.39%). Clearly, domain adaptation alone is not sufficient. We also see that oracle performance is fairly high, suggesting that when there is ambiguity and room for improvement, MEDIATOR is able to rerank ef- fectively.</p><p>Ablation Study for Features. <ref type="table" target="#tab_3">Table 3</ref> displays results of an ablation study on PASCAL-50S and PASCAL-Context-50S to show the importance of the different features. In each row, we retain the module score features and drop a single set of con- sistency features. We can see all consistency fea- tures contribute to the performance of MEDIATOR.</p><p>Visualizing Prepositions. <ref type="figure" target="#fig_2">Figure 5</ref> shows a vi- sualization for what our MEDIATOR model has im- plicitly learned about 3 prepositions ("on", "by", "with"). These visualizations show the score ob- tained by taking the dot product of distance fea- tures (Euclidean and directional) between object 1 and object 2 connected by the preposition with the corresponding learned weights of the model, consid- ering object 2 to be at the center of the visualization. Notice that these were learned without explicit train- ing for spatial learning as in spatial relation learning (SRL) works <ref type="bibr">Fritz, 2014, Lan et al., 2012</ref>). These were simply recovered as an in- termediate step towards reranking SS + PPAR hy- potheses. Also note that SRL cannot handle multi- ple segmentation hypotheses, which our work shows are important <ref type="table">(Table 2 CASCADE</ref>). In addition, our approach is more general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions and Conclusion</head><p>We presented an approach to the simultaneous rea- soning about prepositional phrase attachment res- olution of captions and semantic segmentation in images that integrates beliefs across the modules to pick the best pair of a diverse set of hypothe- ses. Our full model (MEDIATOR) significantly improves the accuracy of PPAR over the Stan- ford Parser by 17.91% for PASCAL-50S and by 12.83% for PASCAL-Context-50S, and achieves a small improvement on semantic segmentation over DeepLab-CRF for PASCAL-50S. These results demonstrate a need for information exchange be- tween the modules, as well as a need for a diverse set of hypotheses to concisely capture the uncertainties of each module. Large gains in PPAR validate our intuition that vision is very helpful for dealing with ambiguity in language. Furthermore, we see even larger gains are possible from the oracle accuracies.</p><p>While we have demonstrated our approach on a task involving simultaneous reasoning about lan- guage and vision, our approach is general and can be used for other applications. Overall, we hope our approach will be useful in a number of settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>next to woman) on couch vs dog next to (woman on couch)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Validation accuracies for different values of M on ABSTRACT-50S, (b) for different values of M y , M z on PASCAL-50S, (c) for different values of M y , M z on PASCAL-Context-50S.</figDesc><graphic url="image-96.png" coords="8,207.23,57.83,141.39,121.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualizations for 3 different prepositions (red = high scores, blue = low scores). We can see that our model has implicitly learned spatial arrangements unlike other spatial relation learning (SRL) works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Ablation study of different feature combinations. Only PPAR Acc. is 
shown for PASCAL-Context-50S because M y = 1. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Larry Zitnick, Mohit Bansal, Kevin Gim-pel, and Devi Parikh for helpful discussions, sugges-tions, and feedback included in this work. A major-ity of this work was done while AL was an intern at Virginia Tech. This work was partially supported by a National Science Foundation CAREER award, an Army Research Office YIP Award, an Office of Naval Research grant N00014-14-1-0679, and GPU donations by NVIDIA, all awarded to DB. GC was supported by DTRA grant HDTRA1-13-1-0015 pro-vided by KK. The views and conclusions contained herein are those of the authors and should not be in-terpreted as necessarily representing official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing Expected Intersection-over-Union with Candidate-Constrained CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dany</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Routledge encyclopedia of philosophy entry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kent</forename><surname>Bach</surname></persName>
		</author>
		<ptr target="http://online.sfsu.edu/kbach/ambguity.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation with Pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">167</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diverse M-Best Solutions in Markov Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payman</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abner</forename><surname>Guzmanrivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Efficient Message-Passing Algorithm for the M-Best MAP Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do You See What I Mean? Visual Resolution of Linguistic Ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Harari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating Typed Dependency Parses from Phrase Structure Parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The Pascal Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From Captions to Visual Concepts and Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Sentence is Worth a Thousand Pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Visual Turing Test for Computer Vision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Systematic Exploration of Diversity in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abner</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">DivMCuts: Faster Training of Structural SVMs with Diverse M-Best Cutting-Planes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascaded Classification Models: Combining Models for Holistic Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Better k-best Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWPT</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Comparative Study of Modern Inference Techniques for Discrete Energy Minimization Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jörg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schnörr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">X</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kausler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Lellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What are you talking about? Text-to-Image Coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Mohit Bansal, Raquel Urtasun, and Sanja Fidler</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image Retrieval with Structured Object Queries Using Latent Ranking SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A pooling approach to modelling spatial relations for image retrieval and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5190</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Globally Optimal Solutions for Energy Minimization in Stereo Vision Using Reweighted Belief Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Talya</forename><surname>Meltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yanover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Role of Context for Object Detection and Semantic Segmentation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Annotating (Anaphoric) ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus Linguistics Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Empirical Minimum Bayes Risk Prediction: How to extract an extra few% performance from vision models with just three more parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vittal Premachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collecting Image Annotations Using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Maximum Entropy Model for Prepositional Phrase Attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology. ACL</title>
		<meeting>the workshop on Human Language Technology. ACL</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Active Learning for Structured Probabilistic Models With Histogram Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based Image Description Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative Re-ranking of Diverse Segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payman</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">See No Evil, Say No Evil: Description Generation from Densely Labeled Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical and Computational Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual Madlibs: Fill in the Blank Description Generation and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
