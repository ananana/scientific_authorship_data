<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Picking Apart Story Salads</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics and Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Holgate</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
							<email>gdurrett@cs.utexas.edu katrin.erk@mail.utexas.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Picking Apart Story Salads</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1455" to="1465"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1455</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>During natural disasters and conflicts, information about what happened is often confusing , messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task and that it is necessary to take into account global context and coherence.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When a natural disaster strikes or a conflict arises, it is often hard to determine what happened. In- formation is messy and confusing, spread out over many messages, buried in irrelevant text, and even conflicting. For example, when flight MH-17 crashed in Ukraine in 2014, there were initially many theories of what happened, including a mis- sile strike initiated by Russia-affiliated militants, a missile strike by the Ukrainian military, and a terrorist attack. There was no single coherent in- terpretation of what happened, but multiple, messy narratives, a story salad. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what hap- pened. This task is also the subject of an upcoming task at the Text Analysis Conference. <ref type="bibr">1</ref> Picking apart a story salad is a hard task that could in principle make use of arbitrary amounts of inference. But it is also a task in which coher-  ence judgments could play a large role, the sim- plest being topical coherence, but also narrative coherence <ref type="bibr">Jurafsky, 2008, 2009;</ref><ref type="bibr" target="#b32">Pichotta and Mooney, 2016;</ref><ref type="bibr" target="#b29">Mostafazadeh et al., 2017)</ref>, overall textual coherence ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b27">Logeswaran et al., 2018)</ref>, and coher- ence in the description of entities. This makes it an attractive task for neural models.</p><p>To make the task accessible to neural mod- els, we propose a simple method for creating simulated story salad data at scale: we mix to- gether sentences from different documents. <ref type="figure" target="#fig_1">Fig- ure 1</ref> shows an example mixture of two arti- cles from Wikipedia, one on the Russia-Chechnya conflict and one on a conflict between the U.S. and Afghanistan. By controlling how similar the source documents are, we can flexibly adjust the difficulty of the task. In particular, as we show be- low, we can generate data that exhibits challenging inference problems by exploiting the Wikipedia category structure. <ref type="bibr">2</ref> While this data is still simpler than story salads arising naturally, it approximates the task, is sufficiently challenging for modeling, and can be generated in large amounts. <ref type="bibr">3</ref> We explore some initial models for our Story Salad task. As the aim of the task is to group story pieces into stories, we start with straight- forward clustering based on topic similarity. But topic similarity is clearly not enough to group the right pieces together. For example, the two articles in <ref type="figure" target="#fig_1">Figure 1</ref> are both about armed conflicts, but the Russia-Chechnya sentences in the example form a group in contrast to the U.S.-Afghanistan sen- tences. To model this, we learn sentence embed- dings adapted to the clustering task and with ac- cess to global information about the salad at hand. We also test an extension where to decide whether to group two sentences together, the model mu- tually attends to the sentences during encoding in order to better focus on the commonalities and dif- ferences of these two sentences. Both extensions lead to better models (6-13% improvement in ac- curacy with a model incorporating both), confirm- ing that the task requires more than just general topical similarity. But there is much room for im- provement, in particular on salads generated to be more difficult, where performance is around 15 points lower than on arbitrary mixtures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Building on early work in script learning <ref type="bibr" target="#b34">(Schank and Abelson, 1977)</ref>, <ref type="bibr" target="#b6">Chambers and Jurafsky (2008)</ref> introduce narrative schema and propose the "narrative cloze" task where the modeling ob- jective is to predict the event happening next. The topic has since seen many extensions and vari- ants coupled with increasingly sophisticated mod- els <ref type="bibr" target="#b7">(Chambers and Jurafsky, 2009</ref>) including neu- ral networks <ref type="bibr" target="#b17">(Granroth-Wilding and Clark, 2016;</ref><ref type="bibr" target="#b32">Pichotta and Mooney, 2016;</ref><ref type="bibr" target="#b29">Mostafazadeh et al., 2017)</ref>. This line of work is related to story sal- ads in that our aim of separating entangled narra- tives in a document mixture also leverages within- narrative coherence. Our work, however, is very different from narrative cloze: (i) we group sen- tences/events rather than predicting what happens next; (ii) crucially, the narrative coherence in story salads is in context, in that a narrative clustering is only meaningful with respect to a particular doc- ument mixture (see Section 5, 6), while in narra- tive cloze the next event is predicted on a "global" salads are available for download directly and we have provided code to reconstruct the NYT salads from English Gigaword 5 (available as LDC2003T05).</p><p>level. <ref type="bibr">4</ref> Working with labeled story salad examples, we draw inspiration from previous work on su- pervised clustering ( <ref type="bibr" target="#b3">Bilenko et al., 2004;</ref><ref type="bibr" target="#b14">Finley and Joachim, 2005</ref>). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space ( <ref type="bibr" target="#b30">Pennington et al., 2014;</ref><ref type="bibr">Kiros et al., 2015;</ref><ref type="bibr" target="#b28">Mekala et al., 2017;</ref> for word/sentence/event encoding; neural compo- nents for enhanced supervised clustering ( <ref type="bibr" target="#b3">Bilenko et al., 2004</ref>), in particular LSTMs <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b12">Dai and Le, 2015)</ref>, CNNs <ref type="bibr" target="#b22">(Kim, 2014;</ref><ref type="bibr" target="#b11">Conneau et al., 2017)</ref>, and atten- tion mechanisms ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b18">Hermann et al., 2015;</ref><ref type="bibr" target="#b25">Lin et al., 2017)</ref>. By exploring our ability to pick apart story salads with these state- of-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive research; (ii) un- derstand the nature of the task and the challenges it sets forth for NLP research in general.</p><p>The task of picking apart story salads is related to the task of conversation disentanglement <ref type="bibr" target="#b13">(Elsner and Charniak, 2008;</ref><ref type="bibr" target="#b36">Wang and Oard, 2009;</ref><ref type="bibr" target="#b21">Jiang et al., 2018)</ref>, which is a clustering task of dividing a transcript into a set of distinct conver- sations. While superficially similar to our Story Salad task, conversation disentanglement focuses on dialogues and has many types of metadata available, such as time stamps, discourse infor- mation, and chat handles. Existing systems draw heavily on this metadata. Another related task is the distinction of on-topic and off-topic docu- ments <ref type="bibr" target="#b2">(Bekkerman and Crammer, 2008)</ref>, which is defined in terms of topical relatedness. In compar- ison, the story salad task offers opportunities for more in-depth reasoning, as we show below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Natural story salads arise when multiple messy narratives exist to describe the same event or out- come. Often this is because each contribution to the explanation only addresses a small aspect of the larger picture. We can directly simulate the confusion this kind of discourse creates by taking multiple narratives, cutting them into small pieces, and mixing them together. Data generation. Story salads are generated by combining content from source documents and randomizing the sentence order of the resulting mixture. In order to ensure appropriately sized salads, we require that each source document con- tain at least eight sentences. Furthermore, to avoid problematically large salads, we pull paragraphs from source documents one at a time until the eight sentence minimum is met. While this proce- dure can be used to mix any number of documents, we currently present mixtures of two documents.</p><p>We utilize two different corpora as sources for story salad generation: (i) the subset of New York Times articles presented within English Gi- gaword ( <ref type="bibr" target="#b16">Graff and Cieri, 2003)</ref> and (ii) English Wikipedia 5 <ref type="bibr" target="#b41">(Wikipedia contributors, 2004</ref>). An overview of the datasets is available in <ref type="table">Table 1</ref>.</p><p>Gigaword. From the New York Times subset of Gigaword, we compiled a set of 573,681 mix- tures we call NYT. Each mixture in this set is con- structed from source articles pulled from the same month and year. Because this temporal constraint is the only restriction put on what articles can be mixed, it is possible for a salad to be constructed from topically disparate source documents (e.g., a restaurant review and a political story). We intend NYT to be relatively easy on the whole as a result of this design choice. However, it is also possible for articles about dominant news stories and trends (e.g., the OJ Simpson trial in the summer of 1994) to be mixed as a result of the same temporal constraint. We therefore pulled out a curated subset of NYT con- sisting only of salads generated from highly top- ically similar source documents which we call NYT-HARD. This subset consists of the 1,000 sal- ads where the source documents are most topically similar. We calculate topic similarity scores by <ref type="bibr">5</ref> Wikipedia dump pulled on January <ref type="bibr">20, 2018.</ref> computing the cosine similarity between the av- erage word embeddings for each source document (denoted cos hereafter)</p><formula xml:id="formula_0">cos(d) = g(ω 1 ) · g(ω 2 ) g(ω 1 ) g(ω 2 ) (1)</formula><p>where ω 1 and ω 2 are the source documents, g is a function that computes the average word embed- ding of a document, and d is the salad under eval- uation. The cos scores on the test portion of the datasets are presented in <ref type="table">Table 1</ref>.</p><p>Wikipedia. From Wikipedia, we present an ad- ditional set of 500k salads constructed by combin- ing random articles which we call WIKI.</p><p>We also leverage Wikipedia category member- ship as a form of human-annotated topic informa- tion. We use this to create a set of 50,374 salads, henceforth called WIKI-HARD, by restricting the domain of articles to only those appearing in cate- gories containing the words conflict and war. Each mixture in this set is generated from source arti- cles from the same category in order to produce highly difficult mixtures. We intend this to be a challenge set in this domain as the constituent ar- ticles for a given mixture are intentionally selected to be closely related. While we have used the cate- gory information to construct an intentionally very difficult set for this paper, we note that this proce- dure can be used to create sets of varying difficulty.</p><p>The fact that WIKI-HARD is generated from human-annotated category labels differentiates it from NYT-HARD in the source of its difficulty. Af- ter manually reviewing 20 samples from each *- HARD dataset, we found that NYT-HARD more fre- quently contains salads that are impossible for hu- mans to pick apart while WIKI-HARD more fre- quently contains salads that are possible, though challenging. In particular, in 9 out of 20 WIKI- HARD salads we found that access to world knowl- edge and inference would be beneficial. Never- theless, the two *-HARD datasets are both high in topic similarity <ref type="table">(Table 1)</ref>.</p><p>In <ref type="figure" target="#fig_3">Figure 2</ref> we present sentences from a sample WIKI-HARD salad that can be solved with world knowledge. In this salad, we learn about two indi- viduals. We can tell that Randle, born in 1855, is unlikely to also have been enrolled in high school in 1913 at the age of 58. We also learn that Randle was a doctor, while Martins, the other individual, was involved in theater. From this, we can deduce that the individual who "also worked as a wrestler" is more likely to be Martins than Randle.  Event Representation. Finally, we explore a form of document representation that has been shown to be useful in narrative schema learning, a related task. We include variants of NYT and NYT- HARD with story salads consisting of event tuple representations instead of natural language sen- tence representations, as in <ref type="bibr" target="#b32">Pichotta and Mooney (2016)</ref>. We label these variants as NYT-EVENT and NYT-EVENT-HARD. Event tuples are in the form &lt;VERB, SUBJ, DOBJ, PREP, POBJ&gt;, where as many preposition and prepositional ob- ject pairs as necessary are allowed. <ref type="bibr">6</ref> Summary. The story salads we present here are, in the end, simpler than those that occur nat- urally in the news or on social media: for one thing, sentences drawn from a document written by a single author should exhibit a high degree of coherence. We have also shown that we can use Wikipedia category annotations to produce large- scale story salad datasets with customizable lev- els of difficulty, enabling us to increase the diffi- culty of the task as performance increases. In the following section, we see that both our standard and *-HARD mixtures are challenging for current models. Furthermore, our WIKI-HARD dataset contains salads featuring conflicting information and is an attractive setting for building models with deeper reasoning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(A) John K. Randle was born on 1 February 1855, son of Thomas Randle, a liberated slave from an Oyo village in the west of what is now Nigeria. (B) In 1913 he was enrolled in Eko Boys High School but dropped out. (B) Martins joined the theatre and from there took on various theatre jobs to survive. (A) Born in Sierra Leone , he was one of the first West Africans to qualify as a doctor in the United Kingdom. (B) He also worked as a wrestler (known as "Black Butcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We treat the story salad task as a narrative clus- tering task where, in our dataset, each salad is comprised of two clusters. Accordingly, the first baselines we consider are standard clustering ap- proaches.</p><p>Baselines. Our first baseline is a simple uni- form baseline (hereafter UNIF), where we assign <ref type="bibr">6</ref> Event tuples are extracted via the extractor presented in <ref type="bibr" target="#b8">Cheng and Erk (2018)</ref>, and copular verbs are not treated as events, meaning that some sentences translate to null events. all sentences in a document mixture to a single cluster. Under UNIF the clustering accuracy is the percentage of the majority-cluster sentences, e.g. if a mixture has 7 sentences from one narrative and 3 from the other, then the accuracy is 0.7.</p><p>Additionally, we explore a family of base- lines that consist of clustering off-the-shelf sen- tence embeddings. We choose k-medoids 7 (here- after KM) as our clustering algorithm. For sen- tence embeddings, we experimented with (i) aver- aged 300D GloVe embeddings ( <ref type="bibr" target="#b30">Pennington et al., 2014)</ref>, which have been shown to produce surpris- ingly strong performance in a variety of text clas- sification tasks <ref type="bibr" target="#b20">(Iyyer et al., 2015;</ref><ref type="bibr" target="#b9">Coates and Bollegala, 2018)</ref>; (ii) skip-thought embeddings ( <ref type="bibr">Kiros et al., 2015)</ref>; and (iii) SCDV ( <ref type="bibr" target="#b28">Mekala et al., 2017</ref>), a multisense-aware sentence embedding algorithm which builds upon pretrained GloVe embeddings using a Gaussian mixture model. Averaged GloVe embeddings gave the best performance in our ex- periments; to avoid clutter, we only report those results henceforth.</p><p>Neural supervised clustering. Our baselines work directly on sentence embeddings and there- fore ignore the task-specific supervision available in our labeled training data. Inspired by the work in <ref type="bibr" target="#b3">Bilenko et al. (2004)</ref> and <ref type="bibr" target="#b14">Finley and Joachim (2005)</ref> on supervised clustering, we aim to exploit this supervision using a learned distance metric in our clustering. 8 <ref type="figure" target="#fig_4">Figure 3</ref> shows our model, which produces a distribution P (same | s 1 , s 2 , d): the probability that two sentences s 1 and s 2 taken from document mixture d are in the same cluster. We train this model as a binary classifier on sampled pairs of sentences to distinguish same-narrative sentence pairs (positive examples) from different-narrative pairs (negative examples). 1−P (same | s 1 , s 2 , d) is then used by the clusterer as the pairwise dis- tance metric. Given the pairwise distance between all sentence pairs in a mixture, the KM algorithm 7 K-medoids is chosen as a substitute for k-means because the latter does not extend easily to our classifier-aided neu- ral models: it does not work when only pairwise distances are available. In empirical evaluation we found k-means and k-medoids to produce very similar accuracy scores when us- ing off-the-shelf embeddings. Experiments with hierarchical agglomerative clustering (not reported here) showed it to per- form worse than either method. <ref type="bibr">8</ref> In early experiments, another strong candidate we tried is a joint model of a sentence autoencoder and a clustering algorithm ( <ref type="bibr" target="#b42">Yang et al., 2017</ref>). However, this produces subpar performance (weaker than the strongest baseline), due par- tially to scalability issues in learning these jointly.  can then be applied to cluster sentences into two narratives.</p><formula xml:id="formula_1">P (same | s 1 , s 2 , d)</formula><p>Our classifier is a neural network model built on top of LSTM sentence encoders, which per- form well at similar text classification tasks <ref type="bibr" target="#b12">(Dai and Le, 2015;</ref><ref type="bibr" target="#b26">Liu et al., 2016)</ref>. <ref type="bibr">9</ref> Denoting a sen- tence as the list of embeddings of its constituent words: s = {w 1 , . . . , w M }, we first encode it as a sentence embedding z with a bidirectional LSTM z = BiLSTM(s) and then compute the probability score with a bilinear layer:</p><formula xml:id="formula_2">P (same | s 1 , s 2 ) = σ(z T 1 W z 2 )<label>(2)</label></formula><p>This model corresponds to the green subset of <ref type="figure" target="#fig_4">Fig- ure 3</ref>. Stronger models. There are two additional ef- fects we might want our model to capture. First, whether two sentences are from the same narra- tive cannot be determined globally: there aren't two "globally-contrasted" 10 narratives (or bag-of- words based topics) from which sentences are sampled. In other words, sentences are always (pairwise) compared in the context of the docu- ment mixture from which they are drawn. Sec- ond, we want to capture more in-depth interac- tions between sentences: our sentence embedding scheme for a sentence s 1 should exploit its point <ref type="bibr">9</ref> Experiments with convolutional encoders here yielded somewhat worse results. <ref type="bibr">10</ref> Two stories may be on the same topic and still form clearly different narratives. For example, both narratives in <ref type="figure" target="#fig_1">Figure 1</ref> are regarding military conflict. of comparison s 2 and encode s 1 with a view of similarities to and differences with s 2 . This type of technique has been useful in tasks like natural language inference (NLI) ( <ref type="bibr" target="#b4">Bowman et al., 2015;</ref><ref type="bibr" target="#b31">Peters et al., 2018)</ref>.</p><p>To improve contextualization, we add a CNN- based context encoder to the BiLSTM classifier: the reader embeds the whole document salad at hand into a vector. Formally, we compute c = CNN(d), where in this case CNN denotes a single convolution layer with max pooling in the style of <ref type="bibr" target="#b22">Kim (2014)</ref> and d is the concatenation of all sen- tences in the mixture. This component is shown in blue in <ref type="figure" target="#fig_4">Figure 3</ref>. The context vector c is then appended to z and fed into the bilinear layer.</p><p>To capture the interaction between two sen- tences in a pair, we employ a mutual attention mechanism, which is similar to the attentive reader ( <ref type="bibr" target="#b18">Hermann et al., 2015)</ref>. Let e i,1...n denote the BiL- STM outputs for the tokens of sentence i. Given the encoding z 1 of sentence s 1 , we compute atten- tion weights and a representation of s 2 as follows:</p><formula xml:id="formula_3">α 1→2 = softmax j (z 1 e 2,j ) m 1→2 = j α 1→2,j e 2,j</formula><p>We compute m 2→1 analogously. This process is shown in purple in <ref type="figure" target="#fig_4">Figure 3</ref>. The m vectors are used as additional inputs to the bilinear layer.</p><p>For comprehensive ablation, we experiment with four variants of neural classifiers: (i) BiL-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STM alone (BILSTM); (ii) BiLSTM + mutual at- tention (BILSTM-MT); (iii) BiLSTM + context (BILSTM-CTX); and (iv) BiLSTM + mutual atten- tion and context (BILSTM-MT-CTX).</head><p>Event-based models. For the event-based vari- ants of the datasets, NYT-EVENT and NYT-EVENT- HARD, we build three models: (i) FFNN-BILSTM: we input a sentence as a sequence of event em- beddings rather than word embeddings as in BIL- STM, where a feedforward layer maps the words in an event tuple to an event embedding; (ii) FFNN-BILSTM-MT-CTX: replacing the base BIL- STM in (i) with our best model which is enhanced with mutual attention and contextualization; (iii) FFNN-BILSTM-MT-CTX-PRETRAIN: a variant of (ii) that is based on the event embedding pretrain- ing method 11 described in <ref type="bibr" target="#b38">Weber et al. (2018)</ref>, where events are encoded with a feedforward net (same as (i)) and trained with a word2Vec-like ob- jective, encouraging events that co-occur in the same narrative to have more similar embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Analysis</head><p>Experimental setup. To stave off sparsity, we impose a vocabulary cut by using only the 100k most frequent lemmas. To evaluate on NYT, NYT-EVENT, WIKI and WIKI-HARD, we sample 20k unique salads (from their respective test por- tions 12 ) to use for both the sentence and event ver- sions of the experiments. For WIKI-HARD, the training combines the training portions of both WIKI and WIKI-HARD. For NYT-HARD, we train on the training portion of NYT and evaluate on NYT-HARD in full as a test set.</p><p>All the neural components are constructed with TensorFlow and use the same hyperparameters across variants: a 2-layer BiLSTM, learning rate 1e-5 with Adam ( <ref type="bibr" target="#b23">Kingma and Ba, 2014</ref>), dropout (Srivastava et al., 2014) rate 0.3 on all layers, and Xavier initialization <ref type="bibr" target="#b15">(Glorot and Bengio, 2010)</ref>. To create training pairs for the neural classifiers, we randomly sample sentence pairs balanced be- tween same-narrative and different-narrative pairs. We train with a batch size of 32 and stop when an epoch yields less than 0.001% accuracy improve- ment on the validation set, which is 5% of mix- tures sampled from the training data beforehand <ref type="bibr">11</ref> In <ref type="bibr" target="#b38">Weber et al. (2018)</ref>, a more complex tensor-based model is applied. Using exactly same method in our experi- ments we obtain weaker results. <ref type="bibr">12</ref> Test sets available with data release.  (the models are not trained on the validation sam- ple). For KM we use the default configurations of off-the-shelf software. <ref type="bibr">13</ref> Evaluation. We evaluate all models in terms of a clustering accuracy metric (hereafter CA), which is a simple extension from the conventional ac- curacy metric: we calculate the ratio of correctly clustered sentences in a document mixture, aver- aged over test mixtures. Given a document mix- ture d i , we call its component documents A and B. Let pred be a function that does the cluster- ing by mapping each sentence s i,n of mixture d i to either A or B, and true AB a function that re- turns the original pseudo-labels (i.e. {A, B}) as they are, and true BA flips the pseudo-labels, i.e. A → B and B → A. Then the clustering accuracy for document d i by pred is CA(di, pred, true) = max{C(di, pred, trueAB),</p><formula xml:id="formula_4">C(di, pred, trueBA)} C(di, pred, true) = 1 Ni n 1[pred(si,n) = true(si,n)]</formula><p>where s i,n is the n-th sentence of mixture d i .</p><p>Sentence based models. First we evaluate the sentence based models. We first run the UNIF baseline on all our datasets, where we obtain near- 50% clustering accuracy. This indicates that the data are all balanced in the number of sentences in the two narratives of the mixtures. We then run k- medoids (KM) on sentence embeddings as a base- line to compare to the classifier-aided models. The results are summarized in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We first observe that the KM is a strong baseline and outperforms the supervised BILSTM system in the harder settings. Adding the mutual attention mechanism and contextualization, however, im- Lighter color indicates higher attention weights. The two heatmaps in the same block are for the attention weights of one sen- tence attending to the other. In (a), we see related con- cepts being identified (video game and sega), while in (b), we see a contrast (family station wagon and sega group).</p><p>prove BILSTM substantially. In addition, the per- formance boost from the two components seems more or less orthogonal, as shown by the much stronger accuracy of the combined model (i.e. BILSTM-MT-CTX) than the models with a single component (i.e. BILSTM-MT and BILSTM-CTX). Overall, the large margin of KM and classifier- aided models above the UNIF baseline indicates that separating story salads is a valid task where generalizable patterns can be exploited by ma- chine learning techniques.</p><p>Why would the mutual attention mechanism help? Plotting the attention weights of randomly selected samples, we see distributionally similar words being attended to in <ref type="figure" target="#fig_5">Figure 4a</ref>. Intuitively, a BiLSTM compresses a sentence into a single vec- tor, leading to information loss ( <ref type="bibr" target="#b10">Conneau et al., 2018)</ref>. Mutual attention enriches this represen- tation by allowing us access to detailed informa- tion in sentences at word-level resolution by cap- turing lexical similarity. Even more interestingly, we observe a synergistic effect between mutual attention and contextualization: with the context reader added, we see high attention weights on words/phrases which bear little distributional sim- ilarity but are important for connecting/contrasting different narratives. For example, in <ref type="figure" target="#fig_5">Figure 4b</ref>, sega group and family station wagon are selected <ref type="table">Table 3</ref>: Spearman's ρ correlation between cluster- ing accuracy (CA) and topic similarity (cos) in the evaluation with NYT and WIKI. The p-values are all below 0.01 (indicated by *). Contextualized models (+CONTEXT) are more robust to high topic similarity than their uncontextualized counterparts (-CONTEXT), indicated by the lower negative correlation between their accuracy and topic similarity.</p><formula xml:id="formula_5">Type Model NYT WIKI -CONTEXT BILSTM −0.40 * −0.43 * BILSTM-MT −0.38 * −0.40 * +CONTEXT BILSTM-CTX −0.31 * −0.30 * BILSTM-MT-CTX −0.27 * −0.25 *</formula><p>by the attention, despite not having similar words in the other sentences. These words are crucial in identifying the two narratives in this mixture: one is about a Japanese video game company, the other is on vehicle manufacturing in the U.S.</p><p>Another observation is that all models see dras- tic reduction in accuracy in the *-HARD version of the data. In fact, the clustering accuracy corre- sponds well with our topic similarity metric (cos, Eq. 1; <ref type="table">Table 1</ref>) across models. In addition, cos is negatively correlated with clustering accuracy for all mixtures <ref type="table">(Table 3)</ref>.</p><p>From the results we also see that contextualiza- tion brings clear performance improvement. This supports our hypothesis that the Story Salad task is a nonstandard clustering task where the contrast of two narratives is only meaningful in the con- text of the particular mixture where they reside, rather than on a corpus-general level. Taking the example in <ref type="figure" target="#fig_1">Figure 1</ref>, the Russian-Chechnya and the U.S.-Afghanistan narratives are contrasted in that mixture, but one can easily imagine a mix- ture where they are in the same narrative and are contrasted to another narrative on business affairs. Further, contextualized models are less vulnera- ble to the performance reduction on mixtures with high topic similarity: for one thing, contextualiza- tion improves performance over the base BILSTM on both regular and *-HARD datasets. Secondly, computing the correlation between clustering ac- curacy and topic similarity, we see a lower nega- tive correlation for contextualized models, true for both NYT and WIKI datasets <ref type="table">(Table 3)</ref>.</p><p>Event based models. While the accuracy scores in the event based experiments are in gen- eral lower than those in the sentence based (Ta- ble 4), overall we observe the same pattern that   mutual attention and contextualization contribute substantially to the performance. More interest- ingly, the performance reduction on the topically highly similar *-HARD is more mild compared to the sentence based experiments, which provides initial evidence that event-based narrative encod- ing allows the models to be more robust to dis- traction by lexical overlap in topically similar nar- ratives. Finally we see that the event pretraining with Weber et al. <ref type="formula" target="#formula_2">(2018)</ref>'s technique brings addi- tional improvement over a contextualized system. The results open up a door for future work: (i) our simple models do not make use of corefer- ence, narrative schema or world knowledge, which are intuitively promising components to introduce (see, e.g., the salad in <ref type="figure" target="#fig_3">Figure 2)</ref>; (ii) more so- phisticated model architectures may help capture the information missed by our models: moving from the sentence version to the event version, we lose many words which may have provided crucial cues in the sentence-based experiments.</p><p>Error analysis. In order to understand the er- rors made by each model, we performed a man- ual analysis of a small sample of bad clusterings. In a sample of 60 mixtures from NYT (test set), we considered all clusterings for which accuracy was less than 0.65. Among the 60 mixtures, the base model had an accuracy this low for 27 mix- tures, the BILSTM-MT and BILSTM-CTX model had 13 low-accuracy mixtures each, and BILSTM- MT-CTX had 3. Each mixture was manually an- notated by 2 annotators as being sourced from (i) thematically closely related documents (e.g., two stories on the same political event), (ii) themati- cally distinct documents (e.g., a political story and a sports story), or (iii) cannot tell.</p><p>Our analysis showed that the base BILSTM model has difficulty even in cases where the source documents for the salad are thematically distinct. This was the case in 9 of 27 bad clusterings. The BILSTM-MT, BILSTM-CTX and BILSTM-MT-CTX (A) lehman brothers be one of several investment bank eager to get UNK hand on state asset, across the nation and in massachusetts (A) former massachusetts governor william f. weld, a staunch supporter of privatization during UNK administration, have UNK in the hall of the state house, now as a corporate lawyer try to drum up support for the sale of lucrative state asset. (B) officially, the rays option dukes, 22, to class a vero beach and place UNK on the temporary inactive list, where UNK will remain for an undetermined amount of time as UNK undergo counseling. (B) UNK apparently will receive UNK $ 380,000 major-league salary .</p><p>Figure 5: An example of (preprocessed) sentences from two unrelated documents being that have been clus- tered into a single cluster by the base model. Docu- ment (A) is an article about proposed privatization of public assets, while Document (B) is an article about happenings in Major League Baseball. models not only have many fewer bad clusterings, they also show low accuracy almost exclusively in mixtures of related documents (2 cases of distinct documents for BILSTM-CTX, none for BILSTM- MT or BILSTM-MT-CTX). <ref type="figure">Figure 5</ref> shows an ex- ample of a bad clustering of two unrelated docu- ments, produced by the base BILSTM model.</p><p>In a second study, we rated the same 60 sam- ples by their difficulty for a human, focusing in particular on mixtures that went from low perfor- mance (0.5-0.65) in the BILSTM model to high performance (0.8-1.0) in another model. For BILSTM-CTX we find that only 2 out of 11 mix- tures with such marked improvement over BIL- STM were hard for humans; for BILSTM-MT only 1 out of 9 markedly improved mixtures was hard for humans. But for BILSTM-MT-CTX, 8 out of 17 markedly improved mixtures were hard for hu- mans, indicating that more sophisticated models do better not only on easy but also on hard cases.</p><p>In a third small study, we compare NYT-HARD and WIKI-HARD for their difficulty for humans, looking at 20 mixtures each. Here, very inter- estingly, we find more mixtures that are impos- sible for humans in NYT-HARD (10 cases, exam- ple in <ref type="figure" target="#fig_7">Figure 6</ref>) than WIKI-HARD (3 cases). This presents a clear discrepancy between difficulty for humans and difficulty for models: the models do better on NYT-HARD which is harder for us. While we would not want to draw strong conclusions from a small sample, this hints at possibilities of future work where world knowledge, which is likely to be orthogonal to the information picked up by the models, can be introduced to improve performance (e.g. ).</p><p>Note that unlike many other NLP tasks where (A) The most basic question face the country on energy be how to keep supply and demand in line. The Democrats would say : "what can UNK do to make good use of what UNK have get?" (B) Oil price dominate the 31-minute news conference, hold here near pittsburgh. (B) Vice President Al Gore hold UNK first news conference in 67 day on Friday, defend UNK call for the release of oil from the govern- ment's stockpile and and vow that UNK would "confront friend and foe alike" over the marketing of violent entertainment to child, despite the million in donation UNK receive from Hollywood. (A) With oil price up, consumer agitate and the winter heating season loom, Vice President Al Gore and Gov. George W. Bush be go at UNK on en- ergy policy, seek to draw sharp distinction over an issue on which both candidate have political vulnerability. (B) On other topic, Gore say UNK be not nervous about the upcoming debate, but be incredulous when a reporter ask whether UNK be confident UNK have the elec- tion lock up. (A) Bush, who criticize the decision as a political ploy to drive down price just ahead of election day, be schedule to discuss energy policy in a speech on Friday. (B) On Friday, Bush call Gore a "flip-flopper", say UNK proposal to tap into the reserve be a political ploy. human performance sets the ceiling for the best achievable results (e.g. span-prediction based question answering ( <ref type="bibr" target="#b33">Rajpurkar et al., 2016)</ref>, where all the information needed for the correct answer is available in the input), successfully picking apart narratives in a story salad may require consulting an external knowledge base, which affords ma- chine learning models a clear advantage over hu- mans. For example, recognizing that Commander Kamal is likely to be Afghani based on his name, which is not knowledge every reader possesses, would allow us to successfully cluster the sentence with the U.S.-Afghanistan narrative rather than the Russian-Chechnya narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a technique to generate Story Salads, mixtures of multiple narratives, at scale. We have demonstrated that the difficulty of these mixtures can be manipulated either based on doc- ument similarity or based on human-created doc- ument categories. This data gives rise to a chal- lenging binary clustering task (but easily extended to n-ary), where the aim is to group sentences that come from the same original narrative. As coher- ence plays an important role in this task, the task is related to work on narrative schemas <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b32">Pichotta and Mooney, 2016)</ref> and textual coherence ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b27">Logeswaran et al., 2018)</ref>. The automated and scal- able data generation technique allows for the use of neural models, which need large amounts of training data. Conducting a series of preliminary experiments on the data with common unsupervised cluster- ing algorithms <ref type="bibr" target="#b5">(Cao and Yang, 2010)</ref> and vari- ants of neural network-based <ref type="bibr" target="#b22">(Kim, 2014;</ref><ref type="bibr" target="#b12">Dai and Le, 2015;</ref><ref type="bibr" target="#b26">Liu et al., 2016</ref>) supervised clustering ( <ref type="bibr" target="#b3">Bilenko et al., 2004;</ref><ref type="bibr" target="#b14">Finley and Joachim, 2005</ref>) models, we have (i) verified the validity of the task where generalizable patterns can be learned through machine learning techniques; (ii) shown that this is a nonstandard clustering task in which the contrast between narratives is in context as op- posed to global; (iii) found that there is a class of mixtures that are doable for humans but very dif- ficult for our current models, and that in particular the category-based method creates a high propor- tion of such mixtures.</p><p>Our work opens up a large number of directions for future research. First, while our models obtain strong results on simpler story salads, they have low performance on more difficult mixtures with high topical similarity. Second, there are many in- tuitively promising sources of information that we have not explored, such as coreference. And third, our models rely on pairwise similarity-based co- herence learning, which leads to the natural ques- tion of whether structured prediction would im- prove performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>A) Some of the prisoners were survivors of the Battle of Qala-i-Jangi in Mazar-i-Sharif. (A) Chechnya came under the influence of warlords. (B) The U.S. invaded Afghanistan the same year when several Taliban prisoners were shot. (A) Russian federal troops entered Chechnya and ended its independence. (A) The Russian casualties included at least two commandos killed and 11 wounded. (B) The dead were buried in the same grave under the authority of Commander Kamal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A story salad involving two articles, about a Russian military operation in Chechnya (A) and about a U.S. operation in Afghanistan (B). These two articles are topically similar but their mixture can still be disentangled based on narrative coherence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Johnson").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A story salad from WIKI-HARD, sourced from articles belonging to the Nigerian people of World War I category. The sentences from this salad have been rearranged for clearer presentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: BiLSTM sentence pair classifier to determine whether s 1 and s 2 are from the same narrative, augmented with a mutual attention and a context reader. The three subcomponents-the BiLSTM, the mutual attention mechanism, and the context reader-each produce vectors, denoted as z, m, c respectively. In the basic BILSTM model, only z is fed to the bilinear layer (Eq. 2), while more sophisticated models incorporate the additional mutual attention and context vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention weight heatmaps for a random sample with BILSTM-MT-CTX. Lighter color indicates higher attention weights. The two heatmaps in the same block are for the attention weights of one sentence attending to the other. In (a), we see related concepts being identified (video game and sega), while in (b), we see a contrast (family station wagon and sega group).</figDesc><graphic url="image-1.png" coords="7,72.00,62.81,218.26,187.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Part of a story salad that is impossible for a human to pick apart (source: NYT-HARD). "UNK" represents out-of-vocabulary tokens, and all the words are lemmatized. Both narratives, i.e. (A) and (B) involve the characters Al Gore and George Bush, and both are on the topic of energy, with strongly overlapping vocabulary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Clustering accuracy (CA) results from the sen-
tence based experiments. More sophisticated models 
do better across all datasets, particularly on *-HARD 
tasks, which are substantially more challenging. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>NYT -EVENT NYT-EVENT-HARD</head><label>NYT</label><figDesc></figDesc><table>KM 

64.7 
55.3 

FFNN-BILSTM 

64.9 
54.8 
*-MT-CTX 
66.8 
59.1 
*-MT-CTX-PRETRAIN 
70.2 
61.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Clustering accuracy (CA) results from the 
event based experiments. *-MT-CTX is a short hand 
for FFNN-BILSTM-MT-CTX. The same notation applies 
for the following models. 

</table></figure>

			<note place="foot" n="1"> https://tac.nist.gov/2018/SM-KBP/ index.html</note>

			<note place="foot" n="4"> The story salad task is more similar to multichoice narrative cloze (Granroth-Wilding and Clark, 2016) in this regard, but formulated categorically differently.</note>

			<note place="foot" n="13"> github.com/letiantian/kmedoids</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the DARPA AIDA program under AFRL grant FA8750-18-2-0017. Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, DoD or the US government. We ac-knowledge the Texas Advanced Computing Cen-ter for providing grid resources that contributed to these results. We are grateful to the anonymous reviewers for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling Local Coherence: An Entity-Based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-class Clustering in the Text Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Bekkerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integrating Constraints and Metric Learning in Semi-Supervised Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Large Annotated Corpus for Learning Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Improved K-Medoids Clustering Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingru</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCAE</title>
		<meeting>ICCAE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Event Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Schemas and Their Participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Implicit Argument Prediction with Event Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Meta-Embedding Computing MetaEmbeddings by Averaging Source Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What You Can Cram into a Single Vector: Probing Sentence Embeddings for Linguistic Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised Clustering with Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<title level="m">English Gigaword LDC2003T05. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What Happens Next? Event Prediction Using a Compositional Neural Network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Wilding</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vrun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Disentangle Interleaved Conversational Threads with a Siamese Hierarchical Network and Similarity Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyun-Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-Thought Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Structured Self-attentive Sentence Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network for Text Classification with Multi-task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentence Ordering and Coherence Modeling Using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SCDV: Sparse Composite Document Vectors using soft clustering over distributional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargavi</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">LSDSem 2017 Shared Task: The Story Cloze Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Shared Task</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using Sentence-Level LSTM Language Models for Script Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proeedings of ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scripts, Plans, Goals and Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<pubPlace>Lawrence Erlbaum</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a Simple Way to Prevent Neural Network from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contextbased Message Expansion for Disentanglement of Interleaved Text Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling Semantic Plausibility by Injecting World Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Event Representations with Tensor-based Compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">PlagiarismWikipedia, the free encyclopedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia Contributors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
