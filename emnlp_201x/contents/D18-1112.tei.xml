<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SQL-to-Text Generation with Graph-to-Sequence Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
							<email>fengyansong@pku.edu.cn, vadims@us.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sheinin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SQL-to-Text Generation with Graph-to-Sequence Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="931" to="936"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>931</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models , which may not fully capture the inherent graph-structured information in SQL query. In this paper, we first introduce a strategy to represent the SQL query as a directed graph and then employ a graph-to-sequence model to encode the global structure information into node embeddings. This model can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model significantly out-performs the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the SQL-to-text task is to automati- cally generate human-like descriptions interpret- ing the meaning of a given structured query lan- guage (SQL) query ( <ref type="figure" target="#fig_1">Figure 1</ref> gives an example). This task is critical to the natural language inter- face to a database since it helps non-expert users to understand the esoteric SQL queries that are used to retrieve the answers through the question- answering process <ref type="bibr">(Simitsis and Ioannidis, 2009</ref>) using varous text embeddings techniques <ref type="bibr" target="#b8">(Kim, 2014;</ref><ref type="bibr" target="#b0">Arora et al., 2017;</ref><ref type="bibr" target="#b20">Wu et al., 2018a)</ref>.</p><p>Earlier attempts for SQL-to-text task are rule- based and template-based ( <ref type="bibr" target="#b11">Koutrika et al., 2010;</ref><ref type="bibr" target="#b12">Ngonga Ngomo et al., 2013)</ref>. Despite requiring intensive human efforts to design temples or rules, these approaches still tend to generate rigid and stylized language that lacks the natural text of the human language. To address this, <ref type="bibr" target="#b7">Iyer et al. (2016)</ref> proposes a sequence-to-sequence (Seq2Seq) net- work to model the SQL query and natural lan- guage jointly. However, since the SQL is designed * Work done when the author was at IBM Research. which company has both the market value and assets higher than val 0, ranking in top val <ref type="bibr">2</ref> and revenue of val 3 to express graph-structured query intent, the se- quence encoder may need an elaborate design to fully capture the global structure information. In- tuitively, varous graph encoding techniques base on deep neural network ( <ref type="bibr" target="#b10">Kipf and Welling, 2016;</ref><ref type="bibr" target="#b6">Hamilton et al., 2017;</ref><ref type="bibr" target="#b17">Song et al., 2018</ref>) or based on Graph Kernels ( <ref type="bibr" target="#b19">Vishwanathan et al., 2010;</ref><ref type="bibr">Wu et al., 2018b</ref>), whose goal is to learn the node-level or graph-level representations for a given graph, are more proper to tackle this problem.</p><p>In this paper, we first introduce a strategy to represent the SQL query as a directed graph (see ยง2) and further make full use of a novel graph- to-sequence (Graph2Seq) model ( <ref type="bibr" target="#b22">Xu et al., 2018)</ref> that encodes this graph-structured SQL query, and then decodes its interpretation (see ยง3). On the en- coder side, we extend the graph encoding work of <ref type="bibr" target="#b6">Hamilton et al. (2017)</ref> by encoding the edge direc- tion information into the node embedding. Our en- coder learns the representation of each node by ag- gregating information from its K-hop neighbors. Different from <ref type="bibr" target="#b6">Hamilton et al. (2017)</ref> which ne- glects the edge direction, we classify the neighbors of a node according to the edge direction, say v, into two classes, i.e., forward nodes (v directs to) and backward nodes (direct to v). We apply two distinct aggregators to aggregate the information of these two types of nodes, resulting two repre- sentations. The node embedding of v is the con- catenation of these two representations. Given the learned node embeddings, we further introduce a pooling-based and an aggregation-based method to generate the graph embedding.</p><p>On the decoder side, we develop an RNN-based decoder which takes the graph vector representa- tion as the initial hidden state to generate the se- quences while employing an attention mechanism over all node embeddings. Experimental results show that our model achieves the state-of-the-art performance on the WikiSQL dataset and Stack- overflow dataset. Our code and data is available at https://github.com/IBM/SQL-to-Text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph Representation of SQL Query</head><p>Representing the SQL query as a graph instead of a sequence could better preserve the inherent structure information in the query. An example is illustrated in the blue dashed frame in <ref type="figure" target="#fig_2">Figure 2</ref>. One can see that representing them as a graph in- stead of a sequence could help the model to bet- ter learn the correlation between this graph pattern and the interpretation "...both X and Y higher than Z...". This observation motivates us to represent the SQL query as a graph. In particular, we use the following method to transform the SQL query to a graph: <ref type="bibr">1</ref> SELECT Clause. For the SELECT clause such as "SELECT company", we first create a node as- signed with text attribute select. This SELECT node connects with column nodes whose text at- tributes are the selected column names such as company. For SQL queries that contain aggrega- tion functions such as count or max, we add one aggregation node which is connected with column nodes. Similarly, their text attributes are the ag- gregation function names.</p><p>WHERE Clause. The WHERE clause usually contains more than one condition. For each condi- tion, we use the same process as for the SELECT clause to create nodes. For example, in <ref type="figure" target="#fig_2">Figure 2</ref>, we create node assets and &gt;val 0 for the first con- dition, the node sales and &gt;val 0 for the second condition. We then integrate the constraint nodes that have the same text attribute (e.g., &gt;val 0 in <ref type="figure" target="#fig_2">Figure 2</ref>). For a logical operator such as AND, OR and NOT, we create a node that connects with all column nodes that the operator works on. Finally, these logical operator nodes connect with the SE- LECT node. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph-to-sequence Model</head><p>Based on the constructed graphs for the SQL queries, we make full use of a novel graph-to- sequence model ( <ref type="bibr" target="#b22">Xu et al., 2018)</ref>, which consists of a graph encoder to learn the embedding for the graph-structured SQL query, and a sequence de- coder with attention mechanism to generate sen- tences. Conceptually, the graph encoder generates the node embedding for each node by accumu- lating information from its K-hop neighbors, and produces a graph embedding for the entire graph by abstracting all node embeddings. Our decoder takes the graph embedding as the initial hidden state and calculates the attention over all node em- beddings on the encoder side to generate natural language interpretations.</p><p>Node Embedding. Given the graph G = (V, E), since the text attribute of a node may include a list of words, we first use a Long Short Term Memory (LSTM) to generate the feature vector a v for all nodes โv โ V from v's text attribute. We use these feature vectors as initial node embeddings. Then, our model incorporates information from a node's neighbors within K hop into its representation by repeating the following process K times:</p><formula xml:id="formula_0">h 0 v = a v , h 0 v = a v , โv โ V<label>(1)</label></formula><formula xml:id="formula_1">h k N (v) = M k ({h kโ1 u , โu โ N (v)})<label>(2)</label></formula><formula xml:id="formula_2">h k v = ฯ(W k ยท CONCAT(h kโ1 v , h k N (v) )) (3) h k N (v) = M k ({h kโ1 u , โu โ N (v)})<label>(4)</label></formula><formula xml:id="formula_3">h k v = ฯ(W k ยท CONCAT(h kโ1 v , h k N (v) )) (5)</formula><p>where k โ {1, ..., K} is the iteration index, N is the neighborhood function 2 , h k v (h k v ) is node v's forward (backward) representation which aggre- gates the information of nodes in N (v) (N (v)),</p><formula xml:id="formula_4">M k</formula><p>and M k are the forward and backward aggre- gator functions, W k denotes weight matrices, ฯ is a non-linearity function.</p><p>For example, for node v โ V, we first aggre- gate the forward representations of its immediate neighbors {h kโ1 u , โu โ N (v)} into a single vec- tor h k N (v) (equation 2). Note that this aggrega- tion step only uses the representations generated at previous iteration and its initial representation is a</p><note type="other">v . Then we concatenate v's current forward rep- resentation h kโ1 v with the newly generated neigh- borhood vector h k N (v) . This concatenated vector is fed into a fully connected layer with nonlinear activation function ฯ, which updates the forward representation of v to be used at the next itera- tion (equation 3). Next, we update the backward representation of v in the similar fashion (equa- tion 4โผ5). Finally, the concatenation of the for- ward and backward representation at last itera- tion K, is used as the resulting representation of v. Since the neighbor information from different hops may have</note><p>the different impact on the node embedding, we learn a distinct aggregator function at each step. This aggregator feeds each neigh- bor's vector to a fully-connected neural network and an element-wise max-pooling operation is ap- plied to capture different aspects of the neighbor set.</p><p>Graph Embedding. Most existing works of graph convolution neural networks focus more on node embeddings rather than graph embeddings (GE) since their focus is on the node-wise clas- sification task. However, graph embeddings that convey the entire graph information are essential to the downstream decoder, which is crucial to our task. For this purpose, we propose two ways to generate graph embeddings, namely, the Pooling- based and Node-based methods.</p><p>Pooling-based GE. This method feeds the ob- tained node embeddings into a fully-connected neural network and applies the element-wise max- pooling operation on all node embeddings. In experiments, we did not observe significant per- formance improvement using min-pooling and average-pooling.</p><p>Node-based GE. Following ( <ref type="bibr" target="#b15">Scarselli et al., 2009)</ref>, this method adds a super node v s that is connected to all other nodes by a special type of edge. The embedding of v s , which is treated as graph embedding, is produced using node embed- ding generation algorithm mentioned above.</p><p>Sequence Decoding. The decoder is an RNN which predicts the next token y i given all the pre- vious words y &lt;i = y 1 , ..., y iโ1 , the RNN hid- den state s i for time-step i and the context vector c i that captures the attention of the encoder side. In particular, the context vector c i depends on a set of node representations (h 1 ,...,h V ) to which the encoder maps the input graph. The context vector c i is dynamically computed using attention mechanism over the node representations. Our model is jointly trained to maximize the condi- tional log-probability of the correct description given a source graph with respect to the parame- ters ฮธ of the model:</p><formula xml:id="formula_5">ฮธ * = arg max ฮธ N n=1 Tn t=1 log p(y n t |y n &lt;t , x n )</formula><p>where (x n , y n ) is the n-th SQL-interpretation pair in the training set, and T n is the length of the n-th target sentence y n . In the inference phase, we use the beam search algorithm with beam size = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We  <ref type="bibr" target="#b7">(Iyer et al., 2016)</ref>. We use the BLEU-4 score ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) as our automatic evaluation metric and also perform a human study. For human evaluation, we randomly sampled 1,000 predicted results and asked three native English speakers to rate each in- terpretation against both the correctness conform- ing to the input SQL and grammaticality on a scale between 1 and 5. We compare some vari- ants of our model against the template, Seq2Seq, and Tree2Seq baselines. Graph2Seq-PGE. This method uses the Pooling method for generating Graph Embedding.</p><p>Graph2Seq-NGE. This method uses the Node based Graph Embedding.</p><p>Template. We implement a template-based method which first maps each element of a SQL query to an utterance and then uses simple rules to assemble these utterances. For example, we BLEU-4 Grammar. Correct. map SELECT to which, WHERE to where, &gt; to more than. This method translates the SQL query of <ref type="figure" target="#fig_1">Figure 1</ref> to which company where assets more than val 0 and sales more than val 0 and industry less than or equal to val 1 and profits equals val 2 . Seq2Seq. We choose two Seq2Seq models as our baselines. The first one is the attention- based Seq2Seq model proposed by <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref>, and the second one additionally introduces the copy mechanism in the decoder side ( <ref type="bibr" target="#b5">Gu et al., 2016)</ref>. To evaluate these models, we employ a template to convert the SQL query into a se- quence: "SELECT + &lt;aggregation function&gt; + &lt;Split Symbol&gt; + &lt;selected column&gt; + WHERE + &lt;condition0&gt;</p><formula xml:id="formula_6">+ &lt;Split Symbol&gt; + &lt;condition1&gt; + ... ".</formula><p>Tree2Seq. We also choose a tree-to-sequence model proposed by <ref type="bibr" target="#b3">(Eriguchi et al., 2016)</ref> as our baseline. We use the SQL Parser tool 3 to convert a SQL query into the tree structure 4 which is fed to the Tree2Seq model.</p><p>Our proposed models are trained using the Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref>, with mini-batch size 30. Our hyper-parameters are set based on performance on the validation set. The learning rate is set to 0.001. We apply the dropout strategy ( <ref type="bibr" target="#b18">Srivastava et al., 2014</ref>) with the ratio of 0.5 at the decoder layer to avoid overfitting. Gra- dients are clipped when their norm is bigger than 20. We initialize word embeddings using GloVe word vectors from <ref type="bibr" target="#b14">Pennington et al. (2014)</ref>, and the word embedding dimension is 300. For the graph encoder, the hop size K is set to 6, the non- linearity function ฯ is implemented as ReLU ( <ref type="bibr" target="#b4">Glorot et al., 2011</ref>), the parameters of weight matrices W k are randomly initialized. The decoder has one layer, and its hidden state size is 300.</p><p>3 http://www.sqlparser.com 4 See Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQL Query &amp; Interpretations</head><p>1. COUNT Player WHERE starter = val0 AND touchdowns = val1 AND position = val2 S: How many players played in position val2 G: number of players with starter val0 and get touchdowns val1 for val2 2. SELECT Tires WHERE engine = val0 AND chassis = val1 AND team = val2 S: which tire has engine val0 and chassis val1 and val2 G: which tire does val2 run with val0 engine and val1 chassis <ref type="table">Table 2</ref>: Example of SQL queries and predicted interpreta- tions where S and G denotes Seq2Seq and Graph2Seq mod- els, respectively. <ref type="table">Table 1</ref> summarizes the results of our models and baselines. Although the template-based method achieves decent BLEU scores, its grammaticality score is substantially worse than other baselines. We can see that on both two datasets, our Graph2Seq models per- form significantly better than the Seq2Seq and Tree2Seq baselines. One possible reason is that in our graph encoder, the node embedding retains the information of neighbor nodes within K hops. However, in the tree encoder, the node embed- ding only aggregates the information of descen- dants while losing the knowledge of ancestors. The pooling-based graph embedding is found to be more useful than the node-based graph em- bedding because Graph2Seq-NGE adds a nonex- istent node into the graph, which introduces the noisy information in calculating the embeddings of other nodes. We also conducted an experiment that treats the SQL query graph as an undirected graph and found the performance degrades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>By manually analyzing the cases in which the Graph2Seq model performs better than Seq2Seq, we find the Graph2Seq model is better at inter- preting two classes of queries: (1) the complicated queries that have more than two conditions (Query 1); (2) the queries whose columns have implicit relationships (Query 2). <ref type="table">Table 2</ref> lists some such SQL queries and their interpretations. One possi- ble reason is that the Graph2Seq model can better learn the correlation between the graph pattern and natural language by utilizing the global structure information.</p><p>We find the hop size has a significant impact on our model since it determines how many neigh- bor nodes to be considered during the node em- bedding generation. As the hop size increasing, the performance is found to be significantly im- proved. However, after the hop size reaches 6, increasing the hop size can not boost the perfor- mance on WikiSQL anymore. By analyzing the most complicated queries (around 6.2%) in Wik- iSQL, we find there are average six hops between a node and its most distant neighbor. This result indicates that the selected hop size should guar- antee each node can receive the information from others nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Previous work approaches the SQL-to-text task using an Seq2Seq model which does not fully cap- ture the global structure information of the SQL query. To address this, we proposed a Graph2Seq model which includes a graph encoder, an atten- tion based sequence decoder. Experimental results show that our model significantly outperforms the Seq2Seq and Tree2Seq models on the WikiSQL and Stackoverflow datasets. We apply the SQL Parser tool <ref type="bibr" target="#b2">5</ref> to convert an SQL query to a tree whose structure is il- lustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. More specifically, the root has two child nodes, namely Select List and Where Clause. The child nodes of Select List rep- resent the selected columns in the SQL query. The Where Clause has the logical operators occurred in the SQL query as its children. The children of a logical operator node are the conditions on which this operator works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>SQL): SELECT company WHERE assets &gt; val 0 AND sales &gt; val 0 AND industry_rank &lt;= val 2 AND revenue = val 3 Interpretation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of SQL query and its interpretation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The graph representation of the SQL query in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Tree representation of the SQL query.</figDesc></figure>

			<note place="foot" n="1"> This method could be simply extended to cope with more general SQL queries that have complex syntaxes such as JOIN and ORDER BY.</note>

			<note place="foot" n="2"> N (v) returns the nodes that v directs to and N (v) returns the nodes that direct to v.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">jointly learning to align and translate</title>
		<idno>abs/1409.0473</idno>
		<ptr target="http://www.sqlparser.com" />
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06075</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>AIS- TATS 2011</idno>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2011-04-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining structured queries in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Koutrika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alkis</forename><surname>Simitsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sorry, i don&apos;t speak sparql: translating sparql queries into natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Bรผhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="977" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Philadelphia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002-07-06" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alkis Simitsis and Yannis Ioannidis</title>
		<idno type="arXiv">arXiv:0909.1786</idno>
	</analytic>
	<monogr>
		<title level="m">Dbmss should talk back too</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A graph-to-sequence model for amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word mover&apos;s embedding: From word2vec to document embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">E H</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04956</idno>
		<title level="m">Pradeep Ravikuma, and Michael Witbrock. 2018b. D2ke: From distance to kernel and embedding</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph2seq: Graph to sequence learning with attention-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
