<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Generation and Snapshot Learning in Neural Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Milica</roleName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaši´</forename><surname>Gaši´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><forename type="middle">Mrkši´</forename><surname>Mrkši´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Generation and Snapshot Learning in Neural Dialogue Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2153" to="2162"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model ar-chitectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Network (RNN)-based condi- tional language models (LM) have been shown to be very effective in tackling a number of real world problems, such as machine translation (MT) ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>) and image caption generation <ref type="bibr" target="#b12">(Karpathy and Fei-Fei, 2015)</ref>. Recently, RNNs were ap- plied to task of generating sentences from an ex- plicit semantic representation <ref type="bibr" target="#b37">(Wen et al., 2015a</ref>). Attention-based methods ( <ref type="bibr" target="#b20">Mei et al., 2016)</ref> and Long Short-term Memory (LSTM)-like <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) gating mechanisms ( <ref type="bibr" target="#b38">Wen et al., 2015b</ref>) have both been studied to improve gen- eration quality. Although it is now clear that LSTM- based conditional LMs can generate plausible nat- ural language, less effort has been put in compar- ing the different model architectures. Furthermore, conditional generation models are typically tested on relatively straightforward tasks conditioned on a single source (e.g. a sentence or an image) and where the goal is to optimise a single metric (e.g. BLEU). In this work, we study the use of condi- tional LSTMs in the generation component of neu- ral network (NN)-based dialogue systems which de- pend on multiple conditioning sources and optimis- ing multiple metrics.</p><p>Neural conversational agents ( <ref type="bibr" target="#b36">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b28">Shang et al., 2015)</ref> are direct extensions of the sequence-to-sequence model <ref type="bibr" target="#b34">(Sutskever et al., 2014</ref>) in which a conversation is cast as a source to target transduction problem. However, these mod- els are still far from real world applications be- cause they lack any capability for supporting domain specific tasks, for example, being able to interact with databases ( <ref type="bibr" target="#b32">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b43">Yin et al., 2016)</ref> and aggregate useful information into their re- sponses. Recent work by <ref type="bibr" target="#b39">Wen et al. (2016a)</ref>, how- ever, proposed an end-to-end trainable neural dia- logue system that can assist users to complete spe- cific tasks. Their system used both distributed and symbolic representations to capture user intents, and these collectively condition a NN language genera- tor to generate system responses. Due to the diver- sity of the conditioning information sources, the best way to represent and combine them is non-trivial.</p><p>In <ref type="bibr" target="#b39">Wen et al. (2016a)</ref>, the objective function for learning the dialogue policy and language generator depends solely on the likelihood of the output sen- tences. However, this sequential supervision signal may not be informative enough to learn a good con- ditioning vector representation resulting in a gener- ation process which is dominated by the LM. This can often lead to inappropriate system outputs.</p><p>In this paper, we therefore also investigate the use of snapshot learning which attempts to mitigate this problem by heuristically applying companion super- vision signals to a subset of the conditioning vector. This idea is similar to deeply supervised nets ( <ref type="bibr" target="#b14">Lee et al., 2015</ref>) in which the final cost from the out- put layer is optimised together with the companion signals generated from each intermediary layer. We have found that snapshot learning offers several ben- efits: (1) it consistently improves performance; (2) it learns discriminative and robust feature representa- tions and alleviates the vanishing gradient problem; (3) it appears to learn transparent and interpretable subspaces of the conditioning vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine learning approaches to task-oriented di- alogue system design have cast the problem as a partially observable Markov Decision Process (POMDP) ( <ref type="bibr" target="#b45">Young et al., 2013</ref>) with the aim of using reinforcement learning (RL) to train dia- logue policies online through interactions with real users . In order to make RL tractable, the state and action space must be care- fully designed <ref type="bibr" target="#b44">(Young et al., 2010</ref>) and the un- derstanding ( <ref type="bibr" target="#b7">Henderson et al., 2014;</ref><ref type="bibr" target="#b24">Mrkši´Mrkši´c et al., 2015</ref>) and generation <ref type="bibr" target="#b38">(Wen et al., 2015b;</ref><ref type="bibr" target="#b40">Wen et al., 2016b</ref>) modules were assumed available or trained standalone on supervised corpora. Due to the under- lying hand-coded semantic representation <ref type="bibr" target="#b35">(Traum, 1999)</ref>, the conversation is far from natural and the comprehension capability is limited. This motivates the use of neural networks to model dialogues from end to end as a conditional generation problem.</p><p>Interest in generating natural language using NNs can be attributed to the success of RNN LMs for large vocabulary speech recognition <ref type="bibr" target="#b21">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b22">Mikolov et al., 2011)</ref>. <ref type="bibr" target="#b33">Sutskever et al. (2011)</ref> showed that plausible sentences can be obtained by sampling characters one by one from the output layer of an RNN. By conditioning an LSTM on a sequence of characters, <ref type="bibr" target="#b6">Graves (2013)</ref> showed that machines can synthesise handwriting indistinguishable from that of a human. Later on, this idea has been tried in several research fields, for example, generating image captions by condi- tioning an RNN on a convolutional neural network (CNN) output <ref type="bibr" target="#b12">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr">Xu et al., 2015)</ref>; translating a source to a target language by conditioning a decoder LSTM on top of an en- coder LSTM ( <ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>; or generating natural language by condition- ing on a symbolic semantic representation <ref type="bibr" target="#b38">(Wen et al., 2015b;</ref><ref type="bibr" target="#b20">Mei et al., 2016</ref>). Among all these meth- ods, attention-based mechanisms ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b8">Hermann et al., 2015;</ref><ref type="bibr">Ling et al., 2016</ref>) have been shown to be very effective improving perfor- mance using a dynamic source aggregation strategy.</p><p>To model dialogue as conditional generation, a sequence-to-sequence learning <ref type="bibr" target="#b34">(Sutskever et al., 2014</ref>) framework has been adopted. <ref type="bibr" target="#b36">Vinyals and Le (2015)</ref> trained the same model on several conversa- tion datasets and showed that the model can gener- ate plausible conversations. However, <ref type="bibr" target="#b27">Serban et al. (2015b)</ref> discovered that the majority of the gener- ated responses are generic due to the maximum like- lihood criterion, which was latter addressed by <ref type="bibr" target="#b15">Li et al. (2016a)</ref> using a maximum mutual information decoding strategy. Furthermore, the lack of a con- sistent system persona was also studied in <ref type="bibr" target="#b16">Li et al. (2016b)</ref>. Despite its demonstrated potential, a ma- jor barrier for this line of research is data collection. Many works ( <ref type="bibr" target="#b19">Lowe et al., 2015;</ref><ref type="bibr" target="#b26">Serban et al., 2015a;</ref><ref type="bibr" target="#b4">Dodge et al., 2016</ref>) have investigated conversation datasets for developing chat bot or QA-like general purpose conversation agents. However, collecting data to develop goal oriented dialogue systems that can help users to complete a task in a specific do- main remains difficult. In a recent work by <ref type="bibr" target="#b39">Wen et al. (2016a)</ref>, this problem was addressed by design- ing an online, parallel version of Wizard-of-Oz data collection <ref type="bibr" target="#b13">(Kelley, 1984)</ref> which allows large scale and cheap in-domain conversation data to be col- lected using Amazon Mechanical Turk. An NN- based dialogue model was also proposed to learn from the collected dataset and was shown to be able to assist human subjects to complete specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2154</head><p>Snapshot learning can be viewed as a special form of weak supervision (also known as distant-or self supervision) <ref type="bibr" target="#b3">(Craven and Kumlien, 1999;</ref><ref type="bibr" target="#b29">Snow et al., 2004</ref>), in which supervision signals are heuristi- cally labelled by matching unlabelled corpora with entities or attributes in a structured database. It has been widely applied to relation extraction ( <ref type="bibr" target="#b23">Mintz et al., 2009</ref>) and information extraction <ref type="bibr" target="#b11">(Hoffmann et al., 2011</ref>) in which facts from a knowledge base (e.g. Freebase) were used as objectives to train classifiers. Recently, self supervision was also used in mem- ory networks ( <ref type="bibr" target="#b9">Hill et al., 2016</ref>) to improve the dis- criminative power of memory attention. Conceptu- ally, snapshot learning is related to curriculum learn- ing ( <ref type="bibr" target="#b1">Bengio et al., 2009</ref>). Instead of learning eas- ier examples before difficult ones, snapshot learning creates an easier target for each example. In prac- tice, snapshot learning is similar to deeply super- vised nets ( <ref type="bibr" target="#b14">Lee et al., 2015</ref>) in which companion ob- jectives are generated from intermediary layers and optimised altogether with the output objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Dialogue System</head><p>The testbed for this work is a neural network-based task-oriented dialogue system proposed by <ref type="bibr" target="#b39">Wen et al. (2016a)</ref>. The model casts dialogue as a source to target sequence transduction problem (modelled by a sequence-to-sequence architecture <ref type="bibr" target="#b34">(Sutskever et al., 2014)</ref>) augmented with the dialogue his- tory (modelled by a belief tracker ( <ref type="bibr" target="#b7">Henderson et al., 2014)</ref>) and the current database search outcome (modelled by a database operator). The model con- sists of both encoder and decoder modules. The de- tails of each module are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder Module</head><p>At each turn t, the goal of the encoder is to produce a distributed representation of the system action m t , which is then used to condition a decoder to gen- erate the next system response in skeletal form <ref type="bibr">1</ref> . It consists of four submodules: intent network, belief tracker, database operator, and policy network. Intent Network The intent network takes a se- quence of tokens 1 and converts it into a sentence em- bedding representing the user intent using an LSTM network. The hidden layer of the LSTM at the last encoding step z t is taken as the representation. As mentioned in <ref type="bibr" target="#b39">Wen et al. (2016a)</ref>, this representation can be viewed as a distributed version of the speech act <ref type="bibr" target="#b35">(Traum, 1999</ref>) used in traditional systems. Belief Trackers In addition to the intent network, the neural dialogue system uses a set of slot-based belief trackers ( <ref type="bibr" target="#b7">Henderson et al., 2014;</ref><ref type="bibr" target="#b24">Mrkši´Mrkši´c et al., 2015</ref>) to track user requests. By taking each user in- put as new evidence, the task of a belief tracker is to maintain a multinomial distribution p over values v ∈ V s for each informable slot 2 s, and a binary distribution for each requestable slot 2 . These prob- ability distributions p s t are called belief states of the system. The belief states p s t , together with the intent vector z t , can be viewed as the system's comprehen- sion of the user requests up to turn t. Database Operator Based on the belief states p s t , a DB query is formed by taking the union of the maximum values of each informable slot. A vector x t representing different degrees of matching in the DB (no match, 1 match, ... or more than 5 matches) is produced by counting the number of matched enti- ties and expressing it as a 6-bin 1-hot encoding. If x t is not zero, an associated entity pointer is maintained which identifies one of the matching DB entities se- lected at random. The entity pointer is updated if the current entity no longer matches the search criteria; otherwise it stays the same. Policy Network Based on the vectors z t , p s t , and x t from the above three modules, the policy network combines them into a single action vector m t by a three-way matrix transformation,</p><formula xml:id="formula_0">m t = tanh(W zm z t + W xm x t + s∈G W s pm p s t ) (1)</formula><p>where matrices W zm , W s pm , and W xm are param- eters and G is the domain ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder Module</head><p>Conditioned on the system action vector m t pro- vided by the encoder module, the decoder mod- ule uses a conditional LSTM LM to generate the required system output token by token in skeletal form 1 . The final system response can then be formed by substituting the actual values of the database en- tries into the skeletal sentence structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Conditional Generation Network</head><p>In this paper we study and analyse three different variants of LSTM-based conditional generation ar- chitectures:</p><p>Language Model Type The most straightforward way to condition the LSTM network on additional source information is to concatenate the condition- ing vector m t together with the input word embed- ding w j and previous hidden layer h j−1 ,</p><formula xml:id="formula_1">    i j f j o j ˆ c j     =     sigmoid sigmoid sigmoid tanh     W 4n,3n   m t w j h j−1   c j = f j c j−1 + i j ˆ c j h j = o j tanh(c j )</formula><p>where index j is the generation step, n is the hidden layer size, i j , f j , o j ∈ [0, 1] n are input, forget, and output gates respectively, ˆ c j and c j are proposed cell value and true cell value at step j, and W 4n,3n are the model parameters. The model is shown in <ref type="figure" target="#fig_0">Fig- ure 1a</ref>. Since it does not differ significantly from the original LSTM, we call it the language model type (lm) conditional generation network. Memory Type The memory type (mem) condi- tional generation network was introduced by <ref type="bibr" target="#b38">Wen et al. (2015b)</ref>, shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, in which the condi- tioning vector m t is governed by a standalone read- ing gate r j . This reading gate decides how much in- formation should be read from the conditioning vec- tor and directly writes it into the memory cell c j ,</p><formula xml:id="formula_2">    i j f j o j r j     =     sigmoid sigmoid sigmoid sigmoid     W 4n,3n   m t w j h j−1   ˆ c j = tanh W c (w j ⊕ h j−1 ) c j = f j c j−1 + i j ˆ c j + r j m t h j = o j tanh(c j )</formula><p>where W c is another weight matrix to learn. The idea behind this is that the model isolates the con- ditioning vector from the LM so that the model has more flexibility to learn to trade off between the two. Hybrid Type Continuing with the same idea as the memory type network, a complete separation of con- ditioning vector and LM (except for the gate con- trolling the signals) is provided by the hybrid type network shown in <ref type="figure" target="#fig_0">Figure 1c</ref>,</p><formula xml:id="formula_3">    i j f j o j r j     =     sigmoid sigmoid sigmoid sigmoid     W 4n,3n   m t w j h j−1   ˆ c j = tanh W c (w j ⊕ h j−1 ) c j = f j c j−1 + i j ˆ c j h j = o j tanh(c j ) + r j m t</formula><p>This model was motivated by the fact that long-term dependency is not needed for the conditioning vec- tor because we apply this information at every step j anyway. The decoupling of the conditioning vector and the LM is attractive because it leads to better in- terpretability of the results and provides the potential to learn a better conditioning vector and LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Attention and Belief Representation</head><p>Attention An attention-based mechanism provides an effective approach for aggregating multiple infor- mation sources for prediction tasks. Like Wen et al.</p><p>(2016a), we explore the use of an attention mecha- nism to combine the tracker belief states in which the policy network in Equation 1 is modified as</p><formula xml:id="formula_4">m j t = tanh(W zm z t + W xm x t + s∈G α j s W s pm p s t )</formula><p>where the attention weights α j s are calculated by,</p><formula xml:id="formula_5">α j s = softmax r tanh W r · (v t ⊕ p s t ⊕ w t j ⊕ h t j−1 )</formula><p>where v t = z t + x t and matrix W r and vector r are parameters to learn. Belief Representation The effect of different be- lief state representations on the end performance are also studied. For user informable slots, the full belief state p s t is the original state containing all categori- cal values; the summary belief state contains only three components: the summed value of all categor- ical probabilities, the probability that the user said they "don't care" about this slot and the probabil- ity that the slot has not been mentioned. For user requestable slots, on the other hand, the full belief state is the same as the summary belief state because the slot values are binary rather than categorical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Snapshot Learning</head><p>Learning conditional generation models from se- quential supervision signals can be difficult, because it requires the model to learn both long-term word dependencies and potentially distant source encod- ing functions. To mitigate this difficulty, we in- troduce a novel method called snapshot learning to create a vector of binary labels</p><formula xml:id="formula_6">Υ j t ∈ [0, 1] d , d &lt; dim(m j t )</formula><p>as the snapshot of the remaining part of the output sentence T t,j:|Tt| from generation step j. Each element of the snapshot vector is an indica- tor function of a certain event that will happen in the future, which can be obtained either from the sys- tem response or dialogue context at training time. A companion cross entropy error is then computed to force a subset of the conditioning vectorˆmvectorˆ vectorˆm j t ⊂ m j t to be close to the snapshot vector,</p><formula xml:id="formula_7">L ss (·) = − t j E[H(Υ j t , ˆ m j t )]<label>(2)</label></formula><p>where H(·) is the cross entropy function, Υ j t andˆmandˆ andˆm j t are elements of vectors Υ j t andˆmandˆ andˆm j t , respectively. In order to make the tanh activations ofˆmofˆ ofˆm j t compat- ible with the 0-1 snapshot labels, we squeeze each <ref type="figure">Figure 2</ref>: The idea of snapshot learning. The snap- shot vector was trained with additional supervisions on a set of indicator functions heuristically labelled using the system response.</p><p>value ofˆmofˆ ofˆm j t by adding 1 and dividing by 2 before computing the cost.</p><p>The indicator functions we use in this work have two forms: (1) whether a particular slot value (e.g., [v.food] 1 ) is going to occur, and (2) whether the sys- tem has offered a venue 3 , as shown in <ref type="figure">Figure 2</ref>. The offer label in the snapshot is produced by checking the delexicalised name token ( <ref type="bibr">[v.name]</ref>) in the en- tire dialogue. If it has occurred, every label in sub- sequent turns is labelled with 1. Otherwise it is la- belled with 0. To create snapshot targets for a partic- ular slot value, the output sentence is matched with the corresponding delexicalised token turn by turn, per generation step. At each generation step, the tar- get is labelled with 0 if that delexicalised token has been generated; otherwise it is set to 1. However, for the models without attention, the targets per turn are set to the same because the condition vector will not be able to learn the dynamically changing behaviour without attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset The dataset used in this work was col- lected in the Wizard-of-Oz online data collection de- scribed by <ref type="bibr" target="#b39">Wen et al. (2016a)</ref>, in which the task of the system is to assist users to find a restaurant in Cambridge, UK area. There are three informable slots (food, pricerange, area) that users can use to constrain the search and six requestable slots (ad- dress, phone, postcode plus the three informable Architecture Belief Success(%) SlotMatch(%) T5-BLEU T1-BLEU   slots) that the user can ask a value for once a restau- rant has been offered. There are 676 dialogues in the dataset (including both finished and unfinished dia- logues) and approximately 2750 turns in total. The database contains 99 unique restaurants. Training The training procedure was divided into two stages. Firstly, the belief tracker parameters θ b were pre-trained using cross entropy errors be- tween tracker labels and predictions. Having fixed the tracker parameters, the remaining parts of the model θ \b are trained using the cross entropy errors from the generation network LM,</p><formula xml:id="formula_8">L(θ \b ) = − t j H(y t j , p t j ) + λL ss (·)<label>(3)</label></formula><p>where y t j and p t j are output token targets and predic- tions respectively, at turn t of output step j, L ss (·) is the snapshot cost from Equation 2, and λ is the tradeoff parameter in which we set to 1 for all mod- els trained with snapshot learning. We treated each dialogue as a batch and used stochastic gradient de- scent with a small l2 regularisation term to train the model. The collected corpus was partitioned into a training, validation, and testing sets in the ratio 3:1:1. Early stopping was implemented based on the validation set considering only LM log-likelihoods. Gradient clipping was set to 1. The hidden layer sizes were set to 50, and the weights were randomly initialised between -0.3 and 0.3 including word em- beddings. The vocabulary size is around 500 for both input and output, in which rare words and words that can be delexicalised have been removed.</p><p>Decoding In order to compare models trained with different recipes rather than decoding strategies, we decode all the trained models with the average log probability of tokens in the sentence. We applied beam search with a beamwidth equal to 10, the search stops when an end-of-sentence token is gen- erated. In order to consider language variability, we ran decoding until 5 candidates were obtained and performed evaluation on them.</p><p>Metrics We compared models trained with differ- ent recipes by performing a corpus-based evaluation in which the model is used to predict each system response in the held-out test set. Three evaluation metrics were used: BLEU score (on top-1 and top- 5 candidates) ( <ref type="bibr" target="#b25">Papineni et al., 2002</ref>), slot matching rate and objective task success rate ( . The dialogue is marked as successful if both: (1) the offered entity matches the task that was speci- fied to the user, and (2) the system answered all the associated information requests (e.g. what is the ad- dress?) from the user. The slot matching rate is the percentage of delexicalised tokens (e.g. reference. We computed the BLEU scores on the skeletal sentence forms before substituting with the actual entity values. All the results were averaged over 10 random initialised networks. Results <ref type="table">Table 1</ref> shows the evaluation results. The numbers to the left and right of each table cell are the same model trained w/o and w/ snapshot learning. The first observation is that snapshot learning con- sistently improves on most metrics regardless of the model architecture. This is especially true for BLEU scores. We think this may be attributed to the more discriminative conditioning vector learned through the snapshot method, which makes the learning of the conditional LM easier.</p><p>In the first block belief state representation, we compare the effect of two different belief represen- tations. As can be seen, using a succinct represen- tation is better (summary&gt;full) because the iden- tity of each categorical value in the belief state does not help when the generation decisions are done in skeletal form. In fact, the full belief state representa- tion may encourage the model to learn incorrect co- adaptation among features when the data is scarce.</p><p>In the conditional architecture block, we com- pare the three different conditional generation archi- tectures as described in section 3.2.1. This result shows that the language model type (lm) and mem- ory type (mem) networks perform better in terms of BLEU score and slot matching rate, while the hybrid type (hybrid) networks achieve higher task success. This is probably due to the degree of separation be-  tween the LM and conditioning vector: a coupling approach (lm, mem) sacrifices the conditioning vec- tor but learns a better LM and higher BLEU; while a complete separation (hybrid) learns a better condi- tioning vector and offers a higher task success. Lastly, in the attention-based model block we train the three architectures with the attention mech- anism and compare them again. Firstly, the char- acteristics of the three models we observed above also hold for attention-based models. Secondly, we found that the attention mechanism improves all the three architectures on task success rate but not BLEU scores. This is probably due to the limita- tions of using n-gram based metrics like BLEU to evaluate the generation quality <ref type="bibr" target="#b30">(Stent et al., 2005</ref>).</p><formula xml:id="formula_9">Model i j f j r j /o j hybrid, full</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Analysis</head><p>Gate Activations We first studied the average ac- tivation of each individual gate in the models by av- eraging them when running generation on the test set. We analysed the hybrid models because their reading gate to output gate activation ratio (r j /o j ) shows clear tradeoff between the LM and the con- ditioning vector components. As can be seen in Ta- ble 2, we found that the average forget gate activa- tions (f j ) and the ratio of the reading gate to the out- put gate activation (r j /o j ) have strong correlations to performance: a better performance (row 3&gt;row 2&gt;row 1) seems to come from models that can learn a longer word dependency (higher forget gate f t ac- tivations) and a better conditioning vector (therefore higher reading to output gate ratio r j /o j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned Attention</head><p>We have visualised the learned attention heat map of models trained with and without snapshot learning in <ref type="figure">Figure 3</ref>. The at- tention is on both the informable slot trackers (first three columns) and the requestable slot trackers (the other columns). We found that the model trained with snapshot learning <ref type="figure">(Figure 3b</ref>) seems to pro- duce a more accurate and discriminative attention heat map comparing to the one trained without it <ref type="figure">(Figure 3a)</ref>. This may contribute to the better perfor- mance achieved by the snapshot learning approach.</p><p>Snapshot Neurons As mentioned earlier, snap- shot learning forces a subspace of the condition- ing vectorˆmvectorˆ vectorˆm j t to become discriminative and in- terpretable. Three example generated sentences together with the snapshot neuron activations are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. As can be seen, when generat- ing words one by one, the neuron activations were changing to detect different events they were as- signed by the snapshot training signals: e.g. in <ref type="figure" target="#fig_4">Fig- ure 4b</ref> the light blue and orange neurons switched their domination role when the token [v.address] was generated; the offered neuron is in a high ac- tivation state in <ref type="figure" target="#fig_4">Figure 4b</ref> because the system was offering a venue, while in <ref type="figure" target="#fig_4">Figure 4a</ref> it is not acti- vated because the system was still helping the user to find a venue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper has investigated different conditional generation architectures and a novel method called snapshot learning to improve response generation in a neural dialogue system framework. The results showed three major findings. Firstly, although the hybrid type model did not rank highest on all met- rics, it is nevertheless preferred because it achieved the highest task success and also it provided more in- terpretable results. Secondly, snapshot learning pro- vided gains on virtually all metrics regardless of the architecture used. The analysis suggested that the benefit of snapshot learning mainly comes from the more discriminative and robust subspace represen- tation learned from the heuristically labelled com- panion signals, which in turn facilitates optimisation of the final target objective. Lastly, the results sug- gested that by making a complex system more inter- pretable at different levels not only helps our under- standing but also leads to the highest success rates.</p><p>However, there is still much work left to do. This work focused on conditional generation architec- tures and snapshot learning in the scenario of gen- erating dialogue responses. It would be very help- ful if the same comparison could be conducted in other application domains such as machine transla- tion or image caption generation so that a wider view of the effectiveness of these approaches can be as- sessed. Furthermore, removing slot-value delexical- isation and learning confirmation behaviour in noisy speech conditions are also main research problems from the system development prospective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three different conditional generation architectures.</figDesc><graphic url="image-2.png" coords="4,237.97,61.31,127.55,110.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>Performance comparison of different model architectures, belief state representations, and snapshot learning. The numbers to the left and right of the / sign are learning without and with snapshot, respectively. The model with the best performance on a particular metric (column) is shown in bold face. The lm models in Conditional architecture and Attention-based model are the same models as in Wen et al. (2016a). Statistical significance was computed using two-tailed Wilcoxon Signed-Rank Test (* p &lt;0.05) to compare models w/ and w/o snapshot learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Learned attention heat maps over trackers. The first three columns in each figure are informable slot trackers and the rest are requestable slot trackers. The generation model is the hybrid type LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three example responses generated from the hybrid model trained with snapshot and attention. Each line represents a neuron that detects a particular snapshot event.</figDesc><graphic url="image-9.png" coords="8,123.22,308.47,362.82,115.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Average activation of gates on test set.</figDesc><table></table></figure>

			<note place="foot" n="1"> Delexicalisation: slots and values are replaced by generic tokens (e.g. keywords like Chinese food are replaced by [v.food] [s.food] to allow weight sharing.</note>

			<note place="foot" n="2"> Informable slots are slots that users can use to constrain the search, such as food type or price range; Requestable slots are slots that users can ask a value for, such as phone number. This information is specified in the domain ontology.</note>

			<note place="foot" n="3"> Details of the specific application used in this study are given in Section 4 below.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Tsung-Hsien Wen and David Vandyke are supported by Toshiba Research Europe Ltd, Cambridge Re-search Laboratory.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">Curriculum learning. In ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Evaluating prerequisite qualities for learning end-to-end dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On-line policy optimisation of bayesian spoken dialogue systems via human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<meeting><address><addrLine>Blaise Thomson</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Pirros Tsiakoulis, and Steve Young</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>arXiv preprint:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Word-based dialog state tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In SIGdial</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sumit Chopra, and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An iterative design methodology for user-friendly natural language office information applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transaction on Information Systems</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno>arXiv perprint:1603.06155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<imprint>
			<pubPlace>Andrew Senior, Fumin Wang, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>arXiv preprint:1603.06744</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGdial</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>InterSpeech</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">H</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-domain Dialog State Tracking using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A survey of available corpora for building data-driven dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno>arXiv preprint:1512.05742</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical neural network generative models for movie dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating evaluation methods for generation in the presence of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Singhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Foundations of Rational Agency, chapter Speech Acts for Dialogue Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGdial</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A network-based end-to-end trainable taskoriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno>arXiv preprint:1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multidomain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural enquirer: Learning to query tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The hidden information state model: A practical framework for pomdp-based spoken dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Proceedings of IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
