<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Unisound Information Technology Co</orgName>
								<address>
									<postCode>100028</postCode>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="182" to="192"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>182</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chi-nese NER task and Chinese word segmen-tation (CWS) task have many similar word boundaries. There are also specificities in each task. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used dataset-s show that our proposed model significantly and consistently outperforms other state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of named entity recognition (NER) is to recognize the named entities in given text. N- ER is a preliminary and important task in natural language processing (NLP) area and can be used in many downstream NLP tasks, such as relation extraction ( <ref type="bibr" target="#b2">Bunescu and Mooney, 2005</ref>), even- t extraction <ref type="bibr" target="#b6">(Chen et al., 2015)</ref> and question an- swering ( <ref type="bibr" target="#b34">Yao and Van Durme, 2014</ref>). In recent years, numerous methods have been carefully s- tudied for NER task, including Hidden Markov Models (HMMs) ( <ref type="bibr" target="#b0">Bikel et al., 1997)</ref>, Support Vec- tor Machines (SVMs) ( <ref type="bibr" target="#b18">Isozaki and Kazawa, 2002)</ref> and Conditional Random Fields (CRFs) <ref type="bibr" target="#b21">(Lafferty et al., 2001</ref>). Currently, with the development of deep learning, neural networks ( <ref type="bibr" target="#b22">Lample et al., 2016;</ref><ref type="bibr" target="#b28">Peng and Dredze, 2016;</ref><ref type="bibr" target="#b25">Luo and Yang, 2016)</ref> have been introduced to NER task. All these methods need to determine entities boundaries and classify them into pre-defined categories.</p><p>Although great improvements have been achieved by these methods on Chinese NER task, some issues still have not been well addressed. One significant drawback is that there is only a very small amount of annotated data available. Weibo NER dataset ( <ref type="bibr" target="#b27">Peng and Dredze, 2015;</ref><ref type="bibr" target="#b14">He and Sun, 2017a</ref>) and Sighan2006 NER dataset <ref type="bibr" target="#b23">(Levow, 2006</ref>) are two widely used datasets for Chinese NER task, containing 1.3k and 45k training examples, respectively. On the two datasets, the highest F1 scores are 48.41% and 89.21%, respectively. As a basic task in NLP area, the performance is not satisfactory. Fortunately, Chinese word segmentation (CWS) task is to recognize word boundaries and the amount of supervised training data for CWS is abundant compared with NER. There are many similarities between Chinese NER task and CWS task, which we call task-shared information. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, given a sentence "•»¯•: : (Hilton leaves Houston Airport)", the two tasks have the same boundaries for some words such as "• (Hilton)" and "» (leaves)", while Chinese NER has more coarse-grained boundaries than CWS task for certain word such as "¯• :: (Houston Airport)" in the example of <ref type="figure" target="#fig_0">Figure  1</ref>, which we call task-specific information. In order to incorporate word boundary information from CWS task into NER task, <ref type="bibr" target="#b28">Peng and Dredze (2016)</ref> propose a joint model that performs Chinese NER with CWS task. However, their proposed model only focuses on task-shared information between Chinese NER and CWS, and ignores filtering the specificities of each task, which will bring noise for both of the tasks. For example, the CWS task splits "¯•:: (Houston Airport)" into "¯• (Houston)" and ":: (Airport)", while the NER task takes " ¯•:: (Houston Airport)" as a whole entity. Thus, how to exploit task-shared information and prevent the noise brought by CWS task to Chinese NER task is a challenging problem.</p><p>Another issue is that most proposed models cannot explicitly model long range dependencies when predicting entity type. Though bidirection- al long short term memory (BiLSTM) can learn long-distance dependencies, it cannot conduct di- rect connections between arbitrary two characters. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, if the model only focuses on the word "• (Hilton)", it can be a person or organization. However, when the model explic- itly captures the dependencies between "• (Hilton)" and "» (leaves)", it is easy to classify "• (Hilton)" into "person" category. Con- text information is very crucial for determining the entity type. While in the sentence "O( • (I will be staying at the Hilton)", the entity type of "• (Hilton)" is "organization". Thus, how to better capture the global dependencies of the w- hole sentence is another challenging problem.</p><p>To address the above problems, we propose an adversarial transfer learning framework to inte- grate the task-shared word boundary information into Chinese NER task in this paper. The adver- sarial transfer learning is incorporating adversari- al training into transfer learning. To better capture long range dependencies and synthesize the infor- mation of the sentence, we extend self-attention mechanism into the framework. Specifically, we try to improve Chinese NER task performance by incorporating shared boundary information from CWS task. To prevent the specific information of CWS task from lowering the performance of the Chinese NER task, we introduce adversarial training to ensure that the Chinese NER task on- ly exploits task-shared word boundary informa- tion. Then, for tackling the long range dependen- cy problems, we utilize self-attention to synthe- size the hidden representation of BiLSTM. Final- ly, we evaluate our model on two different widely used Chinese NER datasets. Experimental results show that our proposed model achieves better per- formance than other state-of-the-art methods and gains new benchmarks.</p><p>In summary, the contributions of this paper are as follows:</p><p>• We propose an adversarial transfer learning framework to incorporate task-shared word boundary information from CWS task into Chinese NER task. To our best knowledge, it is the first work to apply adversarial trans- fer learning method into NER task.</p><p>• We introduce self-attention mechanism into our model, which aims to capture the global dependencies of the whole sentence and learn inner structure features of sentence.</p><p>• We conduct our experiment on two dif- ferent widely used Chinese NER datasets, and the experimental results demonstrate that our proposed model significantly and consis- tently outperforms previous state-of-the-art methods. We release the source code publicly for further research 1 .</p><p>2 Related Work NER Many methods have been proposed for N- ER task. Early studies on NER often exploit SVMs ( <ref type="bibr" target="#b18">Isozaki and Kazawa, 2002)</ref>  NER, which are jointly trained with CWS task. However, the specific features brought by CWS task can lower the performance of the Chinese N- ER task. Adversarial Training Adversarial networks have achieved great success in computer vision ( <ref type="bibr" target="#b12">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b8">Denton et al., 2015)</ref>. In NLP area, adversarial training has been intro- duced for domain adaptation <ref type="bibr" target="#b10">(Ganin and Lempitsky, 2014;</ref><ref type="bibr" target="#b13">Gui et al., 2017)</ref>, cross-lingual transfer learning ( <ref type="bibr" target="#b4">Chen et al., 2016;</ref><ref type="bibr" target="#b19">Kim et al., 2017)</ref>, multi-task learning <ref type="bibr" target="#b24">Liu et al., 2017</ref>) and crowdsourcing learning ( <ref type="bibr" target="#b33">Yang et al., 2018)</ref>. <ref type="bibr" target="#b1">Bousmalis et al. (2016)</ref> pro- pose shared-private model in domain separation network. Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task. To our knowledge, it is the first work to apply adver- sarial transfer learning framework to Chinese NER task.</p><p>Self-Attention Self-attention has been intro- duced to machine translation by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> for capturing global dependencies between input and output and achieves state-of-the-art per- formance. For language understanding task, <ref type="bibr" target="#b29">Shen et al. (2017)</ref> exploit self-attention to learn long range dependencies. <ref type="bibr" target="#b30">Tan et al. (2017)</ref> apply self-attention to semantic role labelling task and achieve state-of-the-art results. We are the first to introduce self-attention mechanism to Chinese N- ER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this paper, we propose a novel adversarial trans- fer learning framework that will learn task-shared word boundary information from CWS task, filter specific information of CWS and explicitly cap- ture the long range dependencies between arbi- trary two characters in sentence. The architecture of our proposed model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The model mainly consists of five components: embedding layer, shared-private feature extractor, self-attention, task-specific CRF and task discrim- inator. In the following sections, we will describe each part of our proposed model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>Similar to other neural network models, the first step of our proposed model is to map discrete characters into the distributed representations. For a given Chinese sentence x = {c 1 , c 2 , . . . , c N } from Chinese NER dataset or CWS dataset, we lookup embedding vector from pre-trained embed- ding matrix for each character c i as x i ∈ R de .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shared-Private Feature Extractor</head><p>Long short term memory (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997</ref>) is a variant of recurrent neu- ral network (RNN) <ref type="bibr" target="#b9">(Elman, 1990)</ref>, which enables to address the gradient vanishing and exploding problems in RNN via introducing gate mechanism and memory cell. The unidirectional LSTM on- ly leverages information from the past, ignoring the future information. In order to incorporate in- formation from both sides of sequence, we adopt BiLSTM to extract features. Specially, the hidden state of BiLSTM could be expressed as follows:</p><formula xml:id="formula_0">− → h i = − −−− → LSTM( − → h i−1 , x i ) (1) ← − h i = ← −−− − LSTM( ← − h i+1 , x i )<label>(2)</label></formula><formula xml:id="formula_1">h i = − → h i ⊕ ← − h i<label>(3)</label></formula><p>where</p><formula xml:id="formula_2">− → h i ∈ R d h and ← − h i ∈ R d h</formula><p>are the hidden states of the forward and backward LSTM at po- sition i, respectively. ⊕ denotes concatenation op- eration.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we propose a shared- private feature extractor, which assigns a private BiLSTM layer and shared BiLSTM layer for task k ∈ {N ER, CW S}. The private BiLSTM lay- er is used to extract task-specific features, and the shared BiLSTM layer is used to learn task-shared word boundaries. Formally, for any sentence in dataset of task k, the hidden states of shared and private BiLSTM layer can be computed as fol- lows:</p><formula xml:id="formula_3">s k i = BiLSTM(x k i , s k i−1 ; θ s )<label>(4)</label></formula><formula xml:id="formula_4">h k i = BiLSTM(x k i , h k i−1 ; θ k )<label>(5)</label></formula><p>where θ s and θ k are the shared BiLSTM param- eters and private BiLSTM parameters of task k, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Attention</head><p>Inspired by the self-attention applied to machine translation ( <ref type="bibr" target="#b31">Vaswani et al., 2017</ref>) and semantic role labelling (Tan et al., 2017), we exploit self- attention to explicitly learn the dependencies be- tween any two characters in sentence and capture the inner structure information of sentence. In this paper, we adopt the multi-head self-attention mechanism. H = {h 1 , h 2 , . . . , h N } denotes the output of private BiLSTM. Correspondingly, S = {s 1 , s 2 , . . . , s N } is the output of shared BiLSTM. We will take the self-attention in private space as example to illustrate how it works. The scaled dot- product attention can be precisely described as fol- lows:</p><formula xml:id="formula_5">Attention(Q, K, V) = softmax( QK T √ d )V (6)</formula><p>where Q ∈ R N ×2d h , K ∈ R N ×2d h and V ∈ R N ×2d h are query matrix, keys matrix and value matrix, respectively. In our setting, Q = K = V = H. d is the dimension of hidden units of BiL- STM, which equals to 2d h . Multi-head attention first linearly projects the queries, keys and values h times by using differ- ent linear projections. Then h projections perfor- m the scaled dot-product attention in parallel. Fi- nally, these results of attention are concatenated and once again projected to get the new represen- tation. Formally, the multi-head attention can be expressed as follows:</p><formula xml:id="formula_6">head i = Attention(QW Q i , KW K i , VW V i ) (7) H = (head i ⊕ . . . ⊕ head h )W o<label>(8)</label></formula><p>where</p><formula xml:id="formula_7">W Q i ∈ R 2d h ×d k , W K i ∈ R 2d h ×d k and W V i ∈ R 2d h ×d k are trainable projection parame- ters and d k = 2d h /h. W o ∈ R 2d h ×2d h is also trainable parameter.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task-Specific CRF</head><p>For a sentence in dataset of task k, we compute the final representation via concatenating the rep- resentations from private space and shared space after self-attention layer:</p><formula xml:id="formula_8">H k = H k ⊕ S k<label>(9)</label></formula><p>where H k and S k are the outputs of private self- attention and shared self-attention of task k, re- spectively.</p><p>Considering the dependencies between succes- sive labels, we exploit CRF ( <ref type="bibr" target="#b21">Lafferty et al., 2001</ref>) to inference tags instead of making tagging de- cisions using h i independently. Due to the d- ifference of labels, we introduce a specific CR- F layer for each task. Given a sentence x = {c 1 , c 2 , . . . , c N } with a predicted tag sequence y = {y 1 , y 2 , . . . , y N }, the CRF tagging process can be formalized as follows:</p><formula xml:id="formula_9">o i = W s h i + b s (10) s(x, y) = N i=1 (o i,y i + T y i−1 ,y i )<label>(11)</label></formula><p>¯ y = arg max</p><formula xml:id="formula_10">y∈Yx s(x, y)<label>(12)</label></formula><p>where W s ∈ R |T |×4d h and b s ∈ R |T | are train- able parameters. |T | denotes the number of output labels. o i,y i represents the score of the y i -th tag of the character c i . T is a transition score matrix which defines the scores of two successive label- s. Y x represents all candidate tag sequences for given sentence x. In decoding, we use Viterbi al- gorithm to get the predicted tag sequence ¯ y. For training, we exploit negative log-likelihood objective as the loss function. The probability of the ground-truth label sequence is computed by:</p><formula xml:id="formula_11">p(ˆ y|x) = e s(x,ˆ y) y∈Yx e s(x, y)<label>(13)</label></formula><p>wherê y denotes the ground-truth label sequence. Given T training examples (x (i) ; ˆ y (i) ), the loss function L T ask can be defined as follows:</p><formula xml:id="formula_12">L T ask = − T i=1 logp(ˆ y (i) |x (i) )<label>(14)</label></formula><p>We use gradient back-propagation method to min- imize the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Task Discriminator</head><p>Inspired by adversarial networks (Goodfellow et al., 2014), we incorporate adversarial training into shared space to guarantee that specific fea- tures of tasks do not exist in shared space. We pro- pose a task discriminator to estimate which task the sentence comes from. Formally, the task dis- criminator can be expressed as follows:</p><formula xml:id="formula_13">s k = Maxpooling(S k )<label>(15)</label></formula><formula xml:id="formula_14">D(s k ; θ d ) = softmax(W d s k + b d )<label>(16)</label></formula><p>where θ d indicates the parameters of task discrim- inator. W d ∈ R K×2d h and b d ∈ R K are trainable parameters. K is the number of tasks. Besides the task loss L T ask , we introduce an ad- versarial loss L Adv to prevent specific features of CWS task from creeping into shared space. The adversarial loss trains the shared model to produce shared features such that the task discriminator cannot reliably recognize which task the sentence comes from. The adversarial loss can be computed as follows:</p><formula xml:id="formula_15">L Adv = min θs (max θ d K k=1 T k i=1 logD(E s (x (i) k )))<label>(17)</label></formula><p>where θ s denotes the trainable parameters of shared BiLSTM. E s denotes the shared feature ex- tractor. T k is the number of training examples of task k. x <ref type="bibr">(i)</ref> k is the i-th example of task k. There is a minimax optimization that the shared BiLST- M generates a representation to mislead the task discriminator and the discriminator tries its best to correctly determine the type of task.</p><p>We add a gradient reversal layer ( <ref type="bibr" target="#b10">Ganin and Lempitsky, 2014</ref>) below the softmax layer to ad- dress the minimax optimization problem. In the training phrase, we minimize the task discrimi- nator errors, and through gradient reversal layer the gradients will become opposed sign to adver- sarially encourage the shared feature extractor to learn task-shared word boundary information. Af- ter training phrase, the shared feature extractor and task discriminator reach a point where the discrim- inator cannot differentiate the tasks according to the representations learned from shared feature ex- tractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training</head><p>The final loss function of our proposed model can be written as follows:</p><formula xml:id="formula_16">L = L N ER · I(x) + L CW S · (1 − I(x)) + λL Adv<label>(18)</label></formula><p>where λ is a hyper-parameter. L N ER and L CW S can be computed via Eq.14. I(x) is a switching function to identify which task the input comes from. It is defined as follows:</p><formula xml:id="formula_17">I(x) = 1, if x ∈ D N ER 0, if x ∈ D CW S<label>(19)</label></formula><p>where D N ER and D CW S are Chinese NER train- ing corpora and CWS training corpora, respective- ly.</p><p>In the training phrase, at each iteration, we first select a task from {N ER, CW S} in turn. Then, we sample a batch of training instances from the given task to update the parameters. We use Adam ( <ref type="bibr" target="#b20">Kingma and Ba, 2014</ref>) algorithm to optimize the final loss function. Since Chinese NER task and CWS task may have different convergence rate, we repeat the above iterations until early stopping ac- cording to the Chinese NER task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate our proposed model on Chinese N- ER, we experiment on two different widely used datasets, including Weibo NER dataset (Wei- boNER) <ref type="bibr" target="#b27">(Peng and Dredze, 2015</ref>  NER results for named entities on the original WeiboNER dataset ( <ref type="bibr" target="#b27">Peng and Dredze, 2015)</ref>. There are three blocks. The first two blocks contain the main and simplified models proposed by <ref type="bibr" target="#b27">Peng and Dredze (2015)</ref> and <ref type="bibr" target="#b28">Peng and Dredze (2016)</ref>, respectively. The last block lists the performance of our proposed model. 2017a) and SIGHAN2006 NER dataset (Sighan- NER) <ref type="bibr" target="#b23">(Levow, 2006</ref>). We use the MSR dataset (from SIGHAN2005) for CWS task. The WeiboNER is annotated with four enti- ty types (person, location, organization and geo- political entities), including named entities and nominal mentions. The SighanNER is simplified Chinese, which contains three entity types (per- son, location and organization). For WeiboNER, we use the same training, development and test- ing splits as <ref type="bibr" target="#b27">Peng and Dredze (2015)</ref>. Since the SighanNER does not have development set, we sample 10% data of training set as development set. We use MSR dataset to improve the perfor- mance of the Chinese NER task. <ref type="table">Table 1</ref> gives the details of the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>For evaluation, we use the Precision (P), Recall (R) and F1 score as metrics in our experiment.</p><p>For hyper-parameter configurations, we adjust them according to the performance on develop- ment set of Chinese NER task. We set the charac- ter embedding size d e to 100. The dimensionality of LSTM hidden states d h is 120. The initial learn- ing rate is set to 0.001. The loss weight coefficient λ is set to 0.06. We set the dropout rate to 0.3.</p><p>The number of projections h is 8. We set the batch size of SighanNER and WeiboNER as 64 and 20, respectively.</p><p>For trainable parameters initialization, we use xavier initializer <ref type="bibr" target="#b11">(Glorot and Bengio, 2010)</ref> to initialize parameters. The character embeddings used in our experiment are pre-trained on Baidu Encyclopedia corpus and Weibo corpus by using word2vec toolkit ( <ref type="bibr" target="#b26">Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared with State-of-the-art Methods</head><p>In this section, we will give the experimental re- sults of our proposed model and previous state- of-the-art methods on WeiboNER dataset and SighanNER dataset, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation on WeiboNER</head><p>We compare our proposed model with the latest models on WeiboNER dataset. <ref type="table" target="#tab_2">Table 2</ref> shows the experimental results for named entities on the o- riginal WeiboNER dataset.</p><p>In the first block of <ref type="table" target="#tab_2">Table 2</ref>, we give the per- formance of the main model and baselines pro- posed by <ref type="bibr" target="#b27">Peng and Dredze (2015)</ref>. They propose a CRF-based model to jointly train the embeddings with NER task, which achieves better results than pipeline models. In addition, they consider the po-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Named Entity Nominal Mention Overall P(%) R(%) F1(%) P(%) R(%) F1(%) F1(%) <ref type="bibr" target="#b27">Peng and Dredze (2015)</ref> 74.78 39.81 51.96 71.92 53.03 61.05 56.05 <ref type="bibr" target="#b28">Peng and Dredze (2016)</ref> 66.67 47.22 55.28 74.48 54.55 62.97 58.99 <ref type="bibr" target="#b14">He and Sun (2017a)</ref> 66.93 40.67 50.60 66.46 53.57 59.32 54.82 <ref type="bibr" target="#b15">He and Sun (2017b)</ref> 61.68 48.82 54.50 74.13 53.54 62.17 58.23 BiLSTM+CRF+adv+self-attention 59.51 50.00 54.34 71.43 47.90 57. <ref type="bibr">35</ref> 58.70 <ref type="table">Table 3</ref>: Experimental results on the updated WeiboNER dataset (He and Sun, 2017a). There are two blocks. The first block is the performance of latest models. The second block reports the performance of our proposed model. With the limited length of the page, we use "adv" to denote "adversarial".  <ref type="table">Table 4</ref>: Results on SighanNER dataset. There are two blocks. The first block reports the result of previous methods. The second block gives the performance of our proposed model. sition of each character in a word to train character and position embeddings.</p><formula xml:id="formula_18">Models P(%) R(%) F1(%) Chen</formula><p>In the second block of <ref type="table" target="#tab_2">Table 2</ref>, we report the performance of the main model and baselines pro- posed by <ref type="bibr" target="#b28">Peng and Dredze (2016)</ref>. Aiming to in- corporate word boundary information into the N- ER task, they propose an integrated model that can joint training CWS task, improving the F1 score from 46.20% to 48.41% as compared with pipeline model (Pipeline Seg.Repr.+NER).</p><p>In the last block of <ref type="table" target="#tab_2">Table 2</ref>, we give the experimental result of our proposed model (BiLSTM+CRF+adversarial+self-attention). We can observe that our proposed model significant- ly outperforms other models. Compared with the model proposed by <ref type="bibr" target="#b28">Peng and Dredze (2016)</ref>, our method gains 4.67% improvement in F1 score. In- terestingly, WeiboNER dataset and MSR dataset are different domains. The WeiboNER dataset is social media domain, while the MSR dataset can be regard as news domain. The improvement of performance indicates that our proposed adver- sarial transfer learning framework may not only learn task-shared word boundary information from CWS task but also tackle the domain adaptation problem.</p><p>We also conduct an experiment on the updated WeiboNER dataset. <ref type="table">Table 3</ref> lists the performance of the latest models and our proposed model on the updated dataset. In the first block of <ref type="table">Table 3,</ref> we report the performance of the latest models. The model proposed by <ref type="bibr" target="#b27">Peng and Dredze (2015)</ref> achieves F1 score of 56.05% on overall perfor- mance. <ref type="bibr" target="#b15">He and Sun (2017b)</ref> propose an unified model for Chinese NER task to exploit the data from out-of-domain corpus and in-domain unla- belled texts. The unified model improves the F1 score from 54.82% to 58.23% compared with the model proposed by <ref type="bibr" target="#b14">He and Sun (2017a)</ref>.</p><p>In the second block of <ref type="table">Table 3</ref>, we give the re- sult of our proposed model. It can be observed that our proposed model achieves a very competi- tive performance. Compared with the latest model proposed by <ref type="bibr" target="#b15">He and Sun (2017b)</ref>, our model im- proves the F1 score from 58.23% to 58.70% on overall performance. The improvement demon- strates the effectiveness of our proposed model. <ref type="table">Table 4</ref> lists the comparisons on SighanNER dataset. We observe that our proposed model achieves new state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation on SighanNER</head><p>In the first block, we give the performance of previous methods for Chinese NER task on SighanNER dataset.  propose a character-based CRF model for Chinese NER task. <ref type="bibr" target="#b37">Zhou et al. (2006)</ref> introduce a pipeline mod- el, which first segments the text with character- level CRF model and then applies word-level CRF to tag. <ref type="bibr" target="#b25">Luo and Yang (2016)</ref> first train a word seg- menter and then use word segmentation as addi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>SighanNER WeiboNER P(%) R(%) F1(%) P(%) R(%) F1 <ref type="formula">(%</ref>   tional features for sequence tagging. Although the model achieves competitive performance, giving the F1 score of 89.21%, it suffers from the error propagation problem.</p><p>In the second block, we report the result of our proposed model. Compared with the state-of- the-art model proposed by <ref type="bibr" target="#b25">Luo and Yang (2016)</ref>, our method improves the F1 score from 89.21% to 90.64% without any additional features, which demonstrates the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Adversarial Transfer</head><p>Learning and Self-Attention <ref type="table" target="#tab_5">Table 5</ref> provides the experimental results of our proposed model and baseline as well as its simpli- fied models on SighanNER dataset and WeiboN- ER dataset. The simplified models are described as follows:</p><p>• BiLSTM+CRF: The model is used as strong baseline in our work, which is trained using Chinese NER training data.</p><p>• BiLSTM+CRF+transfer: We apply transfer learning to BiLSTM+CRF model without ad- versarial loss and self-attention mechanism.</p><p>• BiLSTM+CRF+adversarial: Compared with BiLSTM+CRF+transfer model, the BiLST- M+CRF+adversarial model incorporates ad- versarial training.</p><p>• BiLSTM+CRF+self-attention: The model integrates the self-attention mechanism based on BiLSTM+CRF model.</p><p>From the experimental results of <ref type="table" target="#tab_5">Table 5</ref>, we have following observations:</p><p>• Effectiveness of transfer learning. BiL- STM+CRF+transfer improves F1 score from 89.13% to 89.89% as compared with BiLST- M+CRF on SighanNER dataset and achieves 1.08% improvement on WeiboNER dataset, which indicates the word boundary informa- tion from CWS is very effective for Chinese NER task.</p><p>• Effectiveness of adversarial training. By introducing adversarial training, BiLST- M+CRF+adversarial boosts the performance as compared with BiLSTM+CRF+transfer model, showing 0.15% and 0.36% improve- ment on SighanNER dataset and WeiboNER dataset, respectively. It proves that adversar- ial training can prevent specific features of CWS task from creeping into shared space.</p><p>• Effectiveness of self-attention mechanism.</p><p>When compared with BiLSTM+CRF, the BiLSTM+CRF+self-attention significantly improves the performance on the two dif- ferent datasets with the help of information learned from self-attention, which verifies that the self-attention mechanism is effective for Chinese NER task.</p><p>We observe that our proposed adversarial trans- fer learning framework and self-attention lead to noticeable improvements over the baseline, im- proving F1 score from 51.01% to 53.08% on Wei- boNER dataset and giving 1.51% improvement on SighanNER dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detailed Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Case Study</head><p>Word boundary information from CWS task is very important for Chinese NER task, especially when different entities appear together, . We take a sentence in WeiboNER test set as example for illustrating the effectiveness of our proposed mod- el. As shown in <ref type="figure">Figure 4</ref>(a), when two "person" entities appearing together, our proposed method exploits word segmentation information to deter- mine the boundary between them and then make correct taggings. In <ref type="figure">Figure 4</ref>(b), when labelling the word "ø (the boss)", the self-attention ex- plicitly learns the dependencies with " Í (re- spect)", therefore, our model enables to correctly classify the word into "person" category. It veri- fies that the self-attention is very effective for Chi- nese NER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Error Analysis</head><p>According to the results of <ref type="table" target="#tab_2">Table 2 and Table 4</ref>, our proposed model achieves 4.67% and 1.43% improvement as compared with previous state- of-the-art methods on WeiboNER dataset and SighanNER dataset, respectively. However, the overall performance on WeiboNER dataset is rel- atively low. Two reasons can be explained for this issue. One reason is that the number of training examples in WeiboNER dataset is very limited as compared with SighanNER dataset. There are on- ly 1.3k examples in WeiboNER training corpora, which is not enough to train deep neural network- s. Another reason is that the expression is informal in social media, lowering the performance on Wei- boNER dataset. While the greater improvement on WeiboNER dataset proves that our method is helpful to solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel adversarial trans- fer learning framework for Chinese NER task, which can exploit task-shared word boundaries features and prevent the specific information of CWS task. Besides, we introduce self-attention mechanism to capture the dependencies of arbi- trary two characters and learn the inner structure information of sentence. Experiments on two d- ifferent widely used datasets demonstrate that our method significantly and consistently outperforms previous state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of illustrating the similarities and specificities between Chinese NER and CWS.</figDesc><graphic url="image-1.png" coords="1,307.56,222.54,217.70,80.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The general architecture of our proposed model. The left and right part are Chinese NER space and CWS private space, respectively, including embedding layer, feature extractor (Private BiLSTM), self-attention and CRF layer. The middle part is shared space consisting of feature extractor (Shared BiLSTM), self-attention and task discriminator.</figDesc><graphic url="image-2.png" coords="3,168.08,62.81,261.39,196.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The analysis of Chinese NER cases from WeiboNER dataset.</figDesc><graphic url="image-3.png" coords="8,99.36,212.71,174.99,110.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>;</head><label></label><figDesc></figDesc><table>He and Sun, Dataset 
Task 
# Train sent # Dev sent # Test sent 
WeiboNER Chinese NER 
1350 
270 
270 
SighanNER Chinese NER 
41728 
4636 
4365 
MSR 
CWS 
86924 
-
3985 

Table 1: Statistics of the datasets. 

Models 
P(%) R(%) F1(%) 
CRF (Peng and Dredze, 2015) 
56.98 25.26 35.00 
CRF+word (Peng and Dredze, 2015) 
64.94 25.77 36.90 
CRF+character (Peng and Dredze, 2015) 
57.89 34.02 42.86 
CRF+character+position (Peng and Dredze, 2015) 
57.26 34.53 43.09 
Joint(cp) (main) (Peng and Dredze, 2015) 
57.98 35.57 44.09 
Pipeline Seg.Repr.+NER (Peng and Dredze, 2016) 
64.22 36.08 46.20 
Jointly Train Char.Emb (Peng and Dredze, 2016) 
63.16 37.11 46.75 
Jointly Train LSTM Hidden (Peng and Dredze, 2016) 
63.03 38.66 47.92 
Jointly Train LSTM+Emb (main) (Peng and Dredze, 2016) 63.33 39.18 48.41 
BiLSTM+CRF+adversarial+self-attention 
55.72 50.68 53.08 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison between our proposed model and simplified models on SighanNER dataset and 
original WeiboNER dataset. 

(a) Example for the effectiveness of boundary information. 
(b) Example for the effectiveness of self-attention. 

</table></figure>

			<note place="foot" n="1"> https://github.com/CPF-NLPR/AT4ChineseNER</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work is supported by the Natu-ral Science Foundation of China <ref type="bibr">(No.61533018 and No.61702512)</ref>, and the independent research project of National Laboratory of Pattern Recogni-tion. This work is also supported in part by Beijing Unisound Information Technology Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nymble: a highperformance learning name-finder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth conference on Applied natural language processing</title>
		<meeting>the fifth conference on Applied natural language processing</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition with conditional probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial multi-criteria learning for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno>arX- iv:1704.07556</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter with adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2411" to="2420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">F-score driven max margin neural network for named entity recognition in chinese social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">713</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3216" to="3222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient support vector classifiers for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for pos tagging without cross-lingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2832" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05742</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An empirical study of automatic chinese word segmentation for spoken language understanding and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="238" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="149" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arX- iv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<idno>arX- iv:1712.01586</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Named entity recognition with gated convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adversarial learning for chinese ner from crowd annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaosheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05147</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Aspect-augmented adversarial networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>arX- iv:1701.00188</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition with a multi-phase model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition via joint identification and categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese journal of electronics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="230" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
