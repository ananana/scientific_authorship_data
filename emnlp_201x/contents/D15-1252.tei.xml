<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparative Study on Regularization Strategies for Embedding-based Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
							<email>chenyunchuan11@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparative Study on Regularization Strategies for Embedding-based Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper aims to compare different reg-ularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neu-ral models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparame-ter tuning, and combining different regu-larizations. The results provide a picture on tuning hyperparameters for neural NLP models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have exhibited considerable po- tential in various fields ( <ref type="bibr" target="#b8">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b4">Graves et al., 2013)</ref>. In early years on neural NLP research, neural networks were used in lan- guage modeling ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b12">Morin and Bengio, 2005;</ref><ref type="bibr" target="#b11">Mnih and Hinton, 2009)</ref>; recently, they have been applied to various supervised tasks, such as named entity recognition <ref type="bibr" target="#b2">(Collobert and Weston, 2008)</ref>, sentiment analysis <ref type="bibr" target="#b15">(Socher et al., 2011;</ref>, relation classification ( <ref type="bibr" target="#b20">Zeng et al., 2014;</ref>, etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pre- trained by unsupervised algorithms like <ref type="bibr" target="#b10">Mikolov et al. (2013)</ref>; then they are fed forward to standard neural models, fine-tuned during supervised learn- ing. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. * Equal contribution. <ref type="bibr">† Corresponding author.</ref> A curious question is whether we can regular- ize embedding-based NLP neural models to im- prove generalization. Although existing and newly proposed regularization methods might alleviate the problem, their inherent performance in neural NLP models is not clear: the use of embeddings is sparse; the behaviors may be different from those in other scenarios like image recognition. Further, selecting hyperparameters to pursue the best performance by validation is extremely time- consuming, as suggested in <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>. Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing. Specifically, we focus on the following research questions in this paper. RQ 1: How do different regularization strategies typically behave in embedding-based neural networks? RQ 2: Can regularization coefficients be tuned in- crementally during training so as to ease the burden of hyperparameter tuning? RQ 3: What is the effect of combining different regularization strategies? In this paper, we systematically and quan- titatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding ( <ref type="bibr" target="#b9">Labutov and Lipson, 2013)</ref>, and dropout <ref type="bibr" target="#b17">(Srivastava et al., 2014</ref>). We analyzed these regulariza- tion methods by two widely studied models and tasks. We also emphasized on incremental hyper- parameter tuning and the combination of different regularization methods.</p><p>Our experiments provide some interesting re- sults: (1) Regularizations do help generalization, but their effect depends largely on the datasets' size. (2) Penalizing 2 -norm of embeddings helps optimization as well, improving training accu- racy unexpectedly. (3) Incremental hyperparam- eter tuning achieves similar performance, indicat-ing that regularizations mainly serve as a "local" effect. (4) Dropout performs slightly worse than 2 penalty in our experiments; however, provided very small 2 penalty, dropping out hidden units and penalizing 2 -norm are generally complemen- tary. <ref type="formula">(5)</ref> The newly proposed re-embedding words method is not effective in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks, Models, and Setup</head><p>Experiment I: Relation extraction. The dataset in this experiment comes from SemEval-2010 Task 8. <ref type="bibr">1</ref> The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., <ref type="bibr" target="#b5">Hashimoto et al. (2013)</ref>, <ref type="bibr" target="#b20">Zeng et al. (2014)</ref>, and . To make our task and model general, however, we do not consider entity tag- ging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other.</p><p>Regarding the neural model, we applied Col- lobert's convolutional neural network (CNN) <ref type="bibr" target="#b2">(Collobert and Weston, 2008</ref>) with minor modi- fications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer.</p><p>Experiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to pre- dict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011) 2 ; target labels are strongly/weakly positive/negative, or neutral.</p><p>We used the recursive neural network (RNN), which is proposed in <ref type="bibr" target="#b15">Socher et al. (2011)</ref>, and fur- ther developed in <ref type="bibr" target="#b16">Socher et al. (2012)</ref>; <ref type="bibr" target="#b7">Irsoy and Cardie (2014)</ref>. RNNs make use of binarized con- stituency trees, and recursively encode children's information to their parent's; the root vector is fi- nally used for sentiment classification.</p><p>Experimental Setup. To setup a fair compari- son, we set all layers to be 50-dimensional in ad- vance (rather than by validation). Such setting has been used in previous work like <ref type="bibr" target="#b21">Zhao et al. (2015)</ref>. Our embeddings are pretrained on the Wikipedia corpus using <ref type="bibr" target="#b2">Collobert and Weston (2008)</ref>. The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (probably due to the recursive, and thus chaotic nature). There- fore, we applied power decay ( <ref type="bibr" target="#b14">Senior et al., 2013)</ref> with power equal to −1. For each strategy, we tried a large range of regularization coefficients, 10 −9 , · · · , 10 −2 , extensively from underfitting to no effect with granularity 10x. We ran the model 5 times with different initializations. We used mini-batch stochastic gradient descent; gradients are computed by standard backpropagation. For source code, please refer to our project website. <ref type="bibr">3</ref> It needs to be noticed that, the goal of this paper is not to outperform or reproduce state-of-the-art results. Instead, we would like to have a fair com- parison. The testbed of our work is two widely studied models and tasks, which were not chosen on purpose. During the experiments, we tried to make the comparison as fair as possible. There- fore, we think that the results of this work can be generalized to similar scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Regularization Strategies</head><p>In this section, we describe four regularization strategies used in our experiment.</p><p>• Penalizing 2 -norm of weights. Let E be the cross-entropy error for classification, and R be a regularization term. The overall cost function is J = E + λR, where λ is the co- efficient. In this case, R = W 2 , and the coefficient is denoted as λ W .</p><p>• Penalizing 2 -norm of embeddings. Some studies do not distinguish embeddings or connectional weights for regularization <ref type="bibr" target="#b18">(Tai et al., 2015</ref>). However, we would like to an- alyze their effect separately, for embeddings are sparse in use. Let Φ denote embeddings; then we have R = Φ 2 .</p><p>• Re-embedding words <ref type="bibr" target="#b9">(Labutov and Lipson, 2013)</ref>. Suppose Φ 0 denotes the original em- beddings trained on a large corpus, and Φ de- notes the embeddings fine-tuned during su- pervised training. We would like to penalize the norm of the difference between Φ 0 and Φ, i.e., R = Φ 0 −Φ 2 . In the limit of penalty to infinity, the model is mathematically equiv- alent to "frozen embeddings," where word vectors are used as surface features. • Dropout ( <ref type="bibr" target="#b17">Srivastava et al., 2014</ref>). In this strategy, each neural node is set to 0 with a predefined dropout probability p during train- ing; when testing, all nodes are used, with ac- tivation multiplied by 1 − p.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Individual Regularization Behaviors</head><p>This section compares the behavior of each strat- egy. We first conducted both experiments with- out regularization, achieving accuracies of 54.02± 0.84%, 41.47 ± 2.85%, respectively. Then we plot in <ref type="figure" target="#fig_4">Figure 1</ref> learning curves when each regulariza- tion strategy is applied individually. We report training and validation accuracies through out this paper. The main findings are as follows.</p><p>• Penalizing 2 -norm of weights helps gener- alization; the effect depends largely on the size of training set. Experiment I contains 7,000 training samples and the improvement is 6.98%; Experiment II contains more than 150k samples, and the improvement is only 2.07%. Such results are consistent with other machine learning models.</p><p>• Penalizing 2 -norm of embeddings unexpect- edly helps optimization (improves training accuracy). One plausible explanation is that since embeddings are trained on a large cor- pus by unsupervised methods, they tend to settle down to large values and may not per- fectly agree with the tasks of interest. 2 penalty pushes the embeddings towards small values and thus helps optimization. Regard- ing validation accuracy, Experiment I is im- proved by 6.89%, whereas Experiment II has no significant difference.</p><p>• Re-embedding words does not improve gen- eralization. Particularly, in Experiment II, the ultimate accuracy is improved by 0.44, which is not large. Further, too much penalty hurts the models in both experiments. In the limit λ reembed to infinity, re-embedding words is mathematically equivalent to using embed- dings as surface features, that is, freezing em- beddings. Such strategy is sometimes applied in the literature like <ref type="bibr" target="#b6">Hu et al. (2014)</ref>, but is not favorable as suggested by the experiment.</p><p>• Dropout helps generalization. Under the best settings, the eventual accuracy is improved by 3.12% and 1.76%, respectively. In our ex- periments, dropout alone is not as useful as 2 penalty. However, other studies report that dropout is very effective <ref type="bibr" target="#b7">(Irsoy and Cardie, 2014</ref>). Our results are not consistent; differ- ent dimensionality may contribute to this dis- agreement, but more experiments are needed to confirm the hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Incremental Hyperparameter Tuning</head><p>The above experiments show that regularization generally helps prevent overfitting. To pursue the best performance, we need to try out different hy- perparameters through validation. Unfortunately, training deep neural networks is time-consuming, preventing full grid search from being a practical technique. Things will get easier if we can incre- mentally tune hyperparameters, that is, to train the model without regularization first, and then add penalty.</p><p>In this section, we study whether 2 penalty of weights and embeddings can be tuned incremen- tally. We exclude the dropout strategy because its does not make much sense to incrementally drop out hidden units. Besides, from this section, we only focus on Experiment I due to time and space limit.</p><p>Before continuing, we may envision several possibilities on how regularization works.</p><p>• (On initial effects) As 2 -norm prevents pa- rameters from growing large, adding it at early stages may cause parameters settling down to local optima. If this is the case, de- layed penalty would help parameters get over local optima, leading to better performance.</p><p>• (On eventual effects) 2 penalty lifts er- ror surface of large weights. Adding such penalty may cause parameters settling down to (a) almost the same catchment basin, or (b) different basins. In case (a), when the penalty is added does not matter much. In case (b), however, it makes difference, because param- eters would have already gravitated to catch- ment basins of larger values before regular- ization is added, which means incremental hyperparameter tuning would be ineffective. To verify the above conjectures, we design four settings: adding penalty (1) at the beginning, (2) before overfitting at epoch 2, (3) at peak perfor- mance (epoch 5), and (4) after overfitting (valida- tion accuracy drops) at epoch 10. <ref type="figure" target="#fig_6">Figure 2</ref> plots the learning curves regarding pe- nalizing weights and embeddings, respectively; baseline (without regularization) is also included.</p><p>For both weights and embeddings, all settings yield similar ultimate validation accuracies. This shows 2 regularization mainly serves as a "local" effect-it changes the error surface, but parame- ters tend to settle down to a same catchment basin. We notice a recent report also shows local optima    may not play an important role in training neural networks, if the effect of parameter symmetry is ruled out <ref type="bibr" target="#b1">(Breuel, 2015)</ref>.</p><p>We also observe that regularization helps gener- alization as soon as it is added <ref type="figure" target="#fig_6">(Figure 2a)</ref>, and that regularizing embeddings helps optimization also right after the penalty is applied <ref type="figure" target="#fig_6">(Figure 2b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Combination of Regularizations</head><p>We are further curious about the behaviors when different regularization methods are combined. <ref type="table">Table 1</ref> shows that combining 2 -norm of weights and embeddings results in a further accu- racy improvement of 3-4 percents from applying either single one of them. In a certain range of coefficients, weights and embeddings are comple- mentary: given one hyperparameter, we can tune the other to achieve a result among highest ones.</p><p>Such compensation is also observed in penal- izing 2 -norm versus dropout <ref type="table" target="#tab_1">(Table 2</ref>)-although the peak performance is obtained by pure 2 regu- larization, applying dropout with small 2 penalty also achieves a similar accuracy. The dropout rate is not very sensitive, provided it is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this paper, we systematically compared four regularization strategies for embedding-based neural networks in NLP. Based on the experimen- tal results, we answer our research questions as follows. (1) Regularization methods (except re- embedding words) basically help generalization. Penalizing 2 -norm of embeddings unexpectedly helps optimization as well. Regularization perfor- mance depends largely on the dataset's size. <ref type="formula">(2)</ref> 2 penalty mainly acts as a local effect; hyperpa- rameters can be tuned incrementally. (3) Combin- ing 2 -norm of weights and biases (dropout and 2 penalty) further improves generalization; their co- efficients are mostly complementary within a cer- tain range. These empirical results of regulariza- tion strategies shed some light on tuning neural models for NLP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Training Validation (d) Penalizing embeddings in Experiment II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>λ reembed = 0 λ reembed = 10 −5 λ reembed = 10 −4 λ reembed = 10 −3 Training Validation (f) Re-embedding words in Experiment II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Averaged learning curves. Left: Experiment I, relation extraction with CNN. Right: Experiment II, sentiment analysis with RNN. From top to bottom, we penalize weights, penalize embeddings, re-embed words, and drop out. Dashed lines refer to training accuracies; solid lines are validation accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Incrementally penalizing 2 -norm of biases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tuning hyperparameters incrementally in Experiment I. Penalty is added at epochs 0, 2, 5, 10, respectively. We chose the coefficients yielding the best performance in Figure 1. The controlled trial (no regularization) is early stopped because the accuracy has already decreased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Combining 2 regularization and dropout. 
Left: connectional weights. Right: embeddings. 
(p refers to the dropout rate.) 

</table></figure>

			<note place="foot" n="1"> http://www.aclweb.org/anthology/S10-1006 2 http://nlp.stanford.edu/sentiment/</note>

			<note place="foot" n="3"> https://sites.google.com/site/regembeddingnn/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the National Basic Research Program of China (the 973 Program) un-der Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant No. 61232015. We would also like to thank Hao Jia and Ran Jia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02788</idno>
		<title level="m">The effects of hyperparameters on sgd training of neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple customization of recursive neural networks for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilyz</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tree-based convolution: A new neural architecture for sentence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical study of learning rates in deep neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computational Linguistics</title>
		<meeting>Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intenational Joint Conference in Artificial Intelligence</title>
		<meeting>Intenational Joint Conference in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
