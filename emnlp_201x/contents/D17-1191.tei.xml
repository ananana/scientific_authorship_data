<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Residual Learning for Weakly-Supervised Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Yao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">National Taiwan University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Residual Learning for Weakly-Supervised Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1803" to="1807"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep residual learning (ResNet) (He et al., 2016) is a new method for training very deep neural networks using identity mapping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolu-tional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is the task of predicting at- tributes and relations for entities in a sentence <ref type="bibr" target="#b11">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b0">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b3">GuoDong et al., 2005</ref>). For example, given a sentence "Barack Obama was born in Honolulu, Hawaii.", a relation classifier aims at predicting the relation of "bornInCity". Relation extraction is the key component for building relation knowl- edge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question an- swering, and summarization.</p><p>A major issue for relation extraction is the lack of labeled training data. In recent years, distant supervision ( <ref type="bibr" target="#b7">Mintz et al., 2009;</ref><ref type="bibr" target="#b5">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b10">Surdeanu et al., 2012</ref>) emerges as the most popular method for relation extraction-it uses knowledge base facts to select a set of noisy in- stances from unlabeled data. Among all the ma- chine learning approaches for distant supervision, the recently proposed Convolutional Neural Net- works (CNNs) model ( <ref type="bibr" target="#b13">Zeng et al., 2014</ref>) achieved the state-of-the-art performance. Following their success, <ref type="bibr" target="#b12">Zeng et al. (2015)</ref> proposed a piece-wise max-pooling strategy to improve the CNNs. Var- ious attention strategies ( <ref type="bibr" target="#b6">Lin et al., 2016;</ref><ref type="bibr" target="#b9">Shen and Huang, 2016)</ref> for CNNs are also proposed, obtaining impressive results. However, most of these neural relation extraction models are rela- tively shallow CNNs-typically only one convo- lutional layer and one fully connected layer were involved, and it was not clear whether deeper mod- els could have benefits on distilling signals from noisy inputs in this task.</p><p>In this paper, we investigate the effects of train- ing deeper CNNs for distantly-supervised relation extraction. More specifically, we designed a con- volutional neural network based on residual learn- ing ( <ref type="bibr" target="#b4">He et al., 2016)</ref>-we show how one can in- corporate word embeddings and position embed- dings into a deep residual network, while feed- ing identity feedback to convolutional layers for this noisy relation prediction task. Empirically, we evaluate on the NYT-Freebase dataset ( <ref type="bibr" target="#b8">Riedel et al., 2010)</ref>, and demonstrate the state-of-the-art performance using deep CNNs with identify map- ping and shortcuts. In contrast to popular beliefs in vision that deep residual network only works for very deep CNNs, we show that even with a mod- erately deep CNNs, there are substantial improve- ments over vanilla CNNs for relation extraction. Our contributions are three-fold:</p><p>• We are the first to consider deeper convolu- tional neural networks for weakly-supervised relation extraction using residual learning;</p><p>• We show that our deep residual network model outperforms CNNs by a large margin empirically, obtaining state-of-the-art perfor- mances;</p><p>• Our identity mapping with shortcut feedback approach can be easily applicable to any vari- ants of CNNs for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Residual Networks for Relation Extraction</head><p>In this section, we describe a novel deep residual learning architecture for distantly supervised rela- tion extraction. <ref type="figure" target="#fig_0">Figure 1</ref> describes the architecture of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vector Representation</head><p>Let x i be the i-th word in the sentence and e1, e2 be the two corresponding entities. Each word will ac- cess two embedding look-up tables to get the word embedding WF i and the position embedding PF i . Then, we concatenate the two embeddings and de- note each word as a vector of</p><formula xml:id="formula_0">v i = [WF i , PF i ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Word Embeddings</head><p>Each representation v i corresponding to x i is a real-valued vector. All of the vectors are encoded in an embeddings matrix V w ∈ R dw×|V | where V is a fixed-sized vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Position Embeddings</head><p>In relation classification, we focus on finding a relation for entity pairs. Following ( <ref type="bibr" target="#b13">Zeng et al., 2014</ref>), a PF is the combination of the relative dis- tances of the current word to the first entity e 1 and the second entity e 2 . For instance, in the sentence "Steve Jobs is the founder of Apple.", the relative distances from founder to e 1 (Steve Job) and e 2 are 3 and -2, respectively. We then transform the rel- ative distances into real valued vectors by looking up one randomly initialized position embedding matrices V p ∈ R dp××P where P is fixed-sized dis- tance set. It should be noted that if a word is too far from entities, it may be not related to the rela- tion. Therefore, we choose maximum value e max and minimum value e min for the relative distance.</p><p>In the example shown in <ref type="figure" target="#fig_0">Figure 1</ref>, it is assumed that d w is 4 and d p is 1. There are two position embeddings: one for e 1 , the other for e 2 . Finally, we concatenate the word embeddings and position embeddings of all words and denote a sentence of length n (padded where necessary) as a vector</p><formula xml:id="formula_1">v = v 1 ⊕ v 2 ⊕ ... ⊕ v n</formula><p>where ⊕ is the concatenation operator and</p><formula xml:id="formula_2">v i ∈ R d (d = d w + d p × 2).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolution</head><p>Let v i:i+j refer to the concatenation of words v i , v i+1 , ..., v i+j . A convolution operation in- volves a filter w ∈ R hd , which is applied to a window of h words to produce a new feature. A feature c i is generated from a window of word v i:i+h−1 by</p><formula xml:id="formula_3">c i = f(w · x i:i+h−1 + b)</formula><p>Here b ∈ R is a bias term and f is a non-linear function. This filter is applied to each possible window of words from v 1 to v n to produce fea-</p><formula xml:id="formula_4">ture c = [c 1 , c 2 , ..., c n−h+1 ] with c ∈ R s (s = n − h + 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Residual Convolution Block</head><p>Residual learning connects low-level to high-level representations directly, and tackles the vanishing gradient problem in deep networks. In our model, we design the residual convolution block by ap- plying shortcut connections. Each residual con- volutional block is a sequence of two convolu- tional layers, each one followed by an ReLU ac- tivation. The kernel size of all convolutions is h, with padding such that the new feature will have the same size as the original one. Here are two convolutional filter w 1 , w 2 ∈ R h×1 . For the first convolutional layer:</p><formula xml:id="formula_5">˜ c i = f(w 1 · c i:i+h−1 + b 1 )</formula><p>For the second convolutional layer:</p><formula xml:id="formula_6">´ c i = f(w 2 · ˜ c i:i+h−1 + b 2 )</formula><p>Here b 1 , b 2 are bias terms. For the residual learn- ing operation: c = c + ´ c Conveniently, the notation of c on the left is changed to define as the output vectors of the block. This operation is performed by a shortcut connection and element-wise addition. This block will be multiply concatenated in our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Max Pooling, Softmax Output</head><p>We then apply a max-pooling operation over the feature and take the maximum valuê c = max{c}. We have described the process by which one fea- ture is extracted from one filter. Take all fea- tures into one high level extracted feature z = [ˆ c 1 , ˆ c 2 , ..., ˆ c m ](note that here we have m filters). Then, these features are passed to a fully con- nected softmax layer whose output is the proba- bility distribution over relations. Instead of using y = w · z + b for output unit y in forward prop- agation, dropout uses y = w · (z • r) + b where • is the element-wise multiplication operation and r ∈ R m is a 'masking' vector of Bernoulli random variables with probability p of being 1. In the test procedure, the learned weight vectors are scaled by p such thatˆwthatˆ thatˆw = pw and used (without dropout) to score unseen instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>In this paper, we use the word embeddings re- leased by <ref type="bibr" target="#b6">(Lin et al., 2016</ref>) which are trained on the NYT-Freebase corpus ( <ref type="bibr" target="#b8">Riedel et al., 2010</ref>). We fine tune our model using validation on the train- ing data. The word embedding is of size 50. The input text is padded to a fixed size of 100. Training is performed with tensorflow adam optimizer, us- ing a mini-batch of size 64, an initial learning rate of 0.001. We initialize our convolutional layers following <ref type="bibr" target="#b2">(Glorot and Bengio, 2010)</ref>. The imple- mentation is done using Tensorflow 0.11. All ex- periments are performed on a single NVidia Titan X (Pascal) GPU. In <ref type="table">Table 1</ref> we show all parame- ters used in the experiments. We experiment with several state-of-the-art base- lines and variants of our model.</p><p>• CNN-B: Our implementation of the CNN baseline ( <ref type="bibr" target="#b13">Zeng et al., 2014</ref>) which contains one convolutional layer, and one fully con- nected layer.</p><p>Window size h 3 Word dimension dw 50 Position dimension dp • CNN+ATT: CNN-B with attention over in- stance learning ( <ref type="bibr" target="#b6">Lin et al., 2016</ref>).</p><p>• PCNN+ATT: Piecewise CNN-B with atten- tion over instance learning ( <ref type="bibr" target="#b6">Lin et al., 2016</ref>).</p><p>• CNN: Our CNN model which includes one convolutional layer and three fully connected layers.</p><p>• CNN-x: Deeper CNN model which has x convolutional layers. For example, CNN-9 is a model constructed with 9 convolutional lay- ers (1 + 4 residual cnn block without identity shortcut) and three fully connected layers.</p><p>• ResCNN-x: Our proposed CNN-x model with residual identity shortcuts.</p><p>We evaluate our models on the widely used NYT freebase larger dataset ( <ref type="bibr" target="#b8">Riedel et al., 2010)</ref>. Note that ImageNet dataset used by the original ResNet paper (He et al., 2016) has 1.28 million training instances. NYT freebase dataset includes 522K training sentences, which is the largest dataset in relation extraction, and it is the only suitable dataset to train deeper CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NYT-Freebase Dataset Performance</head><p>The advantage of this dataset is that there are 522,611 sentences in training data and 172,448 sentences in testing data and this size can support  us to train a deep network. Similar to previous work ( <ref type="bibr" target="#b12">Zeng et al., 2015;</ref><ref type="bibr" target="#b6">Lin et al., 2016)</ref>, we eval- uate our model using the held-out evaluation. We report both the aggregate curves precision/recall curves and Precision@N (P@N). In <ref type="figure" target="#fig_1">Figure 2</ref>, we compare the proposed ResCNN model with various CNNs. First, CNNs with mul- tiple fully-connected layers obtained very good re- sults, which is a novel finding. Second, the re- sults also suggest that deeper CNNs with resid- ual learning help extracting signals from noisy dis- tant supervision data. We observe that overfitting happened when we try to add more layers and the performance of CNN-9 is much worse than CNN. We find that ResNet can solve this prob- lem and ResCNN-9 obtains better performance as compared to CNN-B and CNN and dominates the precision/recall curve overall.</p><p>We show the effect of depth in residual net- works in <ref type="figure" target="#fig_2">Figure 3</ref>. We observe that ResCNN-5 is worse than CNN-5 because the ResNet does not work well for shallow CNNs, and this is consis-  tent with the original ResNet paper. As we in- crease the network depth, we see that CNN-9 does overfit to the training data. With residual learning, both ResCNN-9 and ResCNN-13 provide signif- icant improvements over CNN-5 and ResCNN-5 models. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance learning in a noisy input setting. The intuition of ResNet help this task in two as- pect. First, if the lower, middle, and higher lev- els learn hidden lexical, syntactic, and semantic representations respectively, sometimes it helps to bypass the syntax to connect lexical and semantic space directly. Second, ResNet tackles the van- ishing gradient problem which will decrease the effect of noise in distant supervision data.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we compare the performance of our models to state-of-the-art baselines. We show that our ResCNN-9 outperforms all models that do not select training instances. And even without piece- wise max-pooling and instance-based attention, our model is on par with the PCNN+ATT model.</p><p>For the more practical evaluation, we compare the results for precision@N where N is small <ref type="bibr">(1,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">20,</ref><ref type="bibr">50</ref>) in <ref type="table">Table 3</ref>. We observe that our ResCNN-9 model dominates the performance when we predict the relation in the range of higher probability. ResNet helps CNNs to focus on the highly possible candidate and mitigate the noise effect of distant supervision. We believe that resid- ual connections actually can be seen as a form of renormalizing the gradients, which prevents the model from overfitting to the noisy distant super- vision data.</p><p>In our distant-supervised relation extraction ex- perience, we have two important observations: (1) We get significant improvements with CNNs adding multiple fully-connected layers. (2) Resid-P@N(%) 1 5 10 20 50 PCNN+ATT 1 0.8 0.9 0.75 0.7 ResCNN-9 1 1 0.9 0.9 0.88 <ref type="table">Table 3</ref>: P@N for relation extraction with different models where N is small. We get the result of PCNN+ATT using their public source code.</p><p>ual learning could significantly improve the per- formance for deeper CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduce a deep residual learning method for distantly-supervised relation extrac- tion. We show that deeper convolutional models help distill signals from noisy inputs. With short- cut connections and identify mapping, the perfor- mances are significantly improved. These results aligned with a recent study ( <ref type="bibr" target="#b1">Conneau et al., 2017)</ref>, suggesting that deeper CNNs do have positive ef- fects on noisy NLP problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of ResCNN used for relation extraction.</figDesc><graphic url="image-1.png" coords="3,117.36,62.81,362.84,149.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing ResCNN to different CNNs.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Varying the depths of our models.</figDesc><graphic url="image-3.png" coords="4,72.00,257.09,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>P@N for relation extraction with different models. 
Top: models that select training data. Bottom: models with-
out selective attention. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentionbased convolutional neural network for semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
