<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>zptu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
							<email>andy.way@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2997" to="3002"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2997</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations. Recently , Wang et al. (2018) proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared recon-structor to better exploit encoder and decoder representations. Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model. Experimental results show that our approach significantly improves both translation performance and DP prediction accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pronouns are important in natural languages as they imply rich discourse information. How- ever, in pro-drop languages such as Chinese and Japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context. When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models gen- erally fail to translate invisible dropped pronouns (DPs). This phenomenon leads to various trans- lation problems in terms of completeness, syntax and even semantics of translations. A number of approaches have been investigated for DP trans- lation <ref type="bibr" target="#b4">(Le Nagard and Koehn, 2010;</ref><ref type="bibr" target="#b11">Xiang et al., 2013;</ref><ref type="bibr" target="#b10">Wang et al., 2016</ref><ref type="bibr" target="#b8">Wang et al., , 2018</ref>. <ref type="bibr" target="#b8">Wang et al. (2018)</ref> is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University. lation (NMT) models. They employ two sepa- rate reconstructors ( ) to respectively reconstruct encoder and decoder representations back to the DP-annotated source sentence. The annotation of DP is provided by an external pre- diction model, which is trained on the parallel cor- pus using automatically learned alignment infor- mation ( <ref type="bibr" target="#b10">Wang et al., 2016)</ref>. Although this model achieved significant improvements, there nonethe- less exist two drawbacks: 1) there is no interaction between the two separate reconstructors, which misses the opportunity to exploit useful relations between encoder and decoder representations; and 2) the external DP prediction model only has an accuracy of 66% in F1-score, which propagates numerous errors to the translation model.</p><p>In this work, we propose to improve the orig- inal model from two perspectives. First, we use a shared reconstructor to read hidden states from both encoder and decoder. Second, we integrate a DP predictor into NMT to jointly learn to trans- late and predict DPs. Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information rel- evant to DPs. Experimental results on a large- scale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by <ref type="bibr" target="#b8">Wang et al. (2018)</ref>. In addition, the jointly learned DP prediction model significantly outperforms its ex- ternal counterpart by 9% in F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, <ref type="bibr" target="#b8">Wang et al. (2018)</ref> in- troduced two independent reconstructors with their own parameters, which reconstruct the DP- annotated source sentence from the encoder and decoder hidden states, respectively. The central  Prediction F1-score Example DP Position 88% #DP# ?</p><p>DP Words 66% ? <ref type="table">Table 1</ref>: Evaluation of external models on predict- ing the positions of DPs ("DP Position") and the exact words of DP ("DP Words").</p><p>idea underpinning their approach is to guide the corresponding hidden states to embed the recalled source-side DP information and subsequently to help the NMT model generate the missing pro- nouns with these enhanced hidden representations. The DPs can be automatically annotated for training and test data using two different strate- gies ( <ref type="bibr" target="#b10">Wang et al., 2016</ref>). In the training phase, where the target sentence is available, we anno- tate DPs for the source sentence using alignment information. These annotated source sentences can be used to build a neural-based DP predic- tor, which can be used to annotate test sentences since the target sentence is not available during the testing phase. As shown in <ref type="table">Table 1</ref>, <ref type="bibr" target="#b10">Wang et al. (2016</ref><ref type="bibr" target="#b8">Wang et al. ( , 2018</ref> explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score. By analyzing the translation outputs, we found that 16.2% of errors are newly introduced and caused by errors from the DP predictor. For- tunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem. Intu- itively, we can learn to generate DPs at the pre- dicted positions using a jointly trained DP predic- tor, which is fed with informative representations in the reconstructor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shared Reconstructor</head><p>Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, <ref type="bibr" target="#b3">Firat et al. (2016)</ref> share an attention model across languages while <ref type="bibr" target="#b2">Dong et al. (2015)</ref> share an encoder. Our work is most similar to the work of <ref type="bibr" target="#b12">Zoph and Knight (2016)</ref> and <ref type="bibr" target="#b0">Anastasopoulos and Chiang (2018)</ref>, which share a de- coder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames.</p><p>The architectures of our proposed shared recon- struction model are shown in <ref type="figure" target="#fig_3">Figure 2</ref>(a). For- mally, the reconstructor reads from both the en- coder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a re- construction score. It uses two separate attention models to reconstruct the annotated source sen-</p><formula xml:id="formula_0">tencê x = {ˆx{ˆx 1 , ˆ x 2 , .</formula><p>. . , ˆ x T } word by word, and the reconstruction score is computed by</p><formula xml:id="formula_1">R(ˆ x|h enc , h dec ) = T t=1 g r (ˆ x t−1 , h rec t , ˆ c enc t , ˆ c dec t )</formula><p>where h rec t is the hidden state in the reconstructor, and computed by Equation (1):</p><formula xml:id="formula_2">h rec t = f r (ˆ x t−1 , h rec t−1 , ˆ c enc t , ˆ c dec t )<label>(1)</label></formula><p>Here g r (·) and f r (·) are respectively softmax and activation functions for the reconstructor. The context vectorsˆcvectorsˆ vectorsˆc enc t andˆcandˆ andˆc dec t are the weighted sum of h enc and h dec , respectively, as in Equation <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>:</p><formula xml:id="formula_3">ˆ c enc t = J j=1ˆαj=1ˆ j=1ˆα enc t,j · h enc j (2) ˆ c dec t = I i=1ˆαi=1ˆ i=1ˆα dec t,i · h dec i<label>(3)</label></formula><p>Note that the weightsˆαweightsˆ weightsˆα enc andˆαandˆ andˆα dec are calculated by two separate attention models. We propose two attention strategies which differ as to whether the two attention models have interactions or not.</p><p>Independent Attention calculates the two weight matrices independently, as in Equation <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref>:</p><formula xml:id="formula_4">ˆ α enc = ATT enc (ˆ x t−1 , h rec t−1 , h enc ) (4) ˆ α dec = ATT dec (ˆ x t−1 , h rec t−1 , h dec )<label>(5)</label></formula><p>where ATT enc (·) and ATT dec (·) are two separate attention models with their own parameters.   Interactive Attention feeds the context vector produced by one attention model to another atten- tion model. The intuition behind this is that the interaction between two attention models can lead to a better exploitation of the encoder and decoder representations. As the interactive attention is di- rectional, we have two options (Equation (6) and <ref type="formula">(7)</ref>) which modify either ATT enc (·) or ATT dec (·) while leaving the other one unchanged:</p><p>• enc→dec:</p><formula xml:id="formula_5">ˆ α dec = ATT dec (ˆ x t−1 , h rec t−1 , h dec , ˆ c enc t )<label>(6)</label></formula><p>• dec→enc:</p><formula xml:id="formula_6">ˆ α enc = ATT enc (ˆ x t−1 , h rec t−1 , h enc , ˆ c dec t ) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Prediction of Dropped Pronouns</head><p>Inspired by recent successes of multi-task learn- ing ( <ref type="bibr" target="#b2">Dong et al., 2015;</ref><ref type="bibr" target="#b5">Luong et al., 2016)</ref>, we propose to jointly learn to translate and predict DPs (as shown in <ref type="figure" target="#fig_3">Figure 2(b)</ref>). To ease the learn- ing difficulty, we leverage the information of DPPs predicted by an external model, which can achieve an accuracy of 88% in F1-score. Accordingly, we transform the original DP prediction problem to DP word generation given the pre-predicted DP positions. Since the DPP-annotated source sen- tence serves as the reconstructed input, we in- troduce an additional DP-generation loss, which measures how well the DP is generated from the corresponding hidden state in the reconstructor. Let dp = {dp 1 , dp 2 , . . . , dp D } be the list of DPs in the annotated source sentence, and h rec = {h rec 1 , h rec 2 , . . . , h rec D } be the corresponding hid- den states in the reconstructor. The generation probability is computed by</p><formula xml:id="formula_7">P (dp|h rec ) = D d=1 P (dp d |h rec d ) = D d=1 g p (dp d |h rec d )<label>(8)</label></formula><p>where g p (·) is softmax for the DP predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Testing</head><p>We train both the encoder-decoder and the shared reconstructors together in a single end-to-end pro- cess, and the training objective is J(θ, γ, ψ) = arg max θ,γ,ψ log L(y|x; θ) likelihood + log R(ˆ x|h enc , h dec ; θ, γ) reconstruction + log P (dp|ˆhdp|ˆ dp|ˆh rec ; θ, γ, ψ) prediction</p><p>where {θ, γ, ψ} are respectively the parameters associated with the encoder-decoder, shared re- constructor and the DP prediction model. The auxiliary reconstruction objective R(·) guides the related part of the parameter matrix θ to learn better latent representations, which are used to reconstruct the DPP-annotated source sentence. The auxiliary prediction loss P (·) guides the re- lated part of both the encoder-decoder and the re- constructor to learn better latent representations, which are used to predict the DPs in the source sentence.  <ref type="table">Table 2</ref>: Evaluation of translation performance for Chinese-English. "Baseline" is trained and evaluated on the original data, while "Baseline (+DPs)" and "Baseline (+DPPs)" are trained on the data anno- tated with DPs and DPPs, respectively. Training and decoding (beam size is 10) speeds are measured in words/second. " †" and " ‡" indicate statistically significant difference (p &lt; 0.01) from "Baseline (+DDPs)" and "Separate-Recs⇒(+DPs)", respectively.</p><p>as a reranking technique to select the best trans- lation candidate from the generated n-best list at testing time. Different from <ref type="bibr" target="#b8">Wang et al. (2018)</ref>, we reconstruct DPP-annotated source sentence, which is predicted by an external model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>To compare our work with the results reported by previous work ( <ref type="bibr" target="#b8">Wang et al., 2018)</ref>, we conducted experiments on their released Chinese⇒English TV Subtitle corpus. <ref type="bibr">2</ref> The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sen- tence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics ( <ref type="bibr" target="#b6">Papineni et al., 2002</ref>) for evaluation, and sign-test <ref type="bibr" target="#b1">(Collins et al., 2005</ref>) to test for statistical significance.</p><p>We implemented our models on the code repos- itory released by <ref type="bibr" target="#b8">Wang et al. (2018)</ref>. <ref type="bibr">3</ref> We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their re- ported results. It should be emphasized that we did not use the pre-train strategy as done in <ref type="bibr" target="#b8">Wang et al. (2018)</ref>, since we found training from scratch achieved a better performance in the shared recon- structor setting.  <ref type="table">Table 2</ref> shows the translation results. It is clear that the proposed models significantly outperform the baselines in all cases, although there are con- siderable differences among different variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Baselines (Rows 1-4): The three baselines (Rows 1, 2, and 4) differ regarding the training data used. "Separate-Recs⇒(+DPs)" (Row 3) is the best model reported in <ref type="bibr" target="#b8">Wang et al. (2018)</ref>, which we employed as another strong baseline. The baseline trained on the DPP-annotated data ("Baseline (+DPPs)", Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs.</p><p>Our Models (Rows 5-8): Using our shared re- constructor (Row 5) not only outperforms the cor- responding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3). In- troducing a joint prediction objective (Row 6) can achieve a further improvement of +0.61 BLEU points. These results verify that shared reconstruc- tor and jointly predicting DPs can accumulatively improve translation performance. Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by <ref type="bibr" target="#b8">Wang et al. (2018)</ref>  <ref type="bibr">(Row 3</ref>). We attribute the superior per- formance of "Shared-Rec enc→dec " to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corre- sponding pronouns in the decoder side. Similar to <ref type="bibr" target="#b8">Wang et al. (2018)</ref>, the proposed approach im- proves BLEU scores at the cost of decreased train- ing and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Models Precision Recall F1-score External 0.67 0.65 0.66 Joint 0.74 0.76 0.75 <ref type="table">Table 3</ref>: Evaluation of DP prediction accu- racy. "External" model is separately trained on DP-annotated data with external neural methods ( <ref type="bibr" target="#b10">Wang et al., 2016)</ref>, while "Joint" model is jointly trained with the NMT model (Section 3.2).</p><p>DP Prediction Accuracy As shown in <ref type="table">Table 3</ref>, the jointly learned model significantly outper- forms the external one by 9% in F1-score. We attribute this to the useful contextual informa- tion embedded in the reconstructor representa- tions, which are used to generate the exact DP words.  Translation results when reconstruction is used in training only while not used in testing. <ref type="table" target="#tab_3">Table 4</ref> lists translation results when the reconstruction model is used in training only. We can see that the proposed model outperforms both the strong baseline and the best model reported in <ref type="bibr" target="#b8">Wang et al. (2018)</ref>. This is en- couraging since no extra resources and compu- tation are introduced to online decoding, which makes the approach highly practical, for example for translation in industry applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Auto. Man. Seperate-Recs (+DPs) <ref type="bibr">35.08 38.38 +3.30 Shared-Rec (+DPPs)</ref> 36.53 38.94 +2.41 Translation performance gap ("") between manually ("Man.") and automatically ("Auto.") labelling DPs/DPPs for input sentences in testing.</p><p>Effect of DPP Labelling Accuracy For each sentence in testing, the DPs and DPPs are labelled automatically by two separate external prediction models, the accuracy of which are respectively 66% and 88% measured in F1 score. We investi- gate the best performance the models can achieve with manual labelling, which can be regarded as an "Oracle", as shown in <ref type="table" target="#tab_4">Table 5</ref>. As seen, there still exists a significant gap in performance, and this could be improved by improving the accuracy of our DPP generator. In addition, our models show a relatively smaller distance in performance from the oracle performance ("Man"), indicating that the error propagation problem is alleviated to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed effective approaches of translating DPs with NMT models: shared recon- structor and jointly learning to translate and pre- dict DPs. Through experiments we verified that 1) shared reconstruction is helpful to share knowl- edge between the encoder and decoder; and 2) joint learning of the DP prediction model indeed alleviates the error propagation problem by im- proving prediction accuracy. The two approaches accumulatively improve translation performance. The method is not restricted to the DP transla- tion task and could potentially be applied to other sequence generation problems where additional source-side information could be incorporated.</p><p>In future work we plan to: 1) build a fully end-to-end NMT model for DP translation, which does not depend on any external component (i.e. DPP predictor); 2) exploit cross-sentence context ( <ref type="bibr" target="#b9">Wang et al., 2017</ref>) to further improve DP trans- lation; 3) investigate a new research strand that adapts our model in an inverse translation direc- tion by learning to drop pronouns instead of re- covering DPs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of separate reconstructors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architectures in which the words in red are automatically annotated DPs and DPPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2</head><label></label><figDesc>https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Unless otherwise indicated, in the paper, the terms &quot;DP&quot; and &quot;DP word&quot; are identical.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The ADAPT Centre for Digital Content Technol-ogy is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. We thank the anonymous reviewers for their in-sightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tied multitask learning for neural speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kucerova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aiding pronoun translation with co-reference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Nagard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="252" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2965" to="2977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Translating pro-drop languages with reconstruction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4937" to="4945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting cross-sentence context for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2816" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A novel approach for dropped pronoun translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="983" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enlisting the ghost: Modeling empty categories for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="822" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Knight</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
