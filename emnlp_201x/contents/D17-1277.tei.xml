<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Joint Entity Disambiguation with Local Neural Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Joint Entity Disambiguation with Local Neural Attention</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2619" to="2629"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel deep learning model for joint document-level entity disam-biguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for dis-ambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art accuracy at moderate computational costs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity disambiguation (ED) is an important stage in text understanding which automatically re- solves references to entities in a given knowledge base (KB). This task is challenging due to the in- herent ambiguity between surface form mentions such as names and the entities they refer to. This many-to-many ambiguity can often be captured partially by name-entity co-occurrence counts ex- tracted from entity-linked corpora.</p><p>ED research has largely focused on two types of contextual information for disambiguation: lo- cal information based on words that occur in a context window around an entity mention, and, global information, exploiting document-level co- herence of the referenced entities. Many state- of-the-art methods aim to combine the benefits of both, which is also the philosophy we follow in this paper. What is specific to our approach is that we use embeddings of entities as a common repre- sentation to assess local as well as global evidence.</p><p>In recent years, many text and language under- standing tasks have been advanced by neural net- work architectures. However, despite recent work, competitive ED systems still largely employ man- ually designed features. Such features often rely on domain knowledge and may fail to capture all relevant statistical dependencies and interactions. The explicit goal of our work is to use deep learn- ing in order to learn basic features and their com- binations from scratch. To the best of our knowl- edge, our approach is the first to carry out this pro- gram with full rigor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contributions and Related Work</head><p>There is a vast prior research on entity disam- biguation, highlighted by <ref type="bibr" target="#b18">(Ji, 2016)</ref>. We will focus here on a discussion of our main contributions in relation to prior work. Entity Embeddings. We have developed a sim- ple, yet effective method to embed entities and words in a common vector space. This fol- lows the popular line of work on word embed- dings, e.g. ( <ref type="bibr" target="#b22">Mikolov et al., 2013;</ref><ref type="bibr" target="#b25">Pennington et al., 2014)</ref>, which was recently extended to entities and ED by <ref type="bibr" target="#b31">(Yamada et al., 2016;</ref><ref type="bibr" target="#b7">Fang et al., 2016;</ref><ref type="bibr" target="#b32">Zwicklbauer et al., 2016;</ref><ref type="bibr" target="#b17">Huang et al., 2015)</ref>. In contrast to the above methods that require data about entity-entity co-occurrences which of- ten suffers from sparsity, we rather bootstrap en- tity embeddings from their canonical entity pages and local context of their hyperlink annotations. This allows for more efficient training and alle- viates the need to compile co-linking statistics. These vector representations are a key component to avoid hand-engineered features, multiple dis- ambiguation steps, or the need for additional ad hoc heuristics when solving the ED task. Context Attention. We present a novel atten- tion mechanism for local ED. Inspired by mem-ory networks of <ref type="bibr" target="#b29">(Sukhbaatar et al., 2015)</ref> and in- sights of <ref type="bibr" target="#b21">(Lazic et al., 2015)</ref>, our model deploys attention to select words that are informative for the disambiguation decision. A learned combi- nation of the resulting context-based entity scores and a mention-entity prior yields the final local scores. Our local model achieves better accu- racy than the local probabilistic model of ( <ref type="bibr" target="#b12">Ganea et al., 2016)</ref>, as well as the feature-engineered lo- cal model of ( <ref type="bibr" target="#b13">Globerson et al., 2016)</ref>. As an added benefit, our model has a smaller memory footprint and it's very fast for both training and testing.</p><p>There have been other deep learning approaches to define local context models for ED. For in- stance <ref type="bibr" target="#b9">(Francis-Landau et al., 2016;</ref><ref type="bibr" target="#b15">He et al., 2013</ref>) use convolutional neural networks (CNNs) and stacked denoising auto-encoders, respectively, to learn representations of textual documents and canonical entity pages. Entities for each mention are locally scored based on cosine similarity with the respective document embedding. In a similar local setting, ( <ref type="bibr" target="#b30">Sun et al., 2015</ref>) embed mentions, their immediate contexts and their candidate en- tities using word embeddings and CNNs. How- ever, their entity representations are restrictively built from entity titles and entity categories only. Unfortunately, the above models are rather 'black- box' (as opposed to ours which reveals the atten- tion focus) and were never extended to perform joint document disambiguation.</p><p>Collective Disambiguation. Last, a novel deep learning architecture for global ED is proposed. Mentions in a document are resolved jointly, us- ing a conditional random field ( <ref type="bibr" target="#b20">Lafferty et al., 2001</ref>) with parametrized potentials. We suggest to learn the latter by casting loopy belief propagation (LBP) ( <ref type="bibr" target="#b24">Murphy et al., 1999</ref>) as a rolled-out deep network. This is inspired by similar approaches in computer vision, e.g. <ref type="bibr" target="#b5">(Domke, 2013)</ref>, and allows us to backpropagate through the (truncated) mes- sage passing, thereby optimizing the CRF poten- tials to work well in conjunction with the inference scheme. Our model is thus trained end-to-end with the exception of the pre-trained word and entity embeddings. Previous work has investigated dif- ferent approximation techniques, including: ran- dom graph walks <ref type="bibr" target="#b14">(Guo and Barbosa, 2016)</ref>, per- sonalized PageRank ( <ref type="bibr" target="#b26">Pershina et al., 2015)</ref>, inter- mention voting <ref type="bibr" target="#b8">(Ferragina and Scaiella, 2010)</ref>, graph pruning <ref type="bibr" target="#b16">(Hoffart et al., 2011</ref>), integer linear programming ( <ref type="bibr" target="#b1">Cheng and Roth, 2013)</ref>, or ranking <ref type="bibr">SVMs (Ratinov et al., 2011)</ref>. Mostly connected to our approach is ( <ref type="bibr" target="#b12">Ganea et al., 2016)</ref> where LBP is used for inference (but not learning) in a prob- abilistic graphical model and ( <ref type="bibr" target="#b13">Globerson et al., 2016</ref>) where a single round of message passing with attention is performed. To our knowledge, we are one of the first to investigate differentiable message passing for NLP problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Entity Embeddings</head><p>In a first step, we propose to train entity vectors that can be used for the ED task (and potentially for other tasks). These embeddings compress the semantic meaning of entities and drastically re- duce the need for manually designed features or co-occurrence statistics.</p><p>Entity embeddings are bootstrapped from word embeddings and are trained independently for each entity. A few arguments motivate this deci- sion: (i) there is no need for entity co-occurrence statistics that suffer from sparsity issues and/or large memory footprints; (ii) vectors of entities in a subset domain of interest can be trained sepa- rately, obtaining potentially significant speed-ups and memory savings that would otherwise be pro- hibitive for large entity KBs; 1 (iii) entities can be easily added in an incremental manner, which is important in practice; (iv) the approach extends well into the tail of rare entities with few linked occurrences; (v) empirically, we achieve better quality compared to methods that use entity co- occurrence statistics.</p><p>Our model embeds words and entities in the same low-dimensional vector space in order to ex- ploit geometric similarity between them. We start with a pre-trained word embedding map x : W → R d that is known to encode semantic meaning of words w ∈ W; specifically we use word2vec pre- trained vectors ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>). We extend this map to entities E, i.e. x : E → R d , as de- scribed below.</p><p>We assume a generative model in which words that co-occur with an entity e are sampled from a conditional distribution p(w|e) when they are generated. Empirically, we collect word-entity co- occurrence counts #(w, e) from two sources: (i) the canonical KB description page of the entity (e.g. entity's Wikipedia page in our case), and (ii) the windows of fixed size surrounding mentions of the entity in an annotated corpus (e.g. Wikipedia hyperlinks in our case). These counts define a practical approximation of the above word-entity conditional distribution, i.e. ˆ p(w|e) ∝ #(w, e). We call this the "positive" distribution of words related to the entity. Next, let q(w) be a generic word probability distribution which we use for sampling "negative" words unrelated to a specific entity. As in ( <ref type="bibr" target="#b22">Mikolov et al., 2013)</ref>, we choose a smoothed unigram distribution q(w) = ˆ p(w) α for some α ∈ (0, 1). The desired outcome is that vec- tors of positive words are closer (in terms of dot product) to the embedding of entity e compared to vectors of random words. Let w + ∼ ˆ p(w|e) and w − ∼ q(w). Then, we use a max-margin objec- tive to infer the optimal embedding for entity e:</p><formula xml:id="formula_0">J(z; e) := E w + |e E w − h z; w + , w − h(z; w, v) := [γ − z, x w − x v ] +<label>(1)</label></formula><p>x e := arg min z:z=1</p><formula xml:id="formula_1">J(z; e)</formula><p>where γ &gt; 0 is a margin parameter and <ref type="bibr">[·]</ref> + is the ReLU function. The above loss is optimized using stochastic gradient descent with projection over sampled pairs (w + , w − ). Note that the en- tity vector is directly optimized on the unit sphere which is important in order to obtain qualitative embeddings.</p><p>We empirically assess the quality of our entity embeddings on entity similarity and ED tasks as detailed in Section 7 and Appendix A. The tech- nique described in this section can also be applied, in principle, for computing embeddings of general text documents, but a comparison with such meth- ods is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Local Model with Neural Attention</head><p>We now explain our local ED approach that uses word and entity embeddings to steer a neural atten- tion mechanism. We build on the insight that only a few context words are informative for resolving an ambiguous mention, something that has been exploited before in ( <ref type="bibr" target="#b21">Lazic et al., 2015)</ref>. Focusing only on those words helps reducing noise and im- proves disambiguation. ( <ref type="bibr" target="#b31">Yamada et al., 2016</ref>) ob- serve the same problem and adopt the restrictive strategy of removing all non-nouns. Here, we as- sume that a context word may be relevant, if it is strongly related to at least one of the entity candi- dates of a given mention. Context Scores.</p><p>Let us assume that we have computed a mention-entity priorˆppriorˆ priorˆp(e|m) (procedure detailed in Section 6). In addition, for each mention m, a pruned candidate set Γ(m) of at most S entities has been identified. Our model, depicted in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, computes a score for each e ∈ Γ(m) based on the K-word local context c = {w 1 , . . . , w K } surrounding m, as well as on the prior. It is a composition of differentiable functions, thus it is smooth from input to output, allowing us to easily compute gradients and backpropagate through it.</p><p>Each word w ∈ c and entity e ∈ Γ(m) is mapped to its embedding via the pre-trained map x (cf. Section 3). We then compute an unnormal- ized support score for each word in the context as follows:</p><formula xml:id="formula_2">u(w) = max e∈Γ(m) x e Ax w (2)</formula><p>where A is a parameterized diagonal matrix. The weight is high if the word is strongly related to at least one candidate entity. We often observe that uninformative words (e.g. similar to stop words) receive non-negligible scores which add undesired noise to our local context model. As a consequence, we (hard) prune to the top R ≤ K words with the highest scores 2 and apply a soft- max function on these weights. Define the reduced context:</p><formula xml:id="formula_3">¯ c = {w ∈ c|u(w) ∈ topR(u)} (3)</formula><p>Then, the final attention weights are explicitly</p><formula xml:id="formula_4">β(w) = exp[u(w)] v∈¯ c exp[u(v)] . if w ∈ ¯ c 0 otherwise.<label>(4)</label></formula><p>Finally, we define a β-weighted context-based entity-mention score via</p><formula xml:id="formula_5">Ψ(e, c) = w∈¯ c β(w) x e B x w (5)</formula><p>where B is another trainable diagonal matrix. We will later use the same architecture for the unary scores of our global ED model. Local Score Combination.</p><p>We integrate these context scores with the context-independent scores encoded inˆpinˆ inˆp(e|m). Our final (unnormalized) local model is a combi- nation of both Ψ(e, c) and logˆplogˆ logˆp(e|m):</p><formula xml:id="formula_6">Ψ(e, m, c) = f (Ψ(e, c), logˆplogˆ logˆp(e|m))<label>(6)</label></formula><p>We find a flexible choice for f to be important and superior to a na¨ıvena¨ıve weighted average combination model. We therefore use a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities, which we regularize as sug- gested in <ref type="bibr" target="#b3">(Denton et al., 2015)</ref> by constraining the sum of squares of all weights in the linear layer. We use standard projected SGD for training. The same network is also used in Section 5.</p><p>Prediction is done independently for each mention m i and context c i by maximizing the Ψ(e, m i , c i ) score.</p><p>Learning the Local Model.</p><p>Entity and word embeddings are pre-trained as discussed in Section 3. Thus, the only learnable parameters are the diagonal matrices A and B, plus the parameters of f . Having few parameters helps to avoid overfitting and to be able to train with little annotated data. We assume that a set of known mention-entity pairs {(m, e * )} with their respective context windows have been extracted from a corpus. For model fitting, we then utilize a max-margin loss that ranks ground truth entities higher than other candidate entities. This leads us to the objective:</p><formula xml:id="formula_7">θ * = arg min θ D∈D m∈D e∈Γ(m) g(e, m),<label>(7)</label></formula><p>g(e, m) :</p><formula xml:id="formula_8">= [γ − Ψ(e * , m, c) + Ψ(e, m, c)] +</formula><p>where γ &gt; 0 is a margin parameter and D is a training set of entity annotated documents. We aim to find a Ψ (i.e. parameterized by θ) such that the score of the correct entity e * referenced by m is at least a margin γ higher than that of any other candidate entity e. Whenever this is not the case, the margin violation becomes the experi- enced loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Document-Level Deep Model</head><p>Next, we address global ED assuming document coherence among entities. We therefore intro- duce the notion of a document as consisting of a set of mentions m = m 1 , . . . , m n , along with their context windows c = c 1 , . . . c n . Our goal is to define a joint probability distribution over</p><formula xml:id="formula_9">Γ(m 1 ) × . . . × Γ(m n ) e.</formula><p>Each such e selects one candidate entity for each mention in the docu- ment. Obviously, the state space of e grows expo- nentially in the number of mentions n. CRF Model.</p><p>Our model is a fully-connected pairwise condi- tional random field, defined on the log scale as </p><formula xml:id="formula_10">g(e, m, c) = n i=1 Ψ i (e i ) + i&lt;j Φ(e i , e j ) (8)</formula><formula xml:id="formula_11">Φ(e, e ) = 2 n − 1 x e C x e ,<label>(9)</label></formula><p>where C is a diagonal matrix. Similar to ( <ref type="bibr" target="#b12">Ganea et al., 2016)</ref>, the above normalization helps bal- ancing the unary and pairwise terms across docu- ments with different numbers of mentions. The function value g(e, m, c) is supposedly high for semantically related sets of entities that also have local support. The goal of a global ED prediction method is to perform maximum-a- posteriori on this CRF to find the set of entities e that maximize g(e, m, c). Differentiable Inference.</p><p>Training and prediction in binary CRF models as the one above is NP-hard. Therefore, in learn- ing one usually maximizes a likelihood approxi- mation and during operations (i.e. in prediction) one may use an approximate inference procedure, often based on message-passing. Among many challenges of these approaches, it is worth point- ing out that weaknesses of the approximate infer- ence procedure are generally not captured during learning. Inspired by <ref type="bibr" target="#b4">(Domke, 2011</ref><ref type="bibr" target="#b5">(Domke, , 2013</ref>, we use truncated fitting of loopy belief propagation (LBP) to a fixed number of message passing iter- ations. Our model directly optimizes the marginal likelihoods, using the same networks for learn- ing and prediction. As noted by <ref type="bibr" target="#b5">(Domke, 2013)</ref>, this method is robust to model mis-specification, avoids inherent difficulties of partition functions and is faster compared to double-loop likelihood training (where, for each stochastic update, infer- ence is run until convergence is achieved).</p><p>Our architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. A neural network with T layers encodes T message pass- ing iterations of synchronous max-product LBP 3 which is designed to find the most likely (MAP) entity assignments that maximize g(e, m, c). We also use message damping, which is known to speed-up and stabilize convergence of message passing. Formally, in iteration t, mention m i votes for entity candidate e ∈ Γ(m j ) of mention m j using the normalized log-message m t i→j (e) com- puted as:</p><formula xml:id="formula_12">m t+1 i→j (e) = max e ∈Γ(m i ) Ψ i (e ) + Φ(e, e ) + k =j m t k→i (e )} .<label>(10)</label></formula><p>Herein the first part just reflects the CRF poten- tials, whereas the second part is defined as</p><formula xml:id="formula_13">m t i→j (e) = log[δ · softmax(m t i→j (e))<label>(11)</label></formula><formula xml:id="formula_14">+ (1 − δ) · exp(m t−1 i→j (e))]</formula><p>where δ ∈ (0, 1] is a damping factor. Note that, without loss of generality, we simplify the LBP procedure by dropping the factor nodes. The mes- sages at first iteration (layer) are set to zero. After T iterations (network layers), the beliefs (marginals) are computed as:</p><formula xml:id="formula_15">µ i (e) = Ψ i (e) + k =i m T k→i (e)<label>(12)</label></formula><formula xml:id="formula_16">µ i (e) = exp[µ i (e)] e ∈Γ(m i ) exp[µ i (e )]<label>(13)</label></formula><p>Similar to the local case, we obtain accuracy improvement when combining the mention-entity priorˆppriorˆ priorˆp(e|m) with marginal µ i (e) using the same non-linear combination function f from Equa- tion 6 as follows:</p><formula xml:id="formula_17">ρ i (e) := f (µ i (e), logˆplogˆ logˆp(e|m i ))<label>(14)</label></formula><p>The learned function f for global ED is non- trivial (see <ref type="figure" target="#fig_2">Figure 3)</ref>, showing that the influence of the prior tends to weaken for larger µ(e), whereas it has a dominating influence whenever the document-level evidence is weak. We also ex- perimented with the prior integrated directly in- side the unary factors Ψ i (e i ), but results were worse because, in some cases, the global entity interaction is not able to recover from strong in- correct priors (e.g. country names have a strong prior towards the respective countries as opposed to national sports teams). Parameters of our global model are the diago- nal matrices A, B, C and the weights of the f net- work. As before, we find a margin based objective to be the most effective and we suggest to fit pa- rameters by minimizing a ranking loss 4 defined as:</p><formula xml:id="formula_18">L(θ) = D∈D m i ∈D e∈Γ(m i ) h(m i , e) (15) h(m i , e) = [γ − ρ i (e * i ) + ρ i (e)] +<label>(16)</label></formula><p>Computing this objective is trivial by running T times the steps described by Eqs. (10), (11), followed in the end by the step in Eq. <ref type="bibr">(13)</ref>. Each step is differentiable and the gradient of the model parameters can be computed on the result- ing marginals and back-propagated over messages using chain rule. At test time, marginals ρ i (e) are computed jointly per document using this network, but pre- diction is done independently for each mention m i by maximizing its respective marginal score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Candidate Selection</head><p>We use a mention-entity priorˆppriorˆ priorˆp(e|m) both as a feature and for entity candidate selection. It is  computed by averaging probabilities from two in- dexes build from mention entity hyperlink count statistics from Wikipedia and a large Web cor- pus ( <ref type="bibr" target="#b28">Spitkovsky and Chang, 2012</ref>). Moreover, we add the YAGO dictionary of <ref type="bibr" target="#b16">(Hoffart et al., 2011)</ref>, where each candidate receives a uniform prior.</p><p>Candidate selection, i.e. construction of Γ(e), is done for each input mention as follows: first, the top 30 candidates are selected based on the priorˆp priorˆ priorˆp(e|m). Then, in order to optimize for memory and run time (LBP has complexity quadratic in S), we keep only 7 of these entities based on the following heuristic: (i) the top 4 entities based onˆp onˆ onˆp(e|m) are selected, (ii) the top 3 entities based on the local context-entity similarity measured using the function from Eq. 5 are selected. <ref type="bibr">5</ref> . We refrain from annotating mentions without any candidate entity, implying that precision and recall can be different in our case.</p><p>In a few cases, generic mentions of persons (e.g. "Peter") are coreferences of more specific mentions (e.g. "Peter Such") from the same docu- ment. We employ a simple heuristic to address this issue: for each mention m, if there exist mentions of persons that contain m as a continuous subse-Methods AIDA-B Local models priorˆppriorˆ priorˆp(e|m) 71.9 ( <ref type="bibr" target="#b21">Lazic et al., 2015)</ref> 86.4 ( <ref type="bibr" target="#b13">Globerson et al., 2016)</ref> 87.9 ( <ref type="bibr" target="#b31">Yamada et al., 2016)</ref> 87.2 our (local, K=100, R=50)</p><p>88.8 Global models <ref type="figure" target="#fig_0">(Huang et al., 2015)</ref> 86.6 ( <ref type="bibr" target="#b12">Ganea et al., 2016)</ref> 87.6 ( <ref type="bibr" target="#b2">Chisholm and Hachey, 2015)</ref> 88.7 ( <ref type="bibr" target="#b14">Guo and Barbosa, 2016)</ref> 89.0 ( <ref type="bibr" target="#b13">Globerson et al., 2016)</ref> 91.0 ( <ref type="bibr" target="#b31">Yamada et al., 2016)</ref> 91.5 our (global) 92.22 ± 0.14 quence of words, then we consider the merged set of the candidate sets of these specific mentions as the candidate set for the mention m. We decide that a mention refers to a person if its most proba- ble candidate byˆpbyˆ byˆp(e|m) is a person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">ED Datasets</head><p>We validate our ED models on some of the most popular available datasets used by our predeces- sors <ref type="bibr">6</ref> . We provide statistics in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>• AIDA-CoNLL dataset <ref type="bibr" target="#b16">(Hoffart et al., 2011</ref>) is one of the biggest manually annotated ED datasets. It contains training (AIDA-train), validation (AIDA-A) and test (AIDA-B) sets.</p><p>• MSNBC (MSB), AQUAINT (AQ) and ACE2004 (ACE) datasets cleaned and up- dated by (Guo and Barbosa, 2016) 7</p><p>• WNED-WIKI (WW) and WNED-CWEB (CWEB): are larger, but automatically ex- tracted, thus less reliable. Are built from the ClueWeb and Wikipedia corpora by <ref type="bibr" target="#b14">(Guo and Barbosa, 2016;</ref><ref type="bibr" target="#b10">Gabrilovich et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Training Details and (Hyper)Parameters</head><p>We explain training details of our approach. All models are implemented in the Torch framework. Entity Vectors Training &amp; Relatedness Eval- uation. For entity embeddings only, we use Global methods MSB AQ ACE CWEB WW priorˆppriorˆ priorˆp(e|m) 89.3 83.2 84.4 69.8 64. <ref type="bibr">2 (Fang et al., 2016)</ref> 81.2 88.8 85.3 - - ( <ref type="bibr" target="#b12">Ganea et al., 2016)</ref> 91 89.2 88.7 - - ( <ref type="bibr" target="#b23">Milne and Witten, 2008)</ref>   Wikipedia <ref type="bibr">(Feb 2014</ref>) corpus for training. En- tity vectors are initialized randomly from a 0- mean normal distribution with standard deviation 1. We first train each entity vector on the en- tity's Wikipedia canonical description page (title words included) for 400 iterations. Subsequently, Wikipedia hyperlinks of the respective entities are used for learning until validation score (described below) stops improving. In each iteration, 20 pos- itive words, each with 5 negative words, are sam- pled and used for optimization as explained in Sec- tion 3. We use Adagrad (Duchi et al., 2011) with a learning rate of 0.3. We choose embedding size d = 300, pre-trained (fixed) Word2Vec word vec- tors 8 , α = 0.6, γ = 0.1 and window size of 20 for the hyperlinks. We remove stop words before training. Since our method allows to train the em- bedding of each entity independently of other en- tities, we decide for efficiency reasons (and with- out loss of generality) to learn only the vectors of all entities appearing as mention candidates in all the test datasets described in Sec. 7.1, a total of 270000 entities. Training of those takes 20 hours on a single TitanX GPU with 12GB of memory. We test and validate our entity embeddings on the entity relatedness dataset of ( <ref type="bibr" target="#b0">Ceccarelli et al., 2013)</ref>. It contains 3319 and 3673 queries for the test and validation sets. Each query consist of one target entity and up to 100 candidate entities with gold standard binary labels indicating if the two entities are related. The associated task requires ranking of related candidate entities higher than the others. Following previous work, we use dif- ferent evaluation metrics: normalized discounted cumulative gain (NDCG) and mean average pre- cision (MAP). The validation score used during learning is then the sum of the four metrics showed in <ref type="table">Table 1</ref>. We perform candidate ranking based on cosine similarity of entity pairs. Local and Global Model Training. Our local and global ED models are trained on AIDA-train (mul- tiple epochs), validated on AIDA-A and tested on AIDA-B and other datasets mentioned in Sec- tion 7.1. We use Adam ( <ref type="bibr" target="#b19">Kingma and Ba, 2014)</ref> with learning rate of 1e-4 until validation accuracy exceeds 90%, afterwards setting it to 1e-5. Vari- able size mini-batches consisting of all mentions in a document are used during training. We re- move stop words. Hyper-parameters of the best validated global model are: γ = 0.01, K = 100, R = 25, S = 7, δ = 0.5, T = 10. For the local model, R = 50 was best. Validation accu- racy is computed after each 5 epochs. To regular- ize, we use early stopping, i.e. we stop learning if the validation accuracy does not increase after 500 epochs. Training on a single GPU takes, on aver- age, 2ms per mention, or 16 hours for 1250 epochs over AIDA-train.</p><p>By using diagonal matrices A, B, C, we keep the number of parameters very low (approx. 1.2K parameters). This is necessary to avoid overfit- ting when learning from a very small training set. We also experimented with diagonal plus low-rank matrices, but encountered quality degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Entity Similarity Results</head><p>Results for the entity similarity task are shown in <ref type="table">Table 1</ref>. Our method outperforms the well es- tablished Wikipedia link measure and the method of ( <ref type="bibr" target="#b31">Yamada et al., 2016</ref>) using less information (only word -entity statistics). We note that the best result on this dataset was reported in the un- published work of ( <ref type="bibr" target="#b17">Huang et al., 2015)</ref>. Their en- tity embeddings are trained on many more sources of information (e.g. KG links, relations, entity types). However, our focus was to prove that lightweight trained embeddings useful for the ED task can also perform decently for the entity sim-   <ref type="table">Table 6</ref>: ED accuracy on AIDA-B for our best sys- tem splitted by Wikipedia hyperlink frequency and mention prior of the ground truth entity, in cases where the gold entity appears in the candidate set.</p><p>ilarity task. We emphasize that our global ED model outperforms Huang's ED model <ref type="table" target="#tab_2">(Table 3)</ref>, likely due to the power of our local and joint neu- ral network architectures. For example, our at- tention mechanism clearly benefits from explicitly embedding words and entities in the same space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">ED Baselines &amp; Results</head><p>We compare with systems that report state-of-the- art results on the datasets. Some baseline scores from  <ref type="table" target="#tab_2">Tables 3 and 4</ref>. We run our system 5 times, each time we pick the best model on the validation set, and report results on the test set for these models. We obtain state of the art accuracy on AIDA which is the largest and hardest (by the accuracy of thê p(e|m) baseline) manually created ED dataset . We are also competitive on the other datasets. It should be noted that all the other meth- ods use, at least partially, engineered features. The merit of our proposed method is to show that, with the exception of thê p(e|m) feature, a neural net- work is able to learn the best features for ED with- out requiring expert input.</p><p>To gain further insight, we analyzed the accu- racy on the AIDA-B dataset for situations where gold entities have low frequency or mention prior. <ref type="table">Table 6</ref> shows that our method performs well in these harder cases.   <ref type="table">Table 7</ref>: Examples of context words selected by our local attention mechanism. Distinct words are sorted decreasingly by attention weights and only words with non-zero weights are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Hyperparameter Studies</head><p>In <ref type="table" target="#tab_5">Table 5</ref>, we analyze the effect of two hyper- parameters. First, we see that hard attention (i.e. R &lt; K) helps reducing the noise from uninfor- mative context words (as opposed to keeping all words when R = K). Second, we see that a small number of LBP it- erations (hard-coded in our network) is enough to obtain good accuracy. This speeds up training and testing compared to traditional methods that run LBP until convergence. An explanation is that a truncated version of LBP can perform well enough if used at both training and test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Qualitative Analysis of Local Model</head><p>In <ref type="table">Table 7</ref> we show some examples of context words attended by our local model for correctly solved hard cases (where the mention prior of the correct entity is low). One can notice that words relevant for at least one entity candidate are chosen by our model in most of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Error Analysis</head><p>We analyse some of the errors made by our model on the AIDA-B dataset. We mostly observe three situations: i) annotation errors, ii) gold entities that do not appear in mentions' candidate sets, or iii) gold entities with very low p(e|m) prior whose mentions have an incorrect entity candidate with high prior. For example, the mention "Italians" refers in some specific context to the entity "Italy national football team" rather than the entity rep- resenting the country. The contextual information is not strong enough in this case to avoid an in- correct prediction. On the other hand, there are situations where the context can be misleading, e.g. a document heavily discussing about cricket will favor resolving the mention "Australia" to the entity "Australia national cricket team" instead of the gold entity "Australia" (naming a location of cricket games in the given context).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have proposed a novel deep learning architec- ture for entity disambiguation that combines entity embeddings, a contextual attention mechanism, an adaptive local score combination, as well as un- rolled differentiable message passing for global in- ference. Compared to many other methods, we do not rely on hand-engineered features, nor on an ex- tensive corpus for entity co-occurrences or related- ness. Our system is fully differentiable, although we chose to pre-train word and entity embeddings. Extensive experiments show the competitiveness of our approach across a wide range of corpora. In the future, we would like to extend this system to perform nil detection, coreference resolution and mention detection.</p><p>Our code and data are publicly available: http://github.com/dalab/deep-ed</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Local model with neural attention. Inputs: context word vectors, candidate entity priors and embeddings. Outputs: entity scores. All parts are differentiable and trainable with backpropagation.</figDesc><graphic url="image-1.png" coords="4,159.77,62.81,326.54,205.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Global model: unrolled LBP deep network that is end-to-end differentiable and trainable.</figDesc><graphic url="image-2.png" coords="5,72.00,62.81,453.53,91.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Non-linear scoring function of the belief and mention prior learned with a neural network. Achieves a 1.7% improvement on AIDA-B dataset compared to a weighted average scheme.</figDesc><graphic url="image-3.png" coords="6,106.30,62.81,149.67,125.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Freq</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Mention</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of ED datasets. Gold recall is the percentage of mentions for which the entity candidate set contains the ground truth entity. We only train on mentions with at least one candidate.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>In-KB accuracy for AIDA-B test set. 
All baselines use KB+YAGO mention-entity in-
dex. For our method we show 95% confidence 
intervals obtained over 5 runs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Micro F1 results for other datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Effects of two of the hyper-parameters. 
Left: A low T (e.g.5) is already sufficient for accu-
rate approximate marginals. Right: Hard attention 
improves accuracy of a local model with K=100. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 are</head><label>4</label><figDesc></figDesc><table>taken from (Guo and Barbosa, 
2016). The best results for the AIDA datasets are 
reported by (Yamada et al., 2016) and (Globerson 
et al., 2016). We do not compare against (Per-
shina et al., 2015) since, as noted also by (Glober-
son et al., 2016), their mention index artificially 
includes the gold entity (guaranteed gold recall), 
which is not a realistic setting. 
For a fair comparison with prior work, we use 
in-KB accuracy and micro F1 (averaged per men-
tion) metrics to evaluate our approach. Results are 
shown in </table></figure>

			<note place="foot" n="1"> Notably useful with (limited memory) GPU hardware.</note>

			<note place="foot" n="2"> We implement this in a differentiable way by setting the lowest K-R attention weights in u to −∞ and applying a vanila softmax on top of them. We used the layers Threshold and TemporalDynamicKMaxPooling from Torch nn package, which allow subgradient computation.</note>

			<note place="foot" n="3"> Sum-product and mean-field performed worse in our experiments.</note>

			<note place="foot" n="4"> Optimizing a marginal log-likelihood loss function performed worse.</note>

			<note place="foot" n="5"> We have used a simpler context vector here computed by simply averaging all its constituent word vectors.</note>

			<note place="foot" n="6"> TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available. 7 Available at: bit.ly/2gnSBLg</note>

			<note place="foot" n="8"> By Word2Vec authors: http://bit.ly/1R9Wsqr</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Aurelien Lucchi, Marina Ganea, Jason Lee, Florian Schmidt and Hadi Daneshmand for their comments and suggestions. This research was supported by the Swiss Na-tional Science Foundation (SNSF) grant number 407540 167176 under the project "Conversational Agent for Interactive Access to Information".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning relatedness measures for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ceccarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Trani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Urbana</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="16" to="58" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity disambiguation with web links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">User conditional hashtag prediction for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parameter learning with truncated message-passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2937" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning graphical model parameters with approximate marginal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2454" to="2467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Entity disambiguation by knowledge and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Capturing semantic similarity for entity linking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Francis-Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Facc1: Freebase annotation of clueweb corpora, version 1 (release date 2013-06-26, format version 1, correction level 0)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<ptr target="http://lemurproject" />
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Note</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">org/clueweb09/FACC1/Cited by</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic bag-of-hyperlinks model for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="927" to="938" />
		</imprint>
	</monogr>
	<note>ternational World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collective entity resolution with multi-focal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Robust named entity disambiguation with random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning entity representation for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07678</idno>
		<title level="m">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Entity discovery and linking reading list</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on machine learning, ICML</title>
		<meeting>the eighteenth international conference on machine learning, ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Plato: A selective context model for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="503" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ian H Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A cross-lingual dictionary for english wikipedia concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1333" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust and collective entity disambiguation through semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Zwicklbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
