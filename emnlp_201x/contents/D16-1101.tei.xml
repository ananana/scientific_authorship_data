<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Numerically Grounded Language Models for Semantic Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Numerically Grounded Language Models for Semantic Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="987" to="992"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many real world scenarios it is important to de- tect and potentially correct semantic errors and in- consistencies in text. For example, when clinicians compose reports, some statements in the text may be inconsistent with measurements taken from the patient <ref type="bibr" target="#b1">(Bowman, 2013)</ref>. Error rates in clinical data range from 2.3% to 26.9% ( <ref type="bibr" target="#b8">Goldberg et al., 2008</ref>) and many of them are number-based errors ( <ref type="bibr" target="#b0">Arts et al., 2002</ref>). Likewise, a blog writer may make statistical claims that contradict facts recorded in databases <ref type="bibr" target="#b14">(Munger, 2008)</ref>. Numerical concepts constitute 29% of contradictions in Wikipedia and GoogleNews <ref type="bibr" target="#b5">(De Marneffe et al., 2008</ref>) and 8.8% of contradictory pairs in entailment datasets ( <ref type="bibr" target="#b3">Dagan et al., 2006</ref>). "EF" is a clinical term and stands for "ejection fraction".</p><p>These inconsistencies may stem from oversight, lack of reporting guidelines or negligence. In fact they may not even be errors at all, but point to inter- esting outliers or to errors in a reference database. In all cases, it is important to spot and possibly correct such inconsistencies. This task is known as semantic error correction (SEC) ( <ref type="bibr" target="#b4">Dahlmeier and Ng, 2011)</ref>.</p><p>In this paper, we propose a SEC approach to sup- port clinicians with writing patient reports. A SEC system reads a patient's structured background in- formation from a knowledge base (KB) and their clinical report. Then it recommends improvements to the text of the report for semantic consistency. An example of an inconsistency is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The SEC system has been trained on a dataset of records and learnt that the phrases "non dilated" and "severely dilated" correspond to high and low val- ues for "EF" (abbreviation for "ejection fraction", a clinical measurement), respectively. If the system is then presented with the phrase "non dilated" in the context of a low value, it will detect a seman- tic inconsistency and correct the text to "severely di- lated".</p><p>Our contributions are: 1) a straightforward ex- tension to recurrent neural network (RNN) language models for grounding them in numbers available in the text; 2) a simple method for modelling text con- ditioned on an incomplete KB by lexicalising it; 3) our evaluation on a semantic error correction task for clinical records shows that our method achieves F1 improvements of 5 and 6 percentage points with grounding and KB conditioning, respectively, over an ungrounded approach (F1 of 49%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our approach to semantic error correction ( <ref type="figure" target="#fig_0">Figure 1</ref>) starts with training a language model (LM), which can be grounded in numeric quantities mentioned in- line with text (Subsection 2.1) and/or conditioned on a potentially incomplete KB (Subsection 2.2). Given a document for semantic checking, a hypoth- esis generator proposes corrections, which are then scored using the trained language model (Subsec- tion 2.3). A final decision step involves accepting the best scoring hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Numerically grounded language modelling</head><p>Let {w 1 , ..., w T } denote a document, where w t is the one-hot representation of the t-th token and V is the vocabulary size. A neural LM uses a ma- trix, E in ∈ R D×V , to derive word embeddings, e w t = E in w t . A hidden state from the previous time step, h t−1 , and the current word embedding, e w t , are sequentially fed to an RNN's recurrence function to produce the current hidden state, h t ∈ R D . The con- ditional probability of the next word is estimated as softmax(E out h t ), where E out ∈ R V ×D is an output embeddings matrix.</p><p>We propose concatenating a representation, e n t , of the numeric value of w t to the inputs of the RNN's recurrence function at each time step. Through this numeric representation, the model can generalise to out-of-vocabulary numbers. A straightforward representation is defining e n t = float(w t ), where float(.) is a numeric conversion function that returns a floating point number constructed from the string of its input. If conversion fails, it returns zero.</p><p>The proposed mechanism for numerical ground- ing is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Now the probability of each next word depends on numbers that have ap- peared earlier in the text. We treat numbers as a separate modality that happens to share the same medium as natural language (text), but can convey exact measurements of properties of the real world. At training time, the numeric representations medi- ate to ground the language model in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditioning on incomplete KBs</head><p>The proposed extension can also be used in con- ditional language modelling of documents given a knowledge base. Consider a set of KB tuples accom- panying each document and describing its attributes in the form &lt; attribute, value &gt;, where attributes are defined by a KB schema. We can lexicalise the KB by converting its tuples into textual statements of the form "attribute : value". An example of how we lexicalise the KB is shown in <ref type="figure" target="#fig_1">Figure 2</ref> approach can incorporate KB tuples flexibly, even when values of some attributes are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic error correction</head><p>A statistical model chooses the most likely correc- tion from a set of possible correction choices. If the model scores a corrected hypothesis higher than the original document, the correction is accepted. A hypothesis generator function, G, takes the original document, H 0 , as input and generates a set of candidate corrected documents G(H 0 ) = {H 1 , ..., H M }. A simple hypothesis generator uses confusion sets of semantically related words to pro- duce all possible substitutions.</p><p>A scorer model, s, assigns a score s(H i ) ∈ R to a hypothesis H i . The scorer is based on a likeli- hood ratio test between the original document (null hypothesis, H 0 ) and each candidate correction (al- ternative hypotheses,</p><formula xml:id="formula_0">H i ), i.e. s(H i ) = p(H i ) p(H 0 )</formula><p>. The assigned score represents how much more probable a correction is than the original document.</p><p>The probability of observing a document, p(H i ), can be estimated using language models, or grounded and conditional variants thereof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Our dataset comprises 16,003 clinical records from the London Chest Hospital <ref type="table">(Table 1)</ref>. Each patient record consists of a text report and accompanying structured KB tuples. The latter describe 20 possible numeric attributes (age, gender, etc.), which are also description confusion set intensifiers (adv): non, mildly, severely intensifiers (adj): mild, moderate, severe units: cm, mm, ml, kg, bpm viability: viable, non-viable quartiles: 25, 50, 75, 100 inequalities: &lt;, &gt; partly contained in the report. On average, 7.7 tuples are completed per record. Numeric tokens constitute only a small proportion of each sentence (4.3%), but account for a large part of the unique tokens vocab- ulary (&gt;40%) and suffer from high OOV rates.</p><p>To evaluate SEC, we generate a "corrupted" dataset of semantic errors from the test part of the "trusted" dataset <ref type="table">(Table 1</ref>, last column). We manu- ally build confusion sets <ref type="table" target="#tab_1">(Table 2)</ref> by searching the development set for words related to numeric quanti- ties and grouping them if they appear in similar con- texts. Then, for each document in the trusted test set we generate an erroneous document by sampling a substitution from the confusion sets. Documents with no possible substitution are excluded. The re- sulting "corrupted" dataset is balanced, containing 2,926 correct and 2,926 incorrect documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head><p>Our base LM is a single-layer long short-term mem- ory network (LSTM, Hochreiter and Schmidhuber (1997) with all latent dimensions (internal matrices, input and output embeddings) set to D = 50. We extend this baseline to a conditional variant by con- ditioning on the lexicalised KB (see Section 2.2). We also derive a numerically grounded model by concatenating the numerical representation of each token to the inputs of the base LM model (see Sec- tion 2.1). Finally, we consider a model that is both grounded and conditional (g-conditional).</p><p>The vocabulary contains the V = 1000 most fre- quent tokens in the training set. Out-of-vocabulary tokens are substituted with &lt;num unk&gt;, if nu- meric, and &lt;unk&gt;, otherwise. We extract the numerical representations before masking, so that the grounded models can generalise to out-of- vocabulary numbers. Models are trained to min- imise token cross-entropy, with 20 epochs of back-  propagation and adaptive mini-batch gradient de- scent (AdaDelta) <ref type="bibr" target="#b22">(Zeiler, 2012)</ref>.</p><p>For SEC, we use an oracle hypothesis generator that has access to the groundtruth confusion sets (Ta- ble 2). We estimate the scorer (Section 2.3) using the trained base, conditional, grounded or g-conditional LMs. As additional baselines we consider a scorer that assigns random scores from a uniform distribu- tion and always (never) scorers that assign the low- est (highest) score to the original document and uni- formly random scores to the corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment 1: Numerically grounded LM</head><p>We report perplexity and adjusted perplexity <ref type="bibr" target="#b20">(Ueberla, 1994)</ref> of our LMs on the test set for all tokens and token classes <ref type="table" target="#tab_3">(Table 3)</ref>. Adjusted perplexity is not sensitive to OOV-rates and thus allows for mean- ingful comparisons across token classes. Perplexi- ties are high for numeric tokens because they form a large proportion of the vocabulary. The grounded and g-conditional models achieved a 33.3% and 36.9% improvement in perplexity, respectively, over the base LM model. Conditioning without ground- ing yields only slight improvements, because most of the numerical values from the lexicalised KB are out-of-vocabulary.</p><p>The qualitative example in <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates how numeric values influence the probability of to- kens given their history. We select a document from the development set and substitute its numeric val- ues as we vary EF (the rest are set by solving a known system of equations). The selected ex- act values were unseen in the training data. We calculate the probabilities for observing the docu- ment with different word choices {"non", "mildly", "severely"} under the grounded LM and find that "non dilated" is associated with higher EF values. This shows that it has captured semantic dependen- cies on numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment 2: Semantic error correction</head><p>We evaluate SEC systems on the corrupted dataset (Section 3) for detection and correction.</p><p>For detection, we report precision, recall and F1 scores in <ref type="table" target="#tab_5">Table 4</ref>. Our g-conditional model achieves the best results, a total F1 improvement of 2 points over the base LM model and 7 points over the best baseline. The conditional model without ground- ing performs slightly worse in the F1 metric than the base LM. Note that with more hypotheses the random baseline behaves more similarly to always. Our hypothesis generator generated on average 12 hypotheses per document. The results of never are zero as it fails to detect any error.</p><p>For correction, we report mean average precision (MAP) in addition to the same metrics as for detec- tion <ref type="table">(Table 5</ref>). The former measures the position of the ranking of the correct hypothesis. The al- ways (never) baseline ranks the correct hypothesis at the top (bottom). Again, the g-conditional model  yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Grounded language models represent the relation- ship between words and the non-linguistic con- text they refer to. Previous work grounds lan- guage on vision ( <ref type="bibr" target="#b2">Bruni et al., 2014;</ref><ref type="bibr" target="#b18">Socher et al., 2014;</ref><ref type="bibr" target="#b17">Silberer and Lapata, 2014</ref>), audio <ref type="bibr">Clark, 2015), video (Fleischman and</ref><ref type="bibr" target="#b7">Roy, 2008)</ref>, colour <ref type="bibr" target="#b13">(McMahan and Stone, 2015)</ref>, and olfactory perception ( . However, no pre- vious approach has explored in-line numbers as a source of grounding.</p><p>Our language modelling approach to SEC is in- spired by LM approaches to grammatical error de- tection (GEC) ( <ref type="bibr" target="#b15">Ng et al., 2013;</ref><ref type="bibr" target="#b6">Felice et al., 2014</ref>). They similarly derive confusion sets of semantically related words, substitute the target words with al- ternatives and score them with an LM. Existing se- mantic error correction approaches aim at correct- ing word error choices (Dahlmeier and Ng, 2011), collocation errors <ref type="bibr" target="#b12">(Kochmar, 2016)</ref>, and semantic anomalies in adjective-noun combinations <ref type="bibr" target="#b21">(Vecchi et al., 2011</ref>). So far, SEC approaches focus on short distance semantic agreement, whereas our ap- proach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error cor- rection, however they neither ground in numeric quantities nor incorporate background KBs.  <ref type="table">Table 5</ref>: Error correction results on the test set. We report mean average precision (MAP), precision (P), recall (R) and F1. Best results in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a simple technique to model language in relation to numbers it refers to, as well as conditionally on incomplete knowledge bases. We found that the proposed techniques lead to performance improvements in the tasks of language modelling, and semantic error detection and correc- tion. Numerically grounded models make it possible to capture semantic dependencies of content words on numbers.</p><p>In future work, we will plan to apply numeri- cally grounded models to other tasks, such as nu- meric error correction. We will explore alternative ways for deriving the numeric representations, such as accounting for verbal descriptions of numbers. For SEC, a trainable hypothesis generator can po- tentially improve the coverage of the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semantic error correction using language models.</figDesc><graphic url="image-1.png" coords="1,314.28,237.83,224.63,218.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A language model that is numerically grounded and conditioned on a lexicalised KB. Examples of data in rounded rectangles.</figDesc><graphic url="image-2.png" coords="2,314.28,57.83,224.64,195.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Qualitative example. Template document and document probabilities for &lt;WORD&gt;={'non', 'mildly', 'severely'} and varying numbers. Probabilities are renormalised over the set of possible choices.</figDesc><graphic url="image-3.png" coords="4,313.20,57.83,229.31,158.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Confusion sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Language modelling evaluation results on the test set.</head><label>3</label><figDesc></figDesc><table>We report perplexity (PP) and adjusted perplexity (APP). Best 

results in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Error detection results on the test set. We report preci-

sion (P), recall (R) and F1. Best results in bold. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their insightful comments. We also thank Steffen Petersen for providing the dataset and advising us on the clinical aspects of this work. This research was supported by the Farr Institute of Health Informatics Research, an Allen Distin-guished Investigator award and Elsevier.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Defining and improving data quality in medical registries: a literature review, case study, and generic framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Danielle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolette F De</forename><surname>Arts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Keizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="600" to="611" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Impact of Electronic Health Record Systems on Information Integrity: Quality and Safety Implications. Perspectives in Health Information Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Correcting Semantic Collocation Errors with L1-induced Paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding Contradictions in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">N</forename><surname>Rafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1039" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>Helen Yannakoudakis, and Ekaterina Kochmar</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Grounded Language Modeling for Automatic Speech Recognition of Sports Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fleischman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Analysis of data errors in clinical research databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saveli</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Niemierko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turchin</surname></persName>
		</author>
		<editor>AMIA. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2461" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grounding Semantics in Olfactory Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Error Detection in Content Word Combinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A bayesian model of grounded color semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Blogging and political information: truth or truthiness? Public Choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The CoNLL2013 Shared Task on Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Proceedings of the CoNLL: Shared Task</title>
		<editor>Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin Wu, and Christian Hadiwinoto</editor>
		<meeting>the CoNLL: Shared Task</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Grounded Meaning Representations with Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysing a simple language model· some general conclusions for language models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joerg</forename><surname>Ueberla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="176" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Linear) Maps of the Impossible: Capturing semantic anomalies in distributional space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Distributional Semantics and Compositionality</title>
		<meeting>the Workshop on Distributional Semantics and Compositionality</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
