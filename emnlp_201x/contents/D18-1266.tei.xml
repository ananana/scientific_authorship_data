<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2442</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">⇧ JD AI Research ‡ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Google AI Language</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xiaodong He ⇧</roleName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2442" to="2452"/>
							<date type="published">October 31-November 4, 2018. 2018. 2442</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm. We propose effective and general solutions to each of them. Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training. In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration. When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-the-art model that outperforms previous work by 5.0% absolute on exact match accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal rep- resentations (or program) in a situated environ- ment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been han- dled within this framework, including question an- swering ( <ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr" target="#b9">Iyyer et al., 2017)</ref> and instructions for robots <ref type="bibr" target="#b0">(Artzi and Zettlemoyer, 2013;</ref><ref type="bibr" target="#b19">Misra et al., 2015)</ref>.</p><p>Consider the example in <ref type="figure">Figure 1</ref>. Given the question and a table environment, a semantic parser maps the question to an executable pro- gram, in this case a SQL query, and then exe- cutes the query on the environment to generate the answer England. In the SpFD setting, the train- ing data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs Select Nation Where Points is Maximum</p><p>Program:</p><p>Answer:</p><p>Environment:</p><p>England <ref type="figure">Figure 1</ref>: An example of semantic parsing from deno- tations. Given the table environment, map the question to an executable program that evaluates to the answer. and finds suitable candidates, and an update step that uses these programs to update the model. <ref type="figure">Fig- ure 2</ref> shows the two step training procedure for the above example. In this paper, we address two key challenges in model training for SpFD by proposing a novel learning framework, improving both the search and update steps. The first challenge, the exis- tence of spurious programs, lies in the search step. More specifically, while the success of the search step relies on its ability to find programs that are semantically correct, we can only verify if the pro- gram can generate correct answers, given that no gold programs are presented. The search step is complicated by spurious programs, which happen to evaluate to the correct answer but do not rep- resent accurately the meaning of the natural lan- guage question. For example, for the environ- ment in <ref type="figure">Figure 1</ref>, the program Select Nation Where Name = Karen Andrew is spurious. Selecting spurious programs as positive examples can greatly affect the performance of semantic parsers as these programs generally do not gen- Answer: England</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Update</head><p>Step Step</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marginal Likelihood</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy Gradient</head><p>Off-Policy Gradient</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Margin Methods</head><p>Figure 2: An example of semantic parsing from denotation. Given the question and the table environment, there are several programs which are spurious.</p><p>eralize to unseen questions and environments. The second challenge, choosing a learning al- gorithm, lies in the update step. Because of the unique indirect supervision setting of SpFD, the quality of the learned semantic parser is dictated by the choice of how to update the model pa- rameters, often determined empirically. As a re- sult, several families of learning methods, includ- ing maximum marginal likelihood, reinforcement learning and margin based methods have been used. How to effectively explore different model choices could be crucial in practice.</p><p>Our contributions in this work are twofold. To address the first challenge, we propose a policy shaping ( <ref type="bibr" target="#b6">Griffith et al., 2013)</ref> method that incorpo- rates simple, lightweight domain knowledge, such as a small set of lexical pairs of tokens in the ques- tion and program, in the form of a critique policy ( § 3). This helps bias the search towards the cor- rect program, an important step to improve super- vision signals, which benefits learning regardless of the choice of algorithm. To address the second challenge, we prove that the parameter update step in several algorithms are similar and can be viewed as special cases of a generalized update equation ( § 4). The equation contains two variable terms that govern the update behavior. Changing these two terms effectively defines an infinite class of learning algorithms where different values lead to significantly different results. We study this effect and propose a novel learning framework that im- proves over existing methods.</p><p>We evaluate our methods using the sequential question answering (SQA) dataset <ref type="bibr" target="#b9">(Iyyer et al., 2017)</ref>, and show that our proposed improvements to the search and update steps consistently en- hance existing approaches. The proposed algo- rithm achieves new state-of-the-art and outper- forms existing parsers by 5.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We give a formal problem definition of the seman- tic parsing task, followed by the general learning framework for solving it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Semantic Parsing Task</head><p>The problem discussed in this paper can be for- mally defined as follows. Let X be the set of all possible questions, Y programs (e.g., SQL-like queries), T tables (i.e., the structured data in this work) and Z answers. We further assume access to an executor : Y ⇥ T ! Z, that given a pro- gram y 2 Y and a table t 2 T , generates an an- swer (y, t) 2 Z. We assume that the executor and all tables are deterministic and the executor can be called as many times as possible. To facili- tate discussion in the following sections, we define an environment function e t : Y ! Z, by applying the executor to the program as e t (y) = (y, t).</p><p>Given a question x and an environment e t , our aim is to generate a program y ⇤ 2 Y and then exe- cute it to produce the answer e t (y ⇤ ). Assume that for any y 2 Y, the score of y being a correct pro- gram for x is score ✓ (y, x, t), parameterized by ✓. The inference task is thus:</p><formula xml:id="formula_0">y ⇤ = arg max y2Y score ✓ (y, x, t)<label>(1)</label></formula><p>As the size of Y is exponential to the length of the program, a generic search procedure is typi- cally employed for Eq. (1), as efficient dynamic algorithms typically do not exist. These search procedures generally maintain a beam of program states sorted according to some scoring function, where each program state represents an incom- plete program. The search then generates a new program state from an existing state by perform- ing an action. Each action adds a set of tokens (e.g., Nation) and keyword (e.g., Select) to a program state. For example, in order to generate the program in <ref type="figure">Figure 1</ref>, the DynSP parser ( <ref type="bibr" target="#b9">Iyyer et al., 2017)</ref> will take the first action as adding the SQL expression Select Nation. Notice that score ✓ can be used in either probabilistic or non- probabilistic models. For probabilistic models, we assume that it is a Boltzmann policy, meaning that p ✓ (y | x, t) / exp{score ✓ (y, x, t)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning</head><p>Learning a semantic parser is equivalent to learn- ing the parameters ✓ in the scoring function, which is a structured learning problem, due to the large, structured output space Y. Structured learning al- gorithms generally consist of two major compo- nents: search and update. When the gold pro- grams are available during training, the search pro- cedure finds a set of high-scoring incorrect pro- grams. These programs are used by the update step to derive loss for updating parameters. For ex- ample, these programs are used for approximating the partition-function in maximum-likelihood ob- jective ( <ref type="bibr" target="#b15">Liang et al., 2011</ref>) and finding set of pro- grams causing margin violation in margin based methods <ref type="bibr" target="#b4">(Daumé III and Marcu, 2005</ref>). Depend- ing on the exact algorithm being used, these two components are not necessarily separated into iso- lated steps. For instance, parameters can be up- dated in the middle of search (e.g., <ref type="bibr" target="#b8">Huang et al., 2012</ref>). For learning semantic parsers from denotations, where we assume only answers are available in a training set {(x i , t i , z i )} N i=1 of N examples, the basic construction of the learning algorithms re- mains the same. However, the problems that search needs to handle in SpFD is more challeng- ing. In addition to finding a set of high-scoring in- correct programs, the search procedure also needs to guess the correct program(s) evaluating to the gold answer z i . This problem is further com- plicated by the presence of spurious programs, which generate the correct answer but are seman- tically incompatible with the question. For ex- ample, although all programs in <ref type="figure">Figure 2</ref> evalu- ate to the same answer, only one of them is cor- rect. The issue of the spurious programs also af- fects the design of model update. For instance, maximum marginal likelihood methods treat all the programs that evaluate to the gold answer equally, while maximum margin reward networks use model score to break tie and pick one of the programs as the correct reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Addressing Spurious Programs: Policy Shaping</head><p>Given a training example (x, t, z), the aim of the search step is to find a set K(x, t, z) of pro- grams consisting of correct programs that eval- uate to z and high-scoring incorrect programs. The search step should avoid picking up spurious programs for learning since such programs typ- ically do not generalize. For example, in <ref type="figure">Fig- ure 2</ref>, the spurious program Select Nation Where Index is Min will evaluate to an in- correct answer if the indices of the first two rows are swapped <ref type="bibr">1</ref> . This problem is challenging since among the programs that evaluate to the correct answer, most of them are spurious. The search step can be viewed as following an exploration policy b ✓ (y|x, t, z) to explore the set of programs Y. This exploration is often per- formed by beam search and at each step, we ei- ther sample from b ✓ or take the top scoring pro- grams. The set K(x, t, z) is then used by the up- date step for parameter update. Most search strate- gies use an exploration policy which is based on the score function, for example b ✓ (y|x, t, z) / exp{score ✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the fu- ture. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome <ref type="bibr" target="#b24">(Rose, 1998)</ref>. Unfortu- nately such initialization schemes are not appli- cable for deep neural networks which form the model of most successful semantic parsers to- day ( <ref type="bibr" target="#b10">Jia and Liang, 2016;</ref><ref type="bibr" target="#b18">Misra and Artzi, 2016;</ref><ref type="bibr" target="#b9">Iyyer et al., 2017)</ref>. Prior work, such as ✏-greedy exploration ( <ref type="bibr" target="#b7">Guu et al., 2017)</ref>, has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturat- ing the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs.</p><p>Algorithm 1 Learning a semantic parser from denotation us- ing generalized updates.</p><p>Input: Training set {(xi, ti, zi} N i=1 (see Section 2), learning rate µ and stopping epoch T ˜ (see Section 4). Definitions: score ✓ (y, x, t) is a semantic parsing model parameterized by ✓. ps(y | x, t) is the policy used for exploration and search(✓, x, t, z, ps) generates candi- date programs for updating parameters (see Section 3). is the generalized update (see Section 4). Output: Model parameters ✓.</p><p>1: » Iterate over the training data. 2: for t = 1 to T , i = 1 to N do 3: » Find candidate programs using the shaped policy. 4: K = search(✓, xi, ti, zi, ps) 5:</p><p>» Compute generalized gradient updates 6:</p><formula xml:id="formula_1">✓ = ✓ + µ(K) 7: return ✓</formula><p>Policy Shaping Policy shaping is a method to introduce prior knowledge into a policy ( <ref type="bibr" target="#b6">Griffith et al., 2013)</ref>. Formally, let the current behavior policy be b ✓ (y|x, t, z) and a predefined critique policy, the prior knowledge, be p c (y|x, t). Pol- icy shaping defines a new shaped behavior policy p b (y|x, t) given by:</p><formula xml:id="formula_2">p b (y|x, t) = b ✓ (y|x, t, z)p c (y|x, t) P y 0 2Y b ✓ (y 0 |x, t, z)p c (y 0 |x, t)</formula><p>.</p><p>Using the shaped policy for exploration biases the search towards the critique policy's preference. We next describe a simple critique policy that we use in this paper.</p><p>Lexical Policy Shaping We qualitatively ob- served that correct programs often contains tokens which are also present in the question. For exam- ple, the correct program in <ref type="figure">Figure 2</ref> contains the token Points, which is also present in the question. We therefore, define a simple surface form simi- larity feature match(x, y) that computes the ratio of number of non-keyword tokens in the program y that are also present in the question x.</p><p>However, surface-form similarity is often not enough. For example, both the first and fourth pro- gram in <ref type="figure">Figure 2</ref> contain the token Points but only the fourth program is correct. Therefore, we also use a simple co-occurrence feature that triggers on frequently co-occurring pairs of tokens in the pro- gram and instruction. For example, the token most is highly likely to co-occur with a correct program containing the keyword Max. This happens for the example in <ref type="figure">Figure 2</ref>. Similarly the token not may co-occur with the keyword NotEqual. We assume access to a lexicon ⇤ = {(w j , ! j )} k j=1 containing k lexical pairs of tokens and keywords. Each lex- ical pair (w, !) maps the token w in a text to a keyword ! in a program. For a given program y and question x, we define a co-occurrence score as co_occur(y, x) = P (w,!)2⇤ {w 2 x ^ ! 2 y}}. We define critique score critique(y, x) as the sum of the match and co_occur scores. The critique policy is given by:</p><formula xml:id="formula_4">p c (y|x, t) / exp (⌘ ⇤ critique(y, x)) , (3)</formula><p>where ⌘ is a single scalar hyper-parameter denot- ing the confidence in the critique policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Addressing Update Strategy Selection: Generalized Update Equation</head><p>Given the set of programs generated by the search step, one can use many objectives to update the parameters. In this section, we propose a principle and gen- eral update equation such that previous update al- gorithms can be considered as special cases to this equation. Having a general update is important for the following reasons. First, it allows us to understand existing algorithms better by examin- ing their basic properties. Second, the generalized update equation also makes it easy to implement and experiment with various different algorithms. Moreover, it provides a framework that enables the development of new variations or extensions of ex- isting learning methods.</p><p>In the following, we describe how the com- monly used algorithms are in fact very similar - their update rules can all be viewed as special cases of the proposed generalized update equation. Algorithm 1 shows the meta-learning framework. For every training example, we first find a set of candidates using an exploration policy (line 4). We use the program candidates to update the pa- rameters (line 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Commonly Used Learning Algorithms</head><p>We briefly describe three algorithms: maximum marginalized likelihood, policy gradient and max- imum margin reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Marginalized Likelihood</head><p>The max- imum marginalized likelihood method maximizes the log-likelihood of the training data by marginal- izing over the set of programs.</p><formula xml:id="formula_5">J MML = log p(z i |x i , t i ) = log X y2Y p(z i |y, t i )p(y|x i , t i )<label>(4)</label></formula><p>Because an answer is deterministically com- puted given a program and a table, we define p(z | y, t) as 1 or 0 depending upon whether the y evaluates to z given t, or not. Let Gen(z, t) ✓ Y be the set of compatible programs that evaluate to z given the table t. The objective can then be ex- pressed as:</p><formula xml:id="formula_6">J MML = log X y2Gen(zi,ti) p(y|x i , t i ) (5)</formula><p>In practice, the summation over Gen(.) is approx- imated by only using the compatible programs in the set K generated by the search step.</p><p>Policy Gradient Methods Most reinforcement learning approaches for semantic parsing assume access to a reward function R : Y ⇥ X ⇥ Z ! R, giving a scalar reward R(y, z) for a given pro- gram y and the correct answer z. <ref type="bibr">2</ref> We can fur- ther assume without loss of generality that the re- ward is always in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Reinforcement learning approaches maximize the expected reward J RL :</p><formula xml:id="formula_7">J RL = X y2Y p(y|x i , t i )R(y, z i )<label>(6)</label></formula><p>J RL is hard to approximate using numerical in- tegration since the reward for all programs may not be known a priori. Policy gradient methods solve this by approximating the derivative using a sample from the policy. When the search space is large, the policy may fail to sample a correct pro- gram, which can greatly slow down the learning. Therefore, off-policy methods are sometimes in- troduced to bias the sampling towards high-reward yielding programs. In those methods, an addi- tional exploration policy u(y|x i , t i , z i ) is used to improve sampling. Importance weights are used to make the gradient unbiased (see Appendix for derivation).</p><p>Maximum Margin Reward For every training example (x i , t i , z i ), the maximum margin reward method finds the highest scoring program y i that evaluates to z i , as the reference program, from the set K of programs generated by the search. With a margin function : Y ⇥Y ⇥Z ! R and reference program y, the set of programs V that violate the margin constraint can thus be defined as:</p><formula xml:id="formula_8">V = {y 0 | y 0 2 Y and score ✓ (y, x, t)</formula><p> score ✓ (y 0 , x, t) + (y, y 0 , z)}, <ref type="formula">(7)</ref> where (y, y 0 , z) = R(y, z) R(y 0 , z). Similarly, the program that most violates the constraint can be written as:</p><formula xml:id="formula_9">¯ y = arg max y 0 2Y {score ✓ (y 0 , x, t) + (y, y 0 , z) score ✓ (y, x, t)}<label>(8)</label></formula><p>The most-violation margin objective (negative margin loss) is thus defined as:</p><formula xml:id="formula_10">J MMR = max{0, score ✓ (¯ y, x i , t i ) score ✓ (y i , x i , t i ) + (y i , ¯ y, z i )}</formula><p>Unlike the previous two learning algorithms, mar- gin methods only update the score of the reference program and the program that violates the margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalized Update Equation</head><p>Although the algorithms described in §4.1 seem very different on the surface, the gradients of their loss functions can in fact be described in the same generalized form, given in Eq. (9) <ref type="bibr">3</ref> . In addition to the gradient of the model scoring function, this equation has two variable terms, w(·), q(·). We call the first term w(y, x, t, z) intensity, which is a positive scalar value and the second term q(y|x, t) the competing distribution, which is a probability distribution over programs. Varying them makes the equation equivalent to the update rule of the algorithms we discussed, as shown in <ref type="table">Table 1</ref>. We also consider meritocratic update policy which uses a hyperparameter to sharpen or smooth the intensity of maximum marginal likelihood ( <ref type="bibr" target="#b7">Guu et al., 2017)</ref>. Intuitively, w(y, x, t, z) defines the positive part of the update equation, which defines how aggres- sively the update favors program y. Likewise, q(y|x, t) defines the negative part of the learning Generalized Update Equation:  <ref type="formula" target="#formula_11">(9)</ref>, with different choices of intensity w and competing distribution q. We do not show dependence upon table t for brevity. For off-policy policy gradient, u is the exploration policy. For margin methods, y ⇤ is the reference program (see §4.1), V is the set of programs that violate the margin constraint (cf. Eq. <ref type="formula">(7)</ref>) and ¯ y is the most violating program (cf. Eq. <ref type="formula" target="#formula_9">(8)</ref>). For REINFORCE, ˆ y is sampled from K using p(.) whereas for Off-Policy Policy Gradient, ˆ y is sampled using u(.).</p><formula xml:id="formula_11">(K) = X y2K w(y, x, t, z) 0 @ r ✓ score ✓ (y, x, t) X y 0 2Y q(y 0 |x, t)r ✓ score ✓ (y 0 , x, t) 1 A<label>(9)</label></formula><p>algorithm, namely how aggressively the update penalizes the members of the program set. The generalized update equation provides a tool for better understanding individual algorithm, and helps shed some light on when a particular method may perform better.</p><p>Intensity versus Search Quality In SpFD, the effectiveness of the algorithms for SpFD is closely related to the quality of the search results given that the gold program is not available. Intuitively, if the search quality is good, the update algorithm could be aggressive on updating the model param- eters. When the search quality is poor, the algo- rithm should be conservative.</p><p>The intensity w(·) is closely related to the ag- gressiveness of the algorithm. For example, the maximum marginal likelihood is less aggressive given that it produces a non-zero intensity over all programs in the program set K that evaluate to the correct answer. The intensity for a particular correct program y is proportional to its probabil- ity p(y|x, t). Further, meritocratic update becomes more aggressive as becomes larger.</p><p>In contrast, REINFORCE and maximum mar- gin reward both have a non-zero intensity only on a single program in K. This value is 1.0 for maximum margin reward, while for reinforcement learning, this value is the reward. Maximum mar- gin reward therefore updates most aggressively in favor of its selection while maximum marginal likelihood tends to hedge its bet. Therefore, the maximum margin methods should benefit the most when the search quality improves.</p><p>Stability The general equation also allows us to investigate the stability of a model update algo- rithm. In general, the variance of update direction can be high, hence less stable, if the model update algorithm has peaky competing distribution, or it puts all of its intensity on a single program. For example, REINFORCE only samples one program and puts non-zero intensity only on that program, so it could be unstable depending on the sampling results.</p><p>The competing distribution affects the stability of the algorithm. For example, maximum margin reward penalizes only the most violating program and is benign to other incorrect programs. There- fore, the MMR algorithm could be unstable during training.</p><p>New Model Update Algorithm The general equation provides a framework that enables the de- velopment of new variations or extensions of ex- isting learning methods. For example, in order to improve the stability of the MMR algorithm, we propose a simple variant of maximum margin re- ward, which penalizes all violating programs in- stead of only the most violating one. We call this approach maximum margin average violation re- ward (MAVER), which is included in <ref type="table">Table 1</ref> as well. Given that MAVER effectively considers more negative examples during each update, we expect that it is more stable compared to the MMR algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We describe the setup in §5.1 and results in §5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Dataset We use the sequential question answer- ing (SQA) dataset <ref type="bibr" target="#b9">(Iyyer et al., 2017</ref>) for our ex- periments. SQA contains 6,066 sequences and each sequence contains up to 3 questions, with 17,553 questions in total. The data is partitioned into training (83%) and test (17%) splits. We use 4/5 of the original train split as our training set and the remaining 1/5 as the dev set. We evaluate us- ing exact match on answer. Previous state-of-the- art result on the SQA dataset is 44.7% accuracy, using maximum margin reward learning.</p><p>Semantic Parser Our semantic parser is based on DynSP ( <ref type="bibr" target="#b9">Iyyer et al., 2017)</ref>, which contains a set of SQL actions, such as adding a clause (e.g., Select Column) or adding an operator (e.g., Max). Each action has an associated neural net- work module that generates the score for the ac- tion based on the instruction, the table and the list of past actions. The score of the entire program is given by the sum of scores of all actions.</p><p>We modified DynSP to improve its represen- tational capacity. We refer to the new parser as DynSP++. Most notably, we included new fea- tures and introduced two additional parser actions. See Appendix 8.2 for more details. While these improvements help us achieve state-of-the-art re- sults, the majority of the gain comes from the learning contributions described in this paper.</p><p>Hyperparameters For each experiment, we train the model for 30 epochs. We find the op- timal stopping epoch by evaluating the model on the dev set. We then train on train+dev set till the stopping epoch and evaluate the model on the held-out test set. Model parameters are trained us- ing stochastic gradient descent with learning rate of 0.1. We set the hyperparameter ⌘ for policy shaping to 5. All hyperparameters were tuned on the dev set. We use 40 lexical pairs for defining the co-occur score. We used common English superlatives (e.g., highest, most) and comparators (e.g., more, larger) and did not fit the lexical pairs based on the dataset. Given the model parameter ✓, we use a base exploration policy defined in <ref type="bibr" target="#b9">(Iyyer et al., 2017)</ref>. This exploration policy is given by b ✓ (y | x, t, z) / exp( · R(y, z) + score ✓ (y, ✓, z)). R(y, z) is the reward function of the incomplete program y, given the answer z. We use a reward function R(y, z) given by the Jaccard similarity of the gold answer z and the answer generated by the program y. The value of is set to infinity, which essentially is equivalent to sorting the programs based on the reward and using the current model score for tie breaking. Further, we prune all syn- tactically invalid programs. For more details, we refer the reader to <ref type="bibr" target="#b9">(Iyyer et al., 2017)</ref>. <ref type="table" target="#tab_4">Table 2</ref> contains the dev and test results when us- ing our algorithm on the SQA dataset. We ob- serve that margin based methods perform better than maximum likelihood methods and policy gra- dient in our experiment. Policy shaping in general improves the performance across different algo- rithms. Our best test results outperform previous SOTA by 5.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Policy Gradient vs Off-Policy Gradient RE- INFORCE, a simple policy gradient method, achieved extremely poor performance. This likely due to the problem of exploration and having to sample from a large space of programs. This is further corroborated from observing the much su- perior performance of off-policy policy gradient methods. Thus, the sampling policy is an impor- tant factor to consider for policy gradient methods.</p><p>The Effect of Policy Shaping We observe that the improvement due to policy shaping is 6.0% on the SQA dataset for MAVER and only 1.3% for maximum marginal likelihood. We also ob- serve that as increases, the improvement due to policy shaping for meritocratic update increases. This supports our hypothesis that aggressive up- dates of margin based methods is beneficial when the search method is more accurate as compared to maximum marginal likelihood which hedges its bet between all programs that evaluate to the right answer.</p><p>Stability of MMR In Section 4, the general up- date equation helps us point out that MMR could be unstable due to the peaky competing distribu- tion. MAVER was proposed to increase the stabil- ity of the algorithm. To measure stability, we cal-   culate the mean absolute difference of the devel- opment set accuracy between successive epochs during training, as it indicates how much an al- gorithm's performance fluctuates during training. With this metric, we found mean difference for MAVER is 0.57% where the mean difference for MMR is 0.9%. This indicates that MAVER is in fact more stable than MMR.</p><p>Other variations We also analyze other possi- ble novel learning algorithms that are made pos- sible due to generalized update equations. <ref type="table" target="#tab_5">Ta- ble 3</ref> reports development results using these algo- rithms. By mixing different intensity scalars and competing distribution from different algorithms, we can create new variations of the model update algorithm. In <ref type="table" target="#tab_5">Table 3</ref>, we show that by mixing the MMR's intensity and MML's competing distribu- tion, we can create an algorithm that outperform MMR on the development set.</p><p>Policy Shaping helps against Spurious Pro- grams In order to better understand if policy shaping helps bias the search away from spurious programs, we analyze 100 training examples. We look at the highest scoring program in the beam at the end of training using MAVER. Without policy shaping, we found that 53 programs were spuri- ous while using policy shaping this number came down to 23. We list few examples of spurious pro- gram errors corrected by policy shaping in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy Shaping vs Model Shaping</head><p>Critique policy contains useful information that can bias the search away from spurious programs. There- fore, one can also consider making the critique policy as part of the model. We call this model shaping. We define our model to be the shaped policy and train and test using the new model. Us- ing MAVER updates, we found that the dev ac- curacy dropped to 37.1%. We conjecture that the strong prior in the critique policy can hinder gen- eralization in model shaping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Semantic Parsing from Denotation Mapping natural language text to formal meaning repre- sentation was first studied by <ref type="bibr" target="#b20">Montague (1970)</ref>. Early work on learning semantic parsers rely on labeled formal representations as the supervision signals <ref type="bibr">Collins, 2005, 2007;</ref><ref type="bibr" target="#b31">Zelle and Mooney, 1993)</ref>. However, because get- ting access to gold formal representation gener- ally requires expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have be- come the main learning paradigm (e.g., <ref type="bibr" target="#b3">Clarke et al., 2010;</ref><ref type="bibr" target="#b15">Liang et al., 2011;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013;</ref><ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr" target="#b9">Iyyer et al., 2017;</ref><ref type="bibr" target="#b11">Krishnamurthy et al., 2017)</ref>. <ref type="bibr" target="#b7">Guu et al. (2017)</ref> studied the problem of spurious programs and con- sidered adding noise to diversify the search proce- dure and introduced meritocratic updates.</p><p>Reinforcement Learning Algorithms Rein- forcement learning algorithms have been applied to various NLP problems including dialogue ( <ref type="bibr" target="#b14">Li et al., 2016</ref>), text-based games <ref type="bibr" target="#b21">(Narasimhan et al., 2015)</ref>, information extraction ( <ref type="bibr" target="#b22">Narasimhan et al., 2016)</ref>, coreference resolution (Clark and Man-Question without policy shaping with policy shaping "of these teams, which had more SELECT Club SELECT Club than 21 losses?" WHERE Losses = ROW 15 WHERE Losses &gt; 21 "of the remaining, which SELECT Nation WHERE FollowUp WHERE earned the most bronze medals?" Rank = ROW 1 Bronze is Max "of those competitors from germany, SELECT Name WHERE FollowUp WHERE which was not paul sievert?"</p><p>Time (hand) = ROW 3 Name != ROW 5 <ref type="table">Table 4</ref>: Training examples and the highest ranked program in the beam search, scored according to the shaped policy, after training with MAVER. Using policy shaping, we can recover from failures due to spurious programs in the search step for these examples.</p><p>ning, 2016), semantic parsing ( <ref type="bibr" target="#b7">Guu et al., 2017)</ref> and instruction following <ref type="bibr" target="#b17">(Misra et al., 2017)</ref>. <ref type="bibr" target="#b7">Guu et al. (2017)</ref> show that policy gradient methods underperform maximum marginal likelihood ap- proaches. Our result on the SQA dataset sup- ports their observation. However, we show that using off-policy sampling, policy gradient meth- ods can provide superior performance to maxi- mum marginal likelihood methods.</p><p>Margin-based Learning Margin-based meth- ods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing <ref type="bibr" target="#b27">(Taskar et al., 2004;</ref><ref type="bibr" target="#b16">McDonald et al., 2005</ref>), text clas- sification ( <ref type="bibr" target="#b26">Taskar et al., 2003)</ref>, machine transla- tion ( <ref type="bibr" target="#b28">Watanabe et al., 2007</ref>) and semantic pars- ing ( <ref type="bibr" target="#b9">Iyyer et al., 2017)</ref>. <ref type="bibr" target="#b12">Kummerfeld et al. (2015)</ref> found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections be- tween margin based method and likelihood maxi- mization for supervised learning setting. We show them as special cases of our unified update equa- tion for distant supervision learning. Similar to this work, <ref type="bibr" target="#b13">Lee et al. (2016)</ref> also found that in the context of supervised learning, margin-based al- gorithms which update all violated examples per- form better than the one that only updates the most violated example.</p><p>Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant <ref type="bibr" target="#b5">(Dempster et al., 1977</ref>). The graphical model literature has studied latent variable learn- ing on margin-based methods ( <ref type="bibr" target="#b30">Yu and Joachims, 2009</ref>) and probabilistic models ( <ref type="bibr" target="#b23">Quattoni et al., 2007)</ref>. <ref type="bibr" target="#b25">Samdani et al. (2012)</ref> studied various vari- ants of EM algorithm and showed that all of them are special cases of a unified framework. Our gen- eralized update framework is similar in spirit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a general update equa- tion from semantic parsing from denotation and propose a policy shaping method for addressing the spurious program challenge. For the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For example, previous work have utilized maximum marginal likelihood (Krishna- murthy et al., 2017; Guu et al., 2017), reinforce- ment learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017). It could be difficult to choose the suitable algorithm from these options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Question : what nation scored the most points Index</head><label>Question</label><figDesc></figDesc><table>Name 
Nation Points Games Pts/game 

1 
Karen Andrew 
England 
44 
5 
8.8 

2 
Daniella Waterman England 
40 
5 
8 

3 
Christelle Le Duff 
France 
33 
5 
6.6 

4 
Charlotte Barras 
England 
30 
5 
6 

5 
Naomi Thomas 
Wales 
25 
5 
5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Question : what nation scored the most points Index</head><label>Question</label><figDesc></figDesc><table>Name 
Nation Points Games Pts/game 

1 
Karen Andrew 
England 
44 
5 
8.8 

2 
Daniella Waterman England 
40 
5 
8 

3 
Christelle Le Duff 
France 
33 
5 
6.6 

4 
Charlotte Barras 
England 
30 
5 
6 

5 
Naomi Thomas 
Wales 
25 
5 
5 

Select Nation Where Pts/game is Maximum 

Select Nation Where Index is Minimum 

Select Nation Where Points = 44 

Select Nation Where Points is Maximum 

Select Nation Where Name = Karen Andrew 

Programs: 
Program: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on different model update algorithms, with and without policy shaping. 

w 
q 
Dev 
MMR 
MML 41.9 
Off-Policy Policy Gradient MMR 37.0 
MMR 
MMR 40.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The dev set results on the new variations of the update algorithms.</figDesc><table></table></figure>

			<note place="foot" n="1"> This transformation preserves the answer of the question.</note>

			<note place="foot" n="2"> This is essentially a contextual bandit setting. Guu et al. (2017) also used this setting. A general reinforcement learning setting requires taking a sequence of actions and receiving a reward for each action. For example, a program can be viewed as a sequence of parsing actions, where each action can get a reward. We do not consider the general setting here.</note>

			<note place="foot" n="3"> See Appendix for the detailed derivation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We thank Ryan Benmalek, Alane Suhr, Yoav Artzi, Claire Cardie, Chris Quirk, Michel Galley and members of the Cornell NLP group for their valuable comments. We are also grateful to Allen Institute for Artificial Intelligence for the com-puting resource support. This work was initially started when the first author interned at Microsoft Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Policy shaping: Integrating human feedback with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Lee</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">Lockerd</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1051" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured perceptron with inexact search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical analysis of optimization for max-margin nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global neural CCG parsing with optimality guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2366" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Computational Linguistics</title>
		<meeting>the Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mapping instructions and visual observations to actions with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.08795" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural shiftreduce CCG semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Environment-driven lexicon induction for high-level instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejia</forename><surname>Kumar Dipendra Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">English as a formal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language understanding for textbased games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Louis-Philippe Morency, Morency Collins, and Trevor Darrell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sybor</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hidden conditional random fields. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deterministic annealing for clustering, compression, classification, regression, and related optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2210" to="2239" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unified expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max-margin parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online large-margin training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning semantic grammars with constructive inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Seq2SQL: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
