<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
							<email>anthony.rios1@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Division of Biomedical Informatics</orgName>
								<orgName type="institution" key="instit1">University of Kentucky Lexington</orgName>
								<orgName type="institution" key="instit2">University of Kentucky Lexington</orgName>
								<address>
									<region>KY, KY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
							<email>ramakanth.kavuluru@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Division of Biomedical Informatics</orgName>
								<orgName type="institution" key="instit1">University of Kentucky Lexington</orgName>
								<orgName type="institution" key="instit2">University of Kentucky Lexington</orgName>
								<address>
									<region>KY, KY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3132" to="3142"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3132</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few-and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper , we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few-and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2% and 4.8% in R@10 for MIMIC II and MIMIC III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3% and 19%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike in binary or multi-class problems, for multi-label classification a model assigns a set of labels to each input instance ( <ref type="bibr" target="#b32">Tsoumakas et al., 2010)</ref>. Large-scale multi-label text classification problems can be found in several domains. For example, Wikipedia articles are annotated with labels used to organize documents and facilitate search ( <ref type="bibr" target="#b16">Partalas et al., 2015)</ref>. Biomedical arti- cles indexed by the PubMed search engine are manually annotated with medical subject head- ings ( <ref type="bibr" target="#b31">Tsatsaronis et al., 2012</ref>). In healthcare fa- cilities, medical records are assigned a set of standardized codes for billing purposes <ref type="bibr">(NCHS, 1978)</ref>. Automatically annotating tweets with hashtags, while the labels are not fixed, can also be represented as a large-scale multi-label classifi- cation problem <ref type="bibr" target="#b36">(Weston et al., 2014</ref>). There are two major difficulties when devel- oping machine learning methods for large-scale multi-label text classification problems. First, the documents may be long, sometimes containing more than a thousand words ( <ref type="bibr">Mullenbach et al., 2018)</ref>. Finding the relevant information in a large document for a specific label results in needle in a haystack situation. Second, data sparsity is a com- mon problem; as the total number of labels grows, a few labels may occur frequently, but most labels will occur infrequently. <ref type="bibr" target="#b24">Rubin et al. (2012)</ref> refer to datasets that have long-tail frequency distributions as "power-law datasets". Methods that predict in- frequent labels fall under the paradigm of few-shot classification which refers to supervised methods in which only a few examples, typically between 1 and 5, are available in the training dataset for each label. With predefined label spaces, some labels may never appear in the training dataset. Zero- shot problems extend the idea of few-shot classi- fication by assuming no training data is available for the labels we wish to predict at test time. In this paper, we explore both of these issues, long documents and power-law datasets, with an em- phasis on analyzing the few-and zero-shot aspects of large-scale multi-label problems.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we plot the label frequency distri- bution of diagnosis and procedure labels for the entire MIMIC III <ref type="bibr" target="#b8">(Johnson et al., 2016</ref>) dataset. A few labels occur more than 10,000 times, around 5,000 labels occur between 1 and 10 times, and of the 17,000 diagnosis and procedure labels, more than 50% never occur. There are a few reasons a label may never occur in the training dataset. In healthcare, sevearl disorders are rare; there- fore corresponding labels may not have been ob- served yet in a particular clinic. Sometimes new labels may be introduced as the field evolves lead- ing to an emerging label problem. This is intu- itive for applications such as hashtag prediction on Twitter. For example, last year it would not have made sense to annotate tweets with the hashtag #EMNLP2018. Yet, as this year's conference ap- proaches, labeling tweets with the #EMNLP2018 will help users find relevant information.</p><p>Infrequent labels may not contribute heavily to the overall accuracy of a multi-label model, but in some cases, correct prediction of such labels is crucial but not straightforward. For example, in assigning diagnosis labels to EMRs, it is im- portant that trained human coders are both accu- rate and thorough. Errors may cause unfair finan- cial burden on the patient. Coders may have an easier time assigning frequent labels to EMRs be- cause they are encountered more often. Also, fre- quent labels are generally easier to predict using machine-learning based methods. However, infre- quent or obscure labels will be easily confused or missed causing billing mistakes and/or causing the coders to spend more time annotating each record. Thus, we believe methods that handle infrequent and unseen labels in the multi-label setting are im- portant.</p><p>Current evaluation methods for large-scale multi-label classification mostly ignore infrequent and unseen labels. Popular evaluation measures focus on metrics such as micro-F1, recall at k (R@k), precision at k (P@k), and macro-F1. As it is well-known that micro-F1 gives more weight to frequent labels, papers on this topic also report macro-F1, the average of label-wise F1 scores, which equally weights all labels. Unfortunately, macro-F1 scores are generally low and the corre- sponding performance differences between meth- ods are small. Moreover, it is possible to im- prove macro-F1 by only improving a model's per- formance on frequent labels, further confounding its interpretation. Hence we posit that macro-F1 is not enough to compare large-scale multi-label learning methods on infrequent labels and it does not directly evaluate zero-shot labels. Here, we take a step back and ask: can the model predict the correct few-shot (zero-shot) labels from the set of all few-shot (zero-shot) labels? To address this, we test our approach by adapting the general- ized zero-shot classification evaluation methodol- ogy by <ref type="bibr" target="#b37">Xian et al. (2017)</ref> to the multi-label setting.</p><p>In this paper, we propose and evaluate a neural architecture suitable for handling few-and zero- shot labels in the multi-label setting where the out- put label space satisfies two constraints: (1). the labels are connected forming a DAG and (2). each label has a brief natural language descriptor. These assumptions hold in several multi-label scenar- ios including assigning diagnoses/procedures to EMRs, indexing biomedical articles with medical subject headings, and patent classification. Tak- ing advantage of this prior knowledge on labels is vital for zero-shot prediction. Specifically, using the EMR coding use-case, we make the following contributions:</p><p>1. We overcome issues arising from processing long documents by introducing a new neural architecture that expands on recent attention- based CNNs (ACNNs ( <ref type="bibr">Mullenbach et al., 2018)</ref>). Our model learns to predict few-and zero-shot labels by matching discharge sum- maries in EMRs to feature vectors for each label obtained by exploiting structured label spaces with graph CNNs (GCNNs ( <ref type="bibr" target="#b13">Kipf and Welling, 2017)</ref>).</p><p>2. We provide a fine-grained evaluation of state- of-the-art EMR coding methods for frequent, few-shot, and zero-shot labels. By evaluating power-law datasets using an extended gen- eralized zero-shot methodology that also in- cludes few-shot labels, we present a nuanced analysis of model performance on infrequent labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Large-Scale Text Classification. Linear meth- ods have been successfully applied to large-scale problems ( <ref type="bibr" target="#b30">Tang et al., 2009;</ref><ref type="bibr">Papanikolaou et al., 2015;</ref><ref type="bibr" target="#b21">Rios and Kavuluru, 2015)</ref>. For traditional micro-and macro-F1 measures, <ref type="bibr" target="#b30">Tang et al. (2009)</ref> show that linear methods suffer using naive thresh-olding strategies because infrequent labels gener- ally need a smaller threshold. Generative models have also been promising for datasets with many labels ( <ref type="bibr" target="#b24">Rubin et al., 2012)</ref>. Intuitively, by us- ing a prior distribution over the label space, in- frequent labels can be modeled better. Finally, large-scale classification is also pursued as "ex- treme classification" ( <ref type="bibr" target="#b39">Yu et al., 2014;</ref><ref type="bibr" target="#b4">Bhatia et al., 2015)</ref> where the focus is on ranking measures that ignore infrequent labels. Neural networks (NNs) perform well for many small-scale classification tasks <ref type="bibr" target="#b11">(Kim, 2014;</ref><ref type="bibr" target="#b10">Kalchbrenner et al., 2014</ref>). Re- cently, researchers have been exploring NN meth- ods for large-scale problems. <ref type="bibr" target="#b38">Yang et al. (2016)</ref> develop a hierarchical attentive NN for datasets with over a million documents, but their datasets contain few labels. <ref type="bibr">Nam et al. (2014)</ref> show that feed-forward NNs can be successfully applied to large-scale problems through the use of a multi- label binary cross-entropy loss function. <ref type="bibr" target="#b33">Vani et al. (2017)</ref> introduce a grounded recurrent neural net- work (RNN) that iteratively updates its predictions as it processes a document word-by-word. <ref type="bibr" target="#b3">Baumel et al. (2018)</ref> experiment with both CNNs and RNNs for medical coding. Finally, <ref type="bibr">Mullenbach et al. (2018)</ref> expand on prior ACNNs ( <ref type="bibr" target="#b38">Yang et al., 2016;</ref><ref type="bibr" target="#b1">Allamanis et al., 2016</ref>) to develop a label- wise attention framework where the most infor- mative ngrams are extracted for each label in the dataset. Our attention mechanism extends their work to the zero-shot setting.</p><p>Few-Shot and Zero-Shot Learning. While neural networks are generally considered to need large datasets, they have been shown to work well on few-shot classification tasks. To handle in- frequent labels, most NN methods use a k-NN- like approach. Siamese NNs ( <ref type="bibr" target="#b14">Koch et al., 2015</ref>) learn a nonlinear distance metric using a pair- wise loss function. Matching networks ( <ref type="bibr" target="#b34">Vinyals et al., 2016)</ref> introduce an instance-level attention method to find relevant neighbors. Prototypical Networks ( <ref type="bibr" target="#b28">Snell et al., 2017</ref>) average all instances in each class to form "prototype label vectors" and train using a traditional cross-entropy loss.</p><p>In our prior work (Rios and Kavuluru, 2018), we combine matching networks with a sophisticated thresholding strategy. However, in Rios and Kavu- luru (2018) we did not explore the few-and zero- shot settings. Zero-shot learning has not been widely ex- plored in the large-scale multi-label classification scenario. Like neural few-shot methods, neural zero-shot methods use a matching framework. In- stead of matching input instances with other in- stances, they are matched to predefined label vec- tors. For example, the Attributes and Animals Dataset ( <ref type="bibr" target="#b37">Xian et al., 2017</ref>) contains images of an- imals and the label vectors consist of features de- scribing the types of animals (e.g., stripes: yes). When feature vectors for labels are not available, the average of the pretrained word embeddings of the class names have been used. The attribute la- bel embedding method ( <ref type="bibr" target="#b0">Akata et al., 2016</ref>) uses a pairwise ranking loss to match zero-shot label vec- tors to instances. Romera-Paredes and Torr <ref type="formula" target="#formula_1">(2015)</ref> introduced the "embarrassingly simple zero-shot learning" (ESZSL) method which is trained us- ing a mean squared error loss. A few zero-shot methods do not translate well to multi-label prob- lems. CONSE ( <ref type="bibr">Mikolov et al., 2013</ref>) averages the embeddings for the top predicted supervised label vectors to match to zero-shot label vectors. CONSE assumes that both supervised and zero- shot labels cannot be assigned to the same in- stance. In this paper, we expand on the gen- eralized zero-shot evaluation methodology intro- duced by <ref type="bibr" target="#b37">Xian et al. (2017)</ref> to large-scale multi- label classification. Finally, it is important to note that zero-shot classification has been previously studied in the multi-label setting <ref type="bibr">(Mensink et al., 2014</ref>). However, they focus on image classifica- tion and use datasets with around 300 labels.</p><p>Graph Convolutional Neural Networks. GC- NNs generalize CNNs beyond 2d and 1d spaces. <ref type="bibr" target="#b7">Defferrard et al. (2016)</ref> developed spec- tral methods to perform efficient graph convolu- tions. <ref type="bibr" target="#b13">Kipf and Welling (2017)</ref> assume a graph structure is known over input instances and ap- ply GCNNs for semi-supervised learning. GCNNs are applied to relational data (e.g., link prediction) by <ref type="bibr" target="#b26">Schlichtkrull et al. (2018)</ref>. GCNNs have also had success in other NLP tasks such as semantic role labeling , de- pendency parsing <ref type="bibr" target="#b29">(Strubell and McCallum, 2017)</ref>, and machine translation ( <ref type="bibr" target="#b2">Bastings et al., 2017)</ref>.</p><p>There are three GCNN papers that share simi- larities with our work. (i) <ref type="bibr">Peng</ref>  All ICD-9 Descriptors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICD-9 Predictions</head><p>Label-wise Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution</head><p>Layer Output Layer <ref type="figure">Figure 2</ref>: This figure provides a visual overview of our method. Intuitively, our method has two main components. The first component is a CNN that operates operates on the EMRs. The other component is a 2-layer GCNN which creates the label-specific attention vectors and label-vectors used for ranking using ICD-9 descriptions as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-Layer GCNN</head><formula xml:id="formula_0">⎧ ⎨ ⎩ vi ⎧ ⎨ ⎩ v i 2 ⎧ ⎨ ⎩ D ⎧ ⎨ ⎩ ci</formula><p>experiments focus on smaller label spaces and do not handle/assess zero-shot and few-shot labels. Also, their experiments for text classification do not incorporate attention and simply use an aver- age of word vectors to represent each document. (iii)  propose a zero-shot GCNN image classification method for structured multi- -class problems. We believe their method may transfer to the multi-label text classification setting but exact modifications to affect that are not clear (i.e., their semi-supervised approach may not be directly applicable). Likewise, porting to text is nontrivial for long documents. <ref type="figure">Figure 2</ref> shows the overall schematic of our archi- tecture. Intuitively, we incorporate four main com- ponents. First, we assume we have the full English descriptor/gloss for each label we want to predict. We form a vector representation for each label by averaging the word embeddings for each word in its descriptor. Second, the label vectors formed from the descriptor are used as attention vectors (label-wise attention) to find the most informative ngrams in the document for each label. For each label, this will produce a separate vector repre- sentation of the input document. Third, the label vectors are passed through a two layer GCNN to incorporate hierarchical information about the la- bel space. Finally, the vectors returned from the GCNN are matched to the document vectors to generate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Convolutional Neural Network. Contrary to prior CNN methods for text <ref type="bibr" target="#b11">(Kim, 2014)</ref>, instead of using a max-over-time pooling layer, we learn to find relevant ngrams in a document for each label via label-wise attention ( <ref type="bibr">Mullenbach et al., 2018)</ref>. The CNN will return a document feature matrix D ∈ R (n−s+1)×u where each column of D is a feature map, u is the total number of convolu- tion filters, n is the number of words in the docu- ment, and s is the width of convolution filters.</p><p>Label Vectors. To be able to predict labels that were not in the training dataset, we avoid learn- ing label specific parameters. We use the label descriptors to generate a feature vector for each la- bel. First, to preprocess each descriptor, we lower- case all words and remove stop-words. Next, each label vector is formed by averaging the remaining words in the descriptor</p><formula xml:id="formula_1">v i = 1 |N | ∑ j∈N w j , i = 1, . . . , L,<label>(1)</label></formula><p>where v i ∈ R d , L is the number of labels, and N is the index set of the words in the descriptor. Prior zero-shot work has focused on projecting input in- stances into the same semantic space as the label vectors <ref type="bibr" target="#b25">(Sandouk and Chen, 2016)</ref>. For zero-shot image classification, this is a non-trivial task. Be- cause we work with textual data, we simply share the word embeddings between the convolutional layer and the label vector creation step to form v i .</p><p>Label-Wise Attention. Similar to the work by <ref type="bibr">Mullenbach et al. (2018)</ref>, we employ label-wise attention to avoid the needle in the haystack situa- tion encountered with long documents. The issue with simply using a single attention vector or using max-pooling is that we assume a single vector can capture everything required to predict every label. For example, with a single attention, we would only look at one spot in the document and assume that spot contains the relevant information needed to predict all labels. In the multi-class setting, this assumption is plausible. However, for large multi- label problems, the relevant information for each label may be scattered throughout the document -the problem is worse when the documents are very long. Using label-wise attention, our model can focus on different sections. We also need to find relevant information for zero-shot classes. So we use the label vectors v i rather than learning la- bel specific attention parameters. First, we pass the document feature matrix D through a simple feed-forward neural network</p><formula xml:id="formula_2">D 2 = tanh(D W b + b b )</formula><p>where W b ∈ R u×d and b b ∈ R d . This mapping is important because the dimensionality of the ngram vectors (rows) in D depends on u, the number of scores we generate for each ngram. Given D 2 , we generate the label-wise attention vector</p><formula xml:id="formula_3">a i = sof tmax(D 2 v i ), i = 1, . . . , L,<label>(2)</label></formula><p>where a i ∈ R n−s+1 measures how informative each ngram is for the i-th label. Finally, we use D, and generate L label-specific document vector representations</p><formula xml:id="formula_4">c i = a T i D, i = 1, . . . , L,</formula><p>such that c i ∈ R u . Intuitively, c i is the weighted average of the rows in D forming a vector repre- sentation of the document for the i-th label.</p><p>GCNN Output Layer. Traditionally, the output layer of a CNN would learn label specific param- eters optimized via a cross-entropy loss. Instead, our method attempts to match documents to their corresponding label vectors. In essence, this be- comes a retrieval problem. Before using each doc- ument representation c i to score its corresponding label, we take advantage of the structured knowl- edge we have over our label space using a 2-layer GCNN. For both the MIMIC II and MIMIC III datasets, this information is hierarchical. A snip- pet of the hierarchy can be found in <ref type="figure">Figure 2</ref>. Starting with the label vectors v i , we combine the label vectors of the children and parents for the i-th label to form</p><formula xml:id="formula_5">v 1 i = f (W 1 v i + ∑ j∈Np W 1 p v j |N p | + ∑ j∈Nc W 1 c v j |N c | +b 1 g )</formula><p>where</p><formula xml:id="formula_6">W 1 ∈ R q×d , W 1 p ∈ R q×d , W 1 c ∈ R q×d , b 1 g ∈ R q</formula><p>, f is the rectified linear unit <ref type="bibr">(Nair and Hinton, 2010</ref>) function, and N c (N p ) is the index set of the i-th label's children (parents). We use different parameters to distinguish each edge type. In this paper, given we only deal with hierarchies, the edge types include edges from parents, from children, and self edges. This can be adapted to arbitrary DAGs, where parent edges represent all incoming edges and the child edges represent all outgoing edges for each node.</p><p>The second layer follows the same formulation as the first layer with</p><formula xml:id="formula_7">v 2 i = f (W 2 v 1 i + ∑ j∈Np W 2 p v 1 j |N p | + ∑ j∈Nc W 2 c v 1 j |N c | +b 2 g )</formula><p>where W 2 ∈ R q×q , W 2 p ∈ R q×q , W 2 c ∈ R q×q , and b 2 g ∈ R q . Next, we concatenate both the av- eraged description vector (from equation <ref type="formula" target="#formula_1">(1)</ref>) with the GCNN label vector to form</p><formula xml:id="formula_8">v 3 i = v i || v 2 i ,</formula><p>where v 3 i ∈ R d+q . Now, to compare the final label vector v 3 i with its document vector c i , we trans- form the document vector into</p><formula xml:id="formula_9">e i = ReLU (W o c i + b o ), i = 1, . . . , L,</formula><p>where W o ∈ R (q+d)×u and b o ∈ R q+d . This transformation is required to match the dimension to that of v 3 i . Finally, the prediction for each label i is generated viâ</p><formula xml:id="formula_10">viâ y i = sigmoid(e T i v 3 i ), i = 1, . . . , L.</formula><p>During experiments, we found that using either the output layer GCNN or a separate GCNN for the attention vectors (equation <ref type="formula" target="#formula_3">(2)</ref>) did not result in an improvement and severely slowed convergence.</p><p>Training. We train our model using a multi- label binary cross-entropy loss <ref type="bibr">(Nam et al., 2014</ref>)</p><formula xml:id="formula_11">L = L ∑ i=1 [ − y i log(ˆ y i ) − (1 − y i ) log(1 − ˆ y i ) ] ,</formula><p>where y i ∈ {0, 1} is the ground truth for the i-th label andˆyandˆ andˆy i is our sigmoid score for the i-th label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this paper, we use two medical datasets for evaluation purposes: MIMIC II (Jouhet et al., 2012) and MIMIC III ( <ref type="bibr" target="#b8">Johnson et al., 2016)</ref>. Both datasets contain discharge summaries anno- tated with a set of ICD-9 diagnosis and proce- dure labels. Discharge summaries are textual doc- uments consisting of, but not limited to, physi- cian descriptions of procedures performed, diag- noses made, the patient's medical history, and dis- charge instructions. Following a generalized zero- shot learning evaluation methodology ( <ref type="bibr" target="#b37">Xian et al., 2017)</ref>, we split the ICD-9 labels into three groups based on frequencies in the training dataset: The frequent group S that contains all labels that oc- cur &gt; 5 times, the few-shot group F that contains labels that occur between 1 and 5 times, and the zero-shot group Z of labels that never occur in the training dataset, but occur in the test/dev sets. The groups are only used for evaluation. That is, dur- ing training, systems are optimized over all labels simultaneously. Instances that do not contain few- or zero-shot classes are removed from their re- spective groups during evaluation. This grouping is important to assess how each model performs across labels grouped by label frequency. Our evaluation methodology differs from that of <ref type="bibr" target="#b37">Xian et al. (2017)</ref> in two ways. First, because each in- stance is labeled with multiple labels, the same instance can appear in all groups -S, F, and Z. Second, instead of top-1 accuracy or HIT@k eval- uation measures, we focus on R@k to handle mul- tiple labels. At a high level, we want to examine whether a model can distinguish the correct few- shot (zero-shot) labels from the set of all few-shot (zero-shot) labels. Therefore, the R@k measures in <ref type="table" target="#tab_3">Tables 2 and 3</ref>, and <ref type="figure" target="#fig_1">Figure 3</ref> are computed rela- tive to each group.</p><p>Evaluation Measures. The overall statistics for these two datasets are reported in (2013). Following the procedures in <ref type="bibr" target="#b18">Perotte et al. (2013)</ref> and <ref type="bibr" target="#b33">Vani et al. (2017)</ref>, for each diagnosis and procedure label assigned to each medical re- port, we add its parents using the ICD-9 hierarchy. Each report in MIMIC II is annotated with nearly 37 labels on average using hierarchical label ex- pansion. MIMIC III does not contain a standardized training/test split. Therefore, we create our own split that ensures the same patient does not appear in both the training and test datasets. Unlike the MIMIC II dataset, we do not augment the labels using the ICD-9 hierarchy. The ICD-9 hierarchy has three main levels. For MIMIC III, level 0 la- bels make up about 5% of all occurrences, level 1 labels make up about 62%, and level 2 (leaf level) labels make up about 33%. Also, each MIMIC III instance contains16 ICD-9 labels on average.</p><p>ICD-9 Structure and Descriptors. The Inter- national Classification of Diseases (ICD) contains alphanumeric diagnosis and procedure codes that are used by hospitals to standardize their billing practices. In the following experiments, we use the 9th edition of the ICD 1 . Each ICD-9 identifier contains between 3 to 5 alphanumeric characters of the form abc.xy. The alphanumeric structure defines a simple hierarchy over all ICD-9 codes. For example, "systolic heart failure" (428.2) and "diastolic heart failure" (428.3) are both children of the "heart failure" code 428. Furthermore, se- quential codes are grouped together. For instance, numeric codes in the range 390-459 contain "Dis- eases of the Circulatory System". Furthermore, each code, including groups of codes , contain short descriptors, where the average de- scriptor length contains seven words 2 . In this work, we use both the group descriptors and in- <ref type="bibr">1</ref> The US transitioned from ICD-9 to ICD-10 in 2015. Un- fortunately, at the time of publication, large publicly available ICD-10 EMR datasets are unavailable. <ref type="bibr">2</ref> The descriptors and hierarchy used in this paper can be found at https://bioportal.bioontology.org/ ontologies/ICD9CM   dividual descriptors as input to the GCNN. At test time, we ignore the group codes.</p><p>Implementation Details. For the CNN com- ponent of our model, we use 300 convolution filters with a filter size of 10. We use 300 dimen- sional word embeddings pretrained on PubMed biomedical article titles and abstracts. To avoid overfitting, we use dropout directly after the em- bedding layer with a rate of 0.2. For training we use the ADAM ( <ref type="bibr" target="#b12">Kingma and Ba, 2015</ref>) optimizer with a minibatch size of 8 and a learning rate of 0.001. q, the GCNN hidden layer size, is set to 300. The code for our method is available at https://github.com/bionlproc/ multi-label-zero-shot.</p><p>Thresholding has a large influence on traditional multi-label evaluation measures such as micro-F1 and macro-F1 ( <ref type="bibr" target="#b30">Tang et al., 2009</ref>). Hence, we re- port both recall at k (R@k) and precision at k (P@k) which do not require a specific threshold. R@k is preferred for few-and zero-shot labels, because P@k quickly goes to zero as k increases and gets bigger than the number of group specific labels assigned to each instance. Furthermore, for medical coding, these models are typically used as a recommendation engine to help coders. Unless a label appears at the top of the ranking, the anno- tator will not see it. Thus, ranking metrics better measure the usefulness of our systems.</p><p>Baseline Methods. For the frequent and few- shot labels we compare to state-of-the-art meth- ods on the MIMIC II and MIMIC III datasets in- cluding ACNN ( <ref type="bibr">Mullenbach et al., 2018</ref>) and a CNN method introduced in <ref type="bibr" target="#b3">Baumel et al. (2018)</ref>. We also compare with the L1 regularized logistic regression model used in <ref type="bibr" target="#b33">Vani et al. (2017)</ref>. Fi- nally, we compare against our prior EMR coding method, Match-CNN ( <ref type="bibr" target="#b22">Rios and Kavuluru, 2018</ref>  <ref type="table">Table 4</ref>: P@k, R@k, and macro-F1 results over all la- bels (the union of S, F, and Z).</p><p>For zero-shot learning, we compare our results with ESZSL <ref type="bibr" target="#b23">(Romera-Paredes and Torr, 2015)</ref>. To use ESZSL, we must specify feature vectors for each label. For zero-shot methods, the label vectors used are crucial regardless of the learning method used. Therefore, we evaluate ESZSL with three different sets of label vectors. We average 200 dimensional ICD-9 descriptor word embed- dings generated by <ref type="bibr" target="#b19">Pyysalo et al. (2013)</ref> which are pretrained on PubMed, Wikipedia, and PubMed Central (ESZSL + W2V). We lowercased descrip- tors and removed stop-words. We also compare with label vectors derived from our own 300 di- mensional embeddings (ESZSL + W2V 2) pre- trained on PubMed indexed titles and abstracts. Finally, we generate label vectors using the ICD-9 hierarchy. Specifically, let Y ∈ R N ×L be the doc- ument label matrix where N is the total number of documents. We factorize Y into two matrices U ∈ R N ×300 and V ∈ R 300×L using graph reg- ularized alternating least squares (GRALS) ( <ref type="bibr" target="#b20">Rao et al., 2015</ref>). Finally, we also report a baseline using a random ordering on labels, which is im- portant for zero-shot labels -because the total number of such labels is small, the chance that the correct label is in the top k is higher compared to few-shot and frequent labels.</p><p>We compare two variants of our method: zero- shot attentive GCNN (ZAGCNN), which is the full method described in Section 3 and a simpler vari- ant without the GCNN layers, zero-shot attentive CNN (ZACNN) <ref type="bibr">3</ref> .</p><p>Results. <ref type="table" target="#tab_3">Table 2</ref> shows the results for MIMIC II. Because the label set for each medical record is augmented using the ICD-9 hierarchy, we expect methods that use the hierarchy to have an advan- tage. <ref type="table" target="#tab_3">Table 2</ref> results do not rely on thresholding because we evaluate using the relative ranking of groups with similar frequencies. ACNN performs best on frequent labels. For few-shot labels, ZA- GCNN outperforms ACNN by over 10% in R@10 and by 8% in R@5; compared to these R@k gains for few-shot labels, our loss on frequent labels is minimal (&lt; 1%). We find that the word embed- ding derived label vectors work best for ESZSL on zero-shot labels. However, this setup is out- performed by GRALS derived label vectors on the frequent and few-shot labels. On zero-shot labels, ZAGCNN outperforms the best ESZSL variant by over 16% for both R@5 and R@10. Also, we find that the GCNN layers help both few-and zero- shot labels. Finally, similar to the setup in <ref type="bibr" target="#b37">Xian et al. (2017)</ref>, we also compute the harmonic av- erage across all R@5 and all R@10 scores. The metric is only computed for methods that can pre- dict zero-shot classes. We find that ZAGCNN out- performs ZACNN by 4% for R@10. We report the MIMIC III results in <ref type="table" target="#tab_4">Table 3</ref>. Unlike for MIMIC II, the label sets were not ex- panded using the ICD-9 hierarchy. Yet, we find substantial improvements on both few-and zero- shot labels using a GCNN. ZAGCNN outperforms ACNN by almost 5% and ZACNN by 1% in R@10 on few-shot classes. However, ACNN still outperforms all other methods on frequent labels, but by only 0.3% when compared with ZAGCNN. For zero-shot labels, ZAGCNN outperforms ZA- CNN by over 5% and outperforms the best ES- ZSL method by nearly 20% in R@10. We find that ZACNN slightly underperforms ZAGCNN on frequent labels with more prominent differences showing up for infrequent labels.</p><p>In <ref type="table">Table 4</ref> we compare the P@10, R@10, and macro-F1 measures across all three groups (the union of S, F , and Z) on the MIMIC III dataset. We emphasize that the evaluation metrics are cal- culated over all labels and are not averages of the metrics computed independently for each group. We find that R@10 is nearly equivalent to the R@10 on the frequent group in <ref type="table" target="#tab_4">Table 3</ref>. Further- more, we find that ACNN outperforms ZAGCNN in P@10 by almost 4%. To compare all meth- ods with respect to macro-F1, we simply threshold each label at 0.5. Both R@k and P@k give more weight to frequent labels, thus it is expected that ACNN outperforms ZAGCNN for frequent labels. However, we also find that ACNN outperforms our methods with respect to Macro-F1. Given macro-F1 equally weights all labels, does the higher macro score mean ACNN performs bet- ter across infrequent labels? In <ref type="figure" target="#fig_1">Figure 3</ref>, we plot the MIMIC III R@k for the neural methods with k ranging from 1 to 100. We find as k increases, the differences between ZAGCNN and ACNN be- come more evident. Given <ref type="figure" target="#fig_1">Figure 3</ref> and the scores in <ref type="table" target="#tab_4">Table 3</ref>, it is clear that ACNN does not per- form better than ZAGCNN with respect to few- and zero-shot labels. The improvement in macro- F1 for ACNN is because it performs better on fre- quent labels. In general, infrequent labels will have scores much less than 0.5. If we rank all labels (S ∪ F ∪ Z), we find that few-shot labels only occur among the top 16 ranked labels (aver- age number of labels for MIMIC III) for 6% of the test documents that contain them. This suggests that many frequent irrelevant labels have higher scores than the correct few-shot label.</p><p>Why do the rankings among few-and zero-shot labels matter if they are rarely ranked above irrel- evant frequent labels? If we can predict which in- stances contain infrequent labels (novelty detec- tion), then we can help human coders by provid- ing them with multiple recommendation lists -a list of frequent labels and a list of infrequent/zero- shot labels. Also, while we would ideally want a single method that performs best for both frequent and infrequent labels, currently we find that there is a trade-off between them. Hence it may be rea- sonable to use different methods in combination depending on label frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we performed a fine-grained evalu- ation of few-and zero-shot label learning in the large-scale multi-label setting. We also introduced a neural architecture that incorporates label de- scriptors and the hierarchical structure of the label spaces for few-and zero-shot prediction. For these infrequent labels, previous evaluation methodolo- gies do not provide a clear picture about what works. By evaluating power-law datasets using a generalized zero-shot learning methodology, we provide a staring point toward a better understand- ing. Our proposed architecture also provides large improvements on infrequent labels over state-of- the-art automatic medical coding methods.</p><p>We believe there are two important avenues for future work.</p><p>1. For medical coding, a wealth of unstructured domain expertise is available in biomedical research articles indexed by PubMed. These articles are annotated with medical subject headings (MeSH terms), which are organized in a hierarchy. Relationships between MeSH terms and ICD-9 codes are available in Uni- fied Medical Language System (UMLS ( <ref type="bibr" target="#b5">Bodenreider, 2004)</ref>). If we can take advantage of all this structured and unstructured infor- mation via methods such as transfer learning or multi-task learning, then we may be able to predict infrequent labels better.</p><p>2. For our method to be useful for human coders, it is important to develop an accurate novelty detector. We plan to study methods for determining if an instance contains an in- frequent label and if it does, how many in- frequent labels it should be annotated with. In essence, this is an extension of the Meta- Labeler ( <ref type="bibr" target="#b30">Tang et al., 2009</ref>) methodology and open classification ( <ref type="bibr" target="#b27">Shu et al., 2017)</ref>. If we can predict if an instance contains infrequent labels, then we can recommend few-and zero-shot labels only when necessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: This plot shows the label frequency distribution of ICD-9 codes in MIMIC III.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: This graph plots the MIMIC III R@k for fewshot (F) labels at different k values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>For 
reproducibility purposes, we use the same train-
ing/test splits of the MIMIC II as Perotte et al. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>MIMIC II results across frequent (S), few-shot (F), and zero-shot (Z) groups. We mark prior methods for 
MIMIC datasets that we implemented with a *. 

S 
F 
Z 
Harmonic Average 
R@5 
R@10 
R@5 
R@10 
R@5 
R@10 
R@5 
R@10 

Random 
0.000 
0.000 
0.000 
0.000 
0.038 
0.052 
0.000 
0.000 

Logistic (Vani et al., 2017) * 
0.273 
0.427 
0.014 
0.014 
-
-
-
-
CNN (Baumel et al., 2018) * 
0.269 
0.413 
0.058 
0.085 
-
-
-
-
ACNN (Mullenbach et al., 2018) * 
0.288 
0.458 
0.130 
0.168 
-
-
-
-
Match-CNN (Rios and Kavuluru, 2018) 
0.278 
0.426 
0.049 
0.060 
-
-
-
-

ESZSL + W2V 
0.135 
0.191 
0.031 
0.051 
0.157 
0.257 
0.065 
0.105 
ESZSL + W2V 2 
0.127 
0.189 
0.031 
0.048 
0.148 
0.305 
0.063 
0.102 
ESZSL + GRALS 
0.256 
0.393 
0.033 
0.060 
0.076 
0.138 
0.064 
0.114 

ZACNN 
0.278 
0.435 
0.152 
0.195 
0.364 
0.442 
0.232 
0.310 
ZAGCNN 
0.283 
0.445 
0.166 
0.216 
0.428 
0.495 
0.252 
0.337 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>MIMIC III results across frequent (S), few-shot (F), and zero-shot (Z) groups. We mark prior methods for 
MIMIC datasets that we implemented with a *. 

</table></figure>

			<note place="foot" n="3"> We name our methods with the &quot;zero-shot&quot; prefix because they are primarily designed for such scenarios, although as we show later that these methods are effective for both few-shot and frequent labels</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to the outstanding reviewers who provided invaluable insights to improve our manuscript. This research is supported by the U.S. National Li-brary of Medicine through grant R21LM012274. We also gratefully acknowledge the support of the NVIDIA Corporation for its donation of the Titan X Pascal GPU used for this research. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label classification of patient notes a case study on icd code assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jumana</forename><surname>Nassour-Kassis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Joint Workshop on Health Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph convolutional networks for classification with a structured label space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04908</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Scientific data</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated classification of free-text pathology reports for registration of incident cases of cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jouhet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burgun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le Beux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levillain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ingrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Claveau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="251" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modeling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">LSHTC: A benchmark for large-scale text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Thierryartì Eres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih-Reza</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallinari</surname></persName>
		</author>
		<idno>abs/1503.08581</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjiao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Wide Web Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diagnosis code assignment: models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rimma</forename><surname>Adler Perotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Pivovarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LBM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing the moving parts of a large-scale multi-label text classification pipeline: Experiences in indexing biomedical articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Healthcare Informatics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emr coding with semi-parametric multi-head matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2081" to="2091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Statistical topic models for multi-label document classification. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><surname>Timothy N Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="157" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-label zeroshot learning via concept embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ubai</forename><surname>Sandouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00282</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 15th European Semantic Web Conference</title>
		<meeting>15th European Semantic Web Conference</meeting>
		<imprint>
			<publisher>ESWC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Doc: Deep open classification of text documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2911" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dependency parsing with dilated iterated graph CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Structured Prediction for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large scale multi-label classification via metalabeler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Suju Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on World wide web</title>
		<meeting>the 18th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bioasq: A challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Michael R Alvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zschunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI fall symposium: Information retrieval and knowledge discovery in biomedical text</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining and Knowledge Discovery Handbook</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Vani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08557</idno>
		<title level="m">Grounded recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08035</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main"># tagspace: Semantic embeddings from hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1822" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3077" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale multi-label learning with missing labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
