<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
							<email>vicente@cs.unc.edu,tlberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Bishop&apos;s School</orgName>
								<address>
									<postCode>92037</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="787" to="798"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much of everyday language and discourse con- cerns the visual world around us, making under- standing the relationship between objects in the physical world and language describing those ob- jects an important challenge problem for AI. From robotics, to image search, to situated language learning, and natural language grounding, there are a number of research areas that would bene- fit from a better understanding of how people refer to physical entities in the world.</p><p>Recent advances in automatic computer vision methods have started to make technologies for rec- ognizing thousands of object categories a near re- ality ( <ref type="bibr" target="#b26">Perronnin et al., 2012;</ref><ref type="bibr" target="#b5">Deng et al., 2012;</ref><ref type="bibr" target="#b4">Deng et al., 2010;</ref><ref type="bibr" target="#b17">Krizhevsky et al., 2012)</ref>. As a result, there has been a spurt of recent work trying to estimate higher level semantics, including ex- citing efforts to automatically produce natural lan- guage descriptions of images and video (Farhadi et * Indicates equal author contribution.</p><p>al., <ref type="bibr" target="#b40">Yang et al., 2011;</ref><ref type="bibr" target="#b24">Ordonez et al., 2011;</ref><ref type="bibr" target="#b19">Kuznetsova et al., 2012;</ref><ref type="bibr" target="#b11">Feng and Lapata, 2013)</ref>. Common challenges en- countered in these pursuits include the fact that descriptions can be highly task dependent, open- ended, and difficult to evaluate automatically.</p><p>Therefore, we look at the related, but more fo- cused problem of referring expression generation (REG). Previous work on REG has made signif- icant progress toward understanding how people generate expressions to refer to objects (a recent survey of techniques is provided in ). In this paper, we study the relatively unexplored setting of how people refer to objects in complex photographs of real-world cluttered scenes. One initial stumbling block to examining this scenario is lack of existing rele- vant datasets, as previous collections for studying REG have used relatively focused domains such as graphics generated objects <ref type="bibr" target="#b31">(van Deemter et al., 2006;</ref><ref type="bibr" target="#b33">Viethen and Dale, 2008)</ref>, crafts ( <ref type="bibr" target="#b20">Mitchell et al., 2010)</ref>, or small everyday (home and office) ob- jects arrayed on a simple background ( <ref type="bibr" target="#b22">Mitchell et al., 2013a;</ref><ref type="bibr" target="#b12">FitzGerald et al., 2013)</ref>.</p><p>In this paper, we collect a new large-scale cor- pus, currently containing 130,525 expressions, re- ferring to 96,654 distinct objects, in 19,894 pho- tographs of real world scenes. Some examples from our dataset are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. To con- struct this corpus efficiently, we design a new two player referring expression game (ReferItGame) to crowd-source the data collection. Popular- ized by efforts like the ESP game (von <ref type="bibr" target="#b36">Ahn and Dabbish, 2004</ref>) and Peekaboom <ref type="bibr" target="#b38">(von Ahn et al., 2006b</ref>), Human Computation based games can be an effective way to engage users and collect large amounts of data inexpensively. Two player games can also automate verification of human provided annotations.</p><p>Our resulting corpus is both more real-world and much bigger than previous datasets, allowing us to examine referring expression generation in a new setting at large scale. To understand and quantify this new dataset, we perform an exten- sive set of analyses. One significant difference from previous work is that we study how refer- ring expressions vary for different categories. We find that an object's category greatly influences the types of attributes used in their referring expres- sion (e.g. people use color words to describe cars more often than mountains). Additionally, we find that references to an object are sometimes made with respect to other nearby objects, e.g. "the ball to left of the man". Interestingly, the types of ref- erence objects (i.e. "the man") used in referring expressions is also biased toward some categories. Finally, we find that the word used to refer to the object category itself displays consistencies across people. This notion is related to ideas of entry- level categories from Psychology <ref type="bibr" target="#b28">(Rosch, 1978)</ref>.</p><p>Given these findings, we propose an optimiza- tion model for generating referring expressions that jointly selects which attributes to include in the expression, and what attribute values to gener- ate. This model incorporates both visual models for selecting attribute-values and object category specific priors. Experimental evaluations indicate that our proposed model produces reasonable re- sults for REG.</p><p>In summary, contributions of our paper include:</p><p>• A two player online game to collect and ver- ify natural language referring expressions.</p><p>• A new large-scale dataset containing natural language expressions referring to objects in photographs of real world scenes.</p><p>• Analyses of the collected dataset, including studying category-specific variations in refer- ring expressions.</p><p>• An optimization based model to generate referring expressions for objects in real- world scenes with experimental evaluations on three labeled test sets.</p><p>The rest of the paper is organized as follows. First we outline related work from the vision and language communities ( §2). Then we describe our online game for collecting referring expressions ( §3) and provide an analysis of our new Refer- ItGame Dataset ( §4). Finally, we present and eval- uate our model for generating referring expres- sions ( §5) and discuss conclusions and future work ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Referring Expression Generation: There has been a long history of research on understanding how people generate referring expressions, dating back to the 1970s <ref type="bibr" target="#b39">(Winograd, 1972)</ref>. One com- mon approach is the Incremental Algorithm ( <ref type="bibr" target="#b2">Dale and Reiter, 1995;</ref><ref type="bibr" target="#b3">Dale and Reiter, 2000</ref>) which uses logical expressions for generation. Much work in REG follows the Gricean maxims <ref type="bibr" target="#b13">(Grice, 1975)</ref> which provide principles for how people will behave in conversation.</p><p>Recently, there has been progress examining other aspects of the referring expression prob- lem such as understanding what types of attributes are used ( <ref type="bibr" target="#b22">Mitchell et al., 2013a</ref>), modeling varia- tions between speakers ( <ref type="bibr" target="#b34">Viethen and Dale, 2010;</ref><ref type="bibr" target="#b35">Viethen et al., 2013;</ref><ref type="bibr" target="#b23">Mitchell et al., 2013b</ref>), incorporating visual classi- fiers <ref type="bibr" target="#b21">(Mitchell et al., 2011)</ref>, producing algorithms to refer to object sets ( <ref type="bibr" target="#b27">Ren et al., 2010;</ref><ref type="bibr" target="#b12">FitzGerald et al., 2013)</ref>, or examining impoverished percep- tion REG <ref type="bibr" target="#b8">(Fang et al., 2013)</ref>. A good survey of work in this area is provided in . We build on past work, extending models to generate attributes jointly in a category specific framework. Referring Expression Datasets: Some initial datasets in REG used graphics engines to pro- duce images of objects <ref type="bibr" target="#b31">(van Deemter et al., 2006;</ref><ref type="bibr" target="#b33">Viethen and Dale, 2008)</ref>. Recently more realis- tic datasets have been introduced, consisting of craft objects like pipecleaners, ribbons, and feath- ers ( <ref type="bibr" target="#b20">Mitchell et al., 2010)</ref>, or everyday home and office objects such as staplers, combs, or rulers ( <ref type="bibr" target="#b22">Mitchell et al., 2013a</ref>), arrayed on a sim- ple background. These datasets helped moved re- ferring expression generation research into the do- main of real world objects. We seek to further these pursuits by constructing a dataset of natural objects in photographs of the real world.</p><p>Image &amp; Video Description Generation: Re- cent research on automatic image description has followed two main directions. Retrieval based methods <ref type="bibr" target="#b0">(Aker and Gaizauskas, 2010;</ref><ref type="bibr" target="#b9">Farhadi et al., 2010;</ref><ref type="bibr" target="#b24">Ordonez et al., 2011;</ref><ref type="bibr" target="#b10">Feng and Lapata, 2010;</ref><ref type="bibr" target="#b11">Feng and Lapata, 2013</ref>) retrieve exist- ing captions or phrases to describe a query image. Bottom up methods <ref type="bibr" target="#b40">Yang et al., 2011;</ref><ref type="bibr" target="#b41">Yao et al., 2010</ref>) rely on visual classi- fiers to first recognize image content and then con- struct captions from scratch, perhaps with some <ref type="figure">Figure 1</ref>: An example game. Player 1 (left) sees an image with an object outlined in red (the man) and provides a referring expression for the object ("man in red shirt on horse"). Player 2 (right) sees the image and the expression from Player 1 and must localize the correct object by clicking on it (click indicated by the red square). Elapsed time and current scores are also provided. input from natural language statistics. Very re- cently, these ideas have been extended to produce descriptions for videos <ref type="bibr" target="#b15">(Guadarrama et al., 2013;</ref><ref type="bibr" target="#b1">Barbu et al., 2012)</ref>. Like these methods, we gen- erate descriptions for natural scenes, but focus on referring to particular objects rather than provid- ing an overall description of an image or video.</p><p>Human Computation Games: Games can be a useful tool for collecting large amounts of la- beled data quickly. Human Computation Games were first introduced by Luis von Ahn in the ESP game <ref type="bibr" target="#b36">(von Ahn and Dabbish, 2004</ref>) for image la- beling, and later extended to segment objects <ref type="bibr" target="#b38">(von Ahn et al., 2006b</ref>), collect common-sense knowl- edge <ref type="bibr" target="#b37">(von Ahn et al., 2006a</ref>), or disambiguate words ( <ref type="bibr" target="#b29">Seemakurty et al., 2010)</ref>. Recently, crowd games have also been introduced into the com- puter vision community for tasks like fine grained category recognition ). These games can be released publicly on the web or used on Mechanical Turk to enhance and encour- age turker participation ). In- spired by the success of previous games, we cre- ate a game to collect and verify natural language expressions referring to objects in natural scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Referring Expression Game (ReferItGame)</head><p>In this section we describe our referring expres- sion game (ReferItGame * ), a simple two player game where players alternate between generating expressions referring to objects in images of nat- ural scenes, and clicking on the locations of de- scribed objects. An example game is shown in <ref type="figure">Figure 1</ref>. * Available online at http://referitgame.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Game Play</head><p>Player 1: is shown an image with an object out- lined in red and provided with a text box in which to write a referring expression. Player 2: is shown the same image and the referring expression writ- ten by Player 1 and must click on the location of the described object (note, Player 2 does not see the object segmentation). If Player 2 clicks on the correct object, then both players receive game points and the Player 1 and Player 2 roles swap for the next image. If Player 2 does not click on the correct object then no points are received and the players remain in their current roles. This provides us with referring expressions for our dataset and verification that the expressions are valid since they led to correct object localiza- tions. Expressions written for games where the object was not correctly localized are kept and re- leased with the dataset for future study, but are not included in our final dataset analyses or statistics. A game timer encourages players to write expres- sions quickly, resulting in more natural expres- sions. Also, IP addresses are filtered to prevent people from simultaneously playing both roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Playing Against the Computer</head><p>To promote engagement, we implement a single player version of the game. When a player con- nects, if there is another player online then the two people are paired. If there are currently no other available players, then the person plays a "canned" game against the computer. If at any point another person connects, the canned game ends and the player is paired with the new person.</p><p>To implement canned games we seed the game with 5000 pre-recorded referring expression games (5 referring expressions and resulting clicks for each of 1000 objects) collected using Ama- zon's Mechanical Turk service. Implementing an automated version of Player 1 is simple; we just show the person one of the pre-collected referring expressions and they click as usual.</p><p>Automating the role of Player 2 is a bit more complicated. In this case, we compare the per- son's written expression against the pre-recorded expressions for the same object. For this compar- ison we use a parser to lemmatize the words in an expression and then compute cosine similarity be- tween expressions with a bag of words representa- tion. Based on this measure the closest matching expression is determined. If there is no similarity between the newly generated expression and the canned expressions, the expression is deemed in- correct and a random click location (outside of the object) is generated. If there is a successful match with a previously generated expression, then the canned click from the most similar pre-recorded game is used. More complex similarities could be used, but since we require real-time performance in our game setting we use this simple implemen- tation which works well for our expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ReferItGame Dataset</head><p>In this section we describe the ReferItGame dataset † , including images and labels, processing the dataset, and analysis of the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Images and Labels</head><p>We build our dataset of referring expressions on top of the ImageCLEF IAPR image retrieval dataset ( <ref type="bibr" target="#b14">Grubinger et al., 2006</ref>). This dataset is a collection of 20,000 images available free of charge without copyright restrictions, depicting a variety of aspects of everyday life, from sports, to animals, to cities, and landscapes. Crucial for our purposes, the SAIAPR TC-12 expansion <ref type="bibr" target="#b7">(Escalante et al., 2010</ref>) includes segmentations of each image into regions indicating the locations of constituent objects. 238 different object categories are labeled, including animals, people, buildings, objects, and background elements like grass or sky. This provides us with information regarding object category, object location, and object size, as well as the location and categories of other objects present in the same image. † Available at http://tamaraberg.com/referitgame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Collecting the Dataset</head><p>From the ImageCLEF dataset, we created a total of over 100k distinct games (one per object labeled in the dataset). For the games we imposed an or- dering to allow for collecting the most interesting expressions first. Initially we prioritized games for objects in images with multiple objects of the same category. Once these games were completed, we prioritized ordering based on object category to include a comprehensive range of objects. Finally, after successfully collecting referring expressions from the prioritized games, we posted games for the remaining objects. In order to evaluate consis- tency of expression generation across people, we also include a probability of repeating previously played games during collection.</p><p>To date, we have collected 130,525 successfully completed games. This includes 10,431 canned games (a person playing against the computer, not including the initial seed set) and 120,094 real games (two people playing). 96,654 distinct ob- jects from 19,984 photographs are represented in the dataset. This covers almost all of the objects present in the IAPR corpus. The remaining ob- jects from the collection were either too small or too ambiguous to result in successful games.</p><p>For data collection, we posted the game online for anyone on the web to play and encouraged par- ticipation through social media and the survey sec- tion of reddit. In this manner we collected over 4 thousand referring expressions over a period of 3 weeks. To speed up data collection, we also posted the game on Mechanical Turk. Turkers were paid upon completion of 10 correct games (games where Player 2 clicks on the correct object of interest). Turkers were pre-screened to have ap- proval ratings above 80% and to be located in the US for language consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Processing the Dataset</head><p>Because of the size of the dataset, hand annotation of all referring expressions is prohibitive. There- fore, similar to past work ( <ref type="bibr" target="#b12">FitzGerald et al., 2013)</ref>, we design an automatic method to pre-process the expressions and extract object and attribute men- tions. These automatically processed expressions are used only for analysis and model training. We also fully hand label portions of the dataset for evaluation ( §5.2).</p><p>By examining the expressions in the collected dataset, we define a set of attributes with broad S ::= subject word color word ::= rel(S, color word) color word =color word | prep in(S, color word) color word =color word size word ::= rel(S, size word) size word =size word abs loc word ::= rel(S, abs loc word) abs loc word =abs loc word | prep on(S, orientation word) ∧ ¬prep of (S, ) abs loc word =on+orientation word rel loc word ::= RL RL ::= prep rel loc word(S, object word) RL=rel loc word | prep on(S, orientation word) ∧ prep of (S, object word) RL=on orientation word | prep to(S, orientation word) ∧ prep of (S, object word) RL=to orientation word | prep at(S, orientation word) ∧ prep of (S, object word) RL=at orientation word generic word ::= amod(S, generic word) coverage of the attribute types used in the re- ferring expressions. We define the set of at- tributes for a referring expression as a 7-tuple R = {r 1 , r 2 , r 3 , r 4 , r 5 , r 6 , r 7 }:</p><p>• r 1 is an entry-level category attribute,</p><p>• r 2 is a color attribute,</p><p>• r 3 is a size attribute, • r 4 is an absolute location attribute, • r 5 is a relative location relation attribute,</p><p>• r 6 is a relative location object attribute, • r 7 is a generic attribute, Color and size attributes refer to the object color (e.g. "blue") and object size (e.g. "tiny") respec- tively. Absolute location refers to the location of the object in the image (e.g. "top of the image"). Relative location relation and relative location ob- ject attributes allow for referring expressions that localize the object with respect to another object in the picture (e.g. "the car to the left of the tree"). Generic attributes cover all less frequently ob- served attribute types (e.g. "wooden" or "round").</p><p>The entry-level category attribute is related to the concept of entry-level categories first proposed by Psychologists in the 1970s <ref type="bibr" target="#b28">(Rosch, 1978)</ref> and recently explored in visual recognition ( <ref type="bibr" target="#b25">Ordonez et al., 2013</ref>). The idea of entry-level categories is that an object can belong to many different cate- gories; an indigo bunting is an oscine, a bird, a vertebrate, a chordate, and so on. But, a person looking at a picture of one would probably call it a bird (unless they are very familiar with ornithol- ogy). Therefore, we include this attribute to cap- ture how people name object categories in refer- ring expressions.</p><p>Parsing the referring expressions: We parse the expressions using the most recent version of the StanfordCoreNLP parser ( <ref type="bibr" target="#b30">Socher et al., 2013)</ref>. We begin by traversing the parse tree in a breadth-first manner and selecting the head noun of the sentence to determine the object of the referring expression, denoted as subject word. We pre-define a dictionary of attribute-values (color word, size word, abs location word, rel location word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.</p><p>We then apply a template-based approach on the collapsed dependency relations to recover the set of attributes (the main template rules are shown in <ref type="figure" target="#fig_0">Figure 2</ref>). The relationship rel indicates any linguistic binary relationship between the subject word S and another word, including the amod re- lationship. Orientation word captures the words like left, right, top and bottom. For generic word we consider any modifier words other than those captured by our other attributes (color, size, loca- tion).</p><p>Using this template-based parser we can for instance parse the following expression: "Red flower on top of pedestal". The first rule would match the prep(S, color word) relation, effectively recovering the attribute color word as "red". The second rule would match the prep on(S, orientation word) ∧ prep of (S, object word) relations, recovering rel loc word as "on top of " and object word as "pedestal".</p><p>The accuracy of our parser based processing is 91%. This was evaluated on 4,500 expressions   <ref type="figure">Figure 3</ref>: Analyses of the ReferItGame Dataset. Plot A shows frequency and attribute occurrence for common object categories. Plot B shows objects frequently used as reference points, ie "to the left of the man". Plot C shows frequencies of using 0, 1 or 2 attributes within the same expression. Plot D shows object locations vs location words used. Plot E shows normalized object size vs size words used (bars show 1 st through 3 rd quartiles). Plot F shows the frequency of usage of each attribute type for images containing either a single instance of the object category or multiple instances of the category. For example, this indicates that "streets" are often called "road", sometimes "ground", sometimes "roadway", etc. Right: example objects pre- dicted to portray some of our color attribute values. Note sometimes our color predictor is quite accurate, and sometimes it makes mistakes (see the man in a red shirt predicted as "yellow").</p><p>that were manually parsed by a human annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dataset Analysis</head><p>In the resulting dataset, we have a range of cov- erage over objects. For 10,304 of the objects we have 2 or more referring expressions while for the rest of the objects we have collected only one ex- pression. This creates a dataset that emphasizes breadth while also containing enough data to study speaker variation. Multiple attribute analyses are provided in <ref type="figure">Fig- ure 3</ref>. We find that most expressions use 0, 1, or 2 attributes (in addition to the entry-level attribute object word), with very few expressions contain- ing more than 2 attributes (frequencies are shown in <ref type="figure">Fig 3c)</ref>. We also examine what types of at- tributes are used most frequently, according to ob- ject category in <ref type="figure">Fig 3a,</ref> and when associated with single or multiple occurrences of the same object category in an image in <ref type="figure">Fig 3f.</ref> The frequency of attribute usage in images containing multiple objects of the same type increases for all types, compared to single object occurrences. Perhaps more interestingly, the use of different attributes is highly category dependent. People use more at- tribute words overall to describe some categories, like "man", "woman", or "plant", and the distribu- tion of attribute types also varies by category. For example, color attributes are used more frequently for categories like "car" or "woman" than for cat- egories like "sky" or "rock".</p><p>We also examine which objects are most fre- quently used as points of reference, e.g.,"the chair next to the man" in <ref type="figure">Fig 3b.</ref> We observe that peo- ple and some background categories like "tree" or "wall" are often used to help localize objects in Finally, we study entry-level category attribute- values to understand how people name objects in referring expressions. Tag clouds indicating the frequencies of words used to name various ob- ject categories are provided in <ref type="figure" target="#fig_2">Fig 4 (left)</ref>. Ob- jects like "street" are usually referred to as "road", but sometimes they are called "ground", "road- way", etc. "Bottles" are usually called "bottle", but sometimes referred to as "coke" or "beer". In- terestingly, "man" is usually called "man" while "woman" is most often called "person" in the re- ferring expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Generating Referring Expressions</head><p>In this section we describe our proposed genera- tion model and provide experimental evaluations on three test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Generation Model</head><p>Given an input tuple I = {P, S}, where P is a target object and S is a scene (image containing multiple objects), our goal is to generate an output referring expression, R. For instance, the repre- sentation R for the referring expression: The big old white cabin beside the tree would be R = {cabin, white, big, ∅, beside, tree, old}.</p><p>To generate referring expressions we construct vocabularies V r i with candidate values for each at- tribute r i ∈ R, where attribute vocabulary V r i con- tains the set of words observed in our parsed refer- ring expressions for attribute r i plus an additional  ε value indicating that the attribute should be om- mited from the referring expression entirely. In this way, our framework can jointly deter- mine which attributes to include in the expression (e.g.,"size" and "color") and what attribute values to generate (e.g.,"small" and "blue") from the list of all possible values. We enforce a constraint to always include an "entry-level category" attribute (e.g. "boy") so that we always generate a word referring to the object.</p><note type="other">Image Human Expressions Generated Expressions picture on the</note><p>We pose our problem as an optimization where we map a tuple {P, S} to a referring expression R * as:</p><formula xml:id="formula_0">R * = argmax R E(R, P, S) s. t. f i (R) ≤ b i<label>(1)</label></formula><p>Where the objective function E is decomposed as:</p><formula xml:id="formula_1">E(R, P, S) = α 6 i=2 φ i (r i , P, S) + β 7 i=1 ψ i (r i , type(P )) + i&gt;j ψ i,j (r i , r j ) (2)</formula><p>Where φ i is the compatibility function between an attribute-value for r i and the properties of the ob- served scene S and object P (described in §5.1.1).</p><p>The terms ψ i and ψ i,j are unary and pairwise pri- ors computed based on observed co-occurrence statistics of attribute-values for r i with categories (where type(P ) denotes the type or category of an object) and between pairs of attribute-values (de- scribed in §5.1.2). Attributes r 1 and r 7 are mod- eled only in the priors since we do not have visual models for these attributes.</p><p>The constraints f i (R) ≤ b i are restricted to be linear constraints and are used to impose hard con- straints on the solution. The first such constraint is used to control the verbosity (length) of the gener- ated referring expression using a constraint func- tion that imposes a minimum attribute length re- quirement by restricting the number of entries r i that can take value ε in the solution.</p><formula xml:id="formula_2">i 1[r i = ε] ≤ 7 − γ(P, S) (3)</formula><p>Where 1 <ref type="bibr">[.]</ref> is the indicator function and γ(P, S) is a term that allows us to change the length require- ment based on the object and scene (so that images with a larger number of objects of the same type have a larger length requirement). Finally we add hard constraints such that r 5 = ε ⇐⇒ r 6 = ε, so that relative location and relative object attributes are produced together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Content-based potentials</head><p>Potentials φ i are defined for attributes r 2 to r 6 . Attribute r 7 represents a variety of different at- tributes, e.g. material or shape attributes, but we lack sufficient data to train visual models for these infrequent attribute terms. Therefore, we model these attributes using only prior statistics- based potentials ( §5.1.2). Visual recognition mod- els for recognizing entry-level object categories could also be incorporated for modeling r 1 , but we leave this as future work.</p><p>Color attribute:</p><formula xml:id="formula_3">φ 2 (r 2 = c k , P, S) = sim(hist c k , hist(P ))</formula><p>Where hist(P ) is the HSV color histogram of the object P . We compute similarity sim using cosine similarity, and hist c k is the mean histogram of all objects in our training data that were referred to with color attribute-value c k ∈ V r 2 .</p><p>Size attribute:</p><formula xml:id="formula_4">φ 3 (r 3 = s k , P, S) = 1 σ s k √ 2π e −(size(P )−µs k ) 2 2σ 2 s k</formula><p>Where size(P ) is the size of object P normalized by image size. We model the probabilities of each size word s k ∈ V r 3 as a Gaussian learned on our training set.</p><p>Absolute-location attribute:</p><formula xml:id="formula_5">φ 4 (r 4 = a k , P, S) = 1 (2π) n |Σ a k | e − 1 2 (loc(P )−µa k ) T Σa k −1 (loc(P )−µa k )</formula><p>Where loc(P ) are the 2-dimensional coordi- nates of the object P normalized to be ∈ [0 − 1]. Parameters µ a k and Σ a k are estimated from training data for each absolute location word a k ∈ V r 4 .</p><p>Relative-location and Relative object:</p><formula xml:id="formula_6">φ 5 (r 5 = l k , P, S) = 1[l k = ε] · g(count(type(P ), S))</formula><p>If there are a larger number of objects of the same type in the image we find that the probability of us- ing a relative-location-object increases (e.g., "the car to the right of the man"). For images where P was the only object of that category type, the prob- ability of using a relative-location-object is 0.12. This increases to 0.22 when there were two ob- jects of the same type and further increases to 0.26 for additional objects of the same type. There- fore, we model the probability of selecting rela- tive location value l k ∈ V r 5 as a function g, where count(type(P ), S) counts the number of objects in the scene S of the same category type as the object P .</p><formula xml:id="formula_7">φ 6 (r 6 = o k , P, S) = 1[o k ∈ objectsnear(location(P ), S)]</formula><p>The above expression filters out potential relative objects o k ∈ V r 6 that are not located in sufficient proximity to object P or are not present in the im- age at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Prior statistics-based potentials</head><p>Prior statistics-based potentials are modeled for all of the attributes r 1 -r 7 . Note that these potentials do not depend on specific attribute-values but only on the given object category type(P ).</p><p>Unary prior potentials ψ i are defined as:</p><formula xml:id="formula_8">ψ i (r i , type(P )) = |D| j=1 1[(r (j) i = ) ∧ (type(P (j) ) = type(P ))] |D| j=1 1[type(P (j) ) = type(P )] + |D| j=1 1[r (j) i = ] |D| + λ</formula><p>Where D = {P (j) , S (j) , R (j) } is our training dataset and λ is a small additive smoothing term. The two terms in the above expression represent category-specific counts and global counts of the number of times a given attribute r i was output in a referring expression in training data. Pairwise prior potentials ψ i,j are defined as:</p><formula xml:id="formula_9">i&lt;j ψ i,j (r i , r j ) = i&lt;j ψ (1) i,j (r i , r j ) + ψ<label>(2)</label></formula><p>5,6 (r 5 , r 6 ) For instance how frequently people use both color and size attributes to refer to an object. The pair- wise potential ψ</p><formula xml:id="formula_10">ψ (1) i,j (r i , r j ) = 1 if r i = r j = ε C + λ o.w. ψ (2) 5,6 (r 5 = a, r 6 = b) = |D| t=1 1[(r (t) 5 = a) ∧ (r (t) 6 = b)] |D| where C = |D| t=1 1[(r (t) i =) ∧ (r (t) j =)] |D| .</formula><p>i,j produces a cohesion score be- tween relative-location words and relative-object words based on global dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head><p>We implement the proposed model using commer- cial binary integer linear programming software (IBM ILOG CPLEX). This requires introducing a set of indicator variables for each of our multi- valued attributes and another set of indicator vari- ables to model pairwise interactions between our variables, as well as incorporating additional con- sistency constraints between variables. Model pa- rameters (α and β) are tuned on data randomly sampled from the training set. Test Sets: We evaluate our model on three test sets, each containing 500 objects. For each ob- ject in the test sets we collect 3 referring expres- sions using the ReferItGame and manually label the attributes mentioned in each expression. We find human agreement to be 72.31% on our dataset (where we measure agreement as mean match- ing accuracy of attribute values for pairs of users across images in our test sets). The three test sets are created to evaluate different aspects of our data.</p><p>Test Set A contains objects sampled randomly from the entire dataset. This test set is meant to closely resemble the full dataset distribution. The goal of the other two test sets is to sample expres- sions for "interesting" objects. We first identify categories that are mainly related to background content elements, e.g. "sky, ground, floor, sand, sidewalk, etc". We consider these categories to be potentially less interesting for study than cat- egories like people, animals, cars, etc. Test Set B contains objects sampled from the most frequently occurring object categories in the dataset, selected to contain a balanced number of objects from each category, excluding the less interesting categories. Test Set C contains objects sampled from images that contain at least 2 objects of the same category, excluding the less interesting categories.</p><p>Results: Qualitative examples are shown in <ref type="figure" target="#fig_4">Fig 5  comparing</ref> our results to the human produced ex- pressions. For some images (left) we do quite well at predicting the correct attributes and values. For others we do less well (right). We also show exam- ple objects predicted for some color words in <ref type="figure" target="#fig_2">Fig 4  (right)</ref>. We see that our model can fail in several ways, such as generating the wrong attribute-value due to inaccurate predictions by visual models or selecting incorrect attributes to include in the gen- erated expression.</p><p>Quantitative results: precision and recall mea- sures for the 3 test sets are reported in <ref type="table">Table 1</ref>, including evaluation of a baseline version of our model which incorporates only the prior potentials ( §5.1.2) without any content based estimates. We see that our model performs reasonably on both measures, and outperforms the baseline by a large margin on all test sets, with highest performance on the broadly sampled interesting category test set. Note that our problem is somewhat differ- ent than traditional REG where the input is often attribute-value pairs and the task is to select which pairs to include in the expression. Our goal is to jointly select which attributes to include and what values to predict from a list of all possible values for the attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions &amp; Future Work</head><p>In this paper we have introduced a new game to crowd-source referring expressions for objects in natural scenes. We have used this game to pro- duce a new large-scale dataset with analysis. We have also proposed an optimization based model for REG and performed experimental evaluations. Future work includes developing fully automatic visual recognition methods for REG in real world scenes, and incorporating linguistically inspired models for entry-level category prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Templates for parsing attributes from referring expressions ( §4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Tag clouds showing entry-Level category words used in referring expressions to name various object categories, with word size indicating frequency. For example, this indicates that "streets" are often called "road", sometimes "ground", sometimes "roadway", etc. Right: example objects predicted to portray some of our color attribute values. Note sometimes our color predictor is quite accurate, and sometimes it makes mistakes (see the man in a red shirt predicted as "yellow").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>referring expressions.</head><label></label><figDesc>Additionally, we provide plots showing the relationship between object lo- cation in the image and use of absolute location words, Fig 3d, as well as size words vs object area, Fig 3e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example results, including human generated expressions, baseline and full model generated expressions. For some images the model does well at mimicking human expressions (left). For others it does not generate the correct attributes (right).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by NSF Awards #1417991 and #1444234. M.M. was supported by the Stony Brook Simons Summer Research Program for High School students. We also thank Alex Berg for many helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating image descriptions using dependency relational patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video in sentences out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Burchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Coroian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mussman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhaval</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lara</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarrell</forename><forename type="middle">W</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational interpretations of the gricean maxims in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science (CogSci)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">233264</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What does classifying more than 10,000 image categories tell us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.image-net.org/challenges/LSVRC/2012/index" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finegrained crowdsourcing for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The segmented and annotated iapr tc-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez-Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Enrique</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Villasenor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards situated dialogue: Revisiting referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Every picture tells a story: generating sentences for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How many words is a picture worth? automatic caption generation for news images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic caption generation for news images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="812" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning distributions over logical forms for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Grice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="page">4158</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The iapr benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop OntoImage (LREC)</title>
		<meeting>the International Workshop OntoImage (LREC)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niveda</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computational generation of referring expressions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kees Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">173218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural reference to objects in a visual domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Natural Language Generation Conference (INLG)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two approaches for generating size modifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Natural Language Generation</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Typicality and object reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Science (CogSci)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating expressions that refer to visible objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards good practice in large-scale learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Charting the potential of description logic for the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Yuan Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Natural Language Generation Conference (INLG)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Principles of categorization. Cognition and Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page">2748</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word sense disambiguation via human computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Seemakurty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Luis Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Computation Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsing With Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a semantically transparent corpus for the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ielka</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Van Der Sluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Language Generation (INLG)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward a computational psycholinguistics of reference production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Van Gompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in Cognitive Science</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">166183</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The use of spatial relations in referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Natural Language Generation Conference (INLG)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speakerdependent variation in content selection for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphs and spatial relations in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Natural Language Generation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Luis Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Verbosity: A game for collecting common-sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Luis Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kedia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Peekaboom: A game for locating objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoran</forename><surname>Luis Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1191</biblScope>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Lik</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">I2t: Image parsing to text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mun</forename><forename type="middle">Wai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
