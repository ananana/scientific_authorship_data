<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretation of Natural Language Rules in Conversational Machine Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Sheldon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bloomsbury</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Interpretation of Natural Language Rules in Conversational Machine Reading</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2087" to="2097"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2087</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader&apos;s background knowledge. One example is the task of interpreting regulations to answer &quot;Can I...?&quot; or &quot;Do I have to...?&quot; questions such as &quot;I am working in Canada. Do I have to carry on paying UK National Insurance?&quot; after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as &quot;How long have you been working abroad?&quot; when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been significant progress in teaching ma- chines to read text and answer questions when the answer is directly expressed in the text ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b10">Joshi et al., 2017;</ref><ref type="bibr" target="#b30">Welbl et al., 2018;</ref><ref type="bibr" target="#b8">Hermann et al., 2015</ref>). However, in many settings, ⇤ These three authors contributed equally Do I need to carry on paying National Insurance?</p><p>I am working for an employer in Germany.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes</head><p>Have you been working abroad 52 <ref type="bibr">weeks</ref>   <ref type="figure">Figure 1</ref>: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer "Yes". the text contains rules expressed in natural lan- guage that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer some- one's question "I am working for an employer in Canada. Do I need to carry on paying National Insurance?" with "Yes", one needs to read that "You'll carry on paying National Insurance if you're working for an employer outside the EEA" and un- derstand how the rule and question determine the answer.</p><p>Answering questions that require rule interpre- tation is often further complicated due to missing information in the question. For example, as il- lustrated in <ref type="figure">Figure 1</ref> (Utterance 1), the actual rule also mentions that National Insurance only needs to be paid for the first 52 weeks when abroad. This means that we cannot answer the original question without knowing how long the user has already been working abroad. Hence, the correct response in this conversational context is to issue another query such as "Have you been working abroad 52 weeks or less?"</p><p>To capture the fact that question answering in the above scenario requires a dialog, we hence con- sider the following conversational machine read- ing (CMR) problem as displayed in <ref type="figure">Figure 1</ref>: Given an input question, a context scenario of the ques- tion, a snippet of supporting rule text containing a rule, and a history of previous follow-up questions and answers, predict the answer to the question ("Yes"or "No") or, if needed, generate a follow-up question whose answer is necessary to answer the original question. Our goal in this paper is to create a corpus for this task, understand its challenges, and develop initial models that can address it.</p><p>To collect a dataset for this task, we could give a textual rule to an annotator and ask them to provide an input question, scenario, and dialog in one go. This poses two problems. First, this setup would give us very little control. For example, users would decide which follow-up questions become part of the scenario and which are answered with "Yes" or "No". Ultimately, this can lead to bias because annotators might tend to answer "Yes", or focus on the first condition. Second, the more complex the task, the more likely crowd annotators are to make mistakes. To mitigate these effects, we aim to break up the utterance annotation as much as possible.</p><p>We hence develop an annotation protocol in which annotators collaborate with virtual users- agents that give system-produced answers to follow-up questions-to incrementally construct a dialog based on a snippet of rule text and a sim- ple underspecified initial question (e.g., "Do I need to ...?"), and then produce a more elaborate ques- tion based on this dialog (e.g., "I am ... Do I need to...?"). By controlling the answers of the virtual user, we control the ratio of "Yes" and "No" an- swers. And by showing only subsets of the dialog to the annotator that produces the scenario, we can control what the scenario is capturing. The ques- tion, rule text and dialogs are then used to produce utterances of the kind we see in <ref type="figure">Figure 1</ref>. Annota- tors show substantial agreement when constructing dialogs with a three-way annotator agreement at a Fleiss' Kappa level of 0.71. 1 Likewise, we find that our crowd-annotators produce questions that are coherent with the given dialogs with high accuracy.</p><p>In theory, the task could be addressed by an end- to-end neural network that encodes the question, history and previous dialog, and then decodes a Yes/No answer or question. In practice, we test this hypothesis using a seq2seq model <ref type="bibr" target="#b26">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>), with and without copy mechanisms ( <ref type="bibr" target="#b6">Gu et al., 2016)</ref> to reflect how follow-up questions often use lexical content from the rule text. We find that despite a training set size of 21,890 training utterances, successful mod- els for this task need a stronger inductive bias due to the inherent challenges of the task: interpret- ing natural language rules, generating questions, and reasoning with background knowledge. We develop heuristics that can work better in terms of identifying what questions to ask, but they still fail to interpret scenarios correctly. To further motivate the task, we also show in oracle experiments that a CMR system can help humans to answer questions faster and more accurately. This paper makes the following contributions: 1. We introduce the task of conversational ma- chine reading and provide evaluation metrics. 2. We develop an annotation protocol to collect annotations for conversational machine read- ing, suitable for use in crowd-sourcing plat- forms such as Amazon Mechanical Turk. 3. We provide a corpus of over 32k conversa- tional machine reading utterances, from do- mains such as grant descriptions, traffic laws and benefit programs, and include an analysis of the challenges the corpus poses. 4. We develop and compare several baseline models for the task and subtasks. <ref type="figure">Figure 1</ref> shows an example of a conversational ma- chine reading problem. A user has a question that relates to a specific rule or part of a regulation, such as "Do I need to carry on paying National Insur- ance?". In addition, a natural language description of the context or scenario, such as "I am working for an employer in Canada", is provided. The ques- tion will need to be answered using a small snippet of supporting rule text. Akin to machine reading problems in previous work <ref type="bibr" target="#b17">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b8">Hermann et al., 2015)</ref>, we assume that this snip- pet is pre-identified. We generally assume that the question is underspecified, in the sense that the question often does not provide enough informa- tion to be answered directly. However, an agent can use the supporting rule text to infer what needs to be asked in order to determine the final answer. In <ref type="figure">Figure 1</ref>, for example, a reasonable follow-up ques- tion is "Have you been working abroad 52 weeks or less?". We formalise the above task on a per-utterance basis. A given dialog corresponds to a sequence of prediction problems, one for each utterance the system needs to produce. Let W be a vocabu- lary. Let q = w 1 . . . w nq be an input question and r = w 1 . . . w nr an input support rule text, where w i 2 W is a word from a vocabulary. Further- more, let h = (f 1 , a 1 ) . . . (f n h , a n h ) be a dialog history where each f i 2 W ⇤ is a follow-up ques- tion, and each a i 2 {YES, NO} is a follow-up an- swer. Let s be a scenario describing the context of the question. We will refer to x = (q, r, h, s) as the input. Given an input x, our task is to predict an answer y 2 {YES, NO, IRRELEVANT} [ W ⇤ that specifies whether the answer to the input ques- tion, in the context of the rule text and the previ- ous follow-up question dialog, is either YES, NO, IRRELEVANT or another follow-up question in W ⇤ . Here IRRELEVANT is the target answer whenever a rule text is not related to the question q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation Protocol</head><p>Our annotation protocol is depicted in <ref type="figure" target="#fig_1">Figure 2</ref> and has four high-level stages: Rule Text Extraction, Question Generation, Dialog Generation and Sce- nario Annotation. We present these stages below, together with discussion of our quality-assurance mechanisms and method to generate negative data. For more details, such as annotation interfaces, we refer the reader to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rule Text Extraction Stage</head><p>First, we identify the source documents that con- tain the rules we would like to annotate. Source documents can be found in Appendix C. We then convert each document to a set of rule texts using a heuristic which identifies and groups paragraphs and bulleted lists. To preserve readability during the annotation, we also split by a maximum rule text length and a maximum number of bullets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Generation Stage</head><p>For each rule text we ask annotators to come up with an input question. Annotators are instructed to ask questions that cannot be answered directly but instead require follow-up questions. This means that the question should a) match the topic of the support rule text, and b) be underspecified. At present, this part of the annotation is done by expert annotators, but in future work we plan to crowd- source this step as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dialog Generation Stage</head><p>In this stage, we view human annotators as assis- tants that help users reach the answer to the input question. Because the question was designed to be broad and to omit important information, human annotators will have to ask for this information us- ing the rule text to figure out which question to ask. The follow-up question is then sent to a vir- tual user, i.e., a program that simply generates a random YES or NO answer. If the input question can be answered with this new information, the an- notator should enter the respective answer. If not, the annotator should provide the next follow-up question and the process is repeated.</p><p>When the virtual user is providing random YES and NO answers in the dialog generation stage, we are traversing a specific branch of a decision tree. We want the corpus to reflect all possible dialogs for each question and rule text. Hence, we ask annotators to label additional branches. For example, if the first annotator received a YES as the answer to the second follow-up question in <ref type="figure" target="#fig_2">Figure 3</ref>, the second annotator (orange) receives a NO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scenario Annotation Stage</head><p>In the final stage, we choose parts of the dialogs cre- ated in the previous stage and present this to an an- notator. For example, the annotator sees "Are you working or preparing for work?" and NO. They are then asked to write a scenario that is consistent with this dialog such as "I am currently out of work after being laid off from my last job, but am not able to look for any yet.". The number of questions and answers that the annotator is presented with for generating a scenario can vary from one to the full length of a dialog. Users are encouraged to para- phrase the questions and not to use many words from the dialog.</p><p>In an attempt to make these scenarios closer to the real-world situations where a user may provide a lot of unnecessary information to an operator, not only do we present users with one or more questions and answers from a specific dialog but    also with one question from a random dialog. The annotators are asked to come up with a scenario that fits all the questions and answers. Finally, a dialog is produced by combining the scenario with the input question and rule text from the previous stages. In addition, all dialog utter- ances that were not shown to the final annotator are included as well as they complement the in- formation in the scenario. Given a dialog of this form, we can create utterances that are described in Section 2.</p><p>As a result of this stage of annotation, we create a corpus of scenarios and questions where the correct answers (YES, NO or IRRELEVANT) to questions can be derived from the related scenarios. This corpus and its challenges will be discussed in Sec- tion 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Negative Examples</head><p>To facilitate the future application of the models to large-scale rule-based documents instead of rule text, we deem it to be imperative for the data to contain negative examples of both questions and scenarios.</p><p>We define a negative question as a question that is not relevant to the rule text. In this case, we ex- pect models to produce the answer IRRELEVANT. For a given rule text and question pair, a negative example is generated by sampling a random ques- tion from the set of all possible questions, exclud- ing the question itself and questions sourced from the same document using a methodology similar to the work of <ref type="bibr" target="#b11">Levy et al. (2017)</ref>.</p><p>The data created so far is biased in the sense that when a scenario is given, at least one of the follow-up questions in a dialog can be answered. In practice, we expect users to also provide back- ground scenarios that are completely irrelevant to the input question. Therefore, we sample a nega- tive scenario for each input question and rule text pair, (q, r) in our data. We uniformly sample from the scenarios created in Section 3.4 for all question and rule text pairs (q 0 , r 0 ) unequal to (q, r). For more details, we point the reader to Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Quality Control</head><p>We employ a range of quality control measures throughout the process. In particular, we:</p><p>1. Re-annotate pre-terminal nodes in the dia- log trees if they have identical YES and NO branches. 2. Ask annotators to validate the previous dialog in case previous utterances where created by different annotators. 3. Assess a sample of annotations for each an-notator and keep only those annotators with quality scores higher than a certain threshold. 4. We require annotators to pass a qualification test before selecting them for our tasks. We also require high approval rates and restrict location to the UK, US, or Canada. Further details are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Cost, Duration and Scalability</head><p>The cost of different stages of annotation is as fol- lows. An annotator was paid $0.15 for an initial question (948 questions), $0.11 for a dialog part (3000 dialog parts) and $0.20 for a scenario (6,600 scenarios). It takes in total 2 weeks to complete the annotation process. Considering that all the annota- tion stages can be done through crowdsourcing and in a relatively short time period and at a reasonable cost using established validation procedures, the dataset can be scaled up without major bottlenecks or an impact on the quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ShARC</head><p>In this section, we present the SHaping Answers with Rules and Conversation (ShARC) dataset. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Size and Quality</head><p>The dataset is built up from of 948 distinct snip- pets of rule text. Each has an input question and a "dialog tree". At each step in the dialog, there is a followup question posed and the tree branches depending on the answer to the followup question (yes/no). The ShARC dataset is comprised of all individual "utterances" from every tree, i.e. ev- ery possible point/node in any dialog tree. There are 6058 of these utterances. In addition, there are 6637 scenarios that provide more information, allowing some questions in the dialog tree to be "skipped" as the answers can be inferred from the scenario. Scenarios therefore modify the dialog trees, which creates new trees. When combined with scenarios and negative sampled scenarios, the total number of distinct utterances became 37087. As a final step, utterances were removed where the scenario referred to a portion of the dialog tree that was unreachable for that utterance, leaving a final dataset size of 32436 utterances. <ref type="bibr">3</ref> We break these into train, development and test sets such that each dataset contains approximately the same proportion of sources from each domain, targeting a 70%/10%/20% split.</p><p>To evaluate the quality of dialog generation HITs, we sample a subset of 200 rule texts and ques- tions and allow each HIT to be annotated by three distinct workers. In terms of deciding whether the answer is a YES, NO or some follow-up question, the three annotators reach an answer agreement of 72.3%. We also calculate Cohen's Kappa, a mea- sure designed for situations with two annotators. We randomly select two out of the three annota- tions and compute the unweighted kappa values, repeated for 100 times and averaged to give a value of 0.82.</p><p>The above metrics measure whether annota- tors agree in terms of deciding between YES, NO or some follow-up question, but not whether the follow-up questions are equivalent. To approxi- mate this, we calculate BLEU scores between pairs of annotators when they both predict follow-up questions Generally, we find high agreement: An- notators reach average BLEU scores of 0.71, 0.63, 0.58 and 0.58 for maximum orders of 1, 2, 3 and 4 respectively.</p><p>To get an indication of human performance on the sub-task of classifying whether a response should be a YES, NO or FOLLOW-UP QUESTION, we use a similar methodology to ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016)</ref> by considering the second answer to each question as the human prediction and taking the majority vote as ground truth. The resulting human accuracy is 93.9%.</p><p>To evaluate the quality of the scenarios, we sam- ple 100 scenarios randomly and ask two expert annotators to validate them. We perform validation for two cases: 1) scenarios generated by turkers who did not attempt the qualification test and were not filtered by our validation process, 2) scenar- ios that are generated by turkers who have passed the qualification test and validation process. In the second case, annotators approved an average of 89 of the scenarios whereas in the first case, they only approved an average of 38. This shows that the qualification test and the validation process im- tasks, relying solely on large datasets to push the boundaries of AI cannot be as practical as developing better models for incorporating common sense and external knowledge which we believe ShARC is a good test-bed for. Furthermore, the proposed annotation protocol and evaluation procedure can be used to reliably extend the dataset or create datasets for new domains.</p><p>proved the quality of the generated scenarios by more than double. In both cases, the annotators agreed on the validity of 91-92 of the scenarios. For further details on dataset quality, the reader if referred to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Challenges</head><p>We analyse the challenges involved in solving con- versational machine reading in ShARC. We divide these into two parts: challenges that arise when interpreting rules, and challenges that arise when interpreting scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Interpreting Rules</head><p>When no scenarios are available, the task reduces to a) identifying the follow-up questions within the rule text, b) understanding whether a follow-up question has already been answered in the history, and c) determining the logical structure of the rule (e.g.disjunction vs. conjunction vs. conjunction of disjunctions) .</p><p>To illustrate the challenges that these sub-tasks involve, we manually categorise a random sample of 100 (q i , r i ) pairs. We identify 9 phenomena of interest, and estimate their frequency within the corpus. Here we briefly highlight some categories of interest, but full details, including examples, can be found in Appendix G.</p><p>A large fraction of problems involve the identification of at least two conditions, and approximately 41% and 27% of the cases involve logical disjunctions and conjunctions respectively. These can appear in linguistic coordination structures as well as bullet points. Often, differ- entiating between conjunctions and disjunctions is easy when considering bullets-key phrases such as "if all of the following hold" can give this away. However, in 13% of the cases, no such cues are given and we have to rely on lan- guage understanding to differentiate. For example:</p><p>Q: Do I qualify for Statutory Maternity Leave? R: You qualify for Statutory Maternity Leave if -you're an employee not a "worker" -you give your employer the correct notice</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Interpreting Scenarios</head><p>Scenario interpretation can be considered as a multi-sentence entailment task. Given a sce- nario (premise) of (usually) several sentences, and a question (hypothesis), a system should out- put YES (ENTAILMENT), NO (CONTRADICTION) or IRRELEVANT (NEUTRAL). In this context, IRRELEVANT indicates that the answer to the ques- tion cannot be inferred from the scenario. Different types of reasoning are required to in- terpret the scenarios. Examples include numeri- cal reasoning, temporal reasoning and implication (common sense and external knowledge). We man- ually label 100 scenarios with the type of reasoning required to answer their questions. <ref type="table">Table 1</ref> shows examples of different types of reasoning and their percentages. Note that these percentages do not add up to 100% as interpreting a scenario may require more than one type of reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To assess the difficulty of ShARC as a machine learning problem, we investigate a set of baseline approaches on the end-to-end task as well as the im- portant sub-tasks we identified. The baselines are chosen to assess and demonstrate both feasibility and difficulty of the tasks.</p><p>Metrics For all following classification tasks, we use micro-and macro-averaged accuracies. For the follow-up generation task, we compute the BLEU scores at orders 1, 2, 3 and 4 computed between the gold follow-up questions, y i and follow-up ques- tionˆytionˆ tionˆy i = w ˆ y i ,1 , w ˆ y i ,2 . . . w ˆ y i ,n for all utterances i in the evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification (excluding Scenarios)</head><p>On each turn, a CMR system needs to decide, either explicitly or implicitly, whether the answer is YES or NO, whether the question is not relevant to the rule text (IRRELEVANT), or whether a follow-up question is necessary-an outcome we label as MORE. In the following experiments, we will test whether one can learn to make this decision using the ShARC training data.</p><p>When a non-empty scenario is given, this task also requires an understanding of how scenarios answer follow-up questions. In order to focus on the challenges of rule interpretation, here we only consider empty scenarios.</p><p>Formally, for an utterance x = (q, r, h, s), we require models to predict an answer y where y 2 {YES, NO, IRRELEVANT, MORE}. Since we con- sider only the classification task without scenario influence, we consider the subset of utterances such that s = NULL. This data subset consists of 4026 train, 431 dev and 1601 test utterances.   <ref type="table">Table 2</ref>: Selected Results of the baseline models on the classification sub-task.</p><p>Baselines We evaluate various baselines includ- ing random, a surface logistic regression applied to a TFIDF representation of the rule text, question and history, a rule-based heuristic which makes predictions depending on the number of overlap- ping words between the rule text and question, de- tecting conjunctive or disjunctive rules, detecting negative mismatch between the rule text and the question and what the answer to the last follow-up history was, a feature-engineered Random Forest and a Convolutional Neural Network applied to the tokenised inputs of the concatenated rule text, question and history.</p><p>Results We find that, for this classification sub- task, Random Forest slightly outperforms the heuristic. All learnt models considerably outper- form the random and majority baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Follow-up Question Generation without Scenarios</head><p>When the target utterance is a follow-up question, we still have to determine what that follow-up ques- tion is. For an utterance x = (q, r, h, s), we re- quire models to predict an answer y where y is the next follow-up question, y = w y,1 , w y,2 . . . w y,n = f m+1 if x has history of length m. We there- fore consider the subset of utterances such that s = NULL and y 6 2 {YES, NO, IRRELEVANT}. This data subset consists of 1071 train, 112 dev and 424 test utterances.</p><p>Baselines We first consider several simple base- lines to explore the relationship between our evalu- ation metric and the task. As annotators are encour- aged to re-use the words from rule text when gen- erating follow-up questions, a baseline that simply returns the final sentence of the rule text performs surprisingly well. We also implement a rule-based model that uses several heuristics. If framed as a seq2seq task, a modified Copy- Net is most promising ( <ref type="bibr" target="#b6">Gu et al., 2016</ref>). We also experiment with span extraction/sequence-tagging approaches to identify relevant spans from the rule text that correspond to the next follow-up ques- tions. We find that Bidirectional Attention Flow ( ) performed well. <ref type="bibr">4</ref> Further imple- mentation details can be found in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Our results, shown in <ref type="table" target="#tab_4">Table 3</ref> indicate that systems that return contiguous spans from the rule text perform better according to our BLEU metric. We speculate that the logical forms in the data are challenging for existing models to extract and manipulate, which may suggest why the ex- plicit rule-based system performed best. We fur- ther note that only the rule-based and NMT-Copy models are capable of generating genuine questions rather than spans or sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scenario Interpretation</head><p>Many utterances require the interpretation of the scenario associated with a question. If the scenario    is understood, certain follow-up questions can be skipped because they are answered within the sce- nario. In this section, we investigate how difficult scenario interpretation is by training models to an- swer follow-up questions based on scenarios.</p><p>Baselines We use a random baseline and also im- plement a surface logistic regression applied to a TFIDF representation of the combined scenario and the question. Results <ref type="table" target="#tab_5">Table 4</ref> shows the result of our baseline models on the entailment corpus of ShARC test set. Results show poor performance especially for the macro accuracy metric of both simple baselines and neural state-of-the-art entailment models. This performance highlights the challenges that the sce- nario interpretation task of ShARC presents, many of which are discussed in Section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Conversational Machine Reading</head><p>The CMR task requires all of the above abilities. To understand its core challenges, we compare base- lines that are trained end-to-end vs. baselines that reuse solutions for the above subtasks.  Results We find that the combined model outper- forms the neural end-to-end model on the CMR task, however, the fact that the neural model has learned to classify better than random and also predict follow-up questions is encouraging for de- signing more sophisticated neural models for this task.</p><p>User Study In order to evaluate the utility of con- versational machine reading, we run a user study that compares CMR to when such an agent is not available, i.e. the user has to read the rule text and determine themselves the answer to the question. On the other hand, with the agent, the user does not read the rule text, instead only responds to follow- up questions. Our results show that users using the conversational agent reach conclusions &gt; 2 times faster than ones that are not, but more importantly, they are also much more accurate (93% as com- pared to 68%). Details of the experiments and the results are included in Appendix I.</p><p>This work relates to several areas of active research.</p><p>Machine Reading In our task, systems answer questions about units of texts. In this sense, it is most related to work in Machine Reading ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b28">Weissenborn et al., 2017</ref>). The core difference lies in the con- versational nature of our task: in traditional Ma- chine Reading the questions can be answered right away; in our setting, clarification questions are often needed. The domain of text we consider is also different (regulatory vs Wikipedia, books, newswire).</p><p>Dialog The task we propose is, at its heart, about conducting a dialog <ref type="bibr" target="#b29">(Weizenbaum, 1966;</ref><ref type="bibr" target="#b23">Serban et al., 2018;</ref><ref type="bibr" target="#b1">Bordes and Weston, 2016)</ref>. Within this scope, our work is closest to work in dialog- based QA where complex information needs are addressed using a series of questions. In this space, previous approaches have been looking primarily at QA dialogs about images ( <ref type="bibr" target="#b4">Das et al., 2017</ref>) and knowledge graphs ( <ref type="bibr" target="#b21">Saha et al., 2018;</ref><ref type="bibr" target="#b9">Iyyer et al., 2017)</ref>. In parallel to our work, both <ref type="bibr" target="#b3">Choi et al. (2018)</ref> and <ref type="bibr" target="#b19">Reddy et al. (2018)</ref> have to began to investigate QA dialogs with background text. Our work not only differs in the domain covered (regula- tory text vs wikipedia), but also in the fact that our task requires the interpretation of complex rules, ap- plication of background knowledge, and the formu- lation of free-form clarification questions. <ref type="bibr" target="#b18">Rao and Daume III (2018)</ref> does investigate how to generate clarification questions but this does not require the understanding of explicit natural language rules.</p><p>Rule Extraction From Text There is a long line of work in the automatic extraction of rules from text <ref type="bibr" target="#b24">(Silvestro, 1988;</ref><ref type="bibr" target="#b14">Moulin and Rousseau, 1992;</ref><ref type="bibr" target="#b5">Delisle et al., 1994;</ref><ref type="bibr" target="#b7">Hassanpour et al., 2011;</ref><ref type="bibr" target="#b14">Moulin and Rousseau, 1992)</ref>. The work tackles a similar problem-interpretation of rules and reg- ulatory text-but frames it as a text-to-structure task as opposed to end-to-end question-answering. For example, <ref type="bibr" target="#b5">Delisle et al. (1994)</ref> maps text to horn clauses. This can be very effective, and good results are reported, but suffers from the general problem of such approaches: they require careful ontology building, layers of error-prone linguistic preprocessing, and are difficult for non-experts to create annotations for.</p><p>Question Generation Our task involves the au- tomatic generation of natural language questions. Previous work in question generation has focussed on producing questions for a given text, such that the questions can be answered using this text (Van- derwende, 2008; M. <ref type="bibr" target="#b12">Olney et al., 2012;</ref><ref type="bibr" target="#b20">Rus et al., 2011</ref>). In our case, the questions to generate are derived from the background text but cannot be answered by them. <ref type="bibr" target="#b13">Mostafazadeh et al. (2016)</ref> investigate how to generate natural follow-up ques- tions based on the content of an image. Besides not working in a visual context, our task is also different because we see question generation as a sub-task of question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we present a new task as well as an annotation protocol, a dataset, and a set of base- lines. The task is challenging and requires models to generate language, copy tokens, and make log- ical inferences. Through the use of an interactive and dialog-based annotation interface, we achieve good agreement rates at a low cost. Initial baseline results suggest that substantial improvements are possible and require sophisticated integration of entailment-like reasoning and question generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Do</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The different stages of the annotation process (excluding the rule text extraction stage). First a human annotator generates an underspecified input question (question generation). Then, a virtual user and a human annotator collaborate to produce a dialog of follow-up questions and answers (dialog generation). Finally, a scenario is generated from parts of the dialog, and these parts are omitted in the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: We use different annotators (indicated by different colors) to create the complete dialog tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Category</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For neural models, we use Decom- posed Attention Model (DAM) (Parikh et al., 2016) trained on each the SNLI and ShARC corpora us- ing ELMO embeddings (Peters et al., 2018). 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>EEA. input output output inputAnswer Instance</head><label></label><figDesc>or less? Yes You'll carry on paying National Insurance for the first 52 weeks you're abroad if you're working for an employer outside the</figDesc><table>Instance 1 

Instance 2 

Support Text 
Scenario 

Question 

Follow-up 

Answer 

Do I need to carry on 
paying UK National 
Insurance? 

I am working for an 
employer in Canada. 

Yes 

Have you been working abroad 52 
weeks or less? 

Yes 

You'll carry on paying National 
Insurance for the first 52 weeks 
you're abroad if you're working for 
an employer outside the EEA. 

input 

output 

output 

input 

Utterance 1 

Utterance 2 

Rule Text 

Scenario 

Question 

Follow-up 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Selected Results of the baseline models on 
follow-up question generation. 

Model 
Micro Acc. Macro Acc. 

Random 
0.330 
0.326 
Surface LR 
0.682 
0.333 

DAM (SNLI) 
0.479 
0.362 
DAM (ShARC) 
0.492 
0.322 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Results of entailment models on ShARC.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 5 : Results of the models on the CMR task.</head><label>5</label><figDesc></figDesc><table>eration model is used to produce a follow-up ques-
tion, f 1 . The rule text and produced follow-up 
question are then passed as inputs to the Sce-
nario Interpretation model. If the output of this is 
IRRELEVANT, then the CM predicts f 1 , otherwise, 
these steps are repeated recursively until the clas-
sification model no longer predicts MORE or the 
entailment model predicts IRRELEVANT, in which 
case the model produces a final answer. We also in-
vestigate an extension of the NMT-copy model on 
the end-to-end task. Input sequences are encoded 
as a concatenation of the rule text, question, sce-
nario and history. The model consists of a shared 
encoder LSTM, a 4-class classification head with 
attention, and a decoder GRU to generate followup 
questions. The model was trained by alternating 
training the classifier via standard softmax-cross en-
tropy loss and the followup generator via seq2seq. 
At test time, the input is first classified, and if the 
predicted class is MORE, the follow-up generator 
is used to generate a followup question, f 1 . A sim-
pler model without the separate classification head 
failed to produce predictive results. 

</table></figure>

			<note place="foot" n="1"> This is well within the range of what is considered as substantial agreement (Artstein and Poesio, 2008).</note>

			<note place="foot" n="2"> The dataset and its Codalab challenge can be found at https://sharc-data.github.io. 3 One may argue that the the size of the dataset is not sufficient for training end-to-end neural models. While we believe that the availability of large datasets such as SNLI or SQuAD has helped drive the state-of-the-art forward on related</note>

			<note place="foot" n="4"> We use AllenNLP implementations of BiDAF &amp; DAM</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by an Allen Distinguished Investigator Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">QuAC : Question Answering in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>EMNLP. ArXiv: 1808.07036</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From text to horn clauses: Combining linguistic analysis and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Sylvain Delisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Franois Delannoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Canadian AI Conf</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A framework for the automatic extraction of rules from online text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search-based Neural Structured Learning for Sequential Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04115</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Question generation from concept maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Olney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Person</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Margaret Mitchell, Xiaodong He, and Lucy Vanderwende</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno>abs/1603.06059</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Generating natural questions about an image</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated knowledge acquisition from regulatory texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Expert</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">A decomposable attention model for natural language inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2737" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CoQA: A Conversational Question Answering Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07042[cs].ArXiv:1808.07042CitationKey:reddyCoQAConversation-alQuestion2018</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Question generation shared task and evaluation challenge: Status report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Wyse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation, ENLG &apos;11</title>
		<meeting>the 13th European Workshop on Natural Language Generation, ENLG &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="318" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Survey of Available Corpora For Building Data-Driven Dialogue Systems: The Journal Version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using explanations for knowledge-base acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Silvestro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="169" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The importance of being important: Question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge</title>
		<meeting>the Workshop on the Question Generation Shared Task and Evaluation Challenge</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fastqa: A simple and efficient neural architecture for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
		<idno>abs/1703.04816</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ELIZAa computer program for the study of natural language communication between man and machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno>abs/1710.06481</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
