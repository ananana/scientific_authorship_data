<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Named Entity Recognition and Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Named Entity Recognition and Disambiguation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Extracting named entities in text and linking extracted names to a given knowledge base are fundamental tasks in applications for text understanding. Existing systems typically run a named entity recognition (NER) model to extract entity names first, then run an entity linking model to link extracted names to a knowledge base. NER and linking models are usually trained separately , and the mutual dependency between the two tasks is ignored. We propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to improve the performance of the other. To the best of our knowledge, JERL is the first model to jointly optimize NER and linking tasks together completely. In experiments on the CoNLL&apos;03/AIDA data set, JERL outper-forms state-of-art NER and linking systems , and we find improvements of 0.4% absolute F 1 for NER on CoNLL&apos;03, and 0.36% absolute precision@1 for linking on AIDA.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In applications of complex Natural Language Pro- cessing tasks, such as automatic knowledge base construction, entity summarization, and question answering systems, it is essential to first have high quality systems for lower level tasks, such as part- of-speech (POS) tagging, chunking, named en- tity recognition (NER), entity linking, and parsing among others. These lower level tasks are usually decoupled and optimized separately to keep the system tractable. The disadvantage of the decou- pled approach is that each lower level task is not aware of other tasks and thus not able to leverage information provided by others to improve perfor- mance. What is more, there is no guarantee that their outputs will be consistent.</p><p>This paper addresses the problem by building a joint model for Entity Recognition and Disam- biguation (ERD). The goal of ERD is to extract named entities in text and link extracted names to a knowledge base, usually Wikipedia or Freebase. ERD is closely related to NER and linking tasks. NER aims to identify named entities in text and classify mentions into predefined categories such as persons, organizations, locations, etc. Given a mention and context as input, entity linking con- nects the mention to a referent entity in a knowl- edge base.</p><p>Existing ERD systems typically run a NER to extract entity mentions first, then run an entity linking model to link mentions to a knowledge base. Such a decoupled approach makes the sys- tem tractable, and both NER and linking models can be optimized separately. The disadvantages are also obvious: 1) errors caused by NER will be propagated to linking and are not recoverable 2) NER can not benefit from information available used in entity linking; 3) NER and linking may create inconsistent outputs.</p><p>We argue that there is strong mutual depen- dency between NER and linking tasks. Consider the following two examples:</p><p>1. The New York Times (NYT) is an American daily newspaper. 2. Clinton plans to have more news conferences in 2nd term. <ref type="bibr">WASHINGTON 1996-12-06</ref> Example 1 is the first sentence from the Wikipedia article about "The New York Times". It is reasonable but incorrect for NER to identify "New York Times" without "The" as a named en- tity, while entity linking has no trouble connect- ing "The New York Times" to the correct entity.</p><p>Example 2 is a news title where our NER classi- fies "WASHINGTON" as a location, since a lo- cation followed by a date is a frequent pattern in news articles it learned, while the entity link- ing prefers linking this mention to the U.S. pres- ident "George Washington" since another presi- dent's name "Clinton" is mentioned in the con- text. Both the entity boundaries and entity types predicted by NER are correlated to the knowledge of entities linked by entity linking. Modeling such mutual dependency is helpful in resolving incon- sistency and improving performance for both NER and linking.</p><p>We propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to im- prove the performance of the other. If NER is highly confident on its outputs of entity boundaries and types, it will encourage entity linking to link an entity which is consistent with NER's outputs, and vice versa. In other words, JERL is able to model how consistent NER and linking's outputs are, and predict coherent outputs. According to our experiments, this approach does improve the end to end performance. To the best of our knowl- edge, JERL is the first model to jointly optimize NER and linking tasks together completely .</p><p>Sil (2013) also proposes jointly conducting NER and linking tasks. They leverage existing NER/chunking systems and Freebase to over gen- erate mention candidates and leave the linking al- gorithm to make final decisions, which is a re- ranking model. Their model captures the depen- dency between entity linking decisions and men- tion boundary decisions with impressive results. The difference between our model and theirs is that our model jointly models NER and linking tasks from the training phrase, while their model is a combined one which depends on an existing state-of-art NER system. Our model is more pow- erful in capturing mutual dependency by consider- ing entity type and confidences information, while in their model the confidence of outputs is lost in the linking phrase. Furthermore, in our model NER can naturally benefit from entity linking's decision since both decisions are made together, while in their model, it is not clear how the linking decision can help the NER decision in return.</p><p>Joint optimization is costly. It increases the problem complexity, is usually inefficient, and requires the careful consideration of features of multiple tasks and mutual dependency, making proper assumptions and approximations to enable tractable training and inference. However, we believe that joint optimization is a promising di- rection for improving performance for NLP tasks since it is closer to how human beings process text information. Experiment result indicates that our joint model does a better job at both NER and linking tasks than separate models with the same features, and outperforms state-of-art systems on a widely used data set. We found improvements of 0.4% absolute F 1 for NER on CoNLL'03 and 0.36% absolute precision@1 for linking on AIDA. NER is a widely studied problem, and we believe our improvement is significant.</p><p>The contributions of this paper are as follows: 1. We identify the mutual dependency between NER and linking tasks, and argue that NER and linking should be conducted together to improve the end to end performance. 2. We propose the first completely joint NER and linking model, JERL, to train and inference the two tasks together. Efficient training and inference algorithms are also presented. 3. The JERL outperforms the best NER record on the CoNLL'03 data set, which demonstrates how NER could be improved further by leverag- ing knowledge base and linking techniques.</p><p>The remainder of this paper is organized as fol- lows: the next section discusses related works on NER, entity linking, and joint optimization; sec- tion 3 presents our Joint Entity Recognition and Linking model in detail; section 4 describes ex- periments, results, and analysis; and section 5 con- cludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of eval- uation campaigns such as MUC <ref type="bibr" target="#b1">(Chinchor and Marsh, 1998)</ref>, the CoNLL 2003 NER shared task <ref type="bibr" target="#b24">(Tjong Kim Sang and De Meulder, 2003</ref>) and ACE ( <ref type="bibr" target="#b3">Doddington et al., 2004</ref>). Along with the im- provement of Machine Learning techniques, sta- tistical approaches have become a major direc- tion for research on NER, especially after Condi- tional Random Field is proposed by <ref type="bibr" target="#b11">Lafferty et al. (2001)</ref>. The well known state-of-art NER systems are Stanford NER ( <ref type="bibr" target="#b5">Finkel et al., 2005</ref>) and UIUC NER ( <ref type="bibr" target="#b16">Ratinov and Roth, 2009)</ref>. <ref type="bibr" target="#b12">Liang (2005)</ref> compares the performance of the 2nd order lin- ear chain CRF and Semi-CRF ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004</ref>) in his thesis. <ref type="bibr" target="#b13">Lin and Wu (2009)</ref> clus- ter tens of millions of phrases and use the result- ing clusters as features in NER reporting the best performance on the CoNLL'03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter.</p><p>Entity linking was initiated with Wikipedia- based works on entity disambiguation ( <ref type="bibr" target="#b0">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b2">Cucerzan, 2007)</ref>. This task is encouraged by the TAC 2009 KB population task 1 first and receives more and more attention from the research community <ref type="bibr" target="#b8">(Hoffart et al., 2011;</ref><ref type="bibr" target="#b17">Ratinov et al., 2011;</ref>). Linking usu- ally takes mentions detected by NER as its input. <ref type="bibr" target="#b21">Stern et al. (2012)</ref> and <ref type="bibr" target="#b25">Wang et al. (2012)</ref> present joint NER and linking systems and evaluate their systems on French and Chinese data sets. <ref type="bibr" target="#b20">Sil and Yates (2013)</ref> take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted "En- tity Recognition and Disambiguation Challenge" which focused on the end to end performance of linking system 2 .</p><p>Joint optimization models have been studied at great length. E.g. Dynamic CRF ( <ref type="bibr" target="#b15">McCallum et al., 2003</ref>) has been proposed to conduct Part- of-Speech Tagging and Chunking tasks together. <ref type="bibr" target="#b4">Finkel and Manning (2009)</ref> show how to model parsing and named entity recognition together. <ref type="bibr" target="#b26">Yu et al. (2011)</ref> work on jointly entity identifica- tion and relation extraction from Wikipedia. Sil's (2013) work on jointly NER and linking is de- scribed in the introduction section of this paper. It is worth noting that joint optimization does not always work. The CoNLL 2008 shared task <ref type="bibr" target="#b22">(Surdeanu et al., 2008</ref>) was intended to encourage jointly optimize parsing and semantic role label- ing, but the top performing systems decoupled the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Joint Entity Recognition and Linking</head><p>Named entity recognition is usually formalized as a sequence labeling task, in which each word is classified to not-an-entity or entity labels. Condi- tional Random Fields (CRFs) is one of the popu- g. a word sequence appears at po- sition i or whether a word contains exactly four digits). It is hard, if not impossible, to encode entity-level features (such as "entity length" and "correlation to known entities") in traditional CRF. Entity linking is typically formalized as a ranking task. Features used for entity linking are at entity- level inherently (such as entity prior probability; whether there are any related entity names or dis- criminative keywords occurring in the context).</p><p>The main challenges of joint optimization be- tween NER and linking are: how to combine a se- quence labeling model and a ranking model; and how to incorporate word-level and entity-level fea- tures. In a linear chain CRF model, each word's label is assumed to depend on the observations and the label of its previous word. Semi-CRF carefully relaxes the Markov assumption between words in CRF, and models the distribution of seg- mentation boundaries directly. We further extend Semi-CRF to model entity distribution and mu- tual dependency over segmentations, and name it Joint Entity Recognition and Linking (JERL). The model is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">JERL</head><p>Let x = {x i } be a word sequence containing |x| words. Let s = {s j } be a segmentation assign- ment over x, where segment s j = (u j , v j ) con- sist of a start position u j and an end position v j . All segments have a positive length and are adja- cent to each other, so every (u j , v j ) always satis- fies 1 ≤ u j ≤ v j ≤ |x| and u j+1 = v j + 1. Let y = {y j } be labels in a fixed label alphabet Y over a segmentation assignment s. Here Y is the set of types NER to predict.</p><formula xml:id="formula_0">x s j = (x u j . . . x v j )</formula><p>is the corresponding word sequence to s j , and E s j = {e j,k } is a set of entities in the knowl- edge base (KB), which may be referred by word sequence x s j in the entity linking task. Each en- tity e j,k is associated with a label y e j,k ∈ {0, 1}. Label y e j,k takes 1 iff x s j referring to entity e j,k , and 0 otherwise. If x s j does not refer to any entity in the KB, y e j,0 takes 1, which is analogous to the NIL 3 identifier in entity linking.</p><p>Based on the preliminaries and notations, <ref type="figure">Fig- ure 1</ref> shows the factor graph ( <ref type="bibr" target="#b9">Kschischang et al., 2001</ref>) of JERL. There are similar factor nodes for every (u j , v j , y e j,k ), we only show the first one (u j , v j , y e j,0 ) for clarity. Given x, let a = (s, y, y e ) be a joint assign- ment, and g(x, j, a) be local functions for x s j , namely features, each of which maps an assign- ment a to a measurement g k (x, j, a) ∈ . Then G(x, a) = |s| j=1 g(x, j, a) is the factor graph defining a probability distribution of assignment a conditioned on word sequence x.</p><p>Then JERL, conditional probability of a over x, is defined as:</p><formula xml:id="formula_1">P (a|x, w) = 1 Z(x) e w·G(x,a)<label>(1)</label></formula><p>where w is the weight vector corresponding to G will be learned later, and Z(x) is the normaliza- tion factor Z(x) = a∈A e w·G(x,a) , in which A is the union of all possible assignments over x.</p><p>JERL is a probabilistic graphical model. More specificly, as shown in <ref type="figure">Figure 1</ref>, there are three groups of local functions and one constrain intro- duced. Each of them take a different role in JERL, as described below:</p><p>Features defined on x, s j , y j , y j−1 are written as g ner (x, s j , y j , y j−1 ). These functions model segmentation and entity types' distribution over x. Actually, every local features used in NER can be formulated in this way, and thus can be included in JERL. We thus refer to them as "NER features".</p><p>Features defined on x, s j , y e j,k are written as g el (x, s j , y e j,k ) and are called "linking features". These features model joint probabilities of word sequence x s j and linking decisions y k j,k = 1(0 ≤ k ≤ |E s j |) given context x. JERL incorporates all linking features in this way.</p><p>Features defined on y j , y e j,k are written as g cr (y j , y e j,k ). These features model "mutual de-pendency" between NER and linking's outputs. For each entity e j,k , there is additional informa- tion available in the knowledge base, e.g. cate- gories information, popularity and relationship to other entities. These features encourage predicting coherent outputs for NER and linking. There is one constrain for each y e j that the cor- responding x s j can refer to only one entity e j,k ∈ E s j or NIL. This is equivalent to |E s j | k=0 y e j,k = 1. Based on the above description, G(x, a) in equation 1 is the sum of conjunction (g ner , g el , g cr ) over s, and can be rewritten as,</p><formula xml:id="formula_2">G(x, a) = |s| j=1 ( g ner (x, s j , y j , y j−1 ) , |E j | k=0 g el (x, s j , y e j,k ) , |E j | k=0 g cr (x, y j , y e j,k ) )</formula><p>In summary, JERL jointly models the NER and linking, and leverages mutual dependency be- tween them to predict coherent outputs. Previ- ous works <ref type="bibr" target="#b2">(Cucerzan, 2007;</ref><ref type="bibr" target="#b17">Ratinov et al., 2011;</ref><ref type="bibr" target="#b20">Sil and Yates, 2013</ref>) on linking argued that en- tity linking systems often suffer because of errors involved in mention detection phrase, especially false negative errors, and try to mitigate it via over- generating mention candidates. From the mention generation perspective, JERL actually considers every possible assignment and is able to find the optimal a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Estimation</head><p>We describe how to conduct parameter estima- tion for JERL in this section. Given independent and identically distributed (i.i.d.) training data T = {(x t , a t )} N t=1 , the goal of parameter estima- tion is to find optimal w * to maximize the joint probability of the assignments {a t } over {x t }.</p><formula xml:id="formula_3">w * = argmax w∈∈ |G| N t=1 P (a t |x t , w)</formula><p>We use conditional log likelihood with 2 norm as the objective function in training,</p><formula xml:id="formula_4">L(T , w) = t logP (a t |x t , w) − 1 2σ 2 ||w|| 2 2</formula><p>The above function is concave, adding regulariza- tion to ensure that it has exactly one global opti- mum. We adopt a limited-memory quasi-Newton method ( <ref type="bibr" target="#b14">Liu and Nocedal, 1989)</ref> to solve the opti- mization problem.</p><p>The gradient of L(T , w) is derived as,</p><formula xml:id="formula_5">∂L ∂w = t (G(x t , a t ) − a G(x t , a )P (a |x t , w)) − w σ 2<label>(2)</label></formula><p>As shown in <ref type="figure">Figure 1</ref>, our model's factor graph is a tree, which means the calculation of the gradient is tractable. Inspired by the forward backward algorithm ( <ref type="bibr" target="#b19">Sha and Pereira, 2003)</ref> and Semi-CRF ( <ref type="bibr" target="#b18">Sarawagi and Cohen, 2004</ref>), we leverage dynamic program- ming techniques to compute the normalization factor Z w and marginal probability P (a j |x t , w) when w is given.( <ref type="bibr" target="#b23">Sutton and McCallum, 2006</ref>) The parameter estimation algorithm is abstracted in Algorithm 1.</p><formula xml:id="formula_6">Algorithm 1: JERL parameter estimation input : training data T = {(x t , a t )} N t=1</formula><p>output: the optimal w w ← 0; while weight w is not converged do Z ← 0; w ← 0; for t ← 1 to N do calculate α t , β t according to eq.3; calculate Z t according to eq.4 calculate w t according to eq.2, 5;</p><formula xml:id="formula_7">Z ← Z + Z t ; w ← w + w t ; end update w to maximize log likelihood L(T , w) under (Z, w ) via L-BFGS; end Let α i,y (i ∈ [0, |x|], y ∈ Y)</formula><p>be the sum of po- tential functions of all possible assignments over (x 1 . . . x i ) whose last segmentation's labels are y. Then α i,y can be calculated recursively from i = 0 to i = |x| as below.</p><p>We first define base cases as α 0,y = 1| {y∈Y} . When i ∈ (0, |x|]:</p><formula xml:id="formula_8">α i,y = L d=1 y ∈Y α i−d,y ψ ner i−d+1,i,y,y ( y e j ∈Y e * j ψ el.cr i−d+1,i,y,y e j )<label>(3)</label></formula><p>where L is the max segmentation length in Semi- CRF, and Y e * j is all valid assignments for y e j which satisfies |E s j | k=0 y e j,k = 1. where w ner , w el and w cr are weights for g ner , g el and g cr in w accordingly. The value of Z w can then be written as</p><formula xml:id="formula_9">Z w (x) = y α |x|,y<label>(4)</label></formula><p>Define β i,y (i ∈ [0, |x|], y ∈ Y) as the sum of potential functions of all possible assignments over (x i+1 . . . x |x| ) whose first segmentation's la- bels are y. β i,y is calculated in a similar way, ex- cept they are calculated from i = |x| to left i = 0.</p><p>Once we get {α i,j } and {β i,j }, the marginal probability of arbitrary assignment a j = (s j , y j , y e j ), where s j = (u j , v j ), can be calculated as below:</p><formula xml:id="formula_10">P (s j , y j |x, w) = ( y ∈Y α u j −1,y ψ ner u j ,v j ,y j ,y )β v j ,y j Z w (x) and P (a j |x t , w) = P (s j , y j |x, w) ψ el.cr u j ,v j ,y j ,y e j y e ∈Y e * j ψ el.cr u j ,v j ,y j ,y e<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>Given a new word sequence x and model weights w trained on a training set, the goal of in- ference is to find the best assignment, a * = argmax a P (a|x, w) for x. We extend the Viterbi algorithm to exactly infer the best assignment. The inference algorithm is shown in Algorithm 2. Let φ(u j , v j , y j , y j−1 ) be the product of poten- tials depending on (s j , y j , y j−1 ) as,</p><formula xml:id="formula_11">φ(u j , v j , y j , y j−1 ) = ψ ner u j ,v j ,y j ,y j−1 ( y e j ∈Y e * j ψ el.cr u j ,v j ,y j ,y e j )<label>(6)</label></formula><p>Algorithm 2: JERL inference input : one word sequence x and weights w output: the best assignment a over x // shrink JERL graph to a Semi-CRF graph; for u ← 1 to |x| do for v ← u + 1 to |x| do for (y, y ) ∈ Y × Y do calculate φ u,v,y,y // see eq.6; end end end // infer the best assignment of (s * , y * ); for i ← 1 to |x| do for y ∈ Y do calculate V i,y // see eq.7; end end (s * , y * ) ← argmax(V i,y ); // infer the best assignment of {y e j };</p><formula xml:id="formula_12">for j ← 1 to |s * | do y e j ← argmax(P (|x, w, s * j , y * j )) end a * ← (s * , y * , y e * );</formula><p>and let V (i, y) denotes the largest value of (w · G(x, a )) where a could be any possible par- tial assignment starting from x 1 to x i . The best (s * , y * ) are derived during the following recur- sive calculation,</p><formula xml:id="formula_13">V i,y =        max y ∈Y,d∈[1,L] (V i−d,y + φ( i−d+1 , i, y, y )) i &gt; 0 0 i = 0 −∞ i &lt; 0 (7)</formula><p>where L is the maximum segmentation length for Semi-CRF. Once (s * , y * ) are found, the corresponding</p><formula xml:id="formula_14">y e * j = argmax {y e ∈Y e * j } (ψ el.cr u * j ,v * j ,y * j ,y e</formula><p>) is also the optimal one. Then a * = (s * , y * , y e * ) is the best assignment for the given x and w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In our experiments, we first construct two base- line models JERL ner and JERL el , which use exact NER and EL feature sets used in JERL. Then eval- uate JERL and the two baseline models against several state-of-art NER and linking systems. Af- ter that, we evaluate JERL under different feature <ref type="table" target="#tab_1">CoNLL'03 Training Dev set  Test  Articles  946  216  231  Sentences  14,987  3,466  3,684  Tokens  203,621 51,362 46,435  Entities  23,499  5,942  5,648  NIL Entities  4,857  1,129</ref> 1,133 For entity linking, we take Wikipedia as the ref- erent knowledge base. We use a Wikipedia snap- shot dumped in May 2013, which contains around 4.8 million articles. We also align our Wikipedia dump with additional knowledge bases, Freebase and Satori (a Microsoft internal knowledge base), to enrich the information of these entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We follow the CoNLL'03 metrics to evaluate NER performance by precision, recall, and F 1 scores, and follow Hoffart's (2011) experiment setting to evaluate linking performance by micro preci- sion@1. Since the linking labels of CONLL'03 were annotated in 2011, it is not completely con- sistent with the Wikipedia dump we used in the case. We only consider mention entity pairs where the ground truth are known, and ignore around 20% of NIL mentions in the ground truth.  <ref type="table" target="#tab_2">Table 2</ref> shows features used in our models. JERL uses all features in the three categories, while JERL ner and JERL el use only one corresponding category. All three models are trained on the train and development set, and evaluated on the test set of CoNLL'03/AIDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Features Word unigram / bigram Lower cased unigram / bigram Word shape unigram / bigram Stemmed unigram / bigram POS unigram / bigram</head><note type="other">NER Chunk unigram / bigram Words in the 4 left/right window Character n-grams, n ≤ 4 Brown clusters WordNet clusters Dictionaries Alternative names Entity priors Entity name priors Linking Entity priors over names Context scores Geo distance Related entities Mutual Type-category correlation</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">NER</head><p>Features in the NER category are relevant to NER. We considered the most commonly used features in literatures ( <ref type="bibr" target="#b5">Finkel et al., 2005;</ref><ref type="bibr" target="#b12">Liang, 2005;</ref><ref type="bibr" target="#b16">Ratinov and Roth, 2009)</ref>. We collect several known name lists, like popular English first/last names for people, organization lists and so on from Wikipedia and Freebase. UIUC NER's lists are also included. In addition, we extract entity name lists from the knowledge base we used for entity linking, and construct 655 more lists. Al- though those lists are noisy, we find that statisti- cally they do improve the performance of our NER baseline by a significant amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Linking</head><p>Features in linking category are relevant to entity linking. An entity can be referred by its canoni- cal name, nick names, alias, and first/last names. Those names are defined as alternative names for this entity. We collect all alternative names for all known entities and build a name to entity index. This index is used to select entity candidates for any word sequence, also known as surface form. Following previous work by , we calculate entity priors and entity name priors from Wikipedia. Context scores are calculated based on discriminative keywords. Geo distance and related entities capture the relatedness among entities in the given context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Mutual</head><p>Features in this category capture the mutual de- pendency between NER and linking's outputs. For each entity in a knowledge base, there is category information available. We aggregate around 1000 distinct categories from multiple sources. One en- tity can have multiple categories. For example, London is connected to 29 categories. We use all combinations between NER types and categories as features in JERL, and let the model learn the correlation of each combination. This encourages coherent NER and EL decisions, which is one of the key contributions of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Non-local features</head><p>Features capturing long distance dependency be- tween hidden labels are classified as non-local features. Those features are very helpful in im- proving NER system performance but are costly. Since this is not the focus of this paper, we take a simple approach to incorporate non-local fea- tures. We cache history results of previous sen- tences in a 1000 words window, and adopt sev- eral heuristic rules for personal names. This ap- proach contributes 0.2 points to the final NER F 1 score. Non-local features are also considered in linking <ref type="bibr" target="#b17">(Ratinov et al., 2011;</ref>). We try several features, which has been proved to be helpful in TAC data set. However, the gain on CoNLL'03/AIDA data set is not obvious, we do not optimize linking globally.</p><p>Lastly, based on preliminary studies and exper- iments, we set the maximum segmentation length to 6 and max candidate count per segmentation to 5 for efficient training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">State-of-Art systems</head><p>We take three state-of-art NER systems: NereL <ref type="bibr" target="#b20">(Sil and Yates, 2013)</ref>   <ref type="bibr">Hof11 (Hoffart et al., 2011</ref>) are compared with our models. NereL achieves the best precision@1. Kul09 formulates the local compatibility and global coherence in en- tity linking, and optimizes the overall entity as- signment for all entities in a document via a lo- cal hill-climbing approach. Hof11 unifies the prior probability of an entity being mentioned, the simi- larity between context and entity, and the coher- ence between entity candidates among all men- tions in a dense graph. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of different NER systems on the CoNLL'03 testb data set. We re- fer the numbers of state-of-art systems reported by <ref type="bibr" target="#b20">Sil and Yates (2013)</ref>. Stanford NER achieves the best precision, but its recall is low. UIUC reports the (almost) best recorded F 1 . JERL ner considers features only in the NER category, which could be treated as a pure NER system implemented in Semi-CRF. Actually CRF-based implementation with a similar feature set has comparable perfor- mance. Our baseline JERL ner is strong enough. We argue that that it is mainly because of the ad- ditional dictionaries derived from the knowledge base. JERL further pushes the F 1 to 91.2, which outperforms UIUC by 0.4 points in F 1 score. To the best of our knowledge, it is the best F 1 on CoNLL'03 since 2009. The reason our model can outperform state-of-art systems is that, it has more knowledge about entities via incorporate en- tity linking techniques. If an entity can be linked to a well known entity via entity linking in high   <ref type="table">Table 5</ref>: JERL features analysis confidence, its mention boundary and entity type are confirmed implicitly. <ref type="table" target="#tab_6">Table 4</ref> shows the performance of different en- tity linking systems on the AIDA test set. Kul09 and Hof11 use only the correct mentions detected by the Stanford NER as input, and thus their re- call is bound by the recall of NER. NereL uses its overgeneration techniques to generate mention candidates, and outperforms Hoff11 in both preci- sion and recall. Our baseline model JERL el is also evaluated on Stanford NER generated mentions, which has comparable performance with Kul09 and Hof11. JERL achieves precision@1 84.58 which is better than NereL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>We run 15 trials for both NER and linking's ex- periments and report the average numbers above. The standard deviations are 0.11% and 0.08% for NER and linking separately, which pass the stan- dard t-test with confidence level 5%, demonstrat- ing the significance of our results.</p><p>In order to investigate how different features contribute to the overall gain. We compare JERL ner with four different feature sets. <ref type="table">Table  5</ref> summaries the results. In the trial "+candidate", JERL expands every possible segmentation with corresponding entity list and builds its factor graph without any linking and mutual features. This ver- sion's F 1 drops to 88.7 which indicates the created structure is quite noisy. In the "+candidate +link- ing" trial, only linking features are enabled and the F 1 is comparable to the baseline. On the other side, in the "+candidate +mutual" trial when mu- tual features are enabled the F 1 increases to 90.6. If we combine both linking and mutual features,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PER</head><p>LOC ORG Other people.person 3.65 0.817 1.260 -1.782 location.city -0.187 0.712 0.491 -0.188 sports.team -0.180 2.382 3.595 -2.019  JERL achieves the reported performance. The re- sult indicates that mutual features are the deter- mining factor to the performance gain. <ref type="table" target="#tab_7">Table 6</ref> shows weights of learned mutual depen- dency of three categories "people.person", "loca- tion.city", and "sports.team". The bigger a weight is, the more consistent this combination would be. From the weights, we find several interest- ing things. If an entity belongs to any of the three categories, it is less likely to be predicted as non-an-entity by NER. If an entity belongs to the category of "people.person", it more likely to be predicted as PER. When an entity belongs to the category "location.city" or "sports.team", NER may predict it as ORG or LOC. This is be- cause in the CoNLL'03/AIDA data set, there are many sports teams mentioned by their city/country names. JERL successfully models such unex- pected mutual dependency. <ref type="table" target="#tab_8">Table 7</ref> compares the performance and training time under different settings of max segmentation length (MSL) and max referent count (MRC). We use machines with Intel Xeon E5620 @ 2.4GHz CPU (8 cores / 16 logical processors) and 48GB memory. We run every setting 10 times and report the averages. As MSL and MRC increasing, the performance is slightly better, but the training time increased a lot. MSL has linear impact on training time, while MRC affects training time more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we address the problem of joint opti- mization of named entity recognition and linking. We propose a novel model, JERL, to jointly train and infer for NER and linking tasks. To the best of our knowledge, this is the first model which trains two tasks at the same time. The joint model is able to leverage mutual dependency of the two tasks, and predict coherent outputs. JERL outperforms the state-of-art systems on both NER and linking tasks on the CoNLL'03/AIDA data set.</p><p>For future works, we would like to study how to leverage existing partial labeled data, either for NER or for linking only, in joint optimization, and incorporate more NLP tasks together for multi- tasks joint optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>Figure 1: The factor graph of JERL model</figDesc><graphic url="image-1.png" coords="3,307.28,62.81,230.40,146.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Overview of CoNLL'03/AIDA data set 

settings to analysis the contributions of each fea-
tures set, and show some examples we find. We 
also compare the training speed under different 
settings. 

4.1 Data set 

We take the CoNLL'03/AIDA English data set 
to evaluate the performance of NER and linking 
systems. CoNLL'03 is extensively used in prior 
work on NER evaluation (Tjong Kim Sang and 
De Meulder, 2003). The English data is taken 
from Reuters news articles published between Au-
gust 1966 and August 1997. Four types of en-
tities persons (PER), organizations (ORG), loca-
tions (LOC), and miscellaneous names (MISC) are 
annotated. Hoffart et al. (2011) hand-annotated 
all proper nouns with corresponding entities wiht 
YAGO2, Freebase and Wikipedia IDs. This data 
is referenced as AIDA here. To the best of our 
knowledge, this data set is the biggest data set 
which has been labeled for both NER and linking 
tasks. It becomes a really good starting point for 
our work. Table 1 contains of an overview of the 
CoNLL'03/AIDA data set. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>JERL feature list 

4.3 JERL Implementation 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>NER evaluation results 

nents, then trains a maximum-entropy model to 
re-rank different assignments. UIUC NER uses 
a regularized averaged perceptron model and ex-
ternal gazetteers to achieve strong performance. 
In Addition, NereL also uses UIUC NER to gen-
erate mentions. Stanford NER uses Conditional 
Random Fields and Gibbs sampling to incorporate 
non-local features into its model. 
For entity linking systems, NereL, Kul09 
(Kulkarni et al., 2009) and </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Linking evaluation results 

# Feature set description 
NER F 1 
0 JERL ner (baseline) 
89.9 
1 + candidate 
88.7 
2 + candidate + linking 
89.9 
3 + candidate + mutual 
90.6 
4 + candidate + mutual + linking 
91.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 : Learned mutual dependency</head><label>6</label><figDesc></figDesc><table>Setting 
NER Linking 
Training 
MSL MRC 
F 1 
prec@1 time (min) 
4 
5 
87.9 
76.74 
195 
5 
5 
90.8 
84.01 
234 
6 
1 
90.8 
80.13 
37 
6 
3 
91.0 
83.21 
109 
6 
5 
91.2 
84.58 
280 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 : Training time under different settings</head><label>7</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> In the entity linking task, if a given mention refers to an entity which is not in the knowledge base, linking system should return a special identifier &quot;NIL&quot;.</note>

			<note place="foot">Roth, 2009) and Stanford NER (Finkel et al., 2005). NereL firstly over generates mentions and decomposes them to sets of connected compo885 Dataset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was performed when the first author was working at Microsoft Research. The first au-thor is sponsored by Microsoft Bing Core Rele-vance team. Thanks Shuming Shi, Bin Gao, and Yohn Cao for their helpful guidance and valuable discussions. Additionally, we would like to thank the three anonymous reviewers for their insightful suggestions and detailed comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Muc-7 information extraction task definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaine</forename><surname>Marsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the seventh message understanding conference (MUC-7), Appendices</title>
		<meeting>eeding of the seventh message understanding conference (MUC-7), Appendices</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Lance A Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A generative entitymention model for linking entities with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collective entity linking in web text: a graph-based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frank R Kschischang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-A</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collective annotation of wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic conditional random fields for jointly labeling multiple sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nips workshop on syntax, semantics and statistics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Re-ranking for joint named-entity recognition and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2369" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A joint named entity recognition and entity linking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Béchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data</title>
		<meeting>the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The conll-2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields for relational learning. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="93" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A joint chinese named entity recognition and disambiguation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CLP</title>
		<imprint>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards a top-down and bottom-up bidirectional approach to joint information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="847" to="856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
