<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Model of Coherence Based on Distributed Sentence Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiweil@stanford.edu ehovy@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Model of Coherence Based on Distributed Sentence Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2039" to="2048"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumenta-tion logic and syntactic relationships. But both argumentation semantics and cross-sentence syntax (such as coreference and tense rules) are very hard to formalize. In this paper, we introduce a neural network model for the coherence task based on distributed sentence representation. The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or re-cursive neural networks. The architecture obviated the need for feature engineering, and learns sentence representations, which are to some extent able to capture the &apos;rules&apos; governing coherent sentence structure. The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coherence is a central aspect in natural language processing of multi-sentence texts. It is essen- tial in generating readable text that the text plan- ner compute which ordering of clauses (or sen- tences; we use them interchangeably in this paper) is likely to support understanding and avoid con- fusion. As <ref type="bibr" target="#b28">Mann and Thompson (1988)</ref> define it, A text is coherent when it can be ex- plained what role each clause plays with regard to the whole.</p><p>Several researchers in the 1980s and 1990s ad- dressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; <ref type="bibr" target="#b28">(Mann and Thompson, 1988)</ref>), which defined about 25 relations that govern clause interde- pendencies and ordering and give rise to text tree structures; the stepwise assembly of seman- tic graphs to support adductive inference toward the best explanation <ref type="bibr" target="#b19">(Hobbs et al., 1988)</ref>; Dis- course Representation Theory (DRT; <ref type="bibr" target="#b24">(Lascarides and Asher, 1991)</ref>), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intention- oriented conversation blocks and their stack-based queueing to model attention flow <ref type="bibr" target="#b14">(Grosz and Sidner, 1986)</ref>, and more recently an inventory of a hundred or so binary inter-clause relations and as- sociated annotated corpus (Penn Discourse Tree- bank. Work in text planning implemented some of these models, especially operationalized RST <ref type="bibr" target="#b20">(Hovy, 1988)</ref> and explanation relations <ref type="bibr" target="#b35">(Moore and Paris, 1989)</ref> to govern the planning of coher- ent paragraphs. Other computational work defined so called schemas <ref type="bibr" target="#b29">(McKeown, 1985)</ref>, frames with fixed sequences of clause types to achieve stereo- typical communicative intentions.</p><p>Little of this work survives. Modern research tries simply to order a collection of clauses or sen- tences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defin- ing a set of increasingly sophisticated features by which algorithms can be trained to propose order- ings. Features being explored include the clause entities, organized into a grid <ref type="bibr" target="#b23">(Lapata and Barzilay, 2005</ref>; <ref type="bibr" target="#b0">Barzilay and Lapata, 2008)</ref>, coreference clues to ordering <ref type="bibr" target="#b10">(Elsner and Charniak, 2008)</ref>, named-entity categories <ref type="bibr" target="#b8">(Eisner and Charniak, 2011)</ref>, syntactic features ( <ref type="bibr" target="#b27">Louis and Nenkova, 2012</ref>), and others. Besides being time-intensive (feature engineering usually requites considerable effort and can depend greatly on upstream feature extraction algorithms), it is not immediately ap- parent which aspects of a clause or a coherent text to consider when deciding on ordering. More im- portantly, the features developed to date are still incapable of fully specifying the acceptable order- ing(s) within a context, let alone describe why they are coherent.</p><p>Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capa- ble of capturing both semantic and syntactic as- pects of tokens (e.g., <ref type="bibr" target="#b3">(Bengio et al., 2006</ref>)), en- tities, N-grams ( <ref type="bibr" target="#b51">Wang and Manning, 2012)</ref>, or phrases ( ). More recent re- searches have begun looking at higher level dis- tributed representations that transcend the token level, such as sentence-level ( <ref type="bibr" target="#b25">Le and Mikolov, 2014)</ref> or even discourse-level <ref type="bibr" target="#b22">(Kalchbrenner and Blunsom, 2013)</ref> aspects. Just as words combine to form meaningful sentences, can we take advan- tage of distributional semantic representations to explore the composition of sentences to form co- herent meanings in paragraphs?</p><p>In this paper, we demonstrate that it is feasi- ble to discover the coherent structure of a text using distributed sentence representations learned in a deep learning framework. Specifically, we consider a WINDOW approach for sentences, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where positive examples are windows of sentences selected from original arti- cles generated by humans, and negatives examples are generated by random replacements 2 . The se- mantic representations for terms and sentences are obtained through optimizing the neural network framework based on these positive vs negative ex-amples and the proposed model produces state-of- art performance in multiple standard evaluations for coherence models ( <ref type="bibr" target="#b1">Barzilay and Lee, 2004</ref>).</p><p>The rest of this paper is organized as follows: We describe related work in Section 2, then de- scribe how to obtain a distributed representation for sentences in Section 3, and the window compo- sition in Section 4. Experimental results are shown in Section 5, followed by a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Coherence In addition to the early computa- tional work discussed above, local coherence was extensively studied within the modeling frame- work of Centering Theory ( <ref type="bibr" target="#b15">Grosz et al., 1995;</ref><ref type="bibr" target="#b50">Walker et al., 1998;</ref><ref type="bibr" target="#b48">Strube and Hahn, 1999;</ref><ref type="bibr" target="#b37">Poesio et al., 2004)</ref>, which provides principles to form a coherence metric <ref type="bibr" target="#b33">(Miltsakaki and Kukich, 2000;</ref><ref type="bibr" target="#b18">Hasler, 2004</ref>). Centering approaches suffer from a severe dependence on manually annotated input.</p><p>A recent popular approach is the entity grid model introduced by <ref type="bibr" target="#b0">Barzilay and Lapata (2008)</ref> , in which sentences are represented by a vec- tor of discourse entities along with their gram- matical roles (e.g., subject or object). Proba- bilities of transitions between adjacent sentences are derived from entity features and then concate- nated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness <ref type="bibr" target="#b13">(Filippova and Strube, 2007)</ref> or adding more useful types of features such as coreference <ref type="bibr" target="#b10">(Elsner and Charniak, 2008)</ref>, named entities ( <ref type="bibr" target="#b8">Eisner and Charniak, 2011)</ref>, and discourse relations ( <ref type="bibr" target="#b26">Lin et al., 2011</ref>).</p><p>Other systems include the global graph model ( <ref type="bibr" target="#b16">Guinaudeau and Strube, 2013</ref>) which projects en- tities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and in- corporate information from the past (i.e., preced- ing tokens) <ref type="bibr" target="#b39">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b49">Sutskever et al., 2011</ref>) for acquisition of the current output. At each step, the recurrent network takes as input both the output of previous steps and the current token, convolutes the inputs, and forwards the re- sult to the next step. It has been successfully ap- plied to tasks such as language modeling <ref type="bibr" target="#b31">(Mikolov et al., 2010</ref>) and spoken language understanding <ref type="bibr" target="#b30">(Mesnil et al., 2013)</ref>. The advantage of recur- rent network is that it does not depend on exter- nal deeper structure (e.g., parse tree) and is easy to implement. However, in the recurrent framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem <ref type="bibr" target="#b2">(Bengio et al., 1994)</ref>; two tokens may be structurally close to each other, even though they are far away in word sequence <ref type="bibr">3</ref> .</p><p>Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It com- putes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tai- lored to different task-specific requirements, such as Matrix-Vector RNN ( <ref type="bibr" target="#b46">Socher et al., 2012</ref>) that represents every word as both a vector and a ma- trix, or Recursive Neural Tensor Networks ( ) that allow the model to have greater interactions between the input vectors. Many tasks have benefited from this recursive framework, in- cluding parsing <ref type="bibr" target="#b45">(Socher et al., 2011b</ref>), sentiment analysis ( ), and paraphrase de- tection ( <ref type="bibr" target="#b44">Socher et al., 2011a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributed Representations</head><p>Both recurrent and recursive networks require a vector representation of each input token. Dis- tributed representations for words were first pro- posed in <ref type="bibr" target="#b38">(Rumelhart et al., 1988</ref>) and have been successful for statistical language modeling <ref type="bibr" target="#b9">(Elman, 1990</ref>). Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus <ref type="bibr" target="#b3">(Bengio et al., 2006;</ref><ref type="bibr" target="#b4">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b34">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b32">Mikolov et al., 2013)</ref>, which might have different generalization capabil- ities and are able to capture the semantic mean- ings depending on the specific task at hand. These vector representations can to some extent cap- ture interesting semantic relationships, such as King −man ≈ Queue−woman ( <ref type="bibr" target="#b31">Mikolov et al., 2010)</ref>, and recently have been successfully used in various NLP applications, including named en- tity recognition, tagging, segmentation ( <ref type="bibr" target="#b52">Wang et al., 2013)</ref>, and machine translation (e.g., <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b53">Zou et al., 2013)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence Model</head><p>In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the de- scriptions in for example ( <ref type="bibr" target="#b31">Mikolov et al., 2010;</ref><ref type="bibr" target="#b49">Sutskever et al., 2011;</ref>. As the details of both approaches can be readily found there, we make this section brief and omit the de- tails for brevity.</p><p>Let s denote a sentence, comprised of a se- quence of words s = {w 1 , w 2 , ..., w ns }, where n s denotes the number of words within sentence s. Each word w is associated with a specific vector embedding e w = {e 1 w , e 2 w , ..., e K w }, where K de- notes the dimension of the word embedding. We wish to compute the vector representation for cur- rent sentence h s = {h 1 s , h 2 s , ..., h K s }.</p><p>Recurrent Sentence Representation (Recur- rent) The recurrent network captures certain general considerations regarding sentential com- positionality. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), for sen- tence s, recurrent network successively takes word w i at step i, combines its vector representation e t w with former input h i−1 from step i − 1, calculates the resulting current embedding h t , and passes it to the next step. The standard recurrent network calculates h t as follows:</p><formula xml:id="formula_0">h t = f (V Recurrent ·h t−1 +W Recurrent ·e t w +b Recurrent )<label>(1)</label></formula><p>where W Recurrent and V Recurrent are K × K ma- trixes. b Recurrent denotes K × 1 bias vector and f = tanh is a standard element-wise nonlinearity.</p><p>Note that calculation for representation at time t = 1 is given by:</p><formula xml:id="formula_1">h 1 = f (V Recurrent ·h 0 +W Recurrent ·e 1 w +b Recurrent )<label>(2)</label></formula><p>where h 0 denotes the global sentence starting vec- tor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recursive Sentence Representation (Recursive)</head><p>Recursive sentence representation relies on the structure of parse trees, where each leaf node of the tree corresponds to a word from the original sentence. It computes a representation for each parent node based on its immediate children re- cursively in a bottom-up fashion until reaching the root of the tree. Concretely, for a given parent p in the tree and its two children c 1 (associated with vector representation h c 1 ) and c 2 (associated with vector representation h c 2 ), standard recursive net- works calculates h p for p as follows:</p><formula xml:id="formula_2">h p = f (W Recursive · [h c 1 , h c 2 ] + b Recursive ) (3)</formula><note type="other">where [h c 1 , h c 2 ] denotes the concatenating vec- tor for children vector representation h c 1 and h c 2 . W Recursive is a K × 2K matrix and b Recursive is the 1 × K bias vector. f (·) is tanh function.</note><p>Recursive neural models compute parent vec- tors iteratively until the root node's representation is obtained, and use the root embedding to repre- sent the whole sentence, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Coherence Model</head><p>The proposed coherence model adopts a window approach ), in which we train a three-layer neural network based on a slid- ing windows of L sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence Convolution</head><p>We treat a window of sentences as a clique C and associate each clique with a tag y C that takes the value 1 if coherent, and 0 otherwise 4 . As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, cliques taken from original articles are treated as coherent and those with sentences ran- domly replaced are used as negative examples. .</p><p>The sentence convolution algorithm adopted in this paper is defined by a three-layer neural net- work, i.e., sentence-level input layer, hidden layer, and overall output layer as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. For- mally, each clique C takes as input a (L × K) × 1 vector h C by concatenating the embeddings of all its contained sentences, denoted as</p><formula xml:id="formula_3">h C = [h s 1 , h s 2 , ..., h s L ].</formula><p>(Note that if we wish to clas- sify the first and last sentences and include their context, we require special beginning and ending sentence vectors, which are defined as h &lt;S&gt; for s start and h &lt;/S&gt; for s end respectively.)</p><p>Let H denote the number of neurons in the hid- den (second) layer. Then each of the hidden lay- ers takes as input h C and performs the convolution using a non-linear tanh function, parametrized by W sen and b sen . The concatenating output vector for hidden layers, defined as q C , can therefore be rewritten as:</p><formula xml:id="formula_4">q C = f (W sen × h C + b sen )<label>(4)</label></formula><p>where W sen is a H × (L × K) dimensional matrix and b sen is a H × 1 dimensional bias vector. The output layer takes as input q C and generates a scalar using linear function U T q C + b. A sigmod function is then adopted to project the value to a [0,1] probability space, which can be interpreted as the probability of whether one clique is coher- ent or not. The execution at the output layer can be summarized as:</p><formula xml:id="formula_5">p(y C = 1) = sigmod(U T q C + b)<label>(5)</label></formula><p>where U is an H × 1 vector and b denotes the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>In the proposed framework, suppose we have M training samples, the cost function for recurrent neural network with regularization on the training set is given by:</p><formula xml:id="formula_6">J(Θ) = 1 M C∈trainset {−y C log[p(y C = 1)] − (1 − y C ) log[1 − p(y C = 1)]} + Q 2M θ∈Θ θ 2<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">Θ = [W Recurrent , W sen , U sen ]</formula><p>The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity.</p><p>To minimize the objective J(Θ), we use the di- agonal variant of AdaGrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) with minibatches, which is widely applied in deep learning literature (e.g., <ref type="bibr" target="#b44">(Socher et al., 2011a;</ref><ref type="bibr" target="#b36">Pei et al., 2014)</ref>). The learning rate in AdaGrad is adapting differently for different parameters at dif- ferent steps. Concretely, for parameter updates, let g i τ denote the subgradient at time step for param- eter θ i , which is obtained from backpropagation 5 , the parameter update at time step t is given by:</p><formula xml:id="formula_8">θ τ = θ τ −1 − α τ t=0 g i2 τ g i τ (7)</formula><p>where α denotes the learning rate and is set to 0.01 in our approach. Optimal performance is achieved when batch size is set between 20 and 30. Word embeddings {e} are borrowed from <ref type="bibr">Senna (Collobert et al., 2011;</ref><ref type="bibr" target="#b6">Collobert, 2011</ref>). The dimension for these embeddings is 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed coherence model on two common evaluation approaches adopted in exist- ing work ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b27">Louis and Nenkova, 2012;</ref><ref type="bibr" target="#b12">Elsner et al., 2007;</ref><ref type="bibr" target="#b26">Lin et al., 2011</ref>): Sentence Ordering and Readability Assess- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sentence Ordering</head><p>We follow ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b27">Louis and Nenkova, 2012;</ref><ref type="bibr" target="#b12">Elsner et al., 2007;</ref><ref type="bibr" target="#b26">Lin et al., 2011</ref>) that all use pairs of articles, one containing the original document order and the other a ran- dom permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is al- ways more coherent than a random permutation; this assumption has been verified in <ref type="bibr">Lin et al.'s work (2011)</ref>.</p><p>We need to define the coherence score S d for a given document d, where d is comprised of a series of sentences, d = {s 1 , s 2 , .., s N d }, and N d denotes the number of sentences within d. Based on our clique definition, document d is comprised of N d cliques. Taking window size L = 3 as ex- ample, cliques generated from document d appear as follows:</p><formula xml:id="formula_9">&lt; s start , s 1 , s 2 &gt;, &lt; s 1 , s 2 , s 3 &gt;, ..., &lt; s N d −2 , s N d −1 , s N d &gt;, &lt; s N d −1 , s N d , s end &gt;</formula><p>The coherence score for a given document S d is the probability that all cliques within d are coher- ent, which is given by:</p><formula xml:id="formula_10">S d = C∈d p(y C = 1)<label>(8)</label></formula><p>For document pair &lt; d 1 , d 2 &gt; in our task, we would say document d 1 is more coherent than</p><formula xml:id="formula_11">d 2 if S d 1 &gt; S d 2<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Dataset</head><p>We use two corpora that are widely employed for coherence prediction ( <ref type="bibr" target="#b1">Barzilay and Lee, 2004;</ref><ref type="bibr" target="#b0">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b12">Elsner et al., 2007)</ref>. One contains reports on airplane accidents from the National Transportation Safety Board and the other contains reports about earthquakes from the Associated Press. These articles are about 10 sentences long and usually exhibit clear sentence structure. For preprocessing, we only lowercase the capital letters to match with tokens in Senna word embeddings. In the recursive network, sen- tences are parsed using the Stanford Parser <ref type="bibr">6</ref> and then transformed into binary trees. The accident corpus ends up with a vocabulary size of 4758 and an average of 10.6 sentences per document. The earthquake corpus contains 3287 distinct terms and an average of 11.5 sentences per document.</p><p>For each of the two corpora, we have 100 arti- cles for training and 100 (accidents) and 99 (earth- quakes) for testing. A maximum of 20 random permutations were generated for each test arti- cle to create the pairwise data (total of 1986 test pairs for the accident corpus and 1956 for earth- quakes) <ref type="bibr">7</ref> .</p><p>Positive cliques are taken from original training documents. For easy training, rather than creating negative examples by replacing centered sentences randomly, the negative dataset contains cliques where centered sentences are replaced only by other sentences within the same document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Training and Testing</head><p>Despite the numerous parameters in the deep learning framework, we tune only two principal ones for each setting: window size L (tried on {3, 5, 7}) and regularization parameter Q (tried on {0.01, 0.1, 0.25, 0.5, 1.0, 1.25, 2.0, 2.5, 5.0}). We trained parameters using 10-fold cross-validation on the training data. Concretely, in each setting, 90 documents were used for training and evalua- tion was done on the remaining articles, following <ref type="bibr" target="#b27">(Louis and Nenkova, 2012)</ref>. After tuning, the final model was tested on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Model Comparison</head><p>We report performance of recursive and recurrent networks. We also report results from some popu- lar approaches in the literature, including:</p><p>Entity Grid Model : Grid model ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008)</ref> obtains the best performance when coreference resolution, expressive syntactic infor- mation, and salience-based features are incorpo- rated. Entity grid models represent each sentence as a column of a grid of features and apply ma- chine learning methods (e.g., SVM) to identify the coherent transitions based on entity features (for details of entity models see ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008)</ref>). Results are directly taken from Barzilay and Lapata's paper <ref type="bibr">(2008)</ref>.</p><p>HMM : Hidden-Markov approach proposed by <ref type="bibr" target="#b27">Louis and Nenkova (2012)</ref> to model the state (cluster) transition probability in the coherent con- text using syntactic features. Sentences need to be clustered in advance where the number of clus- ters is tuned as a parameter. We directly take </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Based Approach : Guinaudeau and</head><p>Strube <ref type="formula" target="#formula_0">(2013)</ref> extended the entity grid model to a bipartite graph representing the text, where the entity transition information needed for local co- herence computation is embedded in the bipartite graph. The Graph Based Approach outperforms the original entity approach in some of feature set- tings ( <ref type="bibr" target="#b16">Guinaudeau and Strube, 2013)</ref>. As can be seen in <ref type="table">Table 1</ref>, the proposed frame- works (both recurrent and recursive) obtain state- of-art performance and outperform all existing baselines by a large margin. One interpretation is that the abstract sentence vector representations computed by the deep learning framework is more powerful in capturing exactly the relevant the se- mantic/logical/syntactic features in coherent con- texts than features or other representations devel- oped by human feature engineering are.</p><p>Another good quality of the deep learning framework is that it can be trained easily and makes unnecessary the effort required of feature engineering. In contrast, almost all existing base- lines and other coherence methods require sophis- ticated feature selection processes and greatly rely on external feature extraction algorithm.</p><p>The recurrent network is easier to implement than the recursive network and does not rely on external resources (i.e., parse trees), but the recur- sive network obtains better performance by build- <ref type="bibr">8</ref> The details for information about parameter and feature of best setting can be found in ( <ref type="bibr" target="#b27">Louis and Nenkova, 2012).</ref> ing the convolution on parse trees rather than sim- ply piling up terms within the sentence, which is in line with common expectation.</p><p>Both recurrent and recursive models obtain bet- ter performance on the Earthquake than the Acci- dent dataset. Scrutiny of the corpus reveals that articles reporting earthquakes exhibit a more con- sistent structure: earthquake outbreak, describing the center and intensity of the earthquake, injuries and rescue operations, etc., while accident articles usually exhibit more diverse scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Readability Assessment</head><p>Barzilay and Lapata (2008) proposed a readability assessment task for stylistic judgments about the difficulty of reading a document. Their approach combines a coherence system with <ref type="bibr" target="#b40">Schwarm and Ostendorf's (2005)</ref> readability features to clas- sify documents into two categories, more read- able (coherent) documents and less readable ones. The evaluation accesses the ability to differentiate "easy to read" documents from difficult ones of each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Dataset</head><p>Barzilay and Lapata's (2008) data corpus is from the Encyclopedia Britannica and the Britannica Elementary, the latter being a new version targeted at children. Both versions con- tain 107 articles. The Encyclopedia Britannica corpus contains an average of 83.1 sentences per document and the Britannica Elementary contains 36.6. The encyclopedia lemmas are written by different authors and consequently vary considerably in structure and vocabulary choice. Early researchers assumed that the chil- dren version (Britannica Elementary) is easier to read, hence more coherent than documents in Encyclopedia Britannica. This is a somewhat questionable assumption that needs further inves- tigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Training and Testing</head><p>Existing coherence approaches again apply a pair- wise ranking strategy and the article associated with the higher score is considered to be the more readable. As the replacement strategy for gener- ating negative example is apparently not well fit- ted to this task, we adopted the following training framework: we use all sliding windows of sen- tences from coherent documents (documents from Britannica Elementary) as positive examples,   <ref type="bibr" target="#b0">(Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b16">Guinaudeau and Strube, 2013)</ref>. S&amp;O: <ref type="bibr" target="#b40">Schwarm and Ostendorf (2005)</ref>. and cliques from Encyclopedia Britannica as negative examples, and again apply Eq. 6 for train- ing and optimization. During testing, we turn to Equations 8 and 9 for pairwise comparison. We adopted five-fold cross-validation in the same way as in ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b16">Guinaudeau and Strube, 2013</ref>) for fair comparison. Parameters were tuned within each training set also using five- fold cross-validation. Parameters to tune included window size L and regularization parameter Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We report results of the proposed approaches in the work along with entity model ( <ref type="bibr" target="#b0">Barzilay and Lapata, 2008)</ref> and graph based approach <ref type="bibr" target="#b10">(Elsner and Charniak, 2008</ref>) in <ref type="table" target="#tab_2">Table 2</ref>. The tabs shows that deep learning approaches again significantly outperform Entry and Global Approach baselines and are nearly comparable to the combination of entity and S&amp;O features. Again, the recursive network outperforms the recurrent network in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we apply two neural network approaches to the sentence-ordering (coherence) task, using compositional sentence representations learned by recurrent and recursive composition. The proposed approach obtains state-of-art per- formance on the standard coherence evaluation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples.</figDesc><graphic url="image-1.png" coords="2,100.77,62.81,396.00,81.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer h s denotes the resulting sentence vector.</figDesc><graphic url="image-2.png" coords="3,100.77,62.81,396.01,120.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of coherence model based on a window of sentences (clique).</figDesc><graphic url="image-3.png" coords="5,118.77,62.81,360.00,152.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Elements in W sen are initialized by randomly drawing from the uniform distribution [−, ], where = √ 6 √ H+K×L as suggested in (Collobert et al., 2011). W recurrent , V recurrent , W recursive and h 0 are initialized by randomly sampling from a uniform distribution U (−0.2, 0.2). All bias vec- tors are initialized with 0. Hidden layer number H is set to 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of Different Coherence 
Frameworks on Readability Assessment. Re-
ported baselines results are are taken from </table></figure>

			<note place="foot" n="1"> Code available at stanford.edu/ ˜ jiweil/ or by request from the first author.</note>

			<note place="foot" n="2"> Our approach is inspired by Collobert et al.&apos;s idea (2011) that a word and its context form a positive training sample while a random word in that same context gives a negative training sample, when training word embeddings in the deep learning framework.</note>

			<note place="foot" n="3"> For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013).</note>

			<note place="foot" n="4"> instead of a binary classification (correct/incorrect), another commonly used approach is the contrastive approach that minimizes the score function max(0, 1 − s + sc) (Collobert et al., 2011; Smith and Eisner, 2005). s denotes the score of a true (coherent) window and sc the score of a corrupt (containing incoherence) one) in an attempt to make the score of true windows larger and corrupt windows smaller. We tried the contrastive one for both recurrent and recursive networks but the binary approach constantly outperformed the contrastive one in this task.</note>

			<note place="foot" n="5"> For more details on backpropagation through RNNs, see Socher et al. (2010).</note>

			<note place="foot" n="6"> http://nlp.stanford.edu/software/ lex-parser.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors want to thank Richard Socher and Pradeep Dasigi for the clarification of deep learn-ing techniques. We also thank the three anony-mous EMNLP reviewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Catching the drift: Probabilistic content models, with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno>number EPFL- CONF-192374</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified local and global model for discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Joseph L Austerweil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="436" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extending the entity-grid coherence model to semantically related entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Workshop on Natural Language Generation</title>
		<meeting>the Eleventh European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="142" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention, intentions, and the structure of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><forename type="middle">L</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="204" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind K</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="203" to="225" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph-based local coherence modeling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An investigation into the use of centering transitions for summarisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Hasler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual CLUK Research Colloquium</title>
		<meeting>the 7th Annual CLUK Research Colloquium</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretation as abduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Stickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 26th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Planning coherent multisentential text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 26th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.0493</idno>
		<title level="m">Bidirectional recursive neural networks for token-level labeling with structure</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.3584</idno>
		<title level="m">Recurrent convolutional neural networks for discourse compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic evaluation of text coherence: Models and representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discourse relations and defeasible knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 29th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discourse strategies for generating natural-language text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The role of centering theory&apos;s rough-shift in the teaching and evaluation of writing skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="408" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Planning text for advisory dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 27th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Baobao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Centering: A parametric theory and its instantiations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosemary</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Hitzeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="363" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reading level assessment using support vector machines and statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Functional centering: Grounding referential coherence in information structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="344" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Centering theory in discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">Krishna</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen Friedman</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Improving chinese word segmentation on micro-blog using rich punctuations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>He Zhengyan, and Ni Sun</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
