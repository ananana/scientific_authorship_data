<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Strength of Character Language Models for Multilingual Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Strength of Character Language Models for Multilingual Named Entity Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3073" to="3077"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3073</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Character-level patterns have been widely used as features in English Named Entity Recognition (NER) systems. However, to date there has been no direct investigation of the inherent differences between name and non-name tokens in text, nor whether this property holds across multiple languages. This paper analyzes the capabilities of corpus-agnostic Character-level Language Models (CLMs) in the binary task of distinguishing name tokens from non-name tokens. We demonstrate that CLMs provide a simple and powerful model for capturing these differences, identifying named entity tokens in a diverse set of languages at close to the performance of full NER systems. Moreover, by adding very simple CLM-based features we can significantly improve the performance of an off-the-shelf NER system for multiple languages. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In English, there is strong empirical evidence that the character sequences that make up proper nouns tend to be distinctive. Even divorced of con- text, a human reader can predict that "hoeksten- berger" is an entity, but "abstractually" 2 is not. Some NER research explores the use of character- level features including capitalization, prefixes and suffixes <ref type="bibr" target="#b1">(Cucerzan and Yarowsky, 1999;</ref><ref type="bibr" target="#b7">Ratinov and Roth, 2009)</ref>, and character-level models (CLMs) ( <ref type="bibr" target="#b3">Klein et al., 2003</ref>) to improve the perfor- mance of NER, but to date there has been no sys- tematic study isolating the utility of CLMs in cap- turing distinctions between name and non-name tokens in English or across other languages.</p><p>We conduct an experimental assessment of the discriminative power of CLMs for a range of lan- <ref type="bibr">1</ref> The code and resources for this publication can be found at: https://cogcomp.org/page/publication_ view/846 2 Not a real name or a real word.</p><p>Figure 1: Perplexity histogram of entity (left) and non- entity tokens (right) in CoNLL Train calculated by en- tity CLM for both sides. The graphs show the percent- age of tokens (y axis) with different levels of CLM per- plexities (x axis). The entity CLM gives a low aver- age perplexity and small variance to entity tokens (left), while giving non-entity tokens much higher perplexity and higher variance (right).</p><p>guages: English, Amharic, Arabic, Bengali, Farsi, Hindi, Somali, and Tagalog. These languages use a variety of scripts and orthographic conventions (for example, only three use capitalization), come from different language families, and vary in their morphological complexity. We demonstrate the effectiveness of CLMs in distinguishing name to- kens from non-name tokens, as illustrated by <ref type="figure">Fig- ure 1</ref>, which shows perplexity histograms from a CLM trained on entity tokens. Our models use only individual tokens, but perform extremely well in spite of taking no account of word context. We then assess the utility of directly adding sim- ple features based on this CLM implementation to an existing NER system, and show that they have a significant positive impact on performance across many of the languages we tried. By adding very simple CLM-based features to the system, our scores approach those of a state-of-the-art NER system ( <ref type="bibr" target="#b4">Lample et al., 2016</ref>) across multiple lan- guages, demonstrating both the unique importance and the broad utility of this approach. <ref type="table" target="#tab_2">Test  Language  Entity Non-entity Entity Non-entity   English  29,450  170,524  7,194  38,554  Amharic  5,886  46,641  2,077  16,235  Arabic  7,640  52,968  1,754  15,073  Bengali  15,288  108,592  4,573  32,929  Farsi  4,547  50,084  1,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Character Language Models</head><p>We propose a very simple model in which we train an entity CLM on a list of entity tokens, and a non- entity CLM on a list of non-entity tokens. Both lists are unordered, with all entries treated inde- pendently. Each token is split into characters and treated as a "sentence" where the characters are the "words." For example, "Obama" is an entity token, and is split into "O b a m a". From these examples we learn a score measuring how likely it is that a sequence of characters forms an entity. At test time, we also split each word into characters and determine perplexity using the entity and non- entity CLMs. We assign the label corresponding to the lower perplexity CLM.</p><p>We experiment with four different kinds of lan- guage model: N-gram model, Skip-gram model, Continuous Bag-of-Words model (CBOW), and Log-Bilinear model (LB). We demonstrate that the N-gram model is best suited for this task.</p><p>Following <ref type="bibr" target="#b6">Peng and Roth (2016)</ref>, we implement N-gram using SRILM <ref type="bibr" target="#b11">(Stolcke, 2002</ref>) with order 6 and Witten-Bell discounting. 3 For Skip-Gram and CBOW CLMs, we use the Gensim implemen- tation <ref type="bibr" target="#b8">(Rehurek and Sojka, 2010</ref>) for training and inference, and we build the LB CLM using the OxLM toolkit ( <ref type="bibr" target="#b0">Baltescu et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>To determine whether name identifiability applies to languages other than English, we conduct ex- periments on a range of languages for which we had previously gathered resources (such as Brown clusters): English, Amharic, Arabic, Ben- gali, Farsi, Hindi, Somali, and Tagalog. For English, we use the original splits from the ubiquitous <ref type="bibr">CoNLL 2003 English dataset (Sang and</ref><ref type="bibr" target="#b9">Meulder, 2003)</ref>, which is a newswire dataset annotated with Person (PER), Organiza- tion (ORG), Location (LOC) and Miscellaneous (MISC). To collect the list of entities and non- entities as the training data for the Entity and Non-Entity CLMs, we sample a large number of PER/ORG/LOC and non-entities from Wikipedia, using types derived from their corresponding Free- Base entities <ref type="bibr" target="#b5">(Ling and Weld, 2012)</ref>.</p><p>For all other languages, we use a subset of the corpora from the LORELEI project annotated for the NER task ( <ref type="bibr" target="#b12">Strassel and Tracey, 2016)</ref>. We build our entity list using the tokens labeled as en- tities in the training data, and our non-entity list from the remaining tokens. These two lists are then used to train two CLMs, as described above.</p><p>Our datasets vary in size of entity and non-entity tokens, as shown in <ref type="table">Table 1</ref>. The smallest, Farsi, has 4.5K entity and 50K non-entity tokens; the largest, English, has 29K entity and 170K non- entity tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLM for Named Entity Identification</head><p>In this section, we first show the power of CLMs for distinguishing between entity and non-entity tokens in English, and then that this power is ro- bust across a variety of languages.</p><p>We refer to this task as Named Entity Identifi- cation (NEI), because we are concerned only with finding an entity span, not its label. We differen- tiate it from Named Entity Recognition (NER), in which both span and label are required. To avoid complicating this straightforward approach by re- quiring a separate mention detection step, we eval- uate at the token-level, as opposed to the more common phrase-level evaluation. We also apply one heuristic: if a word has length 1, we automat- ically predict 'O' (or non-entity). This captures most punctuation and words like 'I' and 'a'. <ref type="figure">Figure 1</ref> shows that for the majority of entity tokens, the entity CLM computes a relatively low perplexity compared to non-entity tokens. Though there also exist some non-entities with low entity CLM perplexity, we can still reliably identify a large proportion of non-entity words by setting a threshold value for entity CLM perplexity. If a to- ken perplexity lies above this threshold, we label it as a non-entity token. The threshold is tuned on development data.  Since we also build a CLM for non-entities, we can also compare the entity and non-entity per- plexity scores for a token. For those tokens not excluded using the threshold as described above, we compare the perplexity scores of the two mod- els and assign the label corresponding to the model yielding the lower score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><note type="other">eng amh ara ben fas hin som tgl avg Exact Match 43.4</note><p>We compare SRILM against Skip-gram and CBOW, as implemented in Gensim, and the Log- Bilinear (LB) model. We trained both CBOW and Skip-gram with window size 3, and size 20. We tuned LB, and report results with embedding size 150, and learning rate 0.1. Despite tuning the neu- ral models, the simple N-gram model outperforms them significantly, perhaps because of the rela- tively small amount of training data. <ref type="bibr">4</ref> We compare the CLM's Entity Identification against two state-of-the-art NER systems: Cog- CompNER ( <ref type="bibr" target="#b2">Khashabi et al., 2018)</ref> and LSTM- CRF ( <ref type="bibr" target="#b4">Lample et al., 2016</ref>). We train the NER sys- tems as usual, but at test time we convert all pre- dictions into binary token-level annotations to get the final score. As <ref type="table" target="#tab_2">Table 2</ref> shows, the result of N- gram CLM, which yields the highest performance, is remarkably close to the result of state-of-the- art NER systems (especially for English) given the simplicity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improving NER with CLM features</head><p>In this section we show that we can augment a standard NER system with simple features based on our entity/non-entity CLMs to improve perfor- mance in many languages. Based on their superior performance as reported in Section 3, we use the N-gram CLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Features</head><p>We define three simple features that capture infor- mation provided by CLMs and which we expect to be useful for NER.</p><p>Entity Feature We define one "isEntity" fea- ture based on the perplexities of the entity and non-entity CLMs. We compare the perplexity cal- culated by entity CLM and non-entity CLM de- scribed in Section 3, and return a boolean value indicating whether the entity CLM score is lower.</p><p>Language Features We define two language- related features: "isArabic" and "isRussian". We observe that there are many names in English text that originate from other languages, result- ing in very different orthography than native En- glish names. We therefore build two language- based CLMs for Arabic and Russian. We collect a list of Arabic names and a list of Russian names by scraping name-related websites, and train an Arabic CLM and a Russian CLM. For each to- ken, when the perplexity of either the Arabic or the Russian CLM is lower than the perplexity of the Non-Entity CLM, we return True, indicating that this entity is likely to be a name from Ara- bic/Russian. Otherwise, we return False.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>eng amh ara ben fas hin som tgl avg   <ref type="table">Table 3</ref>: NER results on 8 languages show that even a simplistic addition of CLM features to a standard NER model boosts performance. CogCompNER is run with standard features, including Brown clusters; ( <ref type="bibr" target="#b4">Lample et al., 2016</ref>) is run with default parameters and pre-trained embeddings. Unseen refers to performance on named entities in Test that were not seen in the training data. Full is performance on all entities in Test. Averages are computed over all languages other than English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We use CogCompNER ( <ref type="bibr" target="#b2">Khashabi et al., 2018)</ref> as our baseline NER system because it allows easy integration of new features, and evaluate on the same datasets as before. For English, we add all features described above. For other languages, due to the limited training data, we only use the "isEn- tity" feature. We compare with the state-of-the- art character-level neural NER system of ( <ref type="bibr" target="#b4">Lample et al., 2016)</ref>, which inherently encodes compara- ble information to CLMs, as a way to investigate how much of that system's performance can be at- tributed directly to name-internal structure.</p><p>The results in <ref type="table">Table 3</ref> show that for six of the eight languages we studied, the baseline NER can be significantly improved by adding simple CLM features; for English and Arabic, it performs bet- ter even than the neural NER model of ( <ref type="bibr" target="#b4">Lample et al., 2016)</ref>. For Tagalog, however, adding CLM features actually impairs system performance.</p><p>In the same table, the rows marked "unseen" report systems' performance on named entities in Test that were not seen in the training data. This setting more directly assesses the robustness of a system to identify named entities in new data. By this measure, Farsi NER is not improved by name- only CLM features and Tagalog is impaired. Ben- efits for English, Hindi, and Somali are limited, but are quite significant for Amharic, Arabic, and Bengali.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Our results demonstrate the power of CLMs for recognizing named entity tokens in a diverse range of languages, and that in many cases they can im- prove off-the-shelf NER system performance even when integrated in a simplistic way.</p><p>However, the results from Section 4.2 show that this is not true for all languages, especially when only considering unseen entities in Test: Tagalog and Farsi do not follow the trend for the other lan- guages we assessed even though CLM performs well for Named Entity Identification.</p><p>While the end-to-end model developed by <ref type="bibr" target="#b4">(Lample et al., 2016</ref>) clearly includes informa- tion comparable to that in the CLM, it requires a fully annotated NER corpus, takes significant time and computational resources to train, and is non-trivial to integrate into a new NER system. The CLM approach captures a very large fraction of the entity/non-entity distinction capacity of full NER systems, and can be rapidly trained using only entity and non-entity token lists -i.e., it is corpus-agnostic. For some languages it can be used directly to improve NER performance; for others (such as Tagalog), the strong NEI perfor- mance indicates that while it does not immediately boost performance, it can ultimately be used to im- prove NER there too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Cucerzan and Yarowsky (1999) is one of the earli- est works to use character-based features (charac- ter tries) for NER. The approach of <ref type="bibr" target="#b3">Klein et al. (2003)</ref> was one of the original papers in the CoNLL 2003 NER shared task. Their approach, which ranked in the top 3 for both English and German shared tasks, used character-based fea- tures for NER. They do two experiments: one with a character-based HMM, another with using char- acter n-grams as features to a maximum entropy model. The focus on character-level patterns is similar to our work, but without the specific ex- ploration of language models alone.</p><p>Using character-based models similar to ours, <ref type="bibr" target="#b10">Smarr and Manning (2002)</ref> show that unseen noun phrases can be accurately classified into a small number of categories using only a character-based model independent of context. We tackle a some- what more challenging task of distinguishing enti- ties from non-entities. <ref type="bibr" target="#b4">Lample et al. (2016)</ref> use character embeddings in an LSTM-CRF model. Their ablation studies show that character-level features improve performance significantly.</p><p>We are not aware of any work that directly eval- uates CLMs for identifying name tokens, nor of work that demonstrates the utility of character- level information for identifying names in multi- ple languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We have shown, in a series of simple experiments, that in many languages names are identifiable by character patterns alone, and that character level patterns have strong potential for building better NER systems.</p><p>In the future, we plan to make a more thorough analysis of reasons for the high variance in NER performance. In particular, we will study why it is possible, as with Tagalog, to have high Named En- tity Identification results but lose points in NER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lample</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Token level identification F1 scores. Averages are computed over all languages other than English. Two baselines are also compared here: Capitalization tags a token in test as entity if it is capitalized; and Exact Match keeps track of entities seen in training, tagging tokens in Test that exactly match some entity in Train. The bottom section shows state-of-the-art models which use complex features for names, including contextual information. Languages in order are: English, Amharic, Arabic, Bengali, Farsi, Hindi, Somali, and Tagalog. The rightmost column is the average of all columns excluding English.</figDesc><table></table></figure>

			<note place="foot" n="3"> We experimented with different orders on development data, but found little difference between them.</note>

			<note place="foot" n="4"> We also tried a simple RNN+GRU language model, but found that the results were underwhelming.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by a grant from Google, and by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribu-tion Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We appreciate the helpful dis-cussions and suggestions from Haoruo Peng and Qiang Ning, and from the anonymous EMNLP re-viewers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Oxlm: A neural language modelling framework for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<ptr target="https://ufal.mff.cuni.cz/pbml/102/art-baltescu-blunsom-hoang.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language independent named entity recognition combining morphological and contextual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CogCompNLP: Your swiss army knife for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Redman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Language Resources and Evaluation Conference</title>
		<editor>Naveen Arivazhagan, Qiang Ning, Shaoshi Ling, and Dan Roth</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with character-level models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Smarr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two discourse driven language models for semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>of the Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Classifying unknown proper noun phrases without context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Smarr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh international conference on spoken language processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lorelei language packs: Data, tools, and resources for technology development in low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Tracey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
