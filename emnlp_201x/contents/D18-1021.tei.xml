<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiansi</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">B-IT</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="227" to="237"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>227</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Joint representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not require parallel corpora, and automatically generates comparable data via distant supervision using multilingual knowledge bases. We utilize two types of regu-larizers to align cross-lingual words and entities , and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitatively and quantitatively, demonstrate the significance of our method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB) store millions of entities and facts in various languages, and pro- vide rich background structural knowledge for un- derstanding texts. On the other hand, text cor- pus contains huge amount of statistical information complementary to KBs. Many researchers lever- age both types of resources to improve various nat- ural language processing (NLP) tasks, such as ma- chine reading <ref type="bibr">(Yang and Mitchell, 2017)</ref>, question answering ( <ref type="bibr" target="#b14">Hao et al., 2017)</ref>.</p><p>Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a unified vector space. For example, <ref type="bibr">Wang et al. (2014)</ref>; <ref type="bibr">Yamada et al. (2016)</ref>;  utilize the co-occurrence infor- mation to align similar words and entities with sim- ilar embedding vectors. <ref type="bibr">Toutanova et al. (2015)</ref>; * Corresponding author. <ref type="bibr">Wu et al. (2016)</ref>; <ref type="bibr" target="#b13">Han et al. (2016)</ref>; <ref type="bibr">Weston et al. (2013a)</ref>; <ref type="bibr">Wang and Li (2016)</ref> represent entities based on their textual descriptions together with the structured relations. These methods focused on mono-lingual settings. However, for cross-lingual tasks (e.g., cross-lingual entity linking), these ap- proaches need to introduce additional tools to do translation, which suffers from extra costs and in- evitable errors <ref type="bibr" target="#b20">(Ji et al., , 2016</ref>.</p><p>In this paper, we carry out cross-lingual joint representation learning, which has not been fully researched in the literature. We aim at creating a unified space for words and entities in various lan- guages, and easing cross-lingual semantic compar- ison, which will benefit from the complementary information in different languages. For instance, two different meanings of word center in English are expressed by two different words in Chinese: center as the activity-specific building is expressed by 中心, center as the basketball player role is 中 锋.</p><p>Our main challenge is the limited availability of parallel corpus, which is usually either expen- sive to obtain, or only available for certain narrow domains ( <ref type="bibr" target="#b12">Gouws et al., 2015</ref>). Many work has been done to alleviate the problem. One school of methods uses adversarial technique or domain adaption to match linguistic distribution ( <ref type="bibr">Zhang et al., 2017b;</ref><ref type="bibr" target="#b2">Barone, 2016;</ref><ref type="bibr" target="#b4">Cao et al., 2016</ref>). These methods do not require parallel corpora. The weakness is that the training process is un- stable and that the high complexity restricts the methods only to small-scale data. Another line of work uses pre-existing multi-lingual resources to automatically generate "pseudo bilingual docu- ments" ( <ref type="bibr">Vulic and</ref><ref type="bibr">Moens, 2015, 2016)</ref>. However, negative results have been observed due to the oc- casional poor quality of training data <ref type="bibr">(Vulic and Moens, 2016)</ref>. All above methods only focus on words. We consider both words and entities, which makes the parallel data issue more challenging.</p><p>In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. The basic idea is to capture mu- tually complementary knowledge in a shared se- mantic space, which enables joint inference among cross-lingual knowledge base and texts without ad- ditional translations. We achieve it by (1) utilizing an existing multi-lingual knowledge base to auto- matically generate cross-lingual supervision data, (2) learning mono-lingual word and entity rep- resentations, (3) applying cross-lingual sentence regularizer and cross-lingual entity regularizer to align similar words and entities with similar em- beddings. The entire framework is trained using a unified objective function, which is efficient and applicable to arbitrary language pairs that exist in multi-lingual KBs.</p><p>Particularly, we build a bilingual entity network from inter-language links 1 in KBs for regulariz- ing cross-lingual entities through a variant of skip- gram model <ref type="bibr">(Mikolov et al., 2013c</ref>). Thus, mono- lingual structured knowledge of entities are not only extended to cross-lingual settings, but also augmented from other languages. On the other hand, we utilize distant supervision to generate comparable sentences for cross-lingual sentence regularizer to model co-occurrence information across languages. Compared with "pseudo bilin- gual documents", comparable sentences achieve higher quality, because they rely not only on the shared semantics at document level, but also on cross-lingual information at sentence level. We further introduce two attention mechanisms, knowledge attention and cross-lingual attention, to select informative data in comparable sentences.</p><p>Our contributions can be concluded as follows:</p><p>• We proposed a novel method that jointly learns representations of not only cross- lingual words but also cross-lingual entities in a unified vector space, aiming to enhance the embedding quality from each other via com- plementary semantics.</p><p>• Our proposed model introduces distant su- pervision coupled with attention mechanisms to generate comparable data as cross-lingual supervision, which can benefit many cross- lingual analysis.</p><p>• We did qualitative analysis to have an in- tuitive impression of our embeddings, and quantitative analysis in three tasks: word translation, entity relatedness, and cross- lingual entity linking. Experiment results show that our method demonstrates signifi- cant improvements in all three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Jointly representation learning of words and enti- ties attracts much attention in the fields of Entity Linking ( <ref type="bibr">Zhang et al., 2017a;</ref><ref type="bibr" target="#b5">Cao et al., 2018)</ref>, Relation Extraction ( <ref type="bibr">Weston et al., 2013b</ref>) and so on, yet little work focuses on cross-lingual set- tings. Inspiringly, we investigate the task of cross- lingual word embedding models ( <ref type="bibr">Ruder et al., 2017)</ref>, and classify them into three groups accord- ing to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embed- ding learning ( <ref type="bibr" target="#b22">Klementiev et al., 2012;</ref><ref type="bibr">Zou et al., 2013;</ref><ref type="bibr">Wu et al., 2014;</ref><ref type="bibr" target="#b27">Luong et al., 2015;</ref><ref type="bibr" target="#b0">Ammar et al., 2016;</ref><ref type="bibr">Soricut and Ding, 2016)</ref>. (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words ( <ref type="bibr" target="#b12">Gouws et al., 2015;</ref><ref type="bibr" target="#b23">Kociský et al., 2014;</ref><ref type="bibr" target="#b23">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b8">Chandar et al., 2014;</ref><ref type="bibr">Shi et al., 2015;</ref><ref type="bibr">Mogadala and Rettinger, 2016)</ref>. (iii) methods requiring bilingual lexicon to map words from one language into the other ( <ref type="bibr">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b10">Faruqui and Dyer, 2014;</ref><ref type="bibr">Xiao and Guo, 2014</ref>). The major weakness of these methods is the lim- ited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multi- lingual KB). <ref type="bibr" target="#b3">Camacho-Collados et al. (2015)</ref> com- bines several KBs (Wikipedia, WordNet and Ba- belNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> starts from a small bilingual lexicon and using a self-learning approach to induce the structural similarity of embedding spaces. <ref type="bibr">Vulic and</ref><ref type="bibr">Moens (2015, 2016)</ref> collect comparable documents on same themes from multi-lingual Wikipedia, shuf- fle and merge them to build "pseudo bilingual doc- uments" as training corpora. However, the qual- ity of "pseudo bilingual documents" are difficult to control, resulting in poor performance in several cross-lingual tasks <ref type="bibr">(Vulic and Moens, 2016)</ref>.</p><p>Another remedy matches linguistic distribu- tion via adversarial training <ref type="bibr" target="#b2">(Barone, 2016;</ref><ref type="bibr">Zhang et al., 2017b;</ref><ref type="bibr" target="#b24">Lample et al., 2018)</ref>, domain adap- tion ( <ref type="bibr" target="#b4">Cao et al., 2016)</ref>. However, these methods suffer from the instability of training process and the high complexity. This either limits the scala- bility of vocabulary size or relies on a strong dis- tribution assumption.</p><p>Inspired by <ref type="bibr">Vulic and Moens (2016)</ref>, we gener- ate highly qualified comparable sentences via dis- tant supervision, which is one of the most promis- ing approaches to addressing the issue of sparse training data, and performs well in relation extrac- tion ( <ref type="bibr" target="#b25">Lin et al., 2017a;</ref><ref type="bibr">Mintz et al., 2009;</ref><ref type="bibr">Zeng et al., 2015;</ref><ref type="bibr" target="#b18">Hoffmann et al., 2011;</ref><ref type="bibr">Surdeanu et al., 2012</ref>). Our comparable sentences may further ben- efit many other cross-lingual analysis, such as in- formation retrieval <ref type="bibr" target="#b9">(Dong et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries and Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Given a multi-lingual KB, we take (i) text cor- pus, (ii) entity and their relations, (iii) a set of an- chors as inputs, and learn embeddings for each word and each entity in various languages. For clarity, we use English and Chinese as sample lan- guages in the rest of the paper, and use superscript y ∈ {en, zh} to denote language-specific parame- ters 2 .</p><p>We use multi-lingual Wikipedia as KB includ- ing a set of entities E y = {e y i } and their articles. We concatenate these articles together, and form text corpus D y = ⟨w y 1 , . . . , w y i , . . . , w y |D| ⟩. Hy- per links in articles are denoted by Anchors A y = {⟨w y i , e y j ⟩}, which indicates that word w y i refers to entity e y j . G y = (E y , R y ) is the mono-lingual Entity Network (EN), where R y = {⟨e y i , e y j ⟩} if there is a link between e y i , e y j . We use inter- language links in Wikipedia as cross-lingual links R en−zh = {⟨e en i , e zh i ′ ⟩}, indicating e en i , e zh i ′ refer to the same thing in English and Chinese. Cross- lingual word and entity representation learning is to map words and entities in different languages into a unified semantic space. Each word and en- tity obtain their embedding vectors 3 w y i and e y j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework</head><p>To alleviate the heavy burden of limited parallel corpora and additional translation efforts, we uti- lize existing multi-lingual resources to distantly supervise cross-lingual word and entity represen- tation learning, so that the shared embedding space supports joint inference among KB and texts across languages. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our framework has two steps: (1) Cross-lingual Su- https://en.wikipedia.org/wiki/Lists_of_ languages_by_number_of_speakers.</p><p>pervision Data Generation builds a bilingual en- tity network and generates comparable sentences based on cross-lingual links; (2) Joint Represen- tation Learning learns cross-lingual word and en- tity embeddings using a unified objective function.</p><p>Our assumption throughout the entire framework is as follows: The more words/entities two contexts share, the more similar they are.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we build a bilingual EN G en−zh by using G en , G zh and cross-lingual links R en−zh . Thus, entities in different languages shall be connected in a unified network to facil- itate cross-lingual entity alignments. Meanwhile, from KB articles, we extract comparable sentences S en−zh = {⟨s en k , s zh k ⟩} as high qualified parallel data to align similar words in different languages.</p><p>Based on generated cross-lingual data G en−zh , S en−zh and mono-lingual data D y , A y , where y ∈ {en, zh}, we jointly learn cross- lingual word and entity embeddings through three components: (1) Mono-lingual Representation Learning, which learns mono-lingual word and entity embeddings for each language by modeling co-occurrence information through a variant of skip-gram model ( <ref type="bibr">Mikolov et al., 2013c)</ref>. <ref type="formula" target="#formula_6">(2)</ref> Cross-lingual Entity Regularizer, which aligns entities that refer to the same thing in different languages by extending the mono-lingual model to bilingual EN. For example, entity Foust in English and entity 福 斯 特 (Foust) in Chinese are closely embedded in the semantic space because they share common neighbors in two languages, All-star and NBA 选 秀 (draft), etc.. (3) Cross-lingual Sentence Regularizer, which models cross-lingual co-occurrence at sentence level in order to learn translated words to have most similar embeddings. For example, English word basketball and the translated Chinese word 篮球 frequently co-occur in a pair of comparable sentences, therefore, their vector representations shall be close in the semantic space. The above components are trained jointly under a unified objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-lingual Supervision Data Generation</head><p>This section introduces how to build a bilingual entity network G en−zh and comparable sentences S en−zh from a multi-lingual KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bilingual Entity Network Construction</head><p>Entities with cross-lingual links refer to the same thing, which implies they are equivalent across languages. Conventional knowledge representa- tion methods only add edges between e en i and e zh i ′ indicating a special "equivalent" relation ( <ref type="bibr">Zhu et al., 2017)</ref>. Instead, we build G en−zh = (E en ∪ E zh , R en ∪R zh ∪ ˜ R en−zh ) by enriching the neigh- bors of cross-lingual linked entities. That is, we add edges˜Redges˜ edges˜R en−zh between two mono-lingual ENs by letting all neighbors of e en i be neighbors of e zh i ′ , and vice versa, if ⟨e en i , e zh i ′ ⟩ ∈ R en−zh . G en−zh extends G en and G zh to bilingual set- tings in a natural way. It not only keeps a con- sistent objective in mono-lingual ENs-entities, no matter in which language, will be embedded closely if share common neighbors-but also en- hances each other with more neighbors in the for- eign language.</p><p>Following the method in <ref type="bibr">Zhu et al. (2017)</ref>, there will be no edge between Chinese entity 福斯特 (Foust) and English entity Pistons, which implies a wrong fact that 福斯特 (Foust) does not belong to Pistons. Our method enriches the missing rela- tion between entities 福斯特 (Foust) and 活塞队 (Pistons) in incomplete Chinese KB through cor- responding English common neighbors, Allstar, NBA, etc., as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparable Sentences Generation</head><p>To supervise the cross-lingual representation learning of words, we automatically generate com- parable sentences as cross-lingual training data. Comparable sentences are not translated paired sentences, but sentences with the same topic in dif- ferent languages. As shown in the middle layer <ref type="figure" target="#fig_0">(Figure 1</ref>), the pair of sentences are comparable sentences: (1) "Lawrence Michael Foust was an American basketball player who spent 12 seasons in NBA",</p><formula xml:id="formula_0">(2) "拉里·福斯特 (Lawrence Foust) 是 (was) 美国 (American) NBA 联盟 (association) 的 (of) 前 (former) 职业 (professional) 篮球 (basket- ball) 运动员 (player)".</formula><p>Inspired  <ref type="figure" target="#fig_0">Fig- ure 1</ref>, the sentences in the second level are compa- rable due to the similar theme of the relation be- tween entity Foust and NBA. To find this type of sentences, we search the anchors in the English aritcle and Chinese article of cross-lingual entity Foust, respectively, and extract the sentences in- cluding another crosslingual entity NBA. Compa- rable sentences can be regarded as cross-lingual contexts.</p><p>Unfortunately, comparable sentences suffer from two issues caused by distant supervision: Wrong labelling. Take English as sample, there may be several sentences s en k,l | L l=1 containing the same entity e en j in the article of e en i . A straightfor- ward solution is to concatenate them into a longer sentence s en k , but this increases the chance to in- clude unrelated sentences. Unbalanced information. Sometimes the pair of sentences convey unbalanced information, e.g., the English sentence in the middle layer <ref type="figure" target="#fig_0">(Figure 1)</ref> contains Foust spent 12 seasons in NBA while the comparable Chinese sentence not.</p><p>To address the issues, we propose knowledge at- tention and cross-lingual attention to filter out un- related information at sentence level, and at word level respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Joint Representation Learning</head><p>As shown in <ref type="figure">Figure 2</ref>, there are three components in learning cross-lingual word and entity represen- tations, which are trained jointly. In this section, we will describe them in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mono-lingual Representation Learning</head><p>Following <ref type="bibr">Yamada et al. (2016)</ref>; , we learn mono-lingual word/entity embeddings based on corpus D y , anchors A y and entity net- work G y . Capturing the cooccurrence information among words and entities, these embeddings serve as the foundation and will be further extended to bilingual settings using the proposed cross-lingual regularizers, which will be detailed in the next sec- tion. Monolingually, we utilize a variant of Skip- gram model ( <ref type="bibr">Mikolov et al., 2013c</ref>) to predict the contexts given current word/entity:</p><formula xml:id="formula_1">L m = ∑ y∈{en,zh} ∑ x y i ∈{D y ,A y ,G y } log P (C(x y i )|x y i )</formula><p>where x y i is either a word or an entity, and C(x y i ) denotes: (i) contextual words in a pre-defined win- dow of x y i if x y i ∈ D y , (ii) neighbor entities that linked to x y i if x y i ∈ G y , (iii) contextual words of w y j if x y i is entity e y i in an anchor ⟨w y j , e y i ⟩ ∈ A y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-lingual Entity Regularizer</head><p>The bilingual EN G en−zh merges entities in dif- ferent languages into a unified network, resulting in the possibility of using the same objective as in mono-lingual ENs. Thus, we naturally extend mono-lingual function to cross-lingual settings:</p><formula xml:id="formula_2">L e = ∑ e y i ∈{G en−zh } log P (C ′ (e y i )|e y i )</formula><p>where C ′ (e y i ) denotes cross-lingual contexts- neighbor entities in different languages that linked to e y i . Thus, by jointly learning mono-lingual rep- resentation with cross-lingual entity regularizer, words and entities share more common contexts, and will have similar embeddings. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, English entity NBA co-occurs with words basketball and player in texts, so they are embed- ded closely in the semantic space. Meanwhile, cross-lingual linked entities NBA and NBA (zh) have similar representations due to the most com- mon neighbor entities, e.g., Foust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cross-lingual Sentence Regularizer</head><p>Comparable sentences provide cross-lingual co- occurrence of words, thus, we can use them to learn similar embeddings for the words that fre- quently co-occur by minimizing the Euclidean dis- tance as follows:</p><formula xml:id="formula_3">L s = ∑ ⟨s en k ,s zh k ′ ⟩∈S en−zh ||s en k − s zh k ′ || 2</formula><p>where s en k , s zh k ′ are sentence embeddings. Take En- glish as sample language, we define it as the aver- age sum of word vectors weighted by the combi- nation of two types of attentions:</p><formula xml:id="formula_4">s en k = L ∑ l=1 ψ(e en m , s en k,l ) ∑ w en i ∈s en k,l ψ ′ (w en i , w zh j )w en i</formula><p>where s en k,l | L l=1 are sentences containing the same entity (as mentioned in Section 4.2), and ψ(e en m , s en k,l ) is knowledge attention that aims e NBA e <ref type="table" target="#tab_11">Allstar   538   539   540   541   542   543   544   545   546   547   548</ref>  <ref type="table" target="#tab_11">5   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449</ref> dow of x y i if x y i ∈ D y , (ii) neighbor entities that linked to x y i if x y i ∈ G y , (iii) contextual words of w y j if x y i is entity e y i in an anchor ⟨w y j , e y i ⟩ ∈ A y .</p><formula xml:id="formula_5">549 L = L m + L e + γL s γ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-lingual Entity Regularizer</head><p>The bilingual EN G en−zh merges entities in dif- ferent languages into a unified network, resulting in the possibility of using the same objective as in mono-lingual ENs. Thus, we naturally extend mono-lingual function to cross-lingual settings:</p><formula xml:id="formula_6">L e = e y i ∈{G en−zh } log P (C ′ (e y i )|e y i )<label>(2)</label></formula><p>where C ′ (e y i ) denotes cross-lingual contexts- neighbor entities in different languages that linked to e y i . Thus, by jointly learning mono-lingual rep- resentation with cross-lingual entity regularizer, words and entities share more common contexts, and will have similar embeddings. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, English entity NBA co-occurs with words basketball and player in texts, so they are embed- ded close in the semantic space. Meanwhile, cross- lingual linked entities NBA and NBA (zh) have sim- ilar representations due to the most common neigh- bor entities, e.g., Foust.  <ref type="table" target="#tab_11">1   022   023   024   025   026   027   028   029   030   031   032   033   034   035   036   037   038   039   040   041   042   043   044   045   046   047   048   049   072   073   074   075   076   077   078   079   080   081   082   083   084   085   086   087   088   089   090   091   092   093   094   095   096   097   098   099</ref> generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p><note type="other">same entities in art belling errors incre almost irrelevant to at filtering out wro smaller weights an weights. Thus, we ilarity between s</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><p>lations. However, these methods only mono-lingual settings, and few researc been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cro word and entity representations in the sam tic space, to enable joint inference amon text across languages without any additio lation mechanism, which is usually expe may introduce inevitable errors. Our em are helpful to break down language gap tasks, such as cross-lingual entity linking the major challenge lies in measuring th ity between entities and corresponding m words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and e various languages share some common meanings 1 , but there are also ways in w differ. On one hand, we utilize their shar tics to align similar words and entities w lar embedding vectors, no matter they same language or not. On the other ha lingual embeddings will benefit from diff guages due to the complementary knowl instance, textual ambiguity in one lang disappear in another language, e.g., the t 1 Some cross-lingual pioneering work observ embeddings trained separately on monolingual hibit isomorphic structure across languages <ref type="bibr">(M 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>). generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><p>been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>). 2016) learns to represent entities based on their textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>). been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cr word and entity representations in the sa tic space, to enable joint inference amo text across languages without any additi lation mechanism, which is usually exp may introduce inevitable errors. Our e are helpful to break down language gap tasks, such as cross-lingual entity linkin the major challenge lies in measuring t ity between entities and corresponding words in different languages.</p><p>e NBA The intuition is that, words and various languages share some commo meanings 1 , but there are also ways in w differ. On one hand, we utilize their sha tics to align similar words and entities lar embedding vectors, no matter they same language or not. On the other ha lingual embeddings will benefit from di guages due to the complementary know instance, textual ambiguity in one lang disappear in another language, e.g., the 1 Some cross-lingual pioneering work obser embeddings trained separately on monolingua hibit isomorphic structure across languages <ref type="bibr">(M 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>  text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>. edge attention and cross-lingual to select the most informative d filter out noise, which will fur- ove the performance. In exper- eparate tasks of word translation relatedness demonstrate the ef- ss of our method with an average 0% and 3% over baselines, re- y. Using entity linking as a case e results on benchmark dataset quality of our embeddings.  <ref type="bibr">Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., al., 2017;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- wledge attention and cross-lingual n to select the most informative and filter out noise, which will fur- prove the performance. In exper- , separate tasks of word translation tity relatedness demonstrate the ef- ness of our method with an average f 20% and 3% over baselines, re- ely. Using entity linking as a case the results on benchmark dataset the quality of our embeddings.  <ref type="bibr">Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., et al., 2017;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><p>word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>. <ref type="table" target="#tab_11">Allstar   534   535   536   537   538   539   540   541   542   543   544   545   546   547   548   549   584   585   586   587   588   589   590   591   592   593   594   595   596   597   598   599   s</ref>  <ref type="table" target="#tab_11">5   422   423   424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   472   473   474   475   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> y∈{en,zh} x y i ∈{D y ,A y ,G y } i i</p><formula xml:id="formula_7">L s = ||s en s zh || 2 第 5 章 跨语⾔的词和实体联合表⽰学习 e NBA e</formula><formula xml:id="formula_8">k = s e k,l ∈s e k ψ(e i , s k,l ) w e m ∈s e k,l ψ (w m , w n )w m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training</head><formula xml:id="formula_9">L = Lm + Le + γLs γ E R</formula><p>(1) where x y i is either a word or an entity, and C(x y i ) denotes: (i) contextual words in a pre-defined win- dow of x y i if x y i ∈ D y , (ii) neighbor entities that linked to x y i if x y i ∈ G y , (iii) contextual words of w y j if x y i is entity e y i in an anchor ⟨w y j , e y i ⟩ ∈ A y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-lingual Entity Regularizer</head><p>The bilingual EN G en−zh merges entities in dif- ferent languages into a unified network, resulting in the possibility of using the same objective as in mono-lingual ENs. Thus, we naturally extend mono-lingual function to cross-lingual settings:</p><formula xml:id="formula_10">Le = e y i ∈{G en−zh } log P (C ′ (e y i )|e y i )<label>(2)</label></formula><p>where C ′ (e y i ) denotes cross-lingual contexts- neighbor entities in different languages that linked to e y i . Thus, by jointly learning mono-lingual rep- resentation with cross-lingual entity regularizer, words and entities share more common contexts, and will have similar embeddings. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, English entity NBA co-occurs with words basketball and player in texts, so they are embed- ded close in the semantic space. Meanwhile, cross- lingual linked entities NBA and NBA (zh) have sim- ilar representations due to the most common neigh- bor entities, e.g., Foust.</p><p>aligned words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Attention</head><p>Suppose that sentences {s en k,l |l ∈ L} contain the same entities in articles of entity e y m, the wrong la- belling errors increase because some of them are almost irrelevant to e y m. Knowledge attention aims at filtering out wrong labelled sentences through smaller weights and related sentences with higher weights. Thus, we define it proportional to the sim- ilarity between s y k,l and e y m:</p><formula xml:id="formula_11">ψ(e y m , s y k,l ) ∝ sim(e y m , w y i ∈s y k,l w y i )<label>(5)</label></formula><p>where sim is similarity measurement, and we use cosine similarity in the rest of the pa- per. We normalize knowledge attention such that L l ψ(e y m, s y k,l ) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual Attention</head><p>Inspired by self-attention mechanism ( <ref type="bibr" target="#b26">Lin et al., 2017b</ref>), we motivate cross-lingual attention focus- ing on potential information from comparable sen- tences themselves. The intuition is to find possible aligned words between languages, and filter out the words without alignments. We define it according to the maximum similarity: <ref type="table" target="#tab_11">4   321   322   323   324   325   326   327   328   329   330   331   332   333   334   335   336   337   338   339   340   341   342   343   344   345   346   347   348   349   371   372   373   374   375   376   377   378   379   380   381   382   383   384   385   386   387   388   389   390   391   392   393   394   395   396   397   398</ref> 399 <ref type="table" target="#tab_11">433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> &lt; een,Kobe, ezh,Kobe &gt; &lt; een,Joe, ezh,Joe &gt; wki</p><formula xml:id="formula_12">L = ei ∈ E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi, C(ei )) + (ei,ej )∈ Ec P(ej |mi, C(ei )) L = ei ∈ E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi , C(ei )) + (ei,ej )∈ Ec P(ej |mi , C(ei )) 432</formula><formula xml:id="formula_13">C l (wki) Ls = &lt;skm,slm∈Sk,l wki∈skm ||wki − C l (wki)|| 2 &lt; skm, slm &gt;∈ Sk,l m k l 1 098 099</formula><p>entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><p>embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>.</p><formula xml:id="formula_14">1 1 1 1 1</formula><p>ter in which language, e.g., English entity Foust and Chinese entity 福 斯 特 (Foust) are embed- ded close in semantic space due to the common neighbors NBA, All-star and NBA 选秀 (draft), etc. (3) Cross-lingual Sentence Regularizer aims to learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i.e. comparable sentences) together. For exam- ple, English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences, and are close in the semantic space. All word/entity embeddings are trained jointly under a unified optimization objective. Next, we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities ( <ref type="bibr">Zhu et al., 2017)</ref>. However, we argue that this may mislead to an inconsistent training ob- multiple relations. For example <ref type="figure" target="#fig_0">(Figure 1</ref>), there will be no direct relation between Chinese entity 福 斯特 (Foust) and English entity Piston by merely adding the equivalence relation between Foust and 福斯特, which is in contradiction with the fact that Foust belongs to Piston, no matter in which lan- guage.</p><p>Therefore, we build bi-lingual entity network by making cross-lingual linked entities that inherit all relations from each other. Concretely, we enhance mono-EN by adding edges from all neighbors of entity e e i to e z j if &lt; e e i , e z j &gt;∈ R e−z (layer 2). e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generation</head><p>We utilize distant supervision to generate com- parable sentences from Wikipedia articles. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, from the page articles of cross- lingual linked entities e e Kobe and e z Kobe , we extract those sentences including another cross-lingual linked entities e e Joe and e z Joe as comparable sen- tences S e−z = {&lt; s e k , s z k &gt;}. The intuition is that we consider each sentence in a Wikipedia article has a pseudo mention of the page entity (talking something about the en- tity) ( <ref type="bibr">Yamada et al., 2017)</ref>. Thus, if a sentence also mentions another entity, it implicitly expresses their relation. Therefore, we make a similar as- sumption as in relation extraction: If two enti- ties participate in a relation, and both of them <ref type="table" target="#tab_11">Zh   1   019   020   021   022   023   024   025   026   027   028   029   030   031   032   033   034   035   036   037   038   039   040   041   042   043   044   045   046   047   048   049   069   070   071   072   073   074   075   076   077   078   079   080   081   082   083   084   085   086   087   088   089   090   091   092   093   094   095   096   097   098   099</ref> joint inference among knowledge base and text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>). 2016) learns to represent entities based on their textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_11">1   017   018   019   020   021   022   023   024   025   026   027   028   029   030   031   032   033   034   035   036   037   038   039   040   041   042   043   044   045   046   047   048   049   067   068   069   070   071   072   073   074   075   076   077   078   079   080   081   082   083   084   085   086   087   088   089   090   091   092   093   094   095   096   097   098   099</ref> method that integrates cross-lingual word and entity representation learning to enable joint inference among knowledge base and text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB)</p><note type="other">, storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction (Weston et al., 2013; Lin et al., 2017), and entity linking (Tsai and Roth, 2016; Yamada et al., 2016; Cao et al., 2017; Ji et al., 2016). ilar embedding vectors. Another approach in (Han et al., 2016; Toutanova et al., 2015; Wu et al., 2016) learns to represent entities based on their textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios. In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism,</note><p>which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean <ref type="table" target="#tab_11">-1   019   020   021   022   023   024   025   026   027   028   029   030   031   032   033   034   035   036   037   038   039   040   041   042   043   044   045   046   047   048   049   069   070   071   072   073   074   075   076   077   078   079   080   081   082   083   084   085</ref>  </p><note type="other">joint inference among knowledge base and text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve</note><p>the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB)</p><note type="other">, storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction (Weston et al., 2013; Lin et al., 2017), and entity linking (Tsai and Roth, 2016; Yamada et al., 2016; Cao et al., 2017; Ji et al., 2016). 2016) learns to represent entities based on their textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios. In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism,</note><p>which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>). </p><note type="other">text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve</note><p>the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB)</p><note type="other">, storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction (Weston et al., 2013; Lin et al., 2017), and entity linking (Tsai and Roth, 2016; Yamada et al., 2016; Cao et al., 2017; Ji et al., 2016). textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios. In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism,</note><p>which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>). </p><note type="other">joint inference among knowledge base and text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve</note><p>the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB)</p><note type="other">, storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction (Weston et al., 2013; Lin et al., 2017), and entity linking (Tsai and Roth, 2016; Yamada et al., 2016; Cao et al., 2017; Ji et al., 2016). 2016) learns to represent entities based on their textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios. In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism,</note><p>which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e NBA , w , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>eNBA The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>.  </p><note type="other">into a longer sentence s en k , but this increases the chance to include unrelated sentences. Unbalanced information. Sometimes the pair of sentences convey different information, e.g., the English sentence in layer 2 (Figure 1) contains Foust spent 12 seasons in NBA while the compa- rable Chinese sentence not.</note><p>To address the issues, we propose knowledge at- tention and cross-lingual attention to filter out un- related information at sentence level and at word level, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[[这里感觉改动较大]] 5 Joint Representation Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mono-lingual Representation Learning</head><p>Following ( <ref type="bibr">Yamada et al., 2016;</ref>, we learn mono-lingual word/entity embeddings based on corpus D y , anchors A y and entity net- work G y . We utilize a variant of Skip-gram model ( <ref type="bibr">Mikolov et al., 2013c</ref>) to predict the con- texts given current word/entity:</p><formula xml:id="formula_15">Lm = y∈{en,zh} x y i ∈{D y ,A y ,G y } log P (C(x y i )|x y i )</formula><p>(1) where x y i is either a word or an entity, and C(x y i ) denotes: (i) contextual words in a pre-defined win- dow of x y i if x y i ∈ D y , (ii) neighbor entities that linked to x y i if x y i ∈ G y , (iii) contextual words of w y j if x y i is entity e y i in an anchor ⟨w y j , e y i ⟩ ∈ A y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-lingual Entity Regularizer</head><p>The bilingual EN G en−zh merges entities in dif- ferent languages into a unified network, resulting in the possibility of using the same objective as in mono-lingual ENs. Thus, we naturally extend mono-lingual function to cross-lingual settings:</p><formula xml:id="formula_16">Le = e y i ∈{G en−zh } log P (C ′ (e y i )|e y i )<label>(2)</label></formula><p>where C ′ (e y i ) denotes cross-lingual contexts- neighbor entities in different languages that linked to e y i . Thus, by jointly learning mono-lingual rep- resentation with cross-lingual entity regularizer, words and entities share more common contexts, and will have similar embeddings. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, English entity NBA co-occurs with words basketball and player in texts, so they are embed- ded close in the semantic space. Meanwhile, cross- lingual linked entities NBA and NBA (zh) have sim- ilar representations due to the most common neigh- bor entities, e.g., Foust.</p><p>Comparable sentences provide cross-lingual co- occurrence of words, thus, we learn similar em- beddings for the words that frequently co-occur to- gether by minimizing the Euclidean distance:</p><formula xml:id="formula_17">Ls = ⟨s en k ,s zh k′ ⟩∈S en−zh ||s en k − s zh k ′ || 2<label>(3)</label></formula><p>where s en k , s zh k ′ are sentence embeddings. Take En- glish as sample language, we define it as the aver- age sum of word vectors weighted by the combi- nation of two types of attentions:</p><formula xml:id="formula_18">s en k = l∈L ψ(e en m , s en k,l ) w en i ∈s en k,l ψ ′ (w en i , w zh j )w en i<label>(4)</label></formula><p>where {s en k,l |l ∈ L} is a set of sentences con- taining the same entity (as mentioned in Sec- tion 4.2), and ψ(e en m , s en k,l ) is knowledge attention that aims at filter out wrong labelling sentences, and ψ ′ (w en i , w zh j ) is cross-lingual attention to deal with the unbalanced information through possible aligned words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Attention</head><p>Suppose that sentences {s en k,l |l ∈ L} contain the same entities in articles of entity e y m, the wrong la- belling errors increase because some of them are almost irrelevant to e y m. Knowledge attention aims at filtering out wrong labelled sentences through smaller weights and related sentences with higher weights. Thus, we define it proportional to the sim- ilarity between s y k,l and e y m:</p><formula xml:id="formula_19">ψ(e y m , s y k,l ) ∝ sim(e y m , w y i ∈s y k,l w y i )<label>(5)</label></formula><p>where sim is similarity measurement, and we use cosine similarity in the rest of the pa- per. We normalize knowledge attention such that L l ψ(e y m, s y k,l ) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual Attention</head><p>Inspired by self-attention mechanism (Lin et al., 2017b), we motivate cross-lingual attention focus- ing on potential information from comparable sen- tences themselves. The intuition is to find possible aligned words between languages, and filter out the words without alignments. We define it according to the maximum similarity:  <ref type="table" target="#tab_11">319   320   321   322   323   324   325   326   327   328   329   330   331   332   333   334   335   336   337   338   339   340   341   342   343   344   345   346   347   348   349   366   367   368   369   370   371   372   373   374   375   376   377   378   379   380   381   382   383   384   385   386   387   388   389   390   391   392   393   394   395   396   397   398</ref> 399  <ref type="table" target="#tab_11">596   597   598   599               424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   474   475   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498</ref>  2016; <ref type="bibr" target="#b20">Ji et al., 2016</ref>). <ref type="bibr">2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>).  <ref type="table" target="#tab_12">45   46   47   48   49   093   094   095   096   097   098   099</ref> while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><formula xml:id="formula_20">L = (ei,ej )∈Ec ||C(ei ) − C(ej )|| 2 Ec L = ei ∈ E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi, C(ei )) + (ei,ej )∈ Ec P(ej |mi, C(ei )) @@@@@@@@@@@@@@@@ L = (ei,ej )∈Ec ||C(ei ) − C(ej )|| 2 Ec L = ei ∈ E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi , C(ei )) + (ei,ej )∈ Ec P(ej |mi , C(ei )) N (ei ) L = (ei,mi )∈ A P(ei |mi , C(ei )) + (ei,ej )∈ Ec P(ej |mi , C(ei ))</formula><note type="other">499 &lt; een,Kobe, ezh,Kobe &gt; &lt; een,Joe, ezh,Joe &gt; een,Joe wen,player wen,NBA een,LosAngelesLakers een,Joe een,Kobe ezh,Kobe</note><p>Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>  entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016).</ref> embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>. KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_14">1   044   045   046   047   048   049   094   095   096   097   098   099</ref> KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_14">1   044   045   046   047   048   049   094   095   096   097   098   099</ref> KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_14">1   044   045   046   047   048   049   094   095   096   097   098   099</ref> KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>   <ref type="table" target="#tab_14">046   047   048   049   094   095   096   097   098   099</ref> of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean-side, and in the left side there are three main components of joint representation learning. Red texts with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual links.</p><p>mon neighbors have similar embeddings, no mat- ter in which language, e.g., English entity Foust and Chinese entity 福 斯 特 (Foust) are embed- ded close in semantic space due to the common neighbors NBA, All-star and NBA 选秀 (draft), etc. (3) Cross-lingual Sentence Regularizer aims to learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i.e. comparable sentences) together. For exam- ple, English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences, and are close in the semantic space.</p><p>All word/entity embeddings are trained jointly under a unified optimization objective. Next, we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities ( <ref type="bibr">Zhu et al., 2017)</ref>. However, we argue that this may mislead to an inconsistent training ob- jective since a cross-lingual link actually contains multiple relations. For example <ref type="figure" target="#fig_0">(Figure 1</ref>), there will be no direct relation between Chinese entity 福 斯特 (Foust) and English entity Piston by merely adding the equivalence relation between Foust and 福斯特, which is in contradiction with the fact that Foust belongs to Piston, no matter in which lan- guage.</p><p>Therefore, we build bi-lingual entity network by making cross-lingual linked entities that inherit all relations from each other. Concretely, we enhance mono-EN by adding edges from all neighbors of entity e e i to e z j if &lt; e e i , e z j &gt;∈ R e−z (layer 2). e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generation</head><p>We utilize distant supervision to generate com- parable sentences from Wikipedia articles. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, from the page articles of cross- lingual linked entities e e Kobe and e z Kobe , we extract those sentences including another cross-lingual linked entities e e Joe and e z Joe as comparable sen- tences S e−z = {&lt; s e k , s z k &gt;}. The intuition is that we consider each sentence in a Wikipedia article has a pseudo mention of the page entity (talking something about the en- tity) ( <ref type="bibr">Yamada et al., 2017)</ref>. Thus, if a sentence also mentions another entity, it implicitly expresses their relation. Therefore, we make a similar as- sumption as in relation extraction: If two enti- ties participate in a relation, and both of them  <ref type="table" target="#tab_11">319   320   321   322   323   324   325   326   327   328   329   330   331   332   333   334   335   336   337   338   339   340   341   342   343   344   345   346   347   348   349   366   367   368   369   370   371   372   373   374   375   376   377   378   379   380   381   382   383   384   385   386   387   388   389   390   391   392   393   394   395   396   397   398</ref> 399  <ref type="table" target="#tab_11">596   597   598   599               424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   474   475   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> &lt; een,Kobe, ezh,Kobe &gt; &lt; een,Joe, ezh,Joe &gt; 2016; <ref type="bibr" target="#b20">Ji et al., 2016</ref>). <ref type="bibr">2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>. <ref type="table" target="#tab_12">1   043   044   045   046   047   048   049   093   094   095   096   097   098   099</ref> while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><formula xml:id="formula_21">L = (ei,ej )∈Ec ||C(ei ) − C(ej )|| 2 Ec L = ei ∈ E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi, C(ei )) + (ei,ej )∈ Ec P(ej |mi, C(ei )) @@@@@@@@@@@@@@@@ L = (ei,ej )∈Ec ||C(ei ) − C(ej )|| 2 Ec L = ei ∈ E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi , C(ei )) + (ei,ej )∈ Ec P(ej |mi , C(ei )) N (ei ) L = (ei,mi )∈ A P(ei |mi , C(ei )) + (ei,ej )∈ Ec P(ej |mi , C(ei ))</formula><note type="other">een,Joe wen,player wen,NBA een,LosAngelesLakers een,Joe een,Kobe ezh,Kobe</note><p>Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>  entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016).</ref> embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017)</ref>. KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_14">1   044   045   046   047   048   049   094   095   096   097   098   099</ref> KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_14">1   044   045   046   047   048   049   094   095   096   097   098   099</ref> KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- <ref type="table" target="#tab_14">1   044   045   046   047   048   049   094   095   096   097   098   099</ref> KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016)</ref>. instance, textual ambiguity in one language may disappear in another language, e.g., the two mean-  <ref type="bibr">Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref> <ref type="bibr">Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref></p><note type="other">047 048 049 095 096 097 098 099 of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction (Weston et al., 2013; Lin et al., 2017), and entity linking (</note><note type="other">). disappear in another language, e.g., the two mean-1 044 045 046 047 048 049 094 095 096 097 098 099 of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction (Weston et al., 2013; Lin et al., 2017), and entity linking (</note><note type="other">). guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean-side, and in the left side there are three main components of joint representation learning. Red texts with brackets are anchors, dashed lines between entities denote relations, and solid lines are cross-lingual links. mon neighbors have similar embeddings, no mat- ter in which language, e.g., English entity Foust and Chinese entity 福 斯 特 (Foust) are embed- ded close in semantic space due to the common neighbors NBA, All-star and NBA 选秀 (draft), etc.</note><p>(3) Cross-lingual Sentence Regularizer aims to learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i.e. comparable sentences) together. For exam- ple, English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences, and are close in the semantic space.</p><p>All word/entity embeddings are trained jointly under a unified optimization objective. Next, we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities ( <ref type="bibr">Zhu et al., 2017)</ref>. However, we argue that this may mislead to an inconsistent training ob- jective since a cross-lingual link actually contains multiple relations. For example <ref type="figure" target="#fig_0">(Figure 1</ref></p><note type="other">), there will be no direct relation between Chinese entity 福 斯特 (Foust) and English entity Piston by merely adding the equivalence relation between Foust and 福斯特</note><p>, which is in contradiction with the fact that Foust belongs to Piston, no matter in which lan- guage.</p><p>Therefore, we build bi-lingual entity network by making cross-lingual linked entities that inherit all relations from each other. Concretely, we enhance mono-EN by adding edges from all neighbors of entity e e i to e z j if &lt; e e i , e z j &gt;∈ R e−z (layer 2). e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generation</head><p>We utilize distant supervision to generate com- parable sentences from Wikipedia articles. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, from the page articles of cross- lingual linked entities e e Kobe and e z Kobe , we extract those sentences including another cross-lingual linked entities e e Joe and e z Joe as comparable sen- tences S e−z = {&lt; s e k , s z k &gt;}. The intuition is that we consider each sentence in a Wikipedia article has a pseudo mention of the page entity (talking something about the en- tity) ( <ref type="bibr">Yamada et al., 2017)</ref>. Thus, if a sentence also mentions another entity, it implicitly expresses their relation. Therefore, we make a similar as- sumption as in relation extraction: If two enti- ties participate in a relation, and both of them </p><note type="other">method that integrates cross-lingual word and entity representation learning to enable joint inference among knowledge base and text across languages, capturing mutually complementary knowledge. Instead of re- liance on parallel data, we automatically generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention</note><p>to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref></p><note type="other">). ilar embedding vectors. Another approach in (Han et al., 2016; Toutanova et al., 2015; Wu et al., 2016) learns to represent entities based on their textual descriptions together with the structured re- lations. However, these methods only focus on mono-lingual settings, and few researches have been done in cross-lingual scenarios.</note><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean-  <ref type="table" target="#tab_11">1   073   074   075   076   077   078   079   080   081   082   083   084   085   086   087   088   089   090   091   092   093   094   095   096   097   098   099</ref> generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>).</p><p>been</p><note type="other">done in cross-lingual scenarios. In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in</note><p>which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>). </p><note type="other">generate cross-lingual training data via dis- tant supervision over multi-lingual knowl- edge bases. We also propose two types of knowledge attention and cross-lingual attention to select the most informative words and filter out noise, which will fur- ther improve the performance. In exper- iments, separate tasks of word translation and entity relatedness demonstrate the ef- fectiveness of our</note><p>method with an average gain of 20% and 3% over baselines, re- spectively. Using entity linking as a case study, the results on benchmark dataset verify the quality of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-lingual knowledge bases (KB), storing mil- lions of entities and their facts in various lan- guages, provide rich structured knowledge for un- derstanding natural language beyond texts. Mean- while, abundant text corpus contains large amount of potential knowledge complementary to existing KBs. Therefore, researchers leverage both types of resources to improve various natural language processing (NLP) related tasks, such as relation ex- traction ( <ref type="bibr">Weston et al., 2013;</ref><ref type="bibr">Lin et al., 2017)</ref>, and entity linking <ref type="bibr">(Tsai and Roth, 2016;</ref><ref type="bibr">Yamada et al., 2016;</ref><ref type="bibr" target="#b20">Ji et al., 2016</ref>). mono-lingual settings, and few researches have been done in cross-lingual scenarios.</p><p>In this paper, we propose to learn cross-lingual word and entity representations in the same seman- tic space, to enable joint inference among KB and text across languages without any additional trans- lation mechanism, which is usually expensive and may introduce inevitable errors. Our embeddings are helpful to break down language gaps in many tasks, such as cross-lingual entity linking, in which the major challenge lies in measuring the similar- ity between entities and corresponding mentioned words in different languages.</p><p>e , w , w The intuition is that, words and entities in various languages share some common semantic meanings 1 , but there are also ways in which they differ. On one hand, we utilize their shared seman- tics to align similar words and entities with simi- lar embedding vectors, no matter they are in the same language or not. On the other hand, cross- lingual embeddings will benefit from different lan- guages due to the complementary knowledge. For instance, textual ambiguity in one language may disappear in another language, e.g., the two mean- 1 Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora ex- hibit isomorphic structure across languages ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Zhang et al., 2017</ref>).  <ref type="table" target="#tab_11">326   327   328   329   330   331   332   333   334   335   336   337   338   339   340   341   342   343   344   345   346   347   348   349   373   374   375   376   377   378   379   380   381   382   383   384   385   386   387   388   389   390   391   392   393   394   395   396   397   398</ref> 399</p><formula xml:id="formula_22">L = ei ∈E P(N (ei )|ei ) + (ei,ej )∈Ec P(N (ej )|ei ) N (ei ) L = (ei,mi )∈A P(ei |mi, C(ei )) + (ei,ej )∈Ec P(ej |mi, C(ei )) L = N + N N L = m A m C + m C &lt; &gt; w w L = w − w &lt; m m &gt; m 1</formula><p>ded c o e n eman c pace due o he common ne ghbo NBA A a and NBA 选秀 d a e c 3 Cro ngua Sen ence Regu ar zer a m o ea n mu ua y an a ed wo d w h m a em bedd ng by pu h ng he c o ngua con ex e compa ab e en ence oge he Fo exam p e Eng h wo d ba ke ba and he an a ed Ch ne e wo d 篮球 equen y co occu n com pa ab e en ence and a e c o e n he eman c pace A wo d en y embedd ng a e a ned o n y unde a un fied op m za on ob ec ve Nex we w n oduce how o gene a e b ngua EN and compa ab e en ence a we a he h ee compo nen o o n ep e en a on ea n ng n u n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cro ngua Superv on Da a Genera on</head><p>Th ec on n oduce how o ex ac mo e c o ngua c ue om mu ngua KB n he o m o b ngua EN and compa ab e en ence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ngua En y Ne work Con ruc on</head><p>Conven ona know edge ep e en a on me h od no ma y ega d c o ngua nk a a pe c a equ va ence ype o e a on be ween wo en e Zhu e a 2017 Howeve we a gue ha h may m ead o an ncon en a n ng ob 斯特 Fou and Eng h en y P on by me e y add ng he equ va ence e a on be ween Fou and 福斯特 wh ch n con ad c on w h he ac ha Fou be ong o P on no ma e n wh ch an guage</p><p>The e o e we bu d b ngua en y ne wo k by mak ng c o ngua nked en e ha nhe a e a on om each o he Conc e e y we enhance mono EN by add ng edge om a ne ghbo o en y e o e &lt; e e &gt;∈ R − aye 2 e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparab e Sen ence Genera on</head><p>We u ze d an upe v on o gene a e com pa ab e en ence om W k ped a a c e A hown n F gu e 1 om he page a c e o c o ngua nked en e e K and e K we ex ac ho e en ence nc ud ng ano he c o ngua nked en e e and e a compa ab e en ence S − = {&lt; s s &gt;}</p><p>The n u on ha we con de each en ence n a W k ped a a c e ha a p eudo men on o he page en y a k ng ome h ng abou he en y Yamada e a 2017 Thu a en ence a o men on ano he en y mp c y exp e e he e a on The e o e we make a m a a ump on a n e a on ex ac on wo en e pa c pa e n a e a on and bo h o hem   </p><formula xml:id="formula_23">e J an L = n 图 5 4 跨语⾔词和实体联合表⽰学习模型 L m = D og P C x x + og P N e e +</formula><formula xml:id="formula_24">399 ∈E ∈E N (e ) L = m ∈A P(e m C(e )) + ∈E P(e m C(e )) L = ∈ E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈ E P(e m C(e )) L = &lt; m m∈S w ∈ m w − C w &lt; s m s m &gt;∈ S m k 1</formula><p>learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i e comparable sentences) together For exam- ple English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences and are close in the semantic space All word/entity embeddings are trained jointly under a unified optimization objective Next we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities (Zhu et al 2017) However we argue that this may mislead to an inconsistent training ob- Foust belongs to Piston no matter in which lan- guage Therefore we build bi-lingual entity network by making cross-lingual linked entities that inherit all relations from each other Concretely we enhance mono-EN by adding edges from all neighbors of entity e e to e z j if &lt; e e , e z j &gt;∈ R e−z (layer 2) e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generation</head><p>We utilize distant supervision to generate com- parable sentences from Wikipedia articles As shown in <ref type="figure" target="#fig_0">Figure 1</ref>   </p><formula xml:id="formula_25">= ∈E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈E P(e m C(e )) L = ∈ E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈ E P(e m C(e )) w C w L = &lt; m m∈S w ∈ m w − C w &lt; s m s m &gt;∈ S m k 1</formula><p>neighbors NBA All-star and NBA 选秀 (draft) etc (3) Cross-lingual Sentence Regularizer aims to learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i e comparable sentences) together For exam- ple English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences and are close in the semantic space All word/entity embeddings are trained jointly under a unified optimization objective Next we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities (Zhu et al 2017) However we argue that this may mislead to an inconsistent training ob- adding the equivalence relation between Foust and 福斯特 which is in contradiction with the fact that Foust belongs to Piston no matter in which lan- guage Therefore we build bi-lingual entity network by making cross-lingual linked entities that inherit all relations from each other Concretely we enhance mono-EN by adding edges from all neighbors of entity e e to e z j if &lt; e e , e z j &gt;∈ R e−z (layer 2) e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generation</head><p>We utilize distant supervision to generate com- parable sentences from Wikipedia articles As shown in <ref type="figure" target="#fig_0">Figure 1</ref>   </p><formula xml:id="formula_26">= ∈E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈E P(e m C(e )) L = ∈ E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈ E P(e m C(e )) &lt; e n K e h K &gt; &lt; e n J e h J &gt; w C w L = &lt; m m∈S w ∈ m w − C w &lt; s m s m &gt;∈ S m k</formula><note type="other">1 098 099 ng (Tsa and Ro h 2016 Yamada e a e a 2017 J e a 2016) h b somorph c s ruc ure ac oss anguages M ko ov e a 2013 Zhang e a 2017 1 1 1</note><p>and Chinese entity 福 斯 特 (Foust) are embed- ded close in semantic space due to the common neighbors NBA All-star and NBA 选秀 (draft) etc (3) Cross-lingual Sentence Regularizer aims to learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i e comparable sentences) together For exam- ple English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences and are close in the semantic space All word/entity embeddings are trained jointly under a unified optimization objective Next we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities (Zhu et al 2017) However we argue that this may mislead to an inconsistent training ob- will be no direct relation between Chinese entity 福 斯特 (Foust) and English entity Piston by merely adding the equivalence relation between Foust and 福斯特 which is in contradiction with the fact tha Foust belongs to Piston no matter in which lan guage Therefore we build bi-lingual entity network by making cross-lingual linked entities that inherit al relations from each other Concretely we enhance mono-EN by adding edges from all neighbors o entity e e to e z j if &lt; e e , e z j &gt;∈ R e−z (layer 2) e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generation</head><p>We utilize distant supervision to generate com parable sentences from Wikipedia articles A shown in <ref type="figure" target="#fig_0">Figure 1</ref>   </p><formula xml:id="formula_27">= ∈E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈E P(e m C(e )) E L = ∈ E P(N (e ) e ) + ∈E P(N (e ) e ) N (e ) L = m ∈A P(e m C(e )) + ∈ E P(e m C(e )) e m ∈A e e ∈ E &lt; e n K e h K &gt; &lt; e n J e h J &gt; e n L An L e n J e n K e h K w C w L = &lt; m m∈S w ∈ m w − C w &lt; s m s m &gt;∈ S m k 1</formula><note type="other">1 095 096 097 098 099 of resources o mprove var ous na ura anguage process ng (NLP) re a ed asks such as re a on ex- rac on (Wes on e a 2013 L n e a 2017) and en y nk ng (Tsa and Ro h 2016 Yamada e a 2016 Cao e a 2017 J e a 2016) d sappear n ano her anguage e g he wo mean- Some c oss ngua p onee ng wo k obse ve ha wo d embedd ngs a ned sepa a e y on mono ngua co po a ex h b somorph c s ruc ure ac oss anguages M ko ov e a 2013 Zhang e a 2017 1 1 1 en y nk ng T a and Ro h 2016 Yamada e a 2016 Cao e a 2017 e a 2016 m ph u u M Z 1 2016 Cao e a 2017 e a 2016 Z 1 2016 Cao e a 2017 e a 2016 Z 1 2016 Cao e a 2017 e a 2016 Z 1 1 048 049 098 099 en y nk ng T a and Ro h 2016 Yamada e a 2016 Cao e a 2017 e a 2016 mb dd ng n d p y on mono ngu o po x h b omo ph u u o ngu g M ko ov 2013 Zh ng 2017</note><p>mon neighbors have similar embeddings no mat- ter in which language e g English entity Foust and Chinese entity 福 斯 特 (Foust) are embed- ded close in semantic space due to the common neighbors NBA All-star and NBA 选秀 (draft) etc (3) Cross-lingual Sentence Regularizer aims to learn mutually translated words with similar em- beddings by pushing their cross-lingual contexts (i e comparable sentences) together For exam- ple English word basketball and the translated Chinese word 篮球 frequently co-occur in com- parable sentences and are close in the semantic space All word/entity embeddings are trained jointly under a unified optimization objective Next we will introduce how to generate bi-lingual EN and comparable sentences as well as the three compo- nents for joint representation learning in turn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-lingual Supervision Data Generation</head><p>This section introduces how to extract more cross- lingual clues from multi-lingual KB in the form of bi-lingual EN and comparable sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-lingual Entity Network Construction</head><p>Conventional knowledge representation meth- ods normally regard cross-lingual links as a spe- cial equivalence type of relation between two en- tities (Zhu et al 2017) However we argue that this may mislead to an inconsistent training ob- jective since a cross-lingual link ac multiple relations For example (F will be no direct relation between Ch 斯特 (Foust) and English entity Pis adding the equivalence relation betw 福斯特 which is in contradiction w Foust belongs to Piston no matter guage Therefore we build bi-lingual ent making cross-lingual linked entities relations from each other Concrete mono-EN by adding edges from al entity e e to e z j if &lt; e e , e z j &gt;∈ R e−z e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable Sentences Generatio</head><p>We utilize distant supervision to parable sentences from Wikipedia shown in <ref type="figure" target="#fig_0">Figure 1</ref>  at filtering out wrong labelling sentences and ψ (w en , w zh j ) is cross-lingual attention to deal with the unbalanced information through possible aligned words Next we will introduce the two types of atten- tions in detail</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Attention</head><p>Suppose that sentences s en k | L =1 contain the same entities in articles of entity e en m the wrong labelling errors increase because some s en k is almost irrele- vant to e en m Knowledge attention assigns smaller weights to wrong labelled sentences and higher weights to related sentences Thus we define it proportional to the similarity between s en k and e en m :</p><formula xml:id="formula_28">ψ(e en m , s en k ) ∝ sim(e en m , ∑ w en ∈s en k w en )</formula><p>where sim is similarity measurement We use cosine similarity in the presented work Knowledge attention is normalized to satisfy</p><formula xml:id="formula_29">∑ L ψ(e en m , s en k ) = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual Attention</head><p>Inspired by self-attention mechanism (Lin et al 2017b) we motivate cross-lingual attention focus- ing on potential information from comparable sen- tences themselves The intuition is to find possible aligned words between languages and filter out the words without alignments We define it according to the maximum similarity computed by our cross- lingual word embeddings:</p><formula xml:id="formula_30">ψ (w en , w zh j ) ∝ max w en ∈s en k w zh j ∈s zh k sim(w en , w zh j )</formula><p>We set a threshold for discarding non-aligned words if ψ (w en , w zh j ) &lt; θ and make a normal- ization for selected words We set θ = 0 in exper- iments Thus unbalanced information is trimmed to the common meanings between s en k and s zh k For example <ref type="figure" target="#fig_0">(Figure 1)</ref> words American basketball player are selected due to their aligned Chinese words 美国 篮球 运动员 while 12 seasons in s en k or 前 (former) in s zh k are discarded due to low attentions</p><p>The reason of using such regularizer lies in two points: (1) the embeddings of cross-lingual aligned words become closer within the pair of comparable sentences and meanwhile (2) the dis- tance between their contexts is also minimized which keeps the same way as used in mono-lingual word embeddings training-the words sharing more contexts have similar embeddings In this way our regularizer follows a similar assumption with (Gouws et al 2015): The more frequently two words occur in parallel/comparable sentence pairs the closer their representation will be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">4 Training</head><p>All above components are jointly trained using the overall objective function as follows:</p><formula xml:id="formula_31">L = L m + L e + γL s</formula><p>where γ is a hyper-parameter to tune the effect of cross-lingual sentence regularizer and set to 1 in experiments. We use Softmax as probability func- tion, and negative sampling and SGD for efficient optimization <ref type="bibr" target="#b29">(Mikolov et al., 2013a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we describe some qualitative analysis with nearest neighbors and quantita- tive experiments with the tasks of word trans- lation, entity relatedness and cross-lingual en- tity linking to verify the quality of cross- lingual word embeddings, entity embeddings and the joint inference among them, respec- tively.</p><p>The codes of our proposed model can be found in https://github.com/ TaoMiner/MultiLingualEmbedding. We choose Wikipedia, the April 2017 dump, as multi-lingual KB and six popular languages for evaluation. The preprocessing consists of follow- ing steps: converting texts into lower cases, filter- ing out symbols and low frequency words and en- tities (less than 5), and tokenizing Chinese corpus using Jieba <ref type="bibr">4</ref> and Japanese corpus using mecab <ref type="bibr">5</ref> . The statistics is listed in <ref type="table">Table 1</ref>. For brevity, we adopt two-letter abbreviations: 'En', 'Zh', 'Es', 'Ja', 'It' and 'Tr' for English, Chinese, Span- ish, Japanese, Italian and Turkish, respectively. The token sub-column denotes the total number of word/entity in the entire training corpus, and we use 'm' to denote million and 'b' for billion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Settings</head><p>For cross-lingual settings, we choose five lan- guage pairs to compare with state-of-the-art meth- ods, whose statistics is listed in <ref type="table" target="#tab_11">Table 2</ref>.</p><p>We trained our method using the suggested parameters in Skip-gram model ( <ref type="bibr">Mikolov et al., 2013c)</ref>   hours on the server with 64 core CPU and 188GB memory. The embedding dimension is set to 200 and context window size is 5. For each positive example, we sample 5 negative examples.  We manually checked nearest neighbors to have a straightforward impression of the quality of our embeddings. The nearest neighbors of English word basketball is listed in <ref type="table" target="#tab_12">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative Analysis</head><p>As <ref type="table" target="#tab_12">Table 3</ref> shows, we find the correct translation ranked at top 1 (marked by +), and the listed words as well as English nearest words are all basketball related, indicating a higher quality of our cross- lingual word embeddings. Interestingly, we found that although all nearest entities are sports related, e.g., NBA or Professional sports, there is an ob- vious culture divergence between Chinese entities and English entities, such as Hong Kong basketball league v.s. All-America.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Word Translation</head><p>Following ( <ref type="bibr">Zhang et al., 2017b)</ref>, we test our cross- lingual word embeddings on benchmark dataset including over 2,000 bilingual word pairs on av- erage. The ground truth is obtained from Open <ref type="table" target="#tab_2">large  small  large  small  large  small  large  small  large  small  TM  - 48.61  - 37.95  - 26.67  - 11.15  4.79  21.79  IA  - 60.41  - 46.52  - 36.35  - 17.11  7.08  32.29  Bilbowa  53  65.96  - - - - - - - - BWESG</ref> 48  Multilingual WordNet 6 or Google translation. We compare all methods using the same vocabulary, and analyze the vocabulary size's impact by set- ting a nearly 5k small scale and 50k large scale.</p><note type="other">Es-En It-En Ja-Zh Tr-En Zh-En</note><p>We choose several state-of-the-art methods as baseline, using different level of parallel data: (1) TM ( <ref type="bibr">Mikolov et al., 2013b</ref>), IA ( <ref type="bibr" target="#b4">Zhang et al., 2016</ref>) are pioneers and popular transformation based methods using bilingual lexicon. (2) Bil- bowa ( <ref type="bibr" target="#b12">Gouws et al., 2015</ref>) is typical work using parallel sentences and performs quite well. <ref type="formula" target="#formula_17">(3)</ref> BWESG <ref type="bibr">(Vulic and Moens, 2016</ref>) is similar to our method and achieves best performance in the literature of using comparable data. (4) Adver- sarial model ( <ref type="bibr">Zhang et al., 2017b</ref>) is the state-of- the-arts without parallel data. Besides, we re- move attention from our method to investigate the impacts from attention mechanisms, marked with Ours-noatt.</p><p>For fair comparison, we report the results in original paper ( <ref type="bibr">Zhang et al., 2017b</ref>) except Bil- bowa and BWESG, which didn't report their re- sults on the same benchmark datasets. So, we care- fully implement them using released codes on the same training corpus as ours with suggested pa- rameters. Nevertheless, we do not have perfor- mance reports of Zh-En, It-En, Tr-En and Ja-Zh with Bilbowa due to the lack of parallel data used in the original paper. As shown in <ref type="table" target="#tab_14">Table 4</ref>, we can see:</p><p>• Our proposed method significantly outper- forms all the baseline methods with average gains of 21% and 9.1% on large and small vocabulary. This proves the high quality of our generated cross-lingual data and the ef- fectiveness of our joint framework.</p><p>• The pair of languages have similar culture achieves better performance (Es-En, It-En, Tr-En, Ja-Zh) than that have different cultural origins, e.g., Zh-En. <ref type="bibr">6</ref> http://compling.hss.ntu.edu.sg/omw</p><p>• Languages with richer corpus have better translations because adequate training data helps to capture more accurate cross-lingual semantics (Es-En, It-En, Tr-En v.s. Ja-Zh).</p><p>• Our method has less performance reduction between small and large vocabulary than methods based on parallel word pairs, be- cause we adopt a consistent objective func- tion which aligns cross-lingual semantics, and simultaneously keeps their own mono- lingual semantics.</p><p>• Attention mechanisms further improve the performance, mainly because they help to select the most informative words and sen- tences, filtering out unrelated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Entity Relatedness</head><p>With respect to our entity embeddings, we have conducted experiments to evaluate English entity relatedness following ( <ref type="bibr" target="#b11">Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b17">Hoffart et al., 2011</ref>), in which the dataset con- tains 3,314 entities, and each entity has 91 candi- date entities labeled with 1 or 0, indicating whether they are semantically related. Given an entity, we rank candidate entities according to their similarity based on our embeddings, and evaluate the rank- ing quality through two standard metrics: normal- ized discounted cumulative gain (NDCG) <ref type="bibr" target="#b19">(Järvelin and Kekäläinen, 2002</ref>) and mean average precision (MAP) ( <ref type="bibr" target="#b28">Manning et al., 2008)</ref>. To give a comprehensive fair comparison, we choose several widely used and state-of-the-art methods as our baselines, and compare with the results in the original papers: (1) WLM <ref type="bibr">(Milne and Witten, 2008)</ref>, the popular semantic similar- ity measurement based on Wikipedia anchor links. (2) ALIGN ( <ref type="bibr">Yamada et al., 2016</ref>) and MPME , state-of-the-arts that jointly learn word and entity embeddings using mono-lingual EN. (3) Deep Joint (DJ) model ( <ref type="bibr" target="#b11">Ganea and Hofmann, 2017</ref>), deep neural model that achieves the best performance of entity relatedness.    <ref type="table" target="#tab_16">Table 5</ref> shows the results of baseline methods as well as our methods based on different languages. We also test the cases of our method without train- ing cross-lingual words, marked as Ours-e. We can see our method outperforms all baseline methods by introducing cross-lingual information, and all bilingual ENs lead to similar results. Strangely, ALIGN and DJ with more embedding dimensions seemly fails to capture overall relatedness (per- formance reduction from top@1 to top@5). The best performance of Ours-e implies that training cross-lingual word slightly harms the performance of entity embeddings. We can introduce additional sense embeddings in future ( .</p><p>Although favorable improvements has been achieved by using our English entity embeddings, it shall be fewer than that of other languages, be- cause resources of English are already quite rich, and even richer than many other languages, thus contributions from other languages will be less sig- nificant than vice versa. Due to the limitation of the publication, we neglect to report experiment results on the vice versa direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Cross-lingual Entity Linking</head><p>Entity linking, the task of identifing the language- specific reference entity for mentions in texts, raises the key challenges of comparing the rel- evance between entities and contextual words around the mentions ( <ref type="bibr" target="#b7">Cao et al., 2015;</ref><ref type="bibr">Nguyen et al., 2016)</ref>. Recently, the surge of cross-lingual analysis pushes the entity linking task on cross- lingual settings ( . Therefore, we comprehensively measure our joint inference abil- ity among words and entities using the tri-lingual EL benchmark dataset KBP2015, which consists of 944 documents and 38,831 mentions, and di- vides them into 444 and 500 documents for train- ing and evaluation. Note that the main purpose of it is not to beat other EL models but to evaluate the quality of our embeddings, so we adopt a simple classifier GBRT (Gradient Boost Regression Tree) based method as in <ref type="bibr">Yamada et al., 2016)</ref>, replace with our cross-lingual embeddings, and filter out mentions that are out of our vocabu- lary.   <ref type="table" target="#tab_18">Table 6</ref> shows the top 1 linking accuracy (%). We can see our method performs much better than the second ranked system, and is competitive with the top ranked system. Considering that the sys- tems utilize additional translation tools ( , we conclude that our embeddings are high qualified for joint inference among entities and words in different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we propose a novel method to jointly learn cross-lingual word and entity representa- tions that enables effective inference among cross- lingual knowledge bases and texts. Instead of par- allel data, we use distant supervision over multi- lingual KB to generate high quality comparable data as cross-lingual supervision signals for two types of regularizer. We introduce attention mech- anism to further improve the training quality. A series of experiments on several tasks verify the effectiveness of our methods as well as the quality of cross-lingual word and entity embeddings.</p><p>In the future, we will enrich semantics of low- resourced languages by cross-lingual linking to rich-resourced languages, and extend more cross- lingual words and entities to multi-lingual settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>The work is supported by NSFC key project (No. 61533018，U1736204，61661146007), Ministry of Education and China Mobile Research Fund <ref type="bibr">(No. 20181770250)</ref>, and THUNUS NExT++ Co- Lab. Partial financial support from P3ML project funded by BMBF of Germany under grant number 01/S17064 is greatly acknowledged.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview framework of our method. The inputs and outputs of each step are listed in the three levels. Particularly, there are three main components of joint representation learning. Red texts with brackets are anchors, dashed lines denote entity relations, and solid lines are cross-lingual links.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>ction knowledge bases (KB), storing mil- ties and their facts in various lan- ide rich structured knowledge for un- atural language beyond texts. Mean- ant text corpus contains large amount nowledge complementary to existing ore, researchers leverage both types to improve various natural language LP) related tasks, such as relation ex- ton et al., 2013; Lin et al., 2017), and (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>uction al knowledge bases (KB), storing mil- ntities and their facts in various lan- vide rich structured knowledge for un- g natural language beyond texts. Mean- ndant text corpus contains large amount l knowledge complementary to existing refore, researchers leverage both types s to improve various natural language (NLP) related tasks, such as relation ex- eston et al., 2013; Lin et al., 2017), and ng (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>att att att … … w basketball w player … …</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>from the page articles of cross- lingual linked entities e e Kobe and e z Kobe we extract those sentences including another cross-lingual linked entities e e Joe and e z Joe as comparable sen- tences S e−z = {&lt; s e k , s z k &gt;} The intuition is that we consider each sentence in a Wikipedia article has a pseudo mention of the page entity (talking something about the en- tity) (Yamada et al 2017) Thus if a sentence also mentions another entity it implicitly expresses their relation Therefore we make a similar as- sumption as in relation extraction: If two enti- ties participate in a relation and both of them</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head></head><label></label><figDesc>from the page articles of cross- lingual linked entities e e Kobe and e z Kobe we extract those sentences including another cross-lingual linked entities e e Joe and e z Joe as comparable sen- tences S e−z = {&lt; s e k , s z k &gt;} The intuition is that we consider each sentence in a Wikipedia article has a pseudo mention of the page entity (talking something about the en- tity) (Yamada et al 2017) Thus if a sentence also mentions another entity it implicitly expresses their relation Therefore we make a similar as- sumption as in relation extraction: If two enti- ties participate in a relation and both of them</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head></head><label></label><figDesc>from the page articles of cross lingual linked entities e e Kobe and e z Kobe we extrac those sentences including another cross-lingua linked entities e e Joe and e z Joe as comparable sen tences S e−z = {&lt; s e k , s z k &gt;} The intuition is that we consider each sentence in a Wikipedia article has a pseudo mention o the page entity (talking something about the en tity) (Yamada et al 2017) Thus if a sentence also mentions another entity it implicitly expresse their relation Therefore we make a similar as sumption as in relation extraction: If two enti ties participate in a relation and both of them</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Translation words (Chinese)</head><label></label><figDesc>篮球 (+), 篮球队 (basketball team), 湖人 (lakers), 男子 篮球 (men's basketball), 湖人队 (the lakers), 国王队 (the Kings), 美式足球 (American football), 中锋 (center) Nearest entities (Chinese) NBA, 篮球 (Basketball) , 控球后卫 (Point guard), NBA 选秀 (draft), 香港男子甲一组男子篮球联赛 (Hong Kong men's top basketball league), 橄榄球 (American football), 东方篮球队 (Eastern basketball team) Nearest words nba, wnba, player, twyman, professional, pick, 76ers Nearest entities Professional sports, Varsity letter, Sports agent, All- America, Final four, All-star, College basketball</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>NDCG</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>en−zh . As shown in</head><label></label><figDesc></figDesc><table>by the distant supervision technique 
in relation extraction, we assume that sentence 
s en 
k in Wikipedia articles of entity e en 
i explicitly 
or implicitly describes e en 
i (Yamada et al., 2017), 
and that s en 
k shall express a relation between e en 

i 

and e en 
j if another entity e en 
j is in s en 
k . Mean-
while, we find a comparable sentence s zh 
k ′ in an-
other language which satisfies s zh 
k ′ containing e zh 

j ′ in Wikipedia articles of Chinese entity e zh 
i ′ , where 
⟨e en 
i , e zh 
i ′ ⟩, ⟨e en 
j , e zh 
j ′ ⟩ ∈ R </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>En 

e NBA 

e Allstar 

w basketball 

w American 

e F oust 

e F oust 

w basketball 
w American 

w player 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Cross-lingual Data Statistics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cross-lingual nearest words and entities of En-
glish word basketball. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 4 : Word Translation.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 5 : Entity Relatedness.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head>Table 6 : Tri-lingual Entity Linking.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://en.wikipedia.org/wiki/Help: Interlanguage_links</note>

			<note place="foot" n="2"> We choose English and Chinese as example languages because they are top-ranked according to total number of speakers, the full list can be found in</note>

			<note place="foot" n="3"> For the cross-lingual linked entities sharing the same strings (e.g., NBA and NBA (zh)), which is an infrequent situation between languages, we use separated representations to keep training objective consistent and avoid confusion.</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>

			<note place="foot" n="1"> Some cross-lingual pioneering work observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages (Mikolov et al., 2013; Zhang et al., 2017).</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Massively multilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards crosslingual distributed representations without parallel text trained with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Valerio Miceli</forename><surname>Barone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In Rep4NLP@ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nasari: a novel approach to a semantically-aware representation of items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A distribution-based model to learn bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural collective entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bridge text and knowledge by learning multi-prototype entity mention embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Name list only? target entity disambiguation in short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuanhu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Sarath Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Query lattice for translation retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Joint representation learning of text and knowledge for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An endto-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Shizhu He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>TOIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overview of tackbp2016 tri-lingual edl and its impact on end-to-end cold-start kbp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Hoa Trang Dang, and Sydney Informatics Hub</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Overview of tac-kbp2015 tri-lingual entity discovery and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning bilingual word representations by marginalizing alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural relation extraction with multi-lingual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS@HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
