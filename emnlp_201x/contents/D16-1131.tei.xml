<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Antecedent Selection for Sluicing: Structure and Content</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Anand</surname></persName>
							<email>panand@ucsc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">IT Management Copenhagen Business School</orgName>
								<orgName type="institution">Linguistics UC Santa Cruz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IT Management Copenhagen Business School</orgName>
								<orgName type="institution">Linguistics UC Santa Cruz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Antecedent Selection for Sluicing: Structure and Content</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1234" to="1243"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sluicing is an elliptical process where the majority of a question can go unpronounced as long as there is a salient antecedent in previous discourse. This paper considers the task of antecedent selection: finding the correct antecedent for a given case of sluicing. We argue that both syntactic and discourse relationships are important in antecedent selection , and we construct linguistically sophisticated features that describe the relevant relationships. We also define features that describe the relation of the content of the antecedent and the sluice type. We develop a linear model which achieves accuracy of 72.4%, a substantial improvement over a strong manually constructed baseline. Feature analysis confirms that both syntactic and discourse features are important in antecedent selection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ellipsis involves sentences with missing subparts, where those subparts must be interpretatively filled in by the hearer. How this is possible has been a ma- jor topic in linguistic theory for decades <ref type="bibr" target="#b28">(Sag, 1976;</ref><ref type="bibr" target="#b7">Chung et al., 1995;</ref><ref type="bibr" target="#b22">Merchant, 2001)</ref>. One widely studied example is verb phrase ellipsis (VPE), ex- emplified by (1).</p><p>(1)</p><p>Harry traveled to southern Denmark to study botany . Tom did too .</p><p>In the second sentence (Tom did too) the verb phrase is entirely missing, yet the hearer effortlessly 're- solves' (understands) its content to be traveled to southern Denmark to study botany.</p><p>Another widely studied case of ellipsis is sluicing, in which the majority of a question is unpronounced, as in <ref type="formula">(2)</ref>. <ref type="formula">(2)</ref> Harry traveled to southern Denmark to study botany . I want to know why .</p><p>Here the content of the question, introduced by the WH-phrase why, is missing, yet it is understood by the hearer to be why did Harry travel to southern Denmark to study botany?. In both of these cases, ellipsis resolution is made possible by the presence of an antecedent, material in prior discourse that, in- formally speaking, is equivalent to what is missing. Ellipsis poses an important challenge for many applications in language technology, as various forms of ellipsis are known to be frequent in a va- riety of languages and text types. This is perhaps most evident in the case of question-answering sys- tems, since elliptical questions and elliptical answers are both very common in discourse. A computa- tional system that can effectively deal with ellipsis involves three subtasks <ref type="bibr" target="#b23">(Nielsen, 2005)</ref>: ellipsis de- tection, in which a case of ellipsis is identified, an- tecedent selection, in which the antecedent for a case of ellipsis is found, and ellipsis resolution, where the content of the ellipsis is filled in with reference to the antecedent and the context of the ellipsis. Here, we focus on antecedent selection for sluicing. In addressing this problem of antecedent selection, we make use of a newly available annotated corpus of sluice occurrences <ref type="bibr" target="#b0">(Anand and McCloskey, 2015)</ref>. This corpus consists of 4100 automatically parsed and annotated examples from the New York Times subset of the Gigaword Corpus, of which 2185 are publicly available.</p><p>Sluicing antecedent selection might appear simple -after all, it typically involves a sentential expres- sion in the nearby context. However, analysis of the annotated corpus data reveals surprising ambiguity in the identification of the antecedent for sluicing.</p><p>In what follows, we describe a series of algo- rithms and models for antecedent selection in sluic- ing. Following section 2 on background, we de- scribe our dataset in section 3. Then in section 4, we describe the structural factors that we have iden- tified as relevant for antecedent selection. In sec- tion 5, we look at ways in which the content of the sluice and the content of the antecedent tend to be related to each other: we address lexical overlap, as well as the probabilistic relation of head verbs to WH-phrase types, and the relation of correlate ex- pressions to sluice types. In section 6 we present two manually constructed baseline classifiers, and then we describe an approach to automatically tun- ing weights for the complete set of features. In sec- tion 7 we present the results of these algorithms and models, including results involving various subsets of features, to better understand their contributions to the overall results. Finally in section 8 we discuss the results in light of plans for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sluicing and ellipsis</head><p>Sluicing is formally defined in theoretical linguis- tics as ellipsis of a question, leaving only a WH- phrase remnant. While VPE is licensed only by a small series of auxiliaries <ref type="bibr">(e.g., modals, do, see Lobeck (1995)</ref>), sluicing can occur wherever ques- tions can, both in unembedded 'root' environments (e.g., Why?) or governed by the range of expres- sions that embed questions, like know in (2). Sluic- ing is argued to be possible principally in contexts where there is uncertainty or vagueness about an is- sue <ref type="bibr" target="#b13">(Ginzburg and Sag, 2000</ref>). In some cases, this manifests as a correlate, an overt indefinite expres- sion whose value is not further specified, like one of the candidates in (3). But in many others, like that in (2) or (4), there is no correlate, and the uncertainty is implicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3)</head><p>They 've made an offer to [ cor one of the can- didates ] , but I 'm not sure which one (4) They were firing , but at what was unclear</p><p>The existence of correlate-sluices suggests an obvi- ous potential feature type for antecedent detection. However, the annotated sluices in <ref type="bibr" target="#b0">(Anand and McCloskey, 2015</ref>) have correlates only 22% of the time, making this process considerably harder. We return to the question of correlates in section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>The first large-scale study of ellipsis is due to <ref type="bibr" target="#b15">Hardt (1997)</ref>, which addresses VPE. Examining 644 cases of VPE in the Penn Treebank, Hardt presents a manually constructed algorithm for locating the an- tecedent for VPE, and reports accuracy of 75% to 94.8%, depending on whether the metric used re- quires exact match or more liberal overlap or con- tainment. Several preference factors for choosing VPE antecedents are identified (Recency, Clausal Relations, Parallelism, and Quotation). One of the central components of the analysis is the identifi- cation of structural constraints which rule out an- tecedents that improperly contain the ellipsis site, an issue we also address here for sluicing. Draw- ing on 1510 instances of VPE in both the British National Corpus (BNC) and the Penn Treebank, <ref type="bibr" target="#b23">Nielsen (2005)</ref> shows that a maxent classifier using refinements of Hardt's features can achieve roughly similar results to Hardt's, but that additional lexical features do not help appreciably. Nielsen chooses to optimize for Hardt's Head Overlap metric, which assigns success to any candi- date containing/contained in the correct antecedent. There are thus many "correct" antecedents for a given instance of VPE, which mitigates the class im- balance problem. However, the approach does not provide a way to discriminate between these con- taining candidates, an important step in the eventual goal of resolving the ellipsis.</p><p>There is no similar work on antecedent selec- tion for sluicing, though there have been small- scale corpora gathered for sluices <ref type="bibr" target="#b24">(Nykiel, 2010;</ref><ref type="bibr" target="#b3">Beecher, 2008)</ref>. In addition, <ref type="bibr">Fernandez et al. (2005)</ref> build rule-based and memory-based classifiers for the pragmatic import of root (unembedded) sluices in the BNC, based on the typology of <ref type="bibr" target="#b13">Ginzburg and Sag (2000)</ref>. Using features for the type of WH-phrase, markers of mood (declarative/interrogative) and polarity (positive/negative) as well as the pres- ence of correlate-like material (e.g., quantifiers, defi- nites, etc.), they can diagnose the purpose of a sluice in a dataset of 300 root sluices with 79% average F-score, a 5% improvement over the MLE. <ref type="bibr">Fernandez et al. (2007)</ref> address the problem of identify- ing sluices and other non-sentential utterances. We don't address that problem in the current work. Fur- thermore, <ref type="bibr">Fernandez et al. (2007)</ref> and <ref type="bibr">Fernandez et al. (2008)</ref> address the general problem of non- sentential utterances or fragments in dialogue, in- cluding sluices. Sluicing in dialogue differs from sluicing in written text in various ways: there is a high proportion of root sluices, and antecedent se- lection is likely mitigated by the length of utterances and the order of conversation. As we discuss, many of our newswire sluices evince difficult patterns of containment inside the antecedent (particularly what we call interpolated and cataphoric sluices), and it does not appear from inspection that root sluices ever participate in such processes.</p><p>Looking more generally, there is an obvious po- tential connection between antecedent selection for ellipsis and the problem of coreference resolution (see <ref type="bibr" target="#b16">Hardt (1999)</ref> for an explicit theoretical link be- tween the two). However, entity coreference reso- lution is a problem with two major differences from ellipsis antecedent detection: a) the antecedent and anaphor often share a variety of syntactic, semantic, and morphological characteristics that can be featu- rally exploited; b) entity expressions in a text are of- ten densely coreferent, which can help provide prox- ies for discourse salience of an entity.</p><p>In contrast, abstract anaphora, particularly dis- course anaphora (this/that anaphora to something sentential), may offer a more parallel case to ours. Here, <ref type="bibr" target="#b19">Kolhatkar et al. (2013)</ref> use a combination of syntactic type, syntactic/word context, length, and lexical features to identify the antecedents of anaphoric shell nouns (this fact) with precision from 0.35-0.72. Because of the sparsity of these cases, <ref type="bibr">Kolhatkar et al. use Denis and Baldridge's (2008)</ref> candidate ranking model (versus a standard mention-pair model ( <ref type="bibr" target="#b29">Soon et al., 2001)</ref>), in which all potential candidates for an anaphor receive a rela- tive rank in the overall candidate pool. In this paper, we will pursue a hillclimbing approach to antecedent selection, inspired by the candidate ranking scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Annotated Dataset</head><p>Our dataset, described in <ref type="bibr" target="#b0">Anand and McCloskey (2015)</ref>, consists of 4100 sluicing examples from the New York Times subset of the Gigaword Corpus, 2nd edition. This dataset is the first systematic, ex- haustive corpus of sluicing. 1 Each example is an- notated with four main tags, given in terms of token sequence offsets: the sluice remnant, the antecedent, and then inside the antecedent the main predicate and the correlate, if any. The annotations also pro- vide a free-text resolution. Of the 4100 annotated, 2185 sluices have been made publicly available; we use that smaller dataset here. We make use of the annotation of the antecedent and remnant tags. See <ref type="bibr" target="#b0">Anand and McCloskey (2015)</ref> for additional infor- mation on the dataset and the annotation scheme. For the feature extraction in section 4, we rely on the the token, parsetree, and dependency parse in- formation in Annotated Gigaword (extracted from Stanford CoreNLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Defining the Correct Antecedent</head><p>Because of disagreements with the automatic parses of their data, <ref type="bibr" target="#b0">Anand and McCloskey (2015)</ref> had annotators tag token sequences, not parsetree con- stituents. As a result, 10% of the annotations are not sentence-level (i.e., S, SBAR, SBARQ) constituents, such as the VP antecedent in (5), and 15% are not constituents at all, such as the case of <ref type="formula">(6)</ref>, where the parse lacks an S node excluding the initial tempo- ral clause. We describe two different ways to define what will count as the correct antecedent in building and assessing our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Constituent-Based Accuracy</head><p>Linguists generally agree that the antecedent for sluicing is a sentential constituent (see <ref type="bibr" target="#b22">Merchant (2001)</ref> and references therein). Thus, it is straight- forward to define the antecedent as the minimal sentence-level constituent containing the token se- quence marked as the antecedent. Then we define CONACCURACY as the percentage of cases in which the system selects the correct antecedent, as defined here.</p><p>While it is linguistically appealing to uniformly define candidates as sentential constituents, the an- notator choices are sometimes not parsed that way, as in the following examples: <ref type="formula">(5)</ref> " I do n't know how , " said Mrs. Kitayeva , " but [ S we want [ V P to bring Lydia home ] , in any condition ] . "</p><formula xml:id="formula_0">(6) [ S [ S BAR When</formula><p>Brown , an all-America tight end , was selected in the first round in 1992 ] he was one of the highest rated play- ers on the Giants ' draft board ]</p><p>In such cases, there is a risk that we will not accu- rately assess the performance of our systems, since the system choice and annotator choice will only partially overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Token-Based Precision and Recall</head><p>Here we define a metric which calculates the pre- cision and recall of individual token occurrences, following Bos and Spenader (2011) (see also <ref type="bibr" target="#b18">Kolhatkar and Hirst (2012)</ref>). This will accurately re- flect the discrepancy in examples like (5) -accord- ing to ConAccuracy, a system choice of we want to bring Lydia home in any condition is simply con- sidered correct, as it is the smallest sentential con- situent containing the annotator choice. According to the Token-Based metric, we see that the system achieves recall of 1; however, since the system in- cludes six extraneous tokens, precision is .4. We de- fine TOKF as the harmonic mean of Token-Based Precision and Recall; for (5), TokF is .57.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Development and Test Data</head><p>The dataset consists of 2185 sluices extracted from the New York Times between July 1994 and De- cember 2000. For feature development, we seg- mented the data into a development set (DS) of the 453 sluices from July 1994 to December 1995. The experiments in section 6 were carried out on a test set (TS) of the 1732 sluices in the remainder of the dataset, January 1996 to December 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structure</head><p>Under our assumptions, the candidate antecedent set for a given sluice is the set of all sentence- level parsetree constituents within a n-sentence ra- dius around the sluice sentence (based on DS, we set n = 2). Because sentence-level constituents em- bed, in DS there are on average 6.4 candidate an- tecedents per sluice. However, because ellipsis res- olution involves identification of an antecedent, we assume that it, like anaphora resolution, should be sensitive to the overall salience of the antecedent. This means that there should be, in principle, proxies for salience that we can exploit to diagnose the plau- sibility of a candidate for sluicing in general. We consider four principle kinds of proxies: measures of candidate-sluice distance, measures of candidate- sluice containment, measures of candidate 'main point', and candidate-sluice discourse relation mark- ers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Distance</head><p>Within DS, 63% of antecedents are within the same sentence as the sluice site, and 33% are in the im- mediately preceding sentence. In terms of candi- dates, the antecedent is on average the 5th candidate from the end of the n-sentence window. The pos- itive integer-valued feature DISTANCE tracks these notions of recency, where DISTANCE is 1 if the can- didate is the candidate immediately preceding or fol- lowing the sluice site (DISTANCE is defined to be 0 only for infinitival Ss like S0 in (7) below). The fea- ture FOLLOWS marks whether a candidate follows the sluice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Containment</head><p>As two-thirds of the antecedents are in the same sentence as the sluice, we need measures to distin- guish the candidates internal to the sentence con- taining the sluice. In general, we want to exclude any candidate that 'contains' (i.e., dominates) the sluice, such as S0 and S-1 in (7). One might have thought that we want to always exclude the entire sentence (here, S-4) as well, but there are several cases where the smallest sentence-level constituent containing the annotated antecedent dominates the sluice, including: parenthetical sluices inside the an- tecedent (8), sluices in subordinating clauses (9), or sluice VPs coordinated with the antecedent VP (10). We thus need features to mark when such candidates are 'non-containers'. Conceptually, what renders S-3 in (9), S-2 in (8), and S-1 in (10) non-containers is that in all three cases the sluice is semantically dissociable from the rest of the sentence. We provide three features to mark this. First, the boolean feature SLUICEINPAR- ENTHETICAL marks when the sluice is dominated by a parenthetical (a PRN node in the parse or an (al)though SBAR delimited by punctuation). Sec- ond, SLUICEINCOORDVP marks the configuration exemplified (10). We also compute a less structure-specific mea- sure of whether the candidate is meaningful once the sluice (and material dependent on it) is removed. This means determining, for example, that S-4 in <ref type="formula">(7)</ref> is meaningful once to explain why . is removed but S-1 is not. But the latter result follows from the fact that the main predicate of S-1, need takes the sluice govering verb explain as an argument, and hence removing that argument renders it semanti- cally incomplete. We operationalize this in terms of complement dependency relations. We first lo- cate the largest subgraph containing the sluice in a chain of ccomp and xcomp relations. This gives us gov max , the highest such governor (i.e., explain) in <ref type="figure">Fig. 1</ref>. The subgraph dependent on gov max is then removed, as indicated by the grayed boxes in <ref type="figure">Fig 1.</ref> If the resulting subgraph contains a verbal governor, the candidate is meaningful and CONTAINSSLUICE is false. By this logic, S-4 in (7) is meaningful be- cause it contains concluded, but S-1 is not, because there is no verbal material remaining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discourse Structure</head><p>It has often been suggested <ref type="bibr" target="#b2">(Asher, 1993;</ref><ref type="bibr" target="#b15">Hardt, 1997;</ref><ref type="bibr" target="#b14">Hardt and Romero, 2004</ref>) that the antecedent selection process is very closely tied to discourse re- lations, in the sense that there is a strong preference or even a requirement for a discourse relation be- tween the antecedent and ellipsis.</p><p>Here we define several features that indicate either that a discourse relation is present or is not present.</p><p>We begin with features indicating that a dis- course relation is not present: the theoretical lin- guistics literature on sluicing has noted that an- tecedents not in the 'main point' of an assertion (e.g., ones in appositives <ref type="bibr" target="#b1">(AnderBois, 2014</ref>) or relative clauses <ref type="bibr" target="#b6">(Cantor, 2013)</ref>) are very poor antecedents for sluices, presumably because their content is not very salient. The boolean features CANDINPAREN- THETICAL (determined as for the sluice above) and CANDINRELCLAUSE mark these patterns. <ref type="bibr">2</ref> We also define features that would tend to indicate the presence of a discourse relation. These have to do with antecedents that occur after the sluice. Al- though antecedents overwhelmingly occur prior to sluices, we observe one prominent cataphoric pat- tern in DS, where the sentence containing the sluice is coordinated with a contrastive discourse relation; this is exemplified in (11). <ref type="formula">(11)</ref> " I do n't know why , but I like Jimmy Carter . "</p><p>Three features are designed to capture this pattern: COORDWITHSLUICE indicates whether the sluice and candidate are connected by a coordination de- pendency, AFTERINITIALSLUICE marks the con- junctive condition where the candidate follows a sluice initial in its sentence, and IMMEDAFTERINI- TIALSLUICE marks a candidate that is the closest following candidate to an initial sluice.   <ref type="figure">Figure 1</ref>: Sluice containment for S-4 and S-1 in (7). Starting at the governor of the sluice, explain, find govmax need and delete its transitive dependents. The candidate does not contain the sluice if the remaining graph contains verbal governors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Content</head><p>In addition to the structural features above, we also compute several features relating the content of the sluice site and the antecedent. The intuition be- hind these relational features is the following: each sluice type (why, who, how much, etc.) represents a certain type of question, and each candidate rep- resents a particular type of predication. For a given a sluice type, some predications might fit more nat- urally than others. More generally, it is a common view that an elliptical expression and its antecedent contain matching "parallel elements". <ref type="bibr">3</ref> Below we describe three approaches to this: one simply looks for lexical overlap -words that occur both in the sluice expression and in the candidate. The second involves a more general notion of how a predication fits with a sluice type. To capture this, we gather co-occurrence counts of main verb and sluice types. The third approach compares potential correlates in candidates with the type of sluice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overlap</head><p>One potential candidate for overlap information is the presence of a correlate in the antecedent. How- ever, 75% of of sluices involve WH-phrases that typ- ically involve no correlate (e.g., how, when, why). The pertinent exception to this are extent sluices ( ones where the remnant is how (much|many|JJ)), which have been argued to heavily favor a correlate <ref type="bibr" target="#b22">(Merchant, 2001</ref>), such as (12) below (though see (13) for a counterexample). We thus compute the number of tokens of OVER- LAP between the content terms in the WH-phrase sluice (non-WH, non prepositional) and the entire antecedent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Wh-Predicate</head><p>Even for correlate-less sluices, the WH-phrase must semantically cohere with the main predicate of the antecedent. Thus, in (13), S-3 is a more likely an- tecedent than S-2 because increase is more likely to take an implicit extent than predict. Although we could have consulted a lexically rich resource (e.g, VerbNet, FrameNet), our hope was that this general approach could carry over to less argument-specific combinations such as how with complete and raise in <ref type="bibr">(14)</ref>. Our assumption is that some main predicates are more likely than others for a given sluice type, and we wish to gather data that reveals these probabil- ities. This is somewhat similar to the approach of <ref type="bibr" target="#b17">Hindle and Rooth (1993)</ref>, who gather probabilities that reflect the association of verbal and nominal heads with prepositions to disambiguiate preposi- tional phrase attachment.</p><p>One way to collect these would be to use our sluicing data, which consists of a total of 2185 an- notated examples. However, the probabilities of in- terest are not about sluicing per se. Rather, they are about how well a given predication fits with a given type of question. Thus instead of using our com- paratively small set of annotated sluicing examples, we used overt WH-constructions in Gigaword to observe cooccurrences between question types and main predicates. To find overt WH-constructions, we extracted all instances where a WH-phrase is: a) a dependent (to exclude cases like Who?) and b) not at the right edge of a VP (to exclude sluices like know who, per <ref type="bibr" target="#b0">Anand and McCloskey (2015)</ref>). To further ensure that we were not overlapping with our dataset, we did this only for the non=NYT sub- sets of Gigaword (i.e., AFP, APW, CNA, and LTW). This procedure generated 687,000 WH-phrase in- stances, and 79,753 WH-phrase-governor bigram types. From these bigrams, we calculated WH- PREDICATE, the normalized pmi of WH-phrase type and governor lemma in Annotated Gigaword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Correlate Overlap</head><p>Twenty-two percent of our data has correlates, and these correlates should be discriminative for partic- ular sluice types. For example, temporal (when) sluices have timespan correlates (e.g., tomorrow, later), while entity (who/what) sluices have individ- uals as correlates (e.g., someone, a book). We ex- tracted four potential integer-valued correlate fea- tures from each candidate: LOCATIVECORR is the number of primarily locative prepositions (those with a locative MLE in The Preposition Project ( <ref type="bibr" target="#b20">Litowski and Hargraves, 2005)</ref>). ENTITYCORR is the number of nominals in the candidate that are in- definite (bare nominals or ones with a determiner re- lation to a, an and weak quantifiers (some, many, much, few, several).TEMPORALCORR is the num- ber of lexical patterns in the candidate for TIMEX3 annotations in Timebank 1. <ref type="bibr">2 (Pustejovesky et al., 2016)</ref>. WHICHCORR is the pattern for entities plus or.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Algorithms</head><p>Mention-pair coreference models reduce corefer- ence resolution to two steps: a local binary clas- sification, and a global resolution of coreference chains. We may see antecedent selection as a sim- ilar two-stage process: classification on the proba- bility a given candidate is an antecedent, and then selection of the most likely candidate for a given sluice. As Denis and Baldridge (2008) note, one limitation of this approach is that the overall rank of the candidates is never directly learned. They instead propose to learn the rank of a candidate c for antecedent a, modeled as the log-linear score of a candidate across a set of coreference models m, (exp j w j m j (c, a)), normalized by the sum of candidate scores. We apply the same approach to our problem, viewing each feature in <ref type="table" target="#tab_2">Table 1</ref> as a model, and estimating weights for the features by hill-climbing. We begin by defining constructed baselines which are implemented by manually as- signing weights. We then consider the results of a maxent classifier over the features. Finally, we de- termine the weights directly by hill-climbing with random restarts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Manual Baselines</head><p>Random simply selects candidates at random. Clst chooses the closest candidate that starts before the sluice. This is done by assigning a weight of -1 to DISTANCE and -10 to FOLLOWING (to exclude the following candidate), and 0 to all other fea- tures. ClstBef chooses the closest candidate that en- tirely precedes the sluice (i.e., starts before and does not contain the sluice site). To construct ClstBef, we change the weight of CONTAINSSLUICE to -10, which means that candidates containing the sluice will never be chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A maxent model</head><p>We trained a maxent classifier on the features in <ref type="table" target="#tab_2">Ta- ble 1</ref> for the binary antecedent-not antecedent task. With 10-fold cross-validation on the test set, the maxent model achieved an average accuracy on the binary antecedent task of 87.1 and an F-score of 53.8 (P=63.9, R=46.5). We then constructed an an- tecedent selector that chose the candidate with the highest classifier score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hill-Climbing</head><p>We define a procedure to hill-climb over weights in order to maximize ConAccuracy over the entire training set (maximizing TokF yielded similar re- sults, and is not reported here). Weights are initial- ized with random values in the interval <ref type="bibr">[-10,10]</ref>. At iteration i, the current weight vector is compared to alternatives differing from it by the current step size on one weight, and the best new vector is selected. For the results reported here, we performed 13 ran- dom restarts and exponential step size 10 * i .5 (values that maximized performance on the DS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We performed 10-fold cross-validation over TS on the hill-climbed and maxent models above, produc- ing average ConAccuracy and TokF as shown in Ta- ble 2, which also gives results of the three base- lines on the entire dataset. The hill-climbed ap- proach with all features substantially outperformed the baselines, achieving a ConAccuracy of 72.4%.</p><p>We investigated the performance of our hill- climbing procedure with ablation of several feature subsets. We ablated features by group, as in <ref type="table" target="#tab_2">Table 1</ref>. <ref type="table" target="#tab_4">Table 2</ref> shows the results for using four groups and only one group, as well as the top two three group and two group combinations.</p><p>Features fall in three tiers. Distance features are the most predictive: all the top systems use them, and they alone perform reasonably well (like Clst).  The Content and Correlate features concern re- lations between the type of sluice and the content of the antecedent; since other features do not cap- ture this, it is puzzling that these provide no fur- ther improvement. To better understand why this is, we investigated the performance of our feature sets by sluice type. For the top performing systems, we found that antecedent selection for sluices over extents (e.g, how much, how tall) performed 11% better than average and those over reasons (why) and manner (how) performed 13% worse than aver- age; no other WH-phrase types differed significantly from average. Importantly, this finding was consis- tent even for the systems without Content or Cor- relate features, which we extracted in large part to help highlight possible correlate material for extent sluices as well as entity (who/what) and temporal (when) sluices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A:Tr F:Tr A:Tes F:Tes</head><p>We also examined systems knocking out our best performing features, Distance, Containment, and Discourse Structure. When Distance features were omitted, we saw a bimodal distribution: reason and manner sluice antecedent selection was 31% better than expected (based on the full system differences discussed above), and the other sluices performed 22% worse. When Containment features were omit- ted, reason sluices performed 10% better than ex- pected, while extent ones were 10% worse. Finally, when Discourse Structure features were removed, entity and temporal sluices had half the error rate we would expect. While it is hard to provide a clear takeaway from these differences, they do point to the relative difficulty in locating sluice antecedents based on WH-phrase type, and they also suggest that different sluice types present quite different chal- lenges. This suggests that one promising line might be to learn different featural weights for each sluice type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have addressed the problem of sluicing an- tecedent selection by defining linguistically sophis- ticated features describing the structure and content of candidates. We described a hill-climbed model which achieves accuracy of 72.4%, a substantial im- provement over a strong manually constructed base- line. We have shown that both syntactic and dis- course relationships are important in antecedent se- lection. In future work, we hope to improve the per- formance of several of our features. Notable among these are the discourse structural proxies we found to make a contribution to the model. These features constitute a quite limited view of discourse struc- ture, and we suspect that a better representation of discourse structure might well lead to further im- provements. One potential path would be to lever- age data where discourse relations are explicitly an- notated, such as that in the Penn Discourse Treebank ( <ref type="bibr" target="#b25">Prasad et al., 2008)</ref>. In addition, although our Con- tent and Correlate features were not useful alongside the others, we hope that more refined versions of those could provide some assistance. We also noted that our performance was impacted by WH-types, and therefore it might be helpful to learn different featural weights per type.</p><p>In closing, we would like to return to the larger question of effectively handling ellipsis. The solu- tion to antecedent selection that we have presented here provides a starting point for addressing the problem of resolution, in which the content of the sluice is filled in. However, even if the correct an- tecedent is selected, the missing content is not al- ways an exact copy of the antecedent -often sub- stantial modifications will be required -and an ef- fective resolution system will have to negotiate such mismatches. As it turns out, many incorrect an- tecedents differ from the correct antecedent in ways highly reminiscent of these mismatches. Thus, some of the errors of our selection algorithm may be most naturally addressed by the resolution system, and it may be that the relative priority of the specific chal- lenges we identified here will become clearer as we address the next step down in the overall pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>S−4 [ S−3 I have concluded that [ S−2 I can not support the nomination ] , and [ S−1 I need [ S0 to explain why ] ]. ] (8) [ S−2 A major part of the increase in coverage , [ S−1 though Mitchell 's aides could not say just how much , ] would come from a pro- vision providing insurance for children and pregnant women . ] (9) [ S−3 Weltlich still plans [ S−2 to go , [ S−1 although he does n't know where ] ] ] (10) [ S−2 State regulators have ordered 20th Century Industries Inc. [ S−1 to begin pay- ing $ 119 million in Proposition 103 rebates or explain why not by Nov. 14 .]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>I</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>12) The 49ers are [ corr very good ] . It 's hard to know how good because the Cowboys were the only team in the league who could test them .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Deliveries would increase as a result of the acquisition ] , [ S−2 he predicted ] , but [ S−1 he would not say by how much ] (14) [ S−4 [ S−3 Once the city and team complete a contract ] , the Firebirds will begin to raise $ 9 million ] , [ S−2 team president Yount said ] , [ S−1 but he would not say how ] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Summary of features used in experiments.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average (Con)A(ccuracy) and (Tok)F(-Score) for 

Tr(ain) and Tes(t) splits on 10-fold cross-validation of data. 

Feature groups: Distance, Containment, Discourse Structure, 

coNtent, coRrelate. (Red marks results not significantly differ-

ent (via paired t-test) from HC-DCSNR.) 

Containment and then Discourse Structure features 
are the next most helpful. The full system has a 
ConAccuracy of 72.4 on the TS, not reliably dif-
ferent from several systems without Content and/or 
Correlate features. At the same time, the scores for 
these feature types on their own show that they are 
predictive of the antecedent: The Correlate feature 
R has a score of 22.2, which is a rather modest, but 
statistically significant, improvement over Random. 
The Content feature N improves quite substantially, 
up to 30.7. This suggests that there is some redun-
dancy with the other features, so that the contribu-
tions of Content and Correlate are not observed in 
combination with them. (HC-N and HC-R's lower 
than Random TokF is a result of precision: Random 
more often selects very small candidates inside the 
correct antecedent, leading to a higher precision.) 
</table></figure>

			<note place="foot" n="1"> 4100 sluices works out to roughly 0.14% of WH-phrases in the NYT portion of Gigaword. However, note that this includes all uses of WH-phrases (e.g., clefts and relative clauses), whereas sluicing is only possible for WH-questions. It&apos;s not clear how many questions there are in the dataset (distinguishing questions and other WH-phrases is non-trivial).</note>

			<note place="foot" n="2"> This feature might be seen as an analog to the apposition features used in nominal coreference resolution (Bengtson and Roth, 2008), but there it is used to link appositives, whereas here it is to exclude candidates.</note>

			<note place="foot" n="3"> This term is from Dalrymple et al. (1991); a similar general view about parallelism in ellipsis arises in many different theories, such as Prüst et al. (1994) and Asher (1993).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the work of Jim Mc-Closkey in helping to create the dataset investi-gated here, as well as the principal annotators on the project. We thank Jordan Boyd-Graber, Ellen Riloff, and three incisive reviewers for helpful com-ments. This research has been sponsored by NSF grant number 1451819.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Annotating the implicit content of sluices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Mccloskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 9th Linguistic Annotation Workshop held in conjuncion with NAACL 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">178</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The semantics of sluicing: Beyond truth conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Anderbois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="887" to="926" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reference to Abstract Objects in English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pramatic inference in the interpretation of sluiced Prepositional Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Beecher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">San Diego Linguistic Papers</title>
		<meeting><address><addrLine>UCSD, La Jolla, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
		<respStmt>
			<orgName>Department of Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An annotated corpus for the analysis of vp ellipsis. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Spenader</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="463" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ungrammatical double-island sluicing as a diagnostic of left-branch positioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Cantor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sluicing and logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ladusaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccloskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Semantics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ellipsis and higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Dalrymple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Philosophy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1991-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Specialized models and ranking for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic bare sluice disambiguation in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
		<ptr target="http://www.dcs.kcl.ac.uk/staff/lappin/recent_papers_index.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IWCS-6 (Sixth International Workshop on Computational Semantics)</title>
		<meeting>the IWCS-6 (Sixth International Workshop on Computational Semantics)<address><addrLine>Tilburg, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-01" />
			<biblScope unit="page" from="115" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classifying non-sentential utterances in dialogue: A machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="427" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>and Shalom Lappin</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shards: Fragment resolution in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing Meaning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="125" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Interrogative Investigations: The Form, Meaning and Use of English Interrogatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>CSLI Publications</publisher>
			<pubPlace>Stanford, Calif</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ellipsis and the structure of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maribel</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="375" to="414" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical approach to vp ellipsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Lingusitics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="541" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic interpretation of verb phrase ellipsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics &amp; Philosophy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="221" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structural ambiguity and lexical relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Resolving &quot;this-issue&quot; anaphora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varada</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1255" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpreting anaphoric shell nouns using antecedents of ctaphoric shell nouns as training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varada</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Zinmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The preposition project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Litowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-SIGSEM Workshop on &quot;The Linguistic Dimension of Prepositions and Their Use in Computational Linguistic Formalisms and Applications</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ellipsis: Functional heads, licensing and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Lobeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The syntax of silence: Sluicing, islands, and identity in ellipsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Merchant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A Corpus-Based Study of Verb Phrase Ellipsis Identification and Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>King&apos;s College London</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Whatever happened to Old English sluicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Nykiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in the History of the English Language V: Variation and Change in English Grammar and Lexicon: Contemporary Approaches</title>
		<editor>Robert A. Cloutier, Anne Marie Hamilton-Brehm, and Jr. William A. Kretzschmar</editor>
		<imprint>
			<publisher>Walter de Gruyter</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="37" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A discourse perspective on verb phrase anaphora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hub</forename><surname>Prüst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Philosophy</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="327" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Sauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Knippen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Setzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
	<note>Timebank 1.2. LDC2006T08</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deletion and Logical Form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>Published 1980 by Garland Publishing</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology.</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="565" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
