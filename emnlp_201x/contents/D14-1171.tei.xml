<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Accurate Misspelling Correction in Large Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><forename type="middle">Popescu</forename><surname>Fondazione</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Trento</settlement>
									<country>Italy, Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Kessler</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Trento</settlement>
									<country>Italy, Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Phuoc</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Trento</settlement>
									<country>Italy, Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Trento</settlement>
									<country>Italy, Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Accurate Misspelling Correction in Large Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1634" to="1642"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There are several NLP systems whose accuracy depends crucially on finding mis-spellings fast. However, the classical approach is based on a quadratic time algorithm with 80% coverage. We present a novel algorithm for misspelling detection, which runs in constant time and improves the coverage to more than 96%. We use this algorithm together with a cross document coreference system in order to find proper name misspellings. The experiments confirmed significant improvement over the state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of finding the misspelled words in a corpus is an important issue for many NLP sys- tems which have to process large collections of text documents, like news or tweets corpora, dig- italized libraries etc. Any accurate systems, such as the ones developed for cross document corefer- ence, text similarity, semantic search or digital hu- manities, should be able to handle the misspellings in corpora. However, the issue is not easy and the required processing time, memory or the de- pendence on external resources grow fast with the size of the analyzed corpus; consequently, most of the existing algorithms are inefficient. In this pa- per, we present a novel algorithm for misspelling detection which overcomes the drawbacks of the previous approaches and we show that this algo- rithm is instrumental in improving the state of the art of a cross document coreference system.</p><p>Many spelling errors in a corpus are acciden- tal and usually just one or two letters in a word are affected, like existnece vs. the dictionary form existence. Such misspellings are rather a unique phenomenon occurring randomly in a text. For an automatic speller which has access to a dictionary, finding and compiling a list of correct candidates for the misspelled words like the one above is not very difficult. However, not all misspellings are in this category. To begin with, proper nouns, espe- cially foreign proper names, are not present in the dictionary and their misspelling may affect more than one or two characters. Moreover, the mis- spelling of proper names may not be random, for example there might be different spellings of the same Chinese or Russian name in English, the in- correct ones occurring with some frequency. Also, especially if the corpus contains documents writ- ten by non native speakers, the number of char- acters varying between the correct and the actual written form may be more than two. In this case, finding and compiling the list of correct candidates is computationally challenging for traditional al- gorithms, as the distance between the source string and the words in the candidates list is high.</p><p>The Levenshtein distance has been used to com- pile a list of correct form candidates for a mis- spelled word. The Levenshtein distance between two strings counts the number of changes needed to transform one string into the other, where a change is one of the basic edit operations: dele- tion, insertion, substitution of a character and the transposition of two characters. The Edit Dis- tance algorithm, (ED) computes the similarity be- tween two strings according to the Levenshtein distance. Most of the random misspellings which are produced by a native speaker are within one or maximum two basic edit operations <ref type="bibr" target="#b5">(Damerau, 1964)</ref>. For this reason the ED algorithm is the most common way to detect and correct the mis- spellings. However, there is a major inconve- nience associated with the use of ED, namely, ED runs in quadratic time considering the length of the strings, O(n 2 ). The computation time for more than a few thousands pairs is up to several tens of seconds, which is impracticably large for most of large scale applications. By comparison, the num- ber of proper names occurring in a medium sized English news corpus is around 200, 000, which means that there are some 200, 000, 000 pairs.</p><p>In order to cope with the need for a lower com- putation time, on the basis of ED, a series of algo- rithms have been developed that run in linear time <ref type="bibr">(Navaro 2001)</ref>. Unfortunately, this improvement is not enough for practical applications which in- volve a large amount of data coming from large corpora. The reason is two-fold: firstly, the linear time is still too slow ( <ref type="bibr" target="#b10">Mihov and Schulz, 2004</ref>) and secondly, the required memory depends both on the strings' length and on the number of differ- ent characters between the source string and the correct word, and may well exceed several GBs. Another solution is to index the corpus using struc- tures like trie trees, or large finite state automata. However, this solution may require large amounts of memory and is inefficient when the number of characters that differ between the source string and the candidate words is more than two characters <ref type="bibr" target="#b3">(Boytsov, 2011</ref>).</p><p>We focus specifically on misspellings for which there is no dictionary containing the correct form and/or for which the Levenshtein distance to the correct word may be higher than two characters. For this purpose, we developed a novel approach to misspelling correction based on a non indexing algorithm, which we call the prime mapping algo- rithm, PM. PM runs in constant time, O(1), with insignificant memory consumption. The running time of the PM algorithm does not depend either on the strings' length or on the number of different characters between the source string and the can- didate word. It requires a static amount of mem- ory, ranging from a few KBs to a maximum of a few MBs, irrespective of the size of the corpus or the number of pairs for which the misspelling rela- tionship is tested. We run a series of experiments using PM on various corpora in English and Ital- ian. The results confirm that PM is practical for large corpora. It successfully finds the candidate words for misspellings even for large Levenshtein distances, being more than 30 times faster than a linear algorithm, and several hundred times faster than ED. The running time difference is due to the fact that PM maps the strings into numbers and performs only one arithmetic operation in order to decide whether the two strings may be in a mis- spelling relationship. Instead of a quadratic num- ber of characters comparisons, PM executes only one arithmetic operation with integers.</p><p>We also report here the results obtained when using PM inside a cross document coreference system for proper nouns. Correcting a proper name misspelling is actually a more complex task than correcting a misspelled common word. Some misspellings may not be random and in order to cope with repetitive misspellings, as the ones re- sulting from the transliteration of foreign names, the PM is combined with a statistical learning al- gorithm which estimates the probability of a cer- tain type of misspelling considering the surround- ing characters in the source string. Unlike with common words, where a misspelling is obvious, in the case of proper names, John vs. Jon for ex- ample, it is unclear whether we are looking at two different names or a misspelling. The string sim- ilarity evidence is combined with contextual evi- dence provided by a CDC system to disambiguate.</p><p>To evaluate the PM algorithm we use publicly available misspelling annotated corpora contain- ing documents created by both native and non- native speakers. The PM within a CDC system for proper names is evaluated using CRIPCO <ref type="bibr" target="#b2">(Bentivogli et al., 2008)</ref>. The experiments confirm that PM is a competitive algorithm and that the CDC system gains in accuracy by using a module of misspelling correction.</p><p>The rest of the paper is organized as follows. In Section 2 we review the relevant literature. In Sec- tion 3 we introduce the PM algorithm and com- pare it against other algorithms. In Section 4 we present the CDC system with misspelling correc- tion for proper names. In Section 5 we present the results obtained on English and Italian corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In a seminal paper <ref type="bibr" target="#b5">(Damerau, 1964)</ref> introduced the ED algorithm. The rationale for this algorithm was the empirical observation that about 80% of the misspelled words produced by native speakers have distance 1 to the correct word. ED cannot be extended to increase the accuracy, because for k = 2, k being the maximal admissible distance to the correct word, the running time is too high. Most of the techniques developed further use ED together with indexing methods and/or parallel processing.</p><p>In <ref type="bibr" target="#b17">(San Segundo et al., 2001</ref>) an M-best can-didate HMM recognizer for 10,000 Spanish city names is built for speech documents. An N-gram language model is incorporated to minimize the search spaces. A 90% recognition rate is reported. The model is not easily generalizable to the situ- ation in which the names are unknown -as it is the case with the personal proper names in a large corpus. The N-gram model is memory demanding and for 200,000 different names the dimension of the requested memory is impracticably big.</p><p>The problem related to personal proper names was discussed in <ref type="bibr" target="#b0">(Allan and Raghavan, 2002</ref>). However, the paper addresses only the problem of clustering together the names which "sound alike" and no cross document coreference check was car- ried out. The technique to find similar names is based on a noisy channel model. The condi- tional probabilities for each two names to be sim- ilarly spelled are computed. The time complex- ity is quadratic, which renders this technique un- feasible for big data. In fact, the results are re- ported for a 100 word set. A different approach comes from considering search queries databases <ref type="bibr" target="#b1">(Bassil and Alwani, 2012</ref>). These techniques are similar to the model based on the noisy channel, as they compute the conditional probabilities of misspellings based on their frequencies in similar queries. Unfortunately, large numbers of queries for proper names are not available. A similar tech- nique, but using morphological features, was pre- sented in <ref type="bibr" target="#b18">(Veronis, 1988)</ref>. The method can man- age complex combinations of typographical and phonographic errors.</p><p>It has been noted in many works dedicated to error correction, see among others ( <ref type="bibr" target="#b10">Mihov and Schulz, 2004)</ref>, that the ED algorithm is imprac- ticably slow when the number of pairs is large. A solution is to build a large tries tree. While this solution improves the searching time drastically, the memory consumption may be large. Automata indexing was used in <ref type="bibr" target="#b13">(Oflazer, 1996)</ref>. While the memory consumption is much less than for the tries tree approaches, it is still high. For Turk- ish, the author reported 28,825 states and 118,352 transitions labeled with surface symbols. The re- covery error rate is 80%. In (Boytsov, 2011) a review of indexing methods is given. Testing on 5,000 strings for k=1,2,3 is reported and the paper shows the problem the systems run into for bigger values of k. In <ref type="bibr" target="#b8">(Huldén, 2009</ref>) a solution employ- ing approximations via an A* strategy with finite automata is presented. The method is much faster for k bigger than the one presented in <ref type="bibr" target="#b4">(Chodorow and Leacock, 2000</ref>). However, the usage of A* for proper names may be less accurate than the one reported in the paper, because unlike the com- mon words in a given language, the names may have unpredictable forms, especially the foreign names. The results reported show how the time and memory vary for indexing methods according to the length of the words for k=1,2,3.</p><p>A method that uses mapping from strings to numbers is presented in <ref type="bibr" target="#b16">(Reynaert, 2004</ref>). This method uses sum of exponentials. The value of the exponential was empirically found. However, the mapping is only approximative. Our mapping is precise and does not use exponential operations which are time consuming.</p><p>The study in <ref type="bibr" target="#b12">(Navarro, 2001</ref>) is focused on non indexing approximate string search methods, in particular on the simple ED distance. The non- indexing methods may reach linear running time, but it is not always the case that they are scalable to big data. In ( <ref type="bibr" target="#b11">Nagata et al., 2006</ref>) a study on the type of errors produced by non-native speakers of English is carried out, but the long distance mis- spellings are not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prime Mapping Misspeling Algorithm</head><p>The algorithms based on the Levenshtein dis- tance use the dynamic programming technique to build a table of character to character comparisons. We present here a novel approach to misspelling which does not build this table, skipping the need to compare characters. In a nutshell, the prime mapping algorithm, PM, replaces the characters compare operations to a unique arithmetic oper- ation.This can be done by associating to any letter of the alphabet a unique prime number. For ex- ample we can associate 2 to a, 3 to b, 5 to c ... 97 to z. Any string will be mapped into a unique number which is the product of the prime numbers corresponding to its letters. For example the name abba is mapped to 2 · 3 · 3 · 2 = 36. By computing the ratio between any two words we can detect the different letters with just one operation. For exam- ple, the difference between abba and aba is 36/12 = 3, which corresponds uniquely to b because the product/ratio of prime numbers is unique.</p><p>Unlike the ED algorithm, the prime mapping does not find the number of edit operations needed to transform one string into another. In fact, two words that have just one letter in the mutual dif- ference set may be quite distinct: all the strings aba, aab, baa differ by one letter when compared with abba. In order to be in a misspelling relation- ship, the two strings should also have a common part, like prefix or middle, or suffix. The com- plete Prime Mapping (PM) algorithm consists of two successive steps: (1) find all the candidate words that differ from the target word by at most k characters and (2) check weather the target word and the candidate word have a common part, suf- fix, prefix or middle part. Both steps above are executed in constant time, therefore they do not depend either on the length of the strings or on k, the maximal number of different characters. Nor- mally, k = 3, because the probability of a mis- spelled word having more than three distinct let- ters is insignificant, but unlike in the case of ED, the choice of k has no influence on the running time. The first step takes an integer ratio and a hash table key check, both being O(1). The sec- ond step checks if the first k letters at the begin- ning or at the end of the word are the same, and it requires 2k character comparisons, which is also an O(1) process, as k is fixed. The pseudo code and detailed description of the PM algorithm are given below. compute a hash table with prime arithmetics of K primes. In the hash table primeKTable we record all the combinations that can result from di- viding two products which have less than k primes: 1/p i , p i , p i /p j etc. If the ratio between two map- pings is in the hash table, then the corresponding words have all the letters in common, except for at most k. The number of all the combination is k letter difference #combination <ref type="table" target="#tab_1">Memory  1  60  480B  2  435  8K  5  142,506  0.9MB  6  593, 775  3.8MB  10</ref> 30, 045, 015 180MB <ref type="table">Table 1</ref>: The PM algorithm memory needs n k . The memory consumption for different val- ues for k is given in <ref type="table">Table 1</ref>. The figures compare extremely favorably with the ones of ED based ap- proaches (gigs magnitude) . (line 7-8) find misspelling candidates by ratio. By com- puting the ratio and by checking the hash table, we found the pairs which use the same letters, except for at most k. The procedure commonpart checks whether the two strings also have a common part by looking at the start and end k. If this is the case, the pair is in a misspelling relationship. The PM is much faster than ED. The fastest variant of ED, which does not compare strings having length difference bigger than 1, theoret- ically finds only 80% of the misspellings. In practice, only around 60% of the misspellings are found because of proper names and words mis- spelled by non-native speakers. The PM algorithm considers all possible pairs, finds more than 99% of misspellings and is 35 times faster. To obtain the same coverage, the ED algorithm must run for more than 100 days. The time comparison for mil- lions of pairs is plotted in <ref type="figure" target="#fig_1">Figure 1</ref>. The experi- ments were performed on an i5, 2.8 GHz proces- sor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Prime Mapping</head><p>There is an immediate improvement we can bring to the basic variant of PM. The figures re- ported above are obtained by doing the whole set of possible pairs. By taking into account the fact that two words differing by k + 1 letters cannot be k similar, we can organize the number represent- ing the names into an array which reduced drasti-cally the number of comparisons. For example, all the words containing the letters x, y, z cannot be k = 2 similar with the words not containing any of these letters. By dividing the mapping of a word to the primes associated with the letters of an k-gram, we know if the words containing the k-gram can be misspelling candidates with at most k differ- ence, and there is no more need to carry out all the ratios. We arrange the mappings of all words into an array such that on the first indexes we have the words containing the less frequent k + 1 gram, on the next indexes we have the words containing the second less frequent k +1 gram and do not contain the first k + 1 gram, on the next indexes the words containing the third less frequent k + 1 gram and do not contain the first two k + 1 gram, etc. In this way, even the most frequent k + 1 gram has only a few words assigned and consequently the num- ber of direct comparisons is reduced to the mini- mum. The mapping corresponding to a k + 1 gram are ordered in this array according to the length of the words. The number of trigrams is theoretically large, the k + 1 power of the size of the alpha- bet. However, the number of actually occurring k-trigrams is only a small fraction of it. For exam- ple, for k = 2, the number of trigrams is a few hun- dred, out of the 2, 700 possible ones. PM2gram runs in almost a quarter of the time needed by the basic PM. For the same set of names we obtained the results reported in <ref type="table" target="#tab_1">Table 2</ref>. The last column indicates how many times the algorithm is slower than the PM in its basic form.    <ref type="table" target="#tab_2">Table 3</ref> for examples of such patterns. Finding and learning such patterns, along with their probability of indicating a true misspelling, bring an important gain to CDC systems both in running time and in alleviating the data-sparseness problem. The CDC system computes the prob- ability of coreference for two mentions t and t' using a similarity metrics into a vectorial space, where vectors are made out of contextual features occurring with t and t' respectively <ref type="bibr" target="#b6">(Grishman, 1994)</ref>. However, the information extracted from documents is often too sparse to decide on coref- erence <ref type="bibr" target="#b15">(Popescu, 2009)</ref>. Coreference has a global effect, as the CDC systems generally improve the coverage creating new vectors by interpolating the information resulting from the documents which were coreferred ( <ref type="bibr" target="#b7">Hastie et al., 2005</ref>). This infor- mation is used to find further coreferences that no single pair of documents would allow. Thus, miss- ing a coreference pair may result in losing the pos- sibility of realizing further coreferences. However, for two mentions matching a misspelling pattern which is highly accurate, the threshold for contex- tual evidence is lowered. Thus, correcting a mis- spelling is not beneficial for a single mention only, but for the accuracy of the whole.</p><p>The strategy we adopt for finding patterns is to work in a bootstrapping manner, enlarging the valid patterns list while maintaining a high accu- racy of the coreference, over 90%. Initially, we start with an empty base of patterns. Considering only the very high precision threshold for coref- erence, above 98% certainty, we obtain a set of misspelling pairs. This set is used to extract pat- terns of misspellings via a parameter estimation found using the EM-algorithm. The pattern is con- sidered valid only if it also has more than a given number of occurrences. The recursion of the pre- vious steps is carried out by lowering with an ε the threshold for accuracy of coreference for pat-tern candidates. The details and the pseudo code are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Misspelling Pattern Extraction</head><p>Require: thCoref , ε, minO, thAcc Require: thCDC Ensure: pattList 1: pattList, candP attList ← ∅ 2: while there is a pair (t, t') to test for coreference do 3: if (t, t') matches p, p in pattList then 4:</p><p>prob ← corefProb(p) 5: else 6:</p><p>use PM algorithm on pair (t, t') 7:</p><p>prob ← thCoref 8:</p><p>end if 9:</p><p>if pair (t, t') coref with prob then 10:</p><p>candP attList ← candP attList + (t, t') 11:</p><p>end if 12:</p><p>extractPatterns from candP attList 13:</p><p>for cp in new extracted patterns do 14:</p><p>if #cp&gt;minO and corefProb(cp)&gt;thAcc then 15:</p><p>pattList ← pattList + (t, t') 16:</p><p>end if 17:</p><p>end for 18:</p><p>if prob&gt;thCDC then 19:</p><p>corefer (t, t') 20: end if 21: end while 22: thCoref ← thCoref -ε 23: goto line 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Compile a list of misspelling candidates</head><p>For each source string, t, try to match t against the list of patterns (initially empty). If there is a pat- tern matching (t, t') then their prior probability of coreference is the probability associated with that pattern (line 4).</p><p>2. CDC coreference evidence For each pair (t ,t') in the canonical candidates list use the CDC system to compute the probability of coreference between t and t'. If the probability of coreference of t and t' is higher than thCoref , the default value is 98%, then consider t as a misspelling of t' and put (t, t') in a candidate pattern list (line 10).</p><p>3. Extract misspelling patterns Find patterns in the candidate pattern list. Consider only pat- terns with more than minO occurrences, whose default value is 10, and which have the probability of coreference higher than thAcc, whose default value is 90% (line 15).</p><p>4. CDC and pattern evidence For each (t,t') pair matching a pattern and the CDC probabil- ity of coreference more then thCDC, whose de- fault value is 80%, then corefer t and t' (line 21). The fact that the pair (t,t') matches a pattern of misspelling is considered supporting evidence for coreference and in this way it plays a direct role in enhancing the system coverage. Decrease thCoref by ε,whose default is value 0.5, and re- peat the process of finding patterns (goto line 2).</p><p>To extract the pattern from a given list of pairs, procedure extractPatterns at line 12 above, we generate all the suffixes and prefixes of the strings. We compute the probability that a group of char- acters represents a spelling error, given a certain suffix and/or prefix. We use the EM algorithm to compute these probabilities. For a pair (P, S) of a prefix and a suffix, the tuples (p(P)=p, p(S)=s, π) are the quantities to be estimated via EM, with π being the coreference probability. A corefer- ence event is directly observable, without know- ing, however, which prefix or suffix contribute to the coreference. The EM equations are given be- low, where X is the observed data; Z are the hid- den variable, p and s respectively; θ the parame- ters (p,s, π); Q(θ,θ (t) ) the expected log likelihood at iteration t.</p><formula xml:id="formula_0">E − step µ (t) i µ (t) i = E[zi|xi, θ (t) ] = p(x i |z i ,θ (t) ) p(z i =P |θ (t) ) p(x i |θ (t) ) = π (t) [p (t) ] x i [(1−p (t) ] (1−x i ) π (t) [p (t) ] x i [(1−p (t) ] ( 1−x i )+(1−π (t) )[s (t) ] x i [(1−s (t) ] (1−x i ) (1) M − step θ (t+1) ∂Q(θ|θ t) ∂π = 0 π (t+1) = i µ (t) i n ∂Q(θ|θ t) ∂p = 0 p (t+1) = i µ (t) i x i i µ (t) i ∂Q(θ|θ t) ∂s = 0 s (t+1) = i (1−µ (t) i )x i i (1−µ (t) i ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We performed a set of experiments on different corpora in order to evaluate: (1) the performances of the PM algorithm for misspelling detection, (2) the accuracy of proper name misspelling pattern acquisition from large corpora, and (3) the im- provements of a CDC system, employing a cor- rection module for proper name misspellings. In Section 5.1 the accuracy of the PM algorithm is tested on various corpora containing annotated misspellings of English words. In particular, we were interested to see the results when the edit dis- tance between the misspelled pair is bigger than 3, because handling bigger values for k is crucial for finding misspelling errors produced by non-native speakers. The evaluation is directly relevant for the correction of the spelling of foreign names.</p><p>In Section 5.2 the proper name misspelling pat- terns were extracted from two large news cor- pora. One corpus is part of the English Gigawords, LDC2009T13 <ref type="bibr" target="#b14">(Parker et al., 2009</ref>) and the sec- ond corpus is Adige500k in Italian ( <ref type="bibr" target="#b9">Magnini et al., 2006</ref>). We use a Named Entity Recognizer which has an accuracy above 90% for proper names. We evaluated the accuracy of the patterns by random sampling.</p><p>In Section 5.3 the accuracy of the CDC system with the correction module for proper name mis- spellings was tested against a gold standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PM Evaluation</head><p>We consider the following publicly available En- glish corpora containing the annotation of the mis- spelled words: Birkbeck, Aspell, Holbrook, Wiki- pidia. Birkbeck is a heterogeneous collection of documents, so in the experiments below we re- fer to each document separately. In particular we distinguish between misspellings of native speak- ers vs. misspelling of non-native speakers. <ref type="figure" target="#fig_2">Fig- ure 2</ref> shows that there are two types of corpora. For the first type, the misspellings found within two characters are between 80% and 100% of the whole number of misspellings. For the sec- ond type, less than 50% of the misspellings are within two characters.The second category is rep- resented by the misspellings of non native speak- ers. The misspellings are far from the correct forms and they represent chunks of phonetically similar phonemes, like boiz vs. boys. The situa- tion of the foreign name misspellings is likely to be similar to the misspellings found in the sec- ond type of corpora. For those cases, handling a k value bigger than 2 is crucial. Not only the non-indexing methods, but also indexing ones are rather inefficient for k values bigger than 2 for large corpora. The PM algorithm does not have this drawback, and we tested the coverage of the errors we found for values of k ranging from 3 to 10. In <ref type="figure" target="#fig_3">Figure 3</ref> we plot the distributions for the The results showed PM is also able to find the phonemically similar misspellings. We can see that for k bigger than 9 the number of misspellings is not significant.</p><p>The PM algorithm performed very well, being able to find the misspellings even for large k val- ues. There were 47, 837 words in Aspell, <ref type="bibr">Holbrrok and Wikipedia, and 30, 671</ref> in Birkbeck, and PM found all the misspelling pairs in a running time of 25 minutes. This is a very competitive time, even for indexing methods. For k above 8 the access to the hash table containing the prime combinations was slower, but not significantly so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pattern Extraction Evaluation</head><p>We extracted the set of names using a NER from the two corpora, LDC2009T13 and Adige500k. The set of proper names is rather large in both cor- pora -160, 869 names from the English corpus and 185, 508 from the Italian corpus. Apparently, the quasi-similar names, which are considered as mis- spelled name candidates, is very high. In <ref type="figure" target="#fig_4">Figure  4</ref> we plot this data. The English Cand and Italian Cand are absolute values, while the English True and Italian True represent percentages. For exam- ple, a name of length 5 is likely to have around 23 misspelling candidates, but only 17% of them are likely to be true misspellings, the rest being differ- ent names. The numbers are estimated considering samples having the size between 30 and 50, for each name length. The percentages change rapidly with the length of the string. For names with the length bigger than 11, the probability that a misspelling candidate is a true misspelling is more than 98%. This fact suggests a strategy for pattern extrac- tion: start from the higher name length towards the lower length names. The patterns found by the al- gorithm described in Section 4 have between 900 and 20 occurrences. There are 12 patterns having more than 400 occurrences, 20 having between 20 and 50 occurrences, see  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CDC and Misspelling correction</head><p>The CRIPCO corpus ( <ref type="bibr" target="#b2">Bentivogli et al., 2008</ref>) is a gold standard for CDC in Italian, contain- ing pieces of news extracted from Adige500k. There are 107 names, the majority being Ital- ian names. We scrambled the names to cre- ate misspelling candidates. For example the name leonardo was scrambled like teonardo, lionaldo, loenarod etc. We considered the top 15 frequency letters and maximum 4 letters for each scrambling. We randomly selected 70% of the original CRIPCO making no modifications, and called this corpus CRwCR. 30% of the original documents were assigned to the invented pseudo- names, and we called this corpus CRwSC (cor- rect documents with scrambled names). From Adige500k we randomly chose 20, 000 documents and assigned them to the scrambled names as well, calling this corpus NCRwSC. From these pieces we created a new corpus: 70% of the initial CRIPCO documents with the original names, 30% of the CRIPCO documents with scrambled names and 20, 000 documents with the same scrambled names. For the names occurring in CRwCR, the scrambled names are valid name misspellings in the CRwSC corpus, and invalid in NCRwSC.</p><p>As expected, the PM algorithm found all the We let the threshold confidence of coreference to vary from 90% to 98%. The number in <ref type="figure" target="#fig_7">Figure  6</ref> refers to the precision and recall for the name misspellings in the CRIPCO corpus created via random scrambling. We were also interested to see how the pattern finding procedure works, but scrambling randomly produced too many contexts. Therefore, we chose to modify the names in a non random way, by replacing the final o to ino, ex. paolo goes to paolino, and modifying one letter in the word for half of the occurrences, ex. paorino. The idea is that ino is a very common suffix for names in Italian. The system was able to learn the pseudo alternatives created in the context ino. The noise introduced was relatively low, see <ref type="figure" target="#fig_7">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Further Research</head><p>In this paper we described a system able to correct misspellings, including proper name misspellings, fast and accurately. The algorithm introduced, PM, overcomes the time/memory limitations of the approaches based on the edit distance.</p><p>The system is built on a novel string compare algorithm which runs in constant time indepen- dently of the length of the names or the number of different letters allowed, with no auxiliary mem- ory request. As such, the algorithm is much faster than any other non-indexing algorithms. Because it is independent of k, it can be used even for large k, where even the indexing methods have limita- tions. We also used an EM based technique to find misspelling patterns. The results obtained are very accurate.</p><p>The system makes a first selection of the docu- ments, drastically reducing the human work load. Another line of future research is to use the PM algorithm in other NLP tasks, where finding the pairs having some particular elements in common is necessary: for example, comparing parsing trees or dependency trees. We think that PM can be used in other NLP tasks as well and we hope the community can take advantage of it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Require: charList wordsList, primeList, k Ensure: misspList 1: misspList ← ∅ 2: foreach α in charList: p(α) ← pi, pi in primeList 3: foreach w in wordsList: p(w) ← p(α) , α in w 4: primeKT able ← n k of prime arithmetics 5: for w in wordsList do 6: for w' in wordsList, w = w' do 7: r ← p(w) p(w ) 8: if r in primeKT able then 9: if commonPart (w, w') = ∅ then 10: misspList ← misspList + (w, w') 11: end if 12: end if 13: end for 14: end for map letters to prime numbers. A helpful way to assign primes to letters is according to their fre- quency; on average, the numbers corresponding to names are smaller and the operation gets less time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PM vs. the fastest ED type algorithm</figDesc><graphic url="image-1.png" coords="4,337.09,343.58,155.90,129.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: k = 1, 2</figDesc><graphic url="image-2.png" coords="7,73.47,580.08,212.59,92.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Foreign Misspellings</figDesc><graphic url="image-3.png" coords="7,322.92,83.57,184.25,96.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Candidates vs. True Misspellings</figDesc><graphic url="image-4.png" coords="7,322.92,669.38,184.25,93.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of the patterns:</figDesc><graphic url="image-5.png" coords="8,87.65,270.29,184.24,104.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Proper Names CRIPCO Evaluation</figDesc><graphic url="image-6.png" coords="8,322.92,83.57,184.25,81.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : ED variants versus MP 4 Correcting Proper Names Misspellings</head><label>2</label><figDesc></figDesc><table>In this section we focus on a class of words which 
do not occur in a priorly given dictionary and for 
which the misspelled variants may not be random. 
Proper names are representative for this class. For 
example, the same Russian name occurs in corpus 
as Berezovski, Berezovsky or Berezovschi because 
of inaccurate transliteration. By convention, we 
consider the most frequent form as the canonical 
one, and all the other forms as misspelled variants. 
Many times, the difference between a canonical 
form and a misspelled variant follows a pattern: a 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Name misspellings patterns</head><label>3</label><figDesc></figDesc><table>particular group of letters substitutes another one 
in the context created by the other characters in 
the name. A misspelling pattern specifies the con-
text, as prefix or suffix of a string, where a particu-
lar group of characters is a misspelling of another. 
See </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using Partof-Speech Patterns to Reduce Query Ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OCR Post-Processing Error Correction Algorithm Using Google&apos;s Online Spelling Suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Bassil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Alwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Emerging Trends in Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2079" to="8407" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Creating a Gold Standard for Person Cross-Document Coreference Resolution in Italian News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop Programme</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing Methods for Approximate Dictionary Searching: Comparative Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics (JEA)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Unsupervised Method for Detecting Grammatical Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference</title>
		<meeting>the 1st North American chapter of the Association for Computational Linguistics conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Technique for Computer Detection and Correction of Spelling Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Whither Written Language Evaluation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Franklin</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference and Prediction. The Mathematical Intelligencer</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="83" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast Approximate String Matching with Finite Automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Måns Huldén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del lenguaje natural</title>
		<meeting>esamiento del lenguaje natural</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">I-CAB: The Italian Content Annotation Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Speranza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><forename type="middle">Bartalesi</forename><surname>Lenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachele</forename><surname>Sprugnoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="963" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast Approximate Search in Large Dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoyan</forename><surname>Mihov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">U</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="477" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Feedback-Augmented Method for Detecting Errors in The Writing of Learners of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Morihiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuo</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Isu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Guided Tour to Approximate String Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="88" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Error-tolerant Finite-state Recognition with Applications to Morphological Analysis and Spelling Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="89" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">English Gigaword Fourth Edition. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguistic</forename><surname>Data Consortium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person Cross Document Coreference with Name Perplexity Estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text Induced Spelling Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Reynaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">834</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection of Recognition Errors and Out of the Spelling Dictionary Names in a Spelled Name Recognizer for Spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Macías</forename><surname>Rubén San Segundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Guarasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferreiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Manuel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2553" to="2556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Morphosyntactic Correction in Natural Language Interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Veronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th conference on Computational linguistics</title>
		<meeting>the 12th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
