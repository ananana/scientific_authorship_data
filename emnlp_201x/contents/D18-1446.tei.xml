<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>feiliu@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4131" to="4141"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4131</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an ab-stractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural abstractive summarization has primarily focused on summarizing short texts written by sin- gle authors. For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary <ref type="bibr" target="#b26">(Rush et al., 2015;</ref><ref type="bibr" target="#b30">Takase et al., 2016;</ref><ref type="bibr" target="#b29">Song et al., 2018)</ref>; single-document summarization (SDS) fo- cuses on condensing a news article to a handful of bullet points ( <ref type="bibr" target="#b24">Paulus et al., 2017;</ref><ref type="bibr" target="#b28">See et al., 2017)</ref>. These summarization studies are empow- ered by large parallel datasets automatically har- vested from online news outlets, including Giga- word ( <ref type="bibr" target="#b26">Rush et al., 2015)</ref>, CNN/Daily Mail <ref type="bibr" target="#b6">(Hermann et al., 2015)</ref>, NYT <ref type="bibr" target="#b27">(Sandhaus, 2008)</ref>, and Newsroom ( <ref type="bibr" target="#b2">Grusky et al., 2018)</ref>.</p><p>To date, multi-document summarization (MDS) has not yet fully benefited from the development <ref type="bibr">DATASET</ref>  <ref type="bibr" target="#b26">(Rush et al., 2015)</ref> of a news article title-like CNN/Daily Mail a news article 56 words 312 K ( <ref type="bibr" target="#b6">Hermann et al., 2015)</ref> multi-sent TAC (08-11) 10 news articles 100 words 728 <ref type="bibr">(Dang et al., 2008)</ref> related to a topic multi-sent DUC (03-04) 10 news articles 100 words 320 <ref type="bibr" target="#b23">(Over and Yen, 2004)</ref> related to a topic multi-sent of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by mul- tiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews <ref type="bibr">(Gerani et al., 2014</ref>), student re- sponses to post-class questionnaires <ref type="bibr" target="#b17">(Luo and Litman, 2015;</ref><ref type="bibr" target="#b18">Luo et al., 2016)</ref>, and sets of news arti- cles discussing certain topics ( <ref type="bibr" target="#b8">Hong et al., 2014</ref>). State-of-the-art MDS systems are mostly extrac- tive ( <ref type="bibr" target="#b22">Nenkova and McKeown, 2011</ref>). Despite their promising results, such systems cannot perform text abstraction, e.g., paraphrasing, generalization, and sentence fusion <ref type="bibr" target="#b10">(Jing and McKeown, 1999</ref>). Further, annotated MDS datasets are often scarce, containing only hundreds of training pairs (see Ta- ble 1). The cost to create ground-truth summaries from multiple-document inputs can be prohibitive. The MDS datasets are thus too small to be used to train neural encoder-decoder models with millions of parameters without overfitting. A promising route to generating an abstractive summary from a multi-document input is to apply a neural encoder-decoder model trained for single- document summarization to a "mega-document" created by concatenating all documents in the set at test time. Nonetheless, such a model may not scale well for two reasons. First, identifying im- portant text pieces from a mega-document can be challenging for the encoder-decoder model, which is trained on single-document summarization data where the summary-worthy content is often con- tained in the first few sentences of an article. This is not the case for a mega-document. Second, re- dundant text pieces in a mega-document can be re- peatedly used for summary generation under the current framework. The attention mechanism of an encoder-decoder model ( <ref type="bibr">Bahdanau et al., 2014</ref>) is position-based and lacks an awareness of se- mantics. If a text piece has been attended to dur- ing summary generation, it is unlikely to be used again. However, the attention value assigned to a similar text piece in a different position is not af- fected. The same content can thus be repeatedly used for summary generation. These issues may be alleviated by improving the encoder-decoder architecture and its attention mechanism <ref type="bibr">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b31">Tan et al., 2017)</ref>. However, in these cases the model has to be re-trained on large-scale MDS datasets that are not available at the current stage. There is thus an increasing need for a lightweight adaptation of an encoder-decoder model trained on SDS datasets to work with multi- document inputs at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOURCE SUMMARY #PAIRS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gigaword the first sentence 8.3 words 4 Million</head><p>In this paper, we present a novel adaptation method, named PG-MMR, to generate abstracts from multi-document inputs. The method is ro- bust and requires no MDS training data. It com- bines a recent neural encoder-decoder model (PG for Pointer-Generator networks; <ref type="bibr" target="#b28">See et al., 2017</ref>) that generates abstractive summaries from single- document inputs with a strong extractive summa- rization algorithm (MMR for Maximal Marginal Relevance; <ref type="bibr">Carbonell and Goldstein, 1998</ref>) that identifies important source sentences from multi- document inputs. The PG-MMR algorithm itera- tively performs the following. It identifies a hand- ful of the most important sentences from the mega- document. The attention weights of the PG model are directly modified to focus on these important sentences when generating a summary sentence. Next, the system re-identifies a number of impor- tant sentences, but the likelihood of choosing cer- tain sentences is reduced based on their similar- ity to the partially-generated summary, thereby re- ducing redundancy. Our research contributions in- clude the following:</p><p>• we present an investigation into a novel adapta- tion method of the encoder-decoder framework from single-to multi-document summarization.</p><p>To the best of our knowledge, this is the first at- tempt to couple the maximal marginal relevance algorithm with pointer-generator networks for multi-document summarization;</p><p>• we demonstrate the effectiveness of the pro- posed method through extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and ab- stractive summarization systems measured by both automatic metrics and human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Popular methods for multi-document summariza- tion have been extractive. Important sentences are extracted from a set of source documents and op- tionally compressed to form a summary <ref type="bibr">(Daume III and Marcu, 2002;</ref><ref type="bibr">Zajic et al., 2007;</ref><ref type="bibr">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr">Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr">Thadani and McKeown, 2013;</ref><ref type="bibr">Wang et al., 2013;</ref><ref type="bibr">Yogatama et al., 2015;</ref><ref type="bibr">Filippova et al., 2015;</ref><ref type="bibr">Durrett et al., 2016)</ref>. In recent years neural networks have been exploited to learn word/sentence rep- resentations for single-and multi-document sum- marization ( <ref type="bibr">Cheng and Lapata, 2016;</ref><ref type="bibr">Cao et al., 2017;</ref><ref type="bibr" target="#b9">Isonuma et al., 2017;</ref><ref type="bibr">Yasunaga et al., 2017;</ref><ref type="bibr" target="#b21">Narayan et al., 2018</ref>). These approaches remain extractive; and despite encouraging results, sum- marizing a large quantity of texts still requires so- phisticated abstraction capabilities such as gener- alization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summariza- tion has been investigated ( <ref type="bibr">Barzilay et al., 1999;</ref><ref type="bibr">Carenini and Cheung, 2008;</ref><ref type="bibr">Ganesan et al., 2010;</ref><ref type="bibr">Gerani et al., 2014;</ref><ref type="bibr">Fabbrizio et al., 2014;</ref><ref type="bibr" target="#b25">Pighin et al., 2014;</ref><ref type="bibr">Bing et al., 2015;</ref><ref type="bibr" target="#b16">Liu et al., 2015;</ref><ref type="bibr" target="#b14">Liao et al., 2018</ref>). These approaches construct domain templates using a text planner or an open-IE sys- tem and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets.</p><p>Neural abstractive summarization utilizing the encoder-decoder architecture has shown promis- ing results but studies focus primarily on single- document summarization <ref type="bibr" target="#b11">Kikuchi et al., 2016;</ref><ref type="bibr">Chen et al., 2016;</ref><ref type="bibr" target="#b19">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b31">Tan et al., 2017;</ref><ref type="bibr">Zeng et al., 2017;</ref><ref type="bibr">Zhou et al., 2017;</ref><ref type="bibr" target="#b24">Paulus et al., 2017;</ref><ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr">Gehrmann et al., 2018</ref>). The point- ing mechanism <ref type="bibr" target="#b3">Gu et al., 2016</ref>) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evalua- tion metrics ( <ref type="bibr" target="#b24">Paulus et al., 2017;</ref><ref type="bibr" target="#b12">Kry´sci´nskiKry´sci´Kry´sci´nski et al., 2018;</ref><ref type="bibr">Chen and Bansal, 2018)</ref>. These studies fo-cus on summarizing single documents in part be- cause the training data are abundant.</p><p>The work of <ref type="bibr">Baumel et al. (2018)</ref> and <ref type="bibr">Zhang et al. (2018)</ref> are related to ours. In particular, <ref type="bibr">Baumel et al. (2018)</ref> propose to extend an abstractive sum- marization system to generate query-focused sum- maries; <ref type="bibr">Zhang et al. (2018)</ref> add a document set en- coder to their hierarchical summarization frame- work. With these few exceptions, little research has been dedicated to investigate the feasibility of extending the encoder-decoder framework to gen- erate abstractive summaries from multi-document inputs, where available training data are scarce.</p><p>This paper presents some first steps towards the goal of extending the encoder-decoder model to a multi-document setting. We introduce an adap- tation method combining the pointer-generator (PG) networks <ref type="bibr" target="#b28">(See et al., 2017</ref>) and the maximal marginal relevance (MMR) algorithm <ref type="bibr">(Carbonell and Goldstein, 1998</ref>). The PG model, trained on SDS data and detailed in Section §3, is capable of generating document abstracts by performing text abstraction and sentence fusion. However, if the model is applied at test time to summa- rize multi-document inputs, there will be limita- tions. Our PG-MMR algorithm, presented in Sec- tion §4, teaches the PG model to effectively recog- nize important content from the input documents, hence improving the quality of abstractive sum- maries, all without requiring any training on multi- document inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Limits of the Encoder-Decoder Model</head><p>The encoder-decoder architecture has become the de facto standard for neural abstractive summa- rization ( <ref type="bibr" target="#b26">Rush et al., 2015</ref>). The encoder is often a bidirectional LSTM (Hochreiter and Schmidhu- ber, 1997) converting the input text to a set of hid- den states {h e i }, one for each input word, indexed by i. The decoder is a unidirectional LSTM that generates a summary by predicting one word at a time. The decoder hidden states are represented by {h d t }, indexed by t. For sentence and single- document summarization <ref type="bibr" target="#b24">Paulus et al., 2017;</ref><ref type="bibr" target="#b28">See et al., 2017)</ref>, the input text is treated as a sequence of words, and the model is expected to capture the source syntax inherently.</p><formula xml:id="formula_0">e t,i = v tanh(W e [h d t ||h e i || α t,i ] + b e ) (1) α t,i = softmax(e t,i ) (2) α t,i = t−1 t =0 α t ,i<label>(3)</label></formula><p>The attention weight α t,i measures how impor- tant the i-th input word is to generating the t-th output word (Eq. (1-2)). Following ( <ref type="bibr" target="#b28">See et al., 2017)</ref>, α t,i is calculated by measuring the strength of interaction between the decoder hidden state h d t , the encoder hidden state h e i , and the cumula- tive attention α t,i (Eq. <ref type="formula" target="#formula_0">(3)</ref>). α t,i denotes the cumu- lative attention that the i-th input word receives up to time step t-1. A large value of α t,i indicates the i-th input word has been used prior to time t and it is unlikely to be used again for generating the t-th output word.</p><p>A context vector (c t ) is constructed (Eq. <ref type="formula">(4)</ref>) to summarize the semantic meaning of the input; it is a weighted sum of the encoder hidden states. The context vector and the decoder hidden state ([h d t ||c t ]) are then used to compute the vocabulary probability P vcb (w) measuring the likelihood of a vocabulary word w being selected as the t-th out- put word (Eq. <ref type="formula" target="#formula_1">(5)</ref>). 1</p><formula xml:id="formula_1">c t = i α t,i h e i (4) P vcb (w) = softmax(W y [h d t ||c t ] + b y )<label>(5)</label></formula><p>In many encoder-decoder models, a "switch" is estimated (p gen ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>) to indicate whether the system has chosen to select a word from the vo- cabulary or to copy a word from the input text (Eq. <ref type="formula">(6)</ref>). The switch is computed using a feedfor- ward layer with σ activation over [h d t ||c t ||y t−1 ], where y t−1 is the embedding of the output word at time t-1. The attention weights (α t,i ) are used to compute the copy probability (Eq. <ref type="formula">(7)</ref>). If a word w appears once or more in the input text, its copy probability ( i:w i =w α t,i ) is the sum of the attention weights over all its occurrences. The final probability P (w) is a weighted combination of the vocabulary probability and the copy proba- bility. A cross-entropy loss function can often be used to train the model end-to-end.</p><formula xml:id="formula_2">p gen =σ(w z [h d t ||c t ||y t−1 ])+b z ) (6) P (w)=p gen P vcb (w)+(1−p gen ) i:w i =w α t,i (7)</formula><p>To thoroughly understand the aforementioned encoder-decoder model, we divide its model pa- rameters into four groups. They include • parameters of the encoder and the decoder;</p><p>• {w z , b z } for calculating the "switch" (Eq. <ref type="formula">(6)</ref> • {W y , b y } for calculating P vcb (w) (Eq. <ref type="formula" target="#formula_1">(5)</ref>);</p><p>• {v, W e , b e } for attention weights (Eq. <ref type="formula">(1)</ref>).</p><p>By training the encoder-decoder model on single- document summarization (SDS) data containing a large collection of news articles paired with sum- maries ( <ref type="bibr" target="#b6">Hermann et al., 2015)</ref>, these model param- eters can be effectively learned.</p><p>However, at test time, we wish for the model to generate abstractive summaries from multi- document inputs. This brings up two issues. First, the parameters are ineffective at identifying salient content from multi-document inputs. Humans are very good at identifying representative sentences from a set of documents and fusing them into an abstract. However, this capability is not supported by the encoder-decoder model. Second, the atten- tion mechanism is based on input word positions but not their semantics. It can lead to redundant content in the multi-document input being repeat- edly used for summary generation. We conjec- ture that both aspects can be addressed by intro- ducing an "external" model that selects represen- tative sentences from multi-document inputs and dynamically adjusts the sentence importance to re- duce summary redundancy. This external model is integrated with the encoder-decoder model to gen- erate abstractive summaries using selected repre- sentative sentences. In the following section we present our adaptation method for multi-document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Method</head><p>Maximal marginal relevance. Our adaptation method incorporates the maximal marginal rele- vance algorithm (MMR; <ref type="bibr">Carbonell and Goldstein, 1998</ref>) into pointer-generator networks <ref type="bibr">(PG;</ref><ref type="bibr" target="#b28">See et al., 2017)</ref> by adjusting the network's attention val- ues. MMR is one of the most successful extractive approaches and, despite its straightforwardness, performs on-par with state-of-the-art systems <ref type="bibr" target="#b17">(Luo and Litman, 2015;</ref><ref type="bibr">Yogatama et al., 2015)</ref>. At each iteration, MMR selects one sentence from the doc- ument (D) and includes it in the summary (S) until a length threshold is reached. The selected sen- tence (s i ) is the most important one amongst the remaining sentences and it has the least content overlap with the current summary. In the equation below, Sim 1 (s i , D) measures the similarity of the sentence s i to the document. It serves as a proxy of sentence importance, since important sentences usually show similarity to the centroid of the doc- ument. max s j ∈S Sim 2 (s i , s j ) measures the max- imum similarity of the sentence s i to each of the summary sentences, acting as a proxy of redun- dancy. λ is a balancing factor.</p><formula xml:id="formula_3">argmax s i ∈D\S λSim 1 (s i ,D) importance −(1−λ)max s j ∈S Sim 2 (s i ,s j ) redundancy</formula><p>Our PG-MMR describes an iterative framework for summarizing a multi-document input to a sum- mary consisting of multiple sentences. At each it- eration, PG-MMR follows the MMR principle to select the K highest-scored source sentences; they serve as the basis for PG to generate a summary sentence. After that, the scores of all source sen- tences are updated based on their importance and redundancy. Sentences that are highly similar to the partial summary receive lower scores. Select- ing K sentences via the MMR algorithm helps the PG system to effectively identify salient source content that has not been included in the summary.</p><p>Muting. To allow the PG system to effectively utilize the K source sentences without retraining the neural model, we dynamically adjust the PG attention weights (α t,i ) at test time. Let S k rep-resent a selected sentence. The attention weights of the words belonging to {S k } K k=1 are calculated as before <ref type="bibr">(Eq. (2)</ref>). However, words in other sen- tences are forced to receive zero attention weights (α t,i =0), and all α t,i are renormalized <ref type="bibr">(Eq. (8)</ref>).</p><formula xml:id="formula_4">α new t,i = α t,i i∈{S k } K k=1 0 otherwise (8)</formula><p>It means that the remaining sentences are "muted" in this process. In this variant, the sentence impor- tance does not affect the original attention weights, other than muting.</p><p>In an alternative setting, the sentence salience is multiplied with the word salience and renormal- ized (Eq. <ref type="formula">(9)</ref>). PG uses the reweighted alpha val- ues to predict the next summary word.</p><formula xml:id="formula_5">α new t,i = α t,i MMR(S k ) i∈{S k } K k=1 0 otherwise (9)</formula><p>Sentence Importance. To estimate sentence im- portance Sim 1 (s i , D), we introduce a supervised regression model in this work. Importantly, the model is trained on single-document summariza- tion datasets where training data are abundant. At test time, the model can be applied to identify im- portant sentences from multi-document input. Our model determines sentence importance based on four indicators, inspired by how humans identify important sentences from a document set. They in- clude (a) sentence length, (b) its absolute and rela- tive position in the document, (c) sentence quality, and (d) how close the sentence is to the main topic of the document set. These features are considered to be important indicators in previous extractive summarization framework ( <ref type="bibr">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr" target="#b8">Hong et al., 2014</ref>).</p><p>Regarding the sentence quality (c), we lever- age the PG model to build the sentence represen- tation. We use the bidirectional LSTM encoder to encode any source sentence to a vector repre-</p><formula xml:id="formula_6">sentation. [ − → h e N || ← − h e 1 ]</formula><p>is the concatenation of the last hidden states of the forward and backward passes. A document vector is the average of all sentence vectors. We use the document vector and the cosine similarity between the document and sentence vectors as indicator (d). A support vec- tor regression model is trained on (sentence, score) pairs where the training data are obtained from the CNN/Daily Mail dataset. The target importance score is the ROUGE-L recall of the sentence com- pared to the ground-truth summary. Our model ar- chitecture leverages neural representations of sen- Algorithm 1 The PG-MMR algorithm for summa- rizing multi-document inputs. Input: SDS data; MDS source sentences {S i } 1: Train the PG model on SDS data 2: I(S i ) and R(S i ) are the importance and re- dundancy scores of the source sentence S i 3: I(S i ) ← SVR(S i ) for all source sentences 4: MMR(S i ) ← λI(S i ) for all source sentences 5: Summary ← {} 6: t ← index of summary words 7: while t &lt; L max do 8:</p><formula xml:id="formula_7">Find {S k } K k=1</formula><p>with highest MMR scores 9:</p><formula xml:id="formula_8">Compute α new t,i based on {S k } K k=1 (Eq. (8)) 10:</formula><p>Run PG decoder for one step to get {w t } 11:</p><p>Summary ← Summary + {w t }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>if w t is the period symbol then 13:</p><p>R(S i ) ← Sim(S i , Summary), ∀i 14:</p><formula xml:id="formula_9">MMR(S i ) ← λI(S i ) −(1 − λ)R(S i ), ∀i 15:</formula><p>end if 16: end while tences and documents, they are data-driven and not restricted to a particular domain.</p><p>Sentence Redundancy. To calculate the redun- dancy of the sentence (max s j ∈S Sim 2 (s i , s j )), we compute the ROUGE-L precision, which mea- sures the longest common subsequence between a source sentence and the partial summary (consist- ing of all sentences generated thus far by the PG model), divided by the length of the source sen- tence. A source sentence yielding a high ROUGE- L precision is deemed to have significant content overlap with the partial summary. It will receive a low MMR score and hence is less likely to serve as basis for generating future summary sentences.</p><p>Alg. 1 provides an overview the PG-MMR al- gorithm and <ref type="figure" target="#fig_0">Fig. 1</ref> is a graphical illustration. The MMR scores of source sentences are updated af- ter each summary sentence is generated by the PG model. Next, a different set of highest-scored sen- tences are used to guide the PG model to generate the next summary sentence. "Muting" the remain- ing source sentences is important because it helps the PG model to focus its attention on the most sig- nificant source content. The code for our model is publicly available to further MDS research. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets. We investigate the effectiveness of the PG-MMR method by testing it on standard multi- document summarization datasets ( <ref type="bibr" target="#b23">Over and Yen, 2004;</ref><ref type="bibr">Dang and Owczarzak, 2008)</ref>. These include DUC-03, DUC-04, TAC-08, TAC-10, and TAC- 11, containing 30/50/48/46/44 topics respectively. The summarization system is tasked with gener- ating a concise, fluent summary of 100 words or less from a set of 10 documents discussing a topic. All documents in a set are chronologically ordered and concatenated to form a mega-document serv- ing as input to the PG-MMR system. Sentences that start with a quotation mark or do not end with a period are excluded ( <ref type="bibr">Wong et al., 2008)</ref>. Each system summary is compared against 4 human ab- stracts created by NIST assessors. Following con- vention, we report results on DUC-04 and TAC-11 datasets, which are standard test sets; DUC-03 and TAC-08/10 are used as a validation set for hyper- parameter tuning. <ref type="bibr">3</ref> The PG model is trained for single-document summarization using the CNN/Daily Mail <ref type="bibr" target="#b6">(Hermann et al., 2015</ref>) dataset, containing single news articles paired with summaries (human-written ar- ticle highlights). The training set contains <ref type="bibr">287,</ref><ref type="bibr">226</ref> articles. An article contains 781 tokens on aver- age; and a summary contains 56 tokens (3.75 sen- tences). During training we use the hyperparam- eters provided by <ref type="bibr" target="#b28">See et al. (2017)</ref>. At test time, the maximum/minimum decoding steps are set to 120/100 words respectively, corresponding to the max/min lengths of the PG-MMR summaries. Be- cause the focus of this work is on multi-document summarization (MDS), we do not report results for the CNN/Daily Mail dataset.</p><p>Baselines. We compare PG-MMR against a broad spectrum of baselines, including state-of-the-art extractive ('ext-') and abstractive ('abs-') systems. They are described below. <ref type="bibr">4</ref> • ext-SumBasic ( <ref type="bibr">Vanderwende et al., 2007</ref>) is an extractive approach assuming words occurring frequently in a docu- ment set are more likely to be included in the summary; • ext-KL-Sum ( <ref type="bibr" target="#b5">Haghighi and Vanderwende, 2009)</ref> greedily adds source sentences to the summary if it leads to a de- crease in KL divergence; • ext-LexRank ( <ref type="bibr">Erkan and Radev, 2004</ref>) uses a graph-based approach to compute sentence importance based on eigen- vector centrality in a graph representation; • ext-Centroid ( <ref type="bibr" target="#b8">Hong et al., 2014</ref>) computes the importance of each source sentence based on its cosine similarity with the document centroid; • ext-ICSISumm ( ) leverages the ILP framework to identify a globally-optimal set of sentences covering the most important concepts in the document set;</p><p>DUC-04 System R-1 R-2 R-SU4</p><p>SumBasic <ref type="bibr">(Vanderwende et al., 2007)</ref> 29.48 4.25 8.64 KLSumm <ref type="bibr" target="#b5">(Haghighi et al., 2009)</ref> 31.04 6.03 10.23 LexRank <ref type="bibr">(Erkan and Radev, 2004)</ref> 34.44 7.11 11.19 Centroid <ref type="bibr" target="#b8">(Hong et al., 2014)</ref> 35.49 7.80 12.02 ICSISumm  37.31 9. <ref type="bibr">36</ref> 13.12 DPP <ref type="bibr">(Taskar, 2012)</ref> 38.78 9.47 13.36 Extract+Rewrite <ref type="bibr" target="#b29">(Song et al., 2018)</ref> 28.90 5. <ref type="bibr">33</ref> 8.76 Opinosis <ref type="bibr">(Ganesan et al., 2010)</ref> 27.07 5.03 8.63 PG-Original <ref type="bibr" target="#b28">(See et al., 2017)</ref> 31   <ref type="bibr">(Erkan and Radev, 2004)</ref> 33.10 7.50 11.13 Extract+Rewrite <ref type="bibr" target="#b29">(Song et al., 2018)</ref> 29.07 6.11 9.20 Opinosis ( <ref type="bibr">Ganesan et al., 2010)</ref> 25.15 5.12 8.12 PG-Original <ref type="bibr" target="#b28">(See et al., 2017)</ref> 31  <ref type="table">Table 3</ref>: ROUGE results on the TAC-11 dataset.</p><p>• ext-DPP (Taskar, 2012) selects an optimal set of sentences per the determinantal point processes that balance the cov- erage of important information and the sentence diversity;</p><p>• abs-Opinosis ( <ref type="bibr">Ganesan et al., 2010)</ref> generates abstractive summaries by searching for salient paths on a word co- occurrence graph created from source documents;</p><p>• abs-Extract+Rewrite ( <ref type="bibr" target="#b29">Song et al., 2018</ref>) is a recent ap- proach that scores sentences using LexRank and generates a title-like summary for each sentence using an encoder- decoder model trained on Gigaword data.</p><p>• abs-PG-Original ( <ref type="bibr" target="#b28">See et al., 2017</ref>) introduces an encoder- decoder model that encourages the system to copy words from the source text via pointing, while retaining the abil- ity to produce novel words through the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Having described the experimental setup, we next compare the PG-MMR method against the base- lines on standard MDS datasets, evaluated by both automatic metrics and human assessors.</p><p>ROUGE <ref type="bibr" target="#b15">(Lin, 2004</ref>). This automatic metric mea- sures the overlap of unigrams (R-1), bigrams (R- 2) and skip bigrams with a maximum distance of 4 words (R-SU4) between the system summary and a set of reference summaries. ROUGE scores of various systems are presented in <ref type="table" target="#tab_2">Table 2</ref> and 3 re- spectively for the DUC-04 and TAC-11 datasets. We explore variants of the PG-MMR method.</p><p>They differ in how the importances of source sen- tences are estimated and how the sentence impor- tance affects word attention weights. "w/ Cosine" computes the sentence importance as the cosine similarity score between the sentence and docu- ment vectors, both represented as sparse TF-IDF vectors under the vector space model. "w/ Summ- Rec" estimates the sentence importance as the predicted R-L recall score between the sentence and the summary. A support vector regression model is trained on sentences from the CNN/Daily Mail datasets (≈33K) and applied to DUC/TAC sentences at test time (see §4). "w/ BestSumm- Rec" obtains the best estimate of sentence impor- tance by calculating the R-L recall score between the sentence and reference summaries. It serves as an upper bound for the performance of "w/ SummRec." For all variants, the sentence impor- tance scores are normalized to the range of <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. "w/ SentAttn" adjusts the attention weights using Eq. <ref type="formula">(9)</ref>, so that words in important sentences are more likely to be used to generate the summary. The weights are otherwise computed using Eq. (8).</p><p>As seen in <ref type="table" target="#tab_2">Table 2</ref> and 3, our PG-MMR method surpasses all unsupervised extractive baselines, in- cluding SumBasic, KLSumm, and LexRank. On the DUC-04 dataset, ICSISumm and DPP show good performance, but these systems are trained directly on MDS datasets, which are not utilized by the PG-MMR method. PG-MMR exhibits su- perior performance compared to existing abstrac- tive systems. It outperforms Opinosis and PG- Original by a large margin in terms of R-2 F-scores (5.03/6.03/8.73 for DUC-04 and 5.12/6.40/10.92 for TAC-11). In particular, PG-Original is the original pointer-generator networks with multi- document inputs at test time. Compared to it, PG- MMR is more effective at identifying summary- worthy content from the input. "w/ Cosine" is used as the default PG-MMR and it shows bet- ter results than "w/ SummRec." It suggests that the sentence and document representations ob- tained from the encoder-decoder model (trained on CNN/DM) are suboptimal, possibly due to a vocabulary mismatch, where certain words in the DUC/TAC datasets do not appear in CNN/DM and their embeddings are thus not learned during train- ing. Finally, we observe that "w/ BestSummRec" yields the highest performance on both datasets. This finding suggests that there is a great potential for improvements of the PG-MMR method as its "extractive" and "abstractive" components can be separately optimized.    Location of summary content. We are inter- ested in understanding why PG-MMR outper- forms PG-Original at identifying summary content from the multi-document input. We ask the ques- tion: where, in the source documents, does each system tend to look when generating their sum- maries? Our findings indicate that PG-Original gravitates towards early source sentences, while PG-MMR searches beyond the first few sentences. In <ref type="figure" target="#fig_1">Figure 2</ref> we show the median location of the first occurrences of summary n-grams, where the n-grams can come from the 1st to 5th summary sentence. For PG-Original summaries, n-grams of the 1st summary sentence frequently come from the 1st and 2nd source sentences, corresponding to the lower/higher quartiles of source sentence in- dices. Similarly, n-grams of the 2nd summary sen- tence come from the 2nd to 7th source sentences. For PG-MMR summaries, the patterns are differ- ent. The n-grams of the 1st and 2nd summary sen- tences come from source sentences of the range (2, 44) and (6, 53), respectively. Our findings sug- gest that PG-Original tends to treat the input as a single-document and identifies summary-worthy content from the beginning of the input, whereas PG-MMR can successfuly search a broader range of the input for summary content. This capability is crucial for multi-document input where impor- tant content can come from any article in the set.</p><p>Degree of extractiveness.  <ref type="table">Table 5</ref>: Linguistic quality and rankings of system summaries. (DUC-04)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Abstract</head><p>• Boeing 737-400 plane with 102 people on board crashed into a moun- tain in the West Sulawesi province of Indonesia, on Monday, January 01, 2007, killing at least 90 passengers, with 12 possible survivors.</p><p>• The plane was Adam Air flight KI-574, departing at 12:59 pm from Surabaya on Java bound for Manado in northeast Sulawesi.</p><p>• The plane crashed in a mountainous region in Polewali, west Su- lawesi province.</p><p>• There were three Americans on board, it is not know if they survived.</p><p>• The cause of the crash is not known at this time but it is possible bad weather was a factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extract+Rewrite Summary</head><p>• Plane with 102 people on board crashes.</p><p>• Three Americans among 102 on board plane in Indonesia.</p><p>• Rescue team arrives in Indonesia after plane crash.</p><p>• Plane with 102 crashes in West Sulawesi, killing at least 90.</p><p>• No word on the fate of Boeing 737-400.</p><p>• Plane carrying 96 passengers loses contact with Makassar.</p><p>• Plane crashes in Indonesia , killing at least 90.</p><p>• Indonesian navy sends two planes to carry bodies of five.</p><p>• Indonesian plane carrying 102 missing.</p><p>• Indonesian lawmaker criticises slow deployment of plane.</p><p>• Hundreds of kilometers plane crash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG-Original Summary</head><p>• Adam Air Boeing 737-400 crashed Monday after vanishing off air traffic control radar screens between the Indonesian islands of Java and Sulawesi.</p><p>• Up to 12 people were thought to have survived, with rescue teams racing to the crash site near Polewali in West Sulawesi , some 180 kilo- metres north of the South Sulawesi provincial capital Makassar.</p><p>• It was the worst air disaster since Sept. <ref type="bibr">5,</ref><ref type="bibr">2005</ref>, when a Mandala Air- line's Boeing 737-200 crashed shortly after taking off from the North Sumatra's airport, killing 103 people.</p><p>• Earlier on Friday, a ferry carrying 628 people sank off the Java coast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PG-MMR Summary</head><p>• The Adam Air Boeing 737-400 crashed Monday afternoon, but search and rescue teams only discovered the wreckage early Tuesday.</p><p>• The Indonesian rescue team arrived at the mountainous area in West Sulawesi province where a passenger plane with 102 people onboard crashed into a mountain in Polewali, West Sulawesi province.</p><p>• Air force rear commander Eddy Suyanto told-Shinta radio station that the plane -operated by local carrier Adam Air -had crashed in a mountainous region in Polewali province on Monday.</p><p>• There was no word on the fate of the remaining 12 people on board the boeing 737-400. percentages of summary n-grams (or entire sen- tences) appearing in the multi-document input. PG-Original and PG-MMR summaries both show a high degree of extractiveness, and similar find- ings have been revealed by <ref type="bibr" target="#b28">See et al. (2017)</ref>. Because PG-MMR relies on a handful of rep- resentative source sentences and mutes the rest, it appears to be marginally more extractive than PG-Original. Both systems encourage generating summary sentences by stitching together source sentences, as about 52% and 41% of the sum- mary sentences do not appear in the source, but about 90% the n-grams do. The Extract+Rewrite summaries ( §5), generated by rewriting selected source sentences to title-like summary sentences, exhibits a high degree of abstraction, close to that of human abstracts.</p><p>Linguistic quality. To assess the linguistic quality of various system summaries, we employ Amazon Mechanical Turk human evaluators to judge the summary quality, including PG-MMR, LexRank, PG-Original, and Extract+Rewrite. A turker is asked to rate each system summary on a scale of 1 (worst) to 5 (best) based on three evaluation crite- ria: informativeness (to what extent is the mean- ing expressed in the ground-truth text preserved in the summary?), fluency (is the summary gram- matical and well-formed?), and non-redundancy (does the summary successfully avoid repeating information?). Human summaries are used as the ground-truth. The turkers are also asked to provide an overall ranking for the four system summaries. Results are presented in <ref type="table">Table 5</ref>. We observe that the LexRank summaries are highest-rated on flu- ency. This is because LexRank is an extractive approach, where summary sentences are directly taken from the input. PG-MMR is rated as the best on both informativeness and non-redundancy. Re- garding overall system rankings, PG-MMR sum- maries are frequently ranked as the 1st-and 2nd- best summaries, outperforming the others.</p><p>Example summaries. In <ref type="table" target="#tab_8">Table 6</ref> we present example summaries generated by various sys- tems. PG-Original cannot effectively identify im- portant content from the multi-document input. Extract+Rewrite tends to generate short, title-like sentences that are less informative and carry sub- stantial redundancy. This is because the system is trained on the Gigaword dataset ( <ref type="bibr" target="#b26">Rush et al., 2015)</ref> where the target summary length is 7 words. PG- MMR generates summaries that effectively con- dense the important source content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We describe a novel adaptation method to gen- erate abstractive summaries from multi-document inputs. Our method combines an extractive sum- marization algorithm (MMR) for sentence extrac- tion and a recent abstractive model (PG) for fusing source sentences. The PG-MMR system demon- strates competitive results, outperforming strong extractive and abstractive baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System framework. The PG-MMR system uses K highest-scored source sentences (in this case, K=2) to guide the PG model to generate a summary sentence. All other source sentences are "muted" in this process. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The median location of summary n-grams in the multi-document input (and the lower/higher quartiles). The n-grams come from the 1st/2nd/3rd/4th/5th summary sentence and the location is the source sentence index. (TAC-11) System 1-grams 2-grams 3-grams Sent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>A comparison of datasets available for sent. sum-

marization (Gigaword), single-doc (CNN/DM) and multi-doc 
summarization (DUC/TAC). The labelled data for multi-doc 
summarization are much less. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : ROUGE results on the DUC-04 dataset.</head><label>2</label><figDesc></figDesc><table>TAC-11 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Percentages of summary n-grams (or the entire sen-

tences) appear in the multi-document input. (TAC-11) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 shows the</head><label>4</label><figDesc></figDesc><table>Linguistic Quality 
Rankings (%) 
System 
Fluency Inform. NonRed. 
1st 
2nd 
3rd 
4th 

Extract+Rewrite 
2.03 
2.19 
1.88 
5.6 
11.6 
11.6 
71.2 
LexRank 
3.29 
3.36 
3.30 
30.0 
28.8 
32.0 
9.2 
PG-Original 
3.20 
3.30 
3.19 
29.6 
26.8 
32.8 
10.8 
PG-MMR 
3.24 
3.52 
3.42 
34.8 
32.8 
23.6 
8.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Example system summaries and human-written abstract. The sentences are manually de-tokenized for readability.</figDesc><table></table></figure>

			<note place="foot" n="1"> Here [·||·] represents the concatenation of two vectors. The pointer-generator networks (See et al., 2017) use two linear layers to produce the vocabulary distribution P vcb (w). We use W y and b y to denote parameters of both layers.</note>

			<note place="foot" n="2"> https://github.com/ucfnlp/multidoc summarization</note>

			<note place="foot" n="3"> The hyperparameters for all PG-MMR variants are K=7 and λ=0.6; except for &quot;w/ BestSummRec&quot; where K=2. 4 We are grateful to Hong et al. (2014) for providing the summaries generated by Centroid, ICSISumm, DPP systems. These are only available for the DUC-04 dataset.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Integer Linear Programming for Natural Langauge Processing</title>
		<meeting>the NAACL Workshop on Integer Linear Programming for Natural Langauge Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ICSI/UTD summarization system at TAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berndt</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC</title>
		<meeting>TAC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring content models for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A repository of state of the art and competitive baseline summaries for generic news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extractive summarization using multi-task learning with document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Isonuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The decomposition of human-written summary sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving abstraction in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Kry´sci´nskikry´sci´kry´sci´nski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>COLING</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ROUGE: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Text Summarization Branches Out</title>
		<meeting>ACL Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarizing student responses to reflection prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic summarization of student course feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic summarization. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An introduction to DUC-2004</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modelling events through memory-based, open-ie patterns for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cornolti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural attention model for sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure-infused copy mechanisms for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>COLING</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural headline generation on abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
