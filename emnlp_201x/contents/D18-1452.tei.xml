<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<email>srjoty@ntu.edu.sg, lluismv@amazon.com, pnakov@qf.org.qa</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<settlement>Singapore Amazon, Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="4196" to="4207"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We address jointly two important tasks for Question Answering in community forums: given a new question, (i) find related existing questions, and (ii) find relevant answers to this new question. We further use an auxiliary task to complement the previous two, i.e., (iii) find good answers with respect to the thread question in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting , performing joint learning over a complex graph structure. While DNNs alone achieve competitive results when trained to produce the embeddings, the CRF, which makes use of the embeddings and the dependencies between the tasks, improves the results significantly and consistently across a variety of evaluation metrics, thus showing the complemen-tarity of DNNs and structured learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>Question answering web forums such as Stack- Overflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question-comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often re- sults in noisy and redundant content.</p><p>Within community Question Answering (cQA) forums, two subtasks are of special relevance when a user poses a new question to the website ( <ref type="bibr" target="#b12">Hoogeveen et al., 2018;</ref><ref type="bibr">Lai et al., 2018</ref>): (i) find- ing similar questions (question-question related- ness), and (ii) finding relevant answers to the new question, if they already exist (answer selection). * Work conducted while this author was at QCRI, HBKU.</p><p>Both subtasks have been the focus of recent re- search as they result in end-user applications. The former is interesting for a user who wants to ex- plore the space of similar questions in the forum and to decide whether to post a new question. It can also be relevant for the forum owners as it can help detect redundancy, eliminate question duplicates, and improve the overall forum struc- ture. Subtask (ii) on the other hand is useful for a user who just wants a quick answer to a spe- cific question, without the need of digging through the long answer threads and winnowing good from bad comments or without having to post a question and then wait for an answer.</p><p>Obviously, the two subtasks are interrelated as the information needed to answer a new ques- tion is usually found in the threads of highly re- lated questions. Here, we focus on jointly solv- ing the two subtasks with the help of yet another related subtask, i.e., determining whether a com- ment within a question-comment thread is a good answer to the question heading that thread.</p><p>An example is shown in <ref type="figure">Figure 1</ref>. A new ques- tion q is posed for which several potentially related questions are identified in the forum (e.g., by us- ing an information retrieval system); q i in the ex- ample is one of these existing questions. Each re- trieved question comes with an associated thread of comments; c i m represents one comment from the thread of question q i . Here, c i m is a good an- swer for q i , q i is indeed a question related to q, and consequently c i m is a relevant answer for the new question q. This is the setting of <ref type="bibr">SemEval-2016</ref> Task 3, and we use its benchmark datasets.</p><p>Our approach has two steps. First, a deep neu- ral network (DNN) in the form of a feed-forward neural network is trained to solve each of the three subtasks separately, and the subtask-specific hid- den layer activations are taken as embedded fea- ture representations to be used in the second step. q: "How can I extend a family visit visa?" qi: "Dear All; I wonder if anyone knows the procedure how I can extend the family visit visa for my wife be- yond 6 months. I already extended it for 5 months and is 6 months running. I would like to get it extended for couple of months more.Any suggestion is highly appre- ciable.Thanks" c i m : "You can get just another month's extension before she completes 6 months by presenting to immigration of- fice a confirmed booking of her return ticket which must not exceed 7 months."</p><p>Figure 1: Example of the three pieces of informa- tion in the cQA problems addressed in this paper.</p><p>Then, a conditional random field (CRF) model uses these embeddings and performs joint learning with global inference to exploit the dependencies between the subtasks.</p><p>A key strength of DNNs is their ability to learn nonlinear interactions between underlying features through specifically-designed hidden lay- ers, and also to learn the features (e.g., vectors for words and documents) automatically. This capa- bility has led to gains in many unstructured output problems. DNNs are also powerful for structured output problems. Previous work has mostly relied on recurrent or recursive architectures to propa- gate information through hidden layers, but has been disregarding the modeling strength of struc- tured conditional models, which use global infer- ence to model consistency in the output structure (i.e., the class labels of all nodes in a graph). In this work, we explore the idea that combining sim- ple DNNs with structured conditional models can be an effective and efficient approach for cQA sub- tasks that offers the best of both worlds.</p><p>Our experimental results show that: (i) DNNs already perform very well on the question- question similarity and answer selection subtasks; (ii) strong dependencies exist between the sub- tasks under study, especially answer-goodness and question-question-relatedness influence answer- selection significantly; (iii) the CRFs exploit the dependencies between subtasks, providing size- ably better results that are on par or above the state of the art. In summary, we demonstrate the ef- fectiveness of this marriage of DNNs and struc- tured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then "rec- onciled" in a multitask CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various neural models have been applied to cQA tasks such as question-question similarity (dos <ref type="bibr" target="#b31">Santos et al., 2015;</ref><ref type="bibr">Lei et al., 2016;</ref> and answer selection ( <ref type="bibr" target="#b39">Wang and Nyberg, 2015;</ref><ref type="bibr" target="#b30">Qiu and Huang, 2015;</ref><ref type="bibr" target="#b36">Tan et al., 2015;</ref><ref type="bibr" target="#b3">Chen and Bunescu, 2017;</ref><ref type="bibr" target="#b42">Wu et al., 2018</ref>). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos <ref type="bibr" target="#b31">Santos et al. (2015)</ref> combined CNN and bag of words for com- paring questions. <ref type="bibr" target="#b36">Tan et al. (2015)</ref> adopted an at- tention mechanism over bidirectional LSTMs to generate better answer representations, and <ref type="bibr">Lei et al. (2016)</ref> combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neu- ral network, which we only use to generate task- specific embeddings, and we defer the joint learn- ing with global inference to the structured model.</p><p>From the perspective of modeling cQA sub- tasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question-comment thread. This has been done from a feature engi- neering perspective, by modeling a comment in the context of the entire thread ( ), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers col- lectively. For example,  treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs.  modeled the relations between pairs of comments at any distance in the thread, and combined the predic- tions of local classifiers using graph-cut and In- teger Linear Programming. In a follow up work,  also modeled the relations be- tween all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Un- like these models, we use DNNs to induce task- specific embeddings, and, more importantly, we perform multitask learning of three different cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a mul- titask learning framework. We do this using the datasets from the SemEval-2016 Task 3 on Com- munity Question Answering ( ), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SUPER TEAM ( <ref type="bibr" target="#b20">Mihaylova et al., 2016</ref>) and KELP ( <ref type="bibr" target="#b7">Filice et al., 2016)</ref>, stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embed- dings, and it is also lighter in terms of features.</p><p>In work following the competition,  used a triangulation approach to answer ranking in cQA, modeling the three types of sim- ilarities occurring in the triangle formed by the original question, the related question, and an an- swer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model. Moreover, they focus on one task only, while we use multitask learning. <ref type="bibr" target="#b2">Bonadiman et al. (2017)</ref> proposed a multitask neural architecture where the three tasks are trained together with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train task- specific embeddings, as we do.</p><p>The general idea of combining DNNs and struc- tured models has been explored recently for other NLP tasks. <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> used Viterbi inference to train their DNN models to capture de- pendencies between word-level tags for a number of sequence labeling tasks: part-of-speech tag- ging, chunking, named entity recognition, and se- mantic role labeling.  pro- posed an LSTM-CRF framework for such tasks. <ref type="bibr">Ma and Hovy (2016)</ref> included a CNN in the framework to compute word representations from character-level embeddings. While these studies consider tasks related to constituents in a sentence, e.g., words and phrases, we focus on methods to represent comments and to model dependencies between comment-level tags. We also experiment with arbitrary graph structures in our CRF model to model dependencies at different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Approach</head><p>Let q be a newly-posed question, and c i m denote the m-th comment (m ∈ {1, 2, . . . , M }) in the answer thread for the i-th potentially related ques- tion q i (i ∈ {1, 2, . . . , I}) retrieved from the fo- rum. We can define three cQA subtasks: (A) clas- sify each comment c i m in the thread for question q i as Good vs. Bad with respect to q i ; (B) determine, for each retrieved question q i , whether it is Related to the new question q in the sense that a good an- swer to q i might also be a good answer to q; and finally, (C) classify each comment c i m in each an- swer thread as either Relevant or Irrelevant with respect to the new question q.</p><p>Let y a i,m ∈ {Good, Bad}, y b i ∈ {Related, N ot- related}, and y c i,m ∈ {Relevant, Irrelevant} denote the corresponding output labels for sub- tasks A, B, and C, respectively. As argued before, subtask C depends on the other two subtasks. In- tuitively, if c i m is a good comment with respect to the existing question q i , and q i is related to the new question q (subtask A), then c i m is likely to be a relevant answer to q. Similarly, subtask B can benefit from subtask C: if comment c i m in the an- swer thread of q i is relevant with respect to q, then q i is likely to be related to q.</p><p>We propose to exploit these inherent correla- tions between the cQA subtasks as follows: (i) by modeling their interactions in the input represen- tations, i.e., in the feature space of (q, q i , c i m ), and more importantly, (ii) by capturing the dependen- cies between the output variables (y a i,m , y b i , y c i,m ). Moreover, we cast each cQA subtask as a struc- tured prediction problem in order to model the de- pendencies between output variables of the same type. Our intuition is that if two comments c i m and c i n in the same thread are similar, then they are likely to have the same labels for both subtask A and subtask C, i.e., y a i,m ≈ y a i,n , and y c i,m ≈ y c i,n . Similarly, if two pre-existing questions q i and q j are similar, they are also likely to have the same labels, i.e., y b i ≈ y b j . Our framework works in two steps. First, we use a DNN, specifically, a feed-forward NN, to learn task-specific embeddings for the three sub- tasks, i.e., output embeddings x a i,m , x b i and x c i,m for subtasks A, B and C ( <ref type="figure" target="#fig_0">Figure 2a</ref>). The DNN uses syntactic and semantic embeddings of the in- put elements, their interactions, and other similar- ity features between them and, as a by-product, learns the output embeddings for each subtask.</p><p>In the second step, a structured conditional model operates on subtask-specific embeddings from the DNNs and captures the dependencies between the</p><formula xml:id="formula_0">! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! ! ! !! y a i,m y b i y c i,m ! !! ! !! q q i c i m ! !! x b i x c i,m x a i, m Task-specific embedding! Input embedding! Interaction layer! φ q i (" )! ,c i m a φ q (" )! , c i m c φ (" )! q,q i b h b 1" h c 1" h a 1" h b 2" h c 2" h a 2"</formula><p>(a) Our feed-forward neural networks subtasks, between existing questions, and between comments for an existing question ( <ref type="figure" target="#fig_0">Figure 2b</ref>). Below, we describe the two steps in detail. ) using their syntactic and semantic embeddings. Depending on the requirements of the subtasks, the network then models the interactions between the inputs by passing their embeddings through non-linear hidden layers ν(·). Additionally, the network also considers pairwise similarity features φ(·) be- tween two input elements that go directly to the output layer, and also through the last hidden layer. The pairwise features together with the activations at the final hidden layer constitute the task-specific embeddings for each subtask t:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Models for cQA Subtasks</head><formula xml:id="formula_1">x t i = [ν t (·), φ t (·)].</formula><p>The final layer defines a Bernoulli distribution for each subtask t ∈ {a, b, c}:</p><formula xml:id="formula_2">p(y t i |q, q i , c i m , θ) = Ber(y t i | sig(w T t x t i )) (1)</formula><p>where x t i , w t , and y t i are the task-specific em- bedding, the output layer weights, and the predic- tion variable for subtask t, respectively, and sig(·) refers to the sigmoid function. We train the models by minimizing the cross- entropy between the predicted distribution and the gold labels. The main difference between the models is how they compute the task-specific em- beddings x t i for subtask t. Neural Model for Subtask A. </p><formula xml:id="formula_3">h a 1 = f (U a [z q i , z c i m ])<label>(2)</label></formula><p>where U a is the weight matrix from the inputs to the first hidden units, f is a non-linear activa- tion function. The activations are then fed to a fi- nal subtask-specific hidden layer, which combines these signals with the pairwise similarity features φ a (q i , c i m ). Formally,</p><formula xml:id="formula_4">h a 2 = f (V a [h a 1 , φ a (q i , c i m )])<label>(3)</label></formula><p>where V a is the weight matrix. The task-specific output embedding is formed by merging h a 2 and</p><formula xml:id="formula_5">φ a (q i , c i m ); x a i,m = [h a 2 , φ a (q i , c i m )]</formula><p>. Neural Model for Subtask B. To determine whether an existing question q i is related to the new question q, we model the interactions between q and q i using their embeddings and pairwise sim- ilarity features similarly to subtask A. The upper part of <ref type="figure" target="#fig_0">Figure 2a</ref> shows the network. The transformation is defined as follows:</p><formula xml:id="formula_6">h b 1 = f (U b [z q , z q i ]); h b 2 = f (V b [h b 1 , φ b (q, q i )])</formula><p>where U b and V b are the weight matrices in the first and second hidden layer. The task-specific embedding is formed by</p><formula xml:id="formula_7">x b i = [h b 2 , φ b (q, q i )].</formula><p>Neural Model for Subtask C. The network for subtask C is shown in the middle of <ref type="figure" target="#fig_0">Figure 2a</ref>.</p><p>To decide if a comment c i m in the thread of q i is relevant to q, we consider how related q i is to q, and how useful c i m is to answer q i . Again, we model the direct interactions between q and c i m us- ing pairwise features φ c (q, c i m ) and a hidden layer transformation</p><formula xml:id="formula_8">h c 1 = f (U c [z q , z c i m ])</formula><p>, where U c is a weight matrix. We then include a second hidden layer to combine the activations from different in- puts and pairwise similarity features. Formally,</p><formula xml:id="formula_9">h c 2 = f (V c [h a 1 , h b 1 , h c 1 , φ a (qi, c i m ), φ b (q, qi), φ c (q, c i m )])</formula><p>The final task-specific embedding for subtask C is formed as</p><formula xml:id="formula_10">x c i = [h c 2 , φ a (qi, c i m ), φ b (q, qi), φ c (q, c i m )].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Learning with Global Inference</head><p>One simple way to exploit the interdependencies between the subtask-specific embeddings (x a i,m ,</p><formula xml:id="formula_11">x b i , x c i,m )</formula><p>is to precompute the predictions for some subtasks (A and B), and then to use the pre- dictions as features for the other subtask (C). How- ever, as shown later in Section 6, such a pipeline approach propagates errors from one subtask to the subsequent ones. A more robust way is to build a joint model for all subtasks.</p><p>We could use the full DNN network in <ref type="figure" target="#fig_0">Figure 2a</ref> to learn the classification functions for the three subtasks jointly as follows:</p><formula xml:id="formula_12">p(y a i,m , y b i , y c i,m |θ) = p(y a i,m |θa)p(y b i |θ b )p(y c i,m |θc)<label>(4)</label></formula><p>where θ = [θ a , θ b , θ c ] are the model parameters. However, this has two key limitations: (i) it as- sumes conditional independence between the sub- tasks given the parameters; (ii) the scores are nor- malized locally, which leads to the so-called label bias problem ( <ref type="bibr">Lafferty et al., 2001</ref>), i.e., the fea- tures for one subtask would have no influence on the other subtasks.</p><p>Thus, we model the dependencies between the output variables by learning (globally nor- malized) node and edge factor functions that jointly optimize a global performance criterion.</p><p>In particular, we represent the cQA setting as a large undirected graph G=(V, E)=(V a ∪V b ∪V c , E aa ∪E bb ∪E cc ∪E ac ∪E bc ∪E ab ). As shown in <ref type="figure" target="#fig_0">Figure 2b</ref>, the graph contains six subgraphs:</p><formula xml:id="formula_13">G a =(V a</formula><note type="other">, E aa ), G b =(V b , E bb ) and G c =(V c , E cc ) are associated with the three subtasks, while the bipartite subgraphs G</note><formula xml:id="formula_14">ac =(V a ∪ V c , E ac ), G bc =(V b ∪ V c , E bc ) and G ab =(V a ∪ V b , E ab ) con- nect</formula><note type="other">nodes across tasks. We associate each node u ∈ V t with an input vector x u , representing the embedding for sub- task t, and an output variable y u , representing the class label for subtask t. Similarly, each edge (u, v) ∈ E st is associated with an input feature vector µ(x u , x v ), derived from the node-level fea- tures, and an output variable y uv ∈ {1, 2, · · · , L}, representing the state transitions for the pair of nodes. 1 For notational simplicity, here we do not distinguish between comment and question nodes, rather we use u and v as general indices. We de- fine the following joint conditional distribution:</note><formula xml:id="formula_15">p(y|θ, x) = 1 Z(θ, x) t∈τ u∈V t ψn(yu|x, w t n ) (s,t)∈τ ×τ (u,v)∈E st ψe(yuv|x, w st e )<label>(5)</label></formula><p>where τ = {a, b, c}, ψ n (·) and ψ e (·) are node and edge factors, respectively, and Z(·) is a global nor- malization constant. We use log-linear factors:</p><formula xml:id="formula_16">ψn(yu|x, w t n ) = exp(σ(yu, x) T w t n )<label>(6)</label></formula><p>ψe(yuv|x, w st e ) = exp(σ(yuv, x) T w st e )</p><p>where σ(·) is a feature vector derived from the in- puts and the labels. This model is essentially a pairwise conditional random field (Murphy, 2012). The global normal- ization allows CRFs to surmount the label bias problem, allowing them to take long-range inter- actions into account. The objective in Equation 5 is a convex function, and thus we can use gradient- based methods to find the global optimum. The gradients have the following form:</p><formula xml:id="formula_18">f (w t n ) = u∈V t σ(yu, x) − E[σ(yu, x)]<label>(8)</label></formula><formula xml:id="formula_19">f (w st e ) = (u,v)∈E st σ(yuv, x) − E[σ(yuv, x)]<label>(9)</label></formula><p>where E[φ(·)] is the expected feature vector.</p><p>Training and Inference. Traditionally, CRFs have been trained using offline methods like LBFGS <ref type="bibr" target="#b24">(Murphy, 2012)</ref>. Online training using first-order methods such as stochastic gradient de- scent was proposed by <ref type="bibr" target="#b38">Vishwanathan et al. (2006)</ref>. Since our DNNs are trained with the RMSprop online adaptive algorithm <ref type="bibr" target="#b37">(Tieleman and Hinton, 2012)</ref>, in order to compare our two models, we use RMSprop to train our CRFs as well.</p><p>For our CRF models, we use Belief Propaga- tion, or BP, <ref type="bibr" target="#b29">(Pearl, 1988)</ref> for inference. BP con- verges to an exact solution for trees. However, ex- act inference is intractable for graphs with loops. Despite this, <ref type="bibr" target="#b29">Pearl (1988)</ref> advocated for the use of BP in loopy graphs as an approximation. Even though BP only gives approximate solutions, it of- ten works well in practice for loopy graphs <ref type="bibr" target="#b25">(Murphy et al., 1999</ref>), outperforming other methods such as mean field <ref type="bibr" target="#b41">(Weiss, 2001</ref>).</p><p>Variations of Graph Structures. A crucial ad- vantage of our CRFs is that we can use arbitrary graph structures, which allows us to capture de- pendencies between different types of variables: (i) intra-subtask, for variables of the same sub- task, e.g., y b i and y b j in <ref type="figure" target="#fig_0">Figure 2b</ref>, and (ii) across- subtask, for variables of different subtasks.</p><p>For intra-subtask, we explore null (i.e., no con- nection between nodes) and fully-connected rela- tions. For subtasks A and C, the intra-subtask con- nections are restricted to the nodes inside a thread, e.g., we do not connect y c i,m and y c j,m in <ref type="figure" target="#fig_0">Figure 2b</ref>. For across-subtask, we explored three types of connections depending on the subtasks in- volved: (i) null or no connection between sub- tasks, (ii) 1:1 connection for A-C, where the cor- responding nodes of the two subtasks in a thread are connected, e.g., y a i,m and y c i,m in <ref type="figure" target="#fig_0">Figure 2b</ref>, and (iii) M:1 connection to B, where we con- nect all the nodes of C or A to the thread-level B node. Each configuration of intra-and across- connections yields a different CRF model. <ref type="figure" target="#fig_0">Fig- ure 2b</ref> shows one such model for two threads each containing two comments, where all subtasks have fully-connected intra-subtask links, 1:1 connec- tion for A-C, and M:1 for C-B and A-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features for the DNN Models</head><p>We have two types of features: (i) input embed- dings, for q, q i and c i m , and (ii) pairwise features, for (q, q i ), (q, c i m ), and (q i , c i m ) -see <ref type="figure" target="#fig_0">Figure 2a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Input Embeddings</head><p>We use three types of pre-trained vectors to repre- sent a question (q or q i ) or a comment (c i m ):</p><p>GOOGLE VECTORS. 300-dimensional em- bedding vectors, trained on 100 billion words from Google News ( <ref type="bibr" target="#b23">Mikolov et al., 2013</ref>). The embed- ding for a question (or comment) is the average of the word embeddings it is composed of.</p><p>SYNTAX. We parse the question (or comment) using the Stanford neural parser <ref type="bibr" target="#b34">(Socher et al., 2013)</ref>, and we use the final 25-dimensional vec- tor produced internally as a by-product of parsing.</p><p>QL VECTORS. We use fine-tuned word em- beddings pretrained on all the available in-domain Qatar Living data ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pairwise Features</head><p>We extract pairwise features for each of (q, q i ), (q, c i m ), and (q i , c i m ) pairs. These include: COSINES. We compute cosines using the above vectors: cos(q, q i ), cos(q, c i m ) and cos(q i , c i m ). MT FEATURES. We use the following ma- chine translation evaluation metrics: <ref type="formula">(1)</ref>  BLEU COMPONENTS. We further use var- ious components involved in the computation of BLEU: 2 n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), lengths of the hy- potheses and of the reference, length ratio between them, and BLEU's brevity penalty.</p><p>QUESTION-COMMENT RATIO.</p><p>(1) question- to-comment count ratio in terms of senten- ces/tokens/nouns/verbs/adjectives/adverbs/pronouns; (2) question-to-comment count ratio of words that are not in WORD2VEC's Google News vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node Features</head><p>COMMENT FEATURES. These include number of (1) nouns/verbs/adjectives/adverbs/pronouns, (2) URLs/images/emails/phone numbers, (3) to- kens/sentences, (4) positive/negative smileys, (5) single/double/triple exclamation/interrogation symbols, (6) interrogative sentences, (7) 'thank' mentions, (8) words that are not in WORD2VEC's Google News vocabulary. Also, (9) average num- ber of tokens, and (10) word type-to-token ratio.</p><p>META FEATURES. (1) is the person answering the question the one who asked it; (2) reciprocal rank of comment c i m in the thread of q i , i.e., 1/m; batch dropout reg. str inter. layer task-spec. layer  (3) reciprocal rank of c i m in the list of comments for q, i.e., 1/[m+10×(i − 1)]; and (4) reciprocal rank of question q i in the list for q, i.e., 1/i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data and Settings</head><p>We experiment with the data from <ref type="bibr">SemEval-2016</ref><ref type="bibr">Task 3 (Nakov et al., 2016b</ref>. Consistently with our notation from Section 3, it features three sub- tasks: subtask A (i.e., whether a comment c i m is a good answer to the question q i in the thread), subtask B (i.e., whether the retrieved question q i is related to the new question q), and subtask C (i.e., whether the comment c i m is a relevant answer for the new question q). Note that the two main subtasks we are interested in are B and C. DNN Setting. We preprocess the data using min-max scaling. We use RMSprop 3 for learn- ing, with parameters set to the values suggested by <ref type="bibr" target="#b37">Tieleman and Hinton (2012)</ref>. We use up to 100 epochs with patience of 25, rectified linear units (ReLU) as activation functions, l 2 regularization on weights, and dropout ( <ref type="bibr" target="#b35">Srivastava et al., 2014</ref>) of hidden units. See <ref type="table" target="#tab_0">Table 1</ref> for more detail. CRF Setting. For the CRF model, we initial- ize the node-level weights from the output layer weights of the DNNs, and we set the edge-level weights to 0. Then, we train using RMSprop with loopy BP. We regularize the node parameters ac- cording to the best settings of the DNN: 0.001, 0.05, and 0.0001 for A, B, and C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>Below, we first present the evaluation results using DNN models (Section 6.1). Then, we discuss the performance of the joint models (Section 6.2). <ref type="table">Table 2</ref> shows the results for our individual DNN models (rows in boldface) for subtasks A, B and C on the TEST set. We report three ranking-based measures that are commonly accepted in the IR community: mean average precision (MAP), which was the official  <ref type="table">Table 2</ref>: Results for our DNN models on all cQA subtasks, compared to the top-2 systems from SemEval-2016 Task 3. Inter-subtask dependencies are explored using gold output labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results for the DNN Models</head><p>evaluation measure of SemEval-2016, average re- call (AvgRec), and mean reciprocal rank (MRR).</p><p>For each subtask, we show two baselines and the results of the top-2 systems at SemEval. The first baseline is a random ordering of the ques- tions/comments, assuming no knowledge about the subtask. The second baseline keeps the chronological order of the comments for subtask A, of the question ranking from the IR engine for subtask B, and both for subtask C.</p><p>We can see that the individual DNN models for subtasks B and C are very competitive, falling be- tween the first and the second best at SemEval- 2016. For subtask A, our model is weaker, but, as we will see below, it can help improve the results for subtasks B and C, which are our focus here.</p><p>Looking at the results for subtask C, we can see that sizeable gains are possible when using gold labels for subtasks A and B as features to DNN C , e.g., adding gold A labels yields +6.90 MAP points. Similarly, using gold labels for subtask B adds +2.05 MAP points absolute. Moreover, the gain is cumulative: using the two gold labels together yields +9.25 MAP points. The same behavior is observed for the other evaluation measures. Of course, as we use gold labels, this is an upper bound on performance, but it justifies our efforts towards a joint multitask learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results for the Joint Model</head><p>Below we discuss the evaluation results for the joint model. We focus on subtasks B and C, which are the main target of our study.</p><p>Results for Subtask C. <ref type="table" target="#tab_3">Table 3</ref> compares sev- eral variants of the CRF model for joint learning, which we described in Section 3.2 above.</p><p>Row 1 shows the results for our individual DNN C model. The following rows 2-4 present a pipeline approach, where we first predict labels for subtasks A and B and then we add these predic- tions as features to DNN C . This is prone to error propagation, and improvements are moderate and inconsistent across the evaluation measures.</p><p>The remaining rows correspond to variants of our CRF model with different graph structures. Overall, the improvements over DNN C are more sizeable than for the pipeline approach (with one single exception out of 24 cases); they are also more consistent across the evaluation measures, and the improvements in MAP over the baseline range from +0.96 to +1.76 points absolute.</p><p>Rows 5-8 show the impact of adding connec- tions to subtasks A and B when solving subtask C (see <ref type="figure" target="#fig_0">Figure 2b)</ref>. Interestingly, we observe the same pattern as with the gold labels: the A-C and B-C connections help individually and in combi- nation, with A-C being more helpful. Yet, further adding A-B does not improve the results (row 8).</p><p>Note that the locally normalized joint model in Eq. 4 yields much lower results than the glob- ally normalized CRF all (row 8): 54.32, 59.87, and 61.76 in MAP, AvgRec and MRR (figures not in- cluded in the table for brevity). This evinces the problems with the conditional independence as- sumption and the local normalization in the model. Finally, rows 9-12 explore variants of the best system from the previous set (row 7), which has connections between subtasks only. Rows 9-12 show the results when using subgraphs for A, B and C that are fully connected (i.e., for all pairs). We can see that none of these variants yields im- provements over the model from row 7, i.e., the fine-grained relations between comments in the threads and between the different related questions do not seem to help solve subtask C in the joint model. Note that our scores from row 7 are bet- ter than the best results achieved by a system at SemEval-2016 Task 3 subtask C: 56.00 vs. 55.41 on MAP, and 63.25 vs. 61.48 on MRR.</p><p>Results for Subtask B. Next, we present in Ta- ble 4 similar experiments, but this time with sub- task B as the target, and we show some more mea- sures (accuracy, precision, recall, and F 1 ).</p><p>Given the insights from <ref type="table">Table 2</ref> (where we used gold labels), we did not expect to see much im- provements for subtask B. Indeed, as rows 2-4 show, using the pipeline approach, the IR mea- sures are basically unaltered. However, classifi- cation accuracy improves by almost one point ab- solute, recall is also higher (trading for lower pre- cision), and F 1 is better by a sizeable margin.</p><p>Coming to the joint models (rows 6-9), we can see that the IR measures improve consistently over the pipeline approach, even though not by much. The effect on accuracy-P-R-F 1 is the same as observed with the pipeline approach but with larger differences. <ref type="bibr">4</ref> In particular, accuracy im- proves by more than two points absolute, and re- call increases, which boosts F 1 to almost 60.</p><p>Row 5 is a special case where we only consider subtask B, but we do the learning and the infer- ence over the set of ten related questions, exploit- ing their relations. This yields a slight increase in all measures; more importantly, it is crucial for obtaining better results with the joint models.</p><p>Rows 6-9 show results for various variants of the A-C and B-C architecture with fully connected B nodes, playing with the fine-grained connec- tion of the A and C nodes. The best results are in this block, with increases over DNN B in MAP (+0.61), AvgRec (+0.69) and MRR (+1.05), and especially in accuracy (+2.18) and F 1 (+11.25 points). This is remarkable given the low expecta- tion we had about improving subtask B.</p><p>Note that the best architecture for subtask C from <ref type="table" target="#tab_3">Table 3</ref> (A-C and B-C with no fully con- nected B layer) does not yield good results for sub- task B. We speculate that subtask B is overlooked by the architecture, which has many more connections and parameters on the nodes for subtasks A and C (ten comments are to be classified for both subtask   <ref type="table">Table 4</ref>: Performance of the pipeline and of the joint models on subtask B (best results in boldface).</p><formula xml:id="formula_20"># System Comments MAP (∆) AvgRec (∆) MRR (∆) 1 DNNC Subtask C</formula><p>A and C, while only one decision is to be made for the related question B). Finally, note that our best results for subtask B are also slightly better than those for the best sys- tem at SemEval-2016 Task 3, especially on MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a framework for multitask learning of two community Question Answering problems: question-question relatedness and an- swer selection. We further used a third, auxil- iary one, i.e., finding the good comments in a question-comment thread. We proposed a two- step framework based on deep neural networks and structured conditional models, with a feed- forward neural network to learn task-specific em- beddings, which are then used in a pairwise CRF as part of a multitask model for all three subtasks.</p><p>The DNN model has its strength in generating compact embedded representations for the sub- tasks by modeling interactions between different input elements. On the other hand, the CRF is able to perform global inference over arbitrary graph structures ac- counting for the dependencies between subtasks to provide globally good solutions. The experi- mental results have proven the suitability of com- bining the two approaches. The DNNs alone al- ready yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results.</p><p>In future work, we plan to model text com- plexity ( <ref type="bibr" target="#b20">Mihaylova et al., 2016)</ref>, veracity , speech act ( <ref type="bibr" target="#b15">Joty and Hoque, 2016)</ref>, user profile ( <ref type="bibr">Mihaylov et al., 2015)</ref>, troll- ness ( <ref type="bibr" target="#b18">Mihaylov et al., 2018)</ref>, and goodness polar- ity ( <ref type="bibr">Mihaylov et al., 2017)</ref>. From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain ( <ref type="bibr" target="#b32">Shah et al., 2018)</ref> or a cross-language setting <ref type="bibr" target="#b5">(Da San Martino et al., 2017;</ref>. Trying an ensemble of neural networks with different initial seeds is an- other possible research direction. <ref type="bibr">Diederik P. Kingma and Jimmy Ba. 2014. Adam:</ref> A method for stochastic optimization. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of our cQA framework. On the left (a), we have three feed-forward neural networks to learn task-specific embeddings for the three cQA subtasks. On the right (b), a global conditional random field (CRF) models intra-and inter-subtask dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2a depicts our complete neural framework for the three subtasks. The input is a tuple (q, q i , c i m ) consisting of a new question q, a retrieved question q i , and a comment c i m from q i 's answer thread. We first map the input elements to fixed-length vectors (z q , z q i , z c i m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The feed- forward network for subtask A is shown in the lower part of Figure 2a. To determine whether a comment c i m is good with respect to the thread question q i , we model the interactions between c i m and q i by merging their embeddings z c i m and z q i , and passing them through a hidden layer:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>BLEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover et al., 2006); (4) METEOR v1.4 (Lavie and Denkowski, 2009); (5) Unigram PRECISION; (6) Unigram RECALL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Best setting for DNNs, as found on DEV.</head><label>1</label><figDesc></figDesc><table>16 
0.3 
0.001 
10 
125 
B 
25 
0.2 
0.05 
5 
75 
C 
32 
0.3 
0.0001 
15 
50 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of the pipeline and of the joint learning models on subtask C. The best results for 
each measure are in bold, and the gains over the single neural network (DNN C ) are shown in parentheses. 

# System 
Comments 
MAP 
AvgRec MRR 
Acc 
P 
R 
F1 

1 DNNB 
Subtask B network 
76.27 90.27 
83.57 76.39 89.53 33.05 48.28 
2 DNNB+P A 
DNNB with A predicted labels 
76.08 89.99 
83.38 77.40 86.41 38.20 52.98 
3 DNNB+P C 
DNNB with C predicted labels 
76.33 90.38 
83.62 77.40 83.19 40.34 54.34 
4 DNNB+P A+P C 
DNNB with A and C predicted labels 
76.43 90.34 
83.62 77.11 78.74 42.92 55.56 

5 CRF B f 
CRF with fully connected B 
76.41 90.34 
83.81 77.00 84.62 37.76 52.23 
6 CRF ACBC,B f 
CRFACBC with fully connected B 
76.89 90.87 
84.19 77.86 76.00 48.93 59.53 
7 CRF ACBC,A f B f 
CRFACBC with fully connected A and B 
76.51 90.64 
84.19 78.29 83.47 43.35 57.06 
8 CRF ACBC,B f C f 
CRFACBC with fully connected B and C 
76.87 90.96 
84.44 77.86 78.68 45.92 58.00 
9 CRFACBC,f 
CRFACBC with all layers fully connected 76.25 90.38 
84.62 78.57 81.20 46.35 59.02 

</table></figure>

			<note place="foot" n="1"> To avoid visual clutter, the input features and the output variables for the edges are not shown in Figure 2b.</note>

			<note place="foot" n="2"> BLEU FEATURES and BLEU COMPONENTS (Guzmán et al., 2016a,b) are ported from an MT evaluation framework (Guzmán et al., 2015; Guzmán et al., 2017) to cQA.</note>

			<note place="foot" n="3"> Other adaptive algorithms such as ADAM (Kingma and Ba, 2014) or ADADELTA (Zeiler, 2012) were slightly worse.</note>

			<note place="foot" n="4"> Note that we have a classification approach, which favors accuracy-P-R-F1; if we want to improve the ranking measures, we should optimize for them directly.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author would like to thank the funding support from MOE Tier-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PMI-cool at SemEval-2016 Task 3: Experiments with PMI and goodness polarity lexicons for community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Balchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasen</forename><surname>Kiprov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="844" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Threadlevel information for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective shared representations with multitask learning for community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Uva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;17</title>
		<meeting>the Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;17<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="726" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An exploration of data augmentation and RNN architectures for question ranking in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP &apos;17</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing, IJCNLP &apos;17<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="442" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-language question re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research, HLT &apos;02</title>
		<meeting>the Second International Conference on Human Language Technology Research, HLT &apos;02<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">KeLP at SemEval-2016 Task 3: Learning semantic relations between questions and answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1116" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pairwise neural machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine translation evaluation with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="180" to="200" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine translation evaluation meets community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL &apos;16</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL &apos;16<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MTE-NN at SemEval-2016 Task 3: Can machine translation evaluation help community question answering?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
		<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Web forum retrieval and text analytics: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="163" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global thread-level inference for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;15</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;15<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="573" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech act modeling of written asynchronous conversations with task-specific embeddings and conditional structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enamul</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL &apos;16</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL &apos;16<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1746" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint learning with global inference for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;16</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="703" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-language learning with adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israa</forename><surname>Jaradat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning, CoNLL &apos;17</title>
		<meeting>the 21st Conference on Computational Natural Language Learning, CoNLL &apos;17<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="226" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The dark side of news community forums: Opinion manipulation trolls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsvetomila</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Internet Research</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SemanticZ at SemEval-2016 Task 3: Ranking relevant answers in community question answering using semantic similarity based on fine-tuned word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="879" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsvetomila</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pepa</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Boyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Yovcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Momchil</forename><surname>Hardalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasen</forename><surname>Kiprov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Balchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<title level="m">Ivelina Nikolova, and Galia Angelova</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SUper Team at SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a feature-rich system for community question answering</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="836" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fact checking in community forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsvetomila</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Karadjov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, AAAI &apos;18</title>
		<meeting>the ThirtySecond AAAI Conference on Artificial Intelligence, AAAI &apos;18<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="879" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;13</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;13<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning A Probabilistic Perspective</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">It takes three to tango: Triangulation approach to answer ranking in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;16</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;16<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1586" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hamdy Mubarak, abed Alhakim Freihat, James Glass, and Bilal Randeree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="545" />
		</imprint>
	</monogr>
	<note>SemEval-2016 task 3: Community question answering</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>40th Annual Meting of the Association for Computational Linguistics, ACL &apos;02<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for communitybased question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence, IJCAI &apos;15</title>
		<meeting>International Joint Conference on Artificial Intelligence, IJCAI &apos;15<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1305" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation for duplicate question detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA &apos;06</title>
		<meeting>the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA &apos;06<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL &apos;13</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL &apos;13<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">LSTM-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">RMSprop. COURSERA: Neural Networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accelerated training of conditional random fields with stochastic gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A long short-term memory model for answer sentence selection in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Concept and attention-based cnn for question retrieval in multi-view learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><surname>Beijing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisansa De</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP &apos;15</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparing the mean field method and belief propagation for approximate inference in MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Mean Field Methods</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Question condensing networks for answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Houfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL &apos;18</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL &apos;18<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1746" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Answer sequence learning with neural networks for answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL-IJCNLP &apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL-IJCNLP &apos;15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="713" to="718" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
