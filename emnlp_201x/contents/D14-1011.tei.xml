<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate Word Segmentation and POS Tagging for Japanese Microblogs: Corpus Annotation and Joint Modeling with Lexical Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Institute of Information and Communications Technology † Institute of Industrial Science</orgName>
								<orgName type="institution">The University of Tokyo ‡ National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Institute of Information and Communications Technology † Institute of Industrial Science</orgName>
								<orgName type="institution">The University of Tokyo ‡ National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate Word Segmentation and POS Tagging for Japanese Microblogs: Corpus Annotation and Joint Modeling with Lexical Normalization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="99" to="109"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Microblogs have recently received widespread interest from NLP researchers. However, current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts. We developed an annotated corpus and proposed a joint model for overcoming this situation. Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance. Our joint model with lexical normalization handles the orthographic diversity of microblog texts. We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microblogs, such as Twitter <ref type="bibr">1</ref> and Weibo 2 , have re- cently become an important target of NLP tech- nology. Since microblogs offer an instant way of posting textual messages, they have been given increasing attention as valuable sources for such actions as mining opinions <ref type="bibr" target="#b11">(Jiang et al., 2011)</ref> and detecting sudden events such as earthquake ( <ref type="bibr" target="#b20">Sakaki et al., 2010)</ref>.</p><p>However, many studies have reported that cur- rent NLP tools do not perform well on microblog texts <ref type="bibr" target="#b3">(Foster et al., 2011;</ref><ref type="bibr" target="#b4">Gimpel et al., 2011</ref>). In the case of Japanese text processing, the most se- rious problem is poor accuracy of word segmen- tation and POS tagging. Since these two tasks are positioned as the fundamental step in the text processing pipeline, their accuracy is vital for all downstream applications. <ref type="bibr">1</ref> https://twitter.com 2 https://www.weibo.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Development of annotated corpus</head><p>The main obstacle that makes word segmentation and POS tagging in the microblog domain chal- lenging is the lack of annotated corpora. Because current annotated corpora are from other domains, such as news articles, it is difficult to train models that perform well on microblog texts. Moreover, system performance cannot be evaluated quantita- tively.</p><p>We remedied this situation by developing an an- notated corpus of Japanese microblogs. We col- lected 1831 sentences from Twitter and manually annotated these sentences with word boundaries, POS tags, and normalized forms of words (c.f., Section 1.2).</p><p>We, for the first time, present a comprehen- sive empirical study of Japanese word segmenta- tion and POS tagging on microblog texts by us- ing this corpus. Specifically, we investigated how well current models trained on existing corpora perform in the microblog domain. We also ex- plored performance gains achieved by using our corpus for training, and by jointly performing lex- ical normalization (c.f., Section 1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Joint modeling with lexical normalization</head><p>Orthographic diversity in microblog texts causes a problem when training a statistical model for word segmentation and POS tagging. Microblog texts frequently contain informal words that are spelled in a non-standard manner, e.g., "oredi (already)", "b4 (before)", and "talkin (talking)" (Han and Baldwin, 2011). Such words, hereafter referred to as ill-spelled words, are so productive that they considerably increase the vocabulary size. This makes training of statistical models difficult.</p><p>We address this problem by jointly conducting lexical normalization. Although a wide variety of ill-spelled words are used in microblog texts, many can be normalized into well-spelled equiva- lents, which conform to standard rules of spelling.</p><p>A joint model with lexical normalization is able to handle orthographic diversity by exploiting in- formation obtainable from the well-spelled equiv- alents.</p><p>The proposed joint model was empirically eval- uated on the microblog corpus we developed. Our experiment demonstrated that the proposed model can perform word segmentation and POS tag- ging substantially better than current state-of-the- art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Summary</head><p>Contributions of this paper are the following:</p><p>• We developed a microblog corpus that en- ables not only training of accurate models but also quantitative evaluation for word segmen- tation and POS tagging in the microblog do- main. 3</p><p>• We propose a joint model with lexical nor- malization for better handling of ortho- graphic diversity in microblog texts. In par- ticular, we present a new method of training the joint model using a partially annotated corpus (c.f., Section 7.4).</p><p>• We, for the first time, present a comprehen- sive empirical study of word segmentation and POS tagging for microblogs. The experi- mental results demonstrated that both the mi- croblog corpus and joint model greatly con- tributes to training accurate models for word segmentation and POS tagging.</p><p>The remainder of this paper is organized as fol- lows. Section 2 reviews related work. Section 3 discusses the task of lexical normalization and in- troduces terminology. Section 4 presents our mi- croblog corpus and results of our corpus analysis. Section 5 presents an overview of our joint model with lexical normalization, and Sections 6 and 7 provide details of the model. Section 8 presents experimental results and discussions, and Section 9 presents concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Researchers have recently developed various mi- croblog corpora annotated with rich linguistic in- formation. <ref type="bibr" target="#b4">Gimpel et al. (2011) and</ref><ref type="bibr" target="#b3">Foster et al. (2011)</ref> annotated English microblog posts with POS tags. Han and Baldwin (2011) released a mi- croblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff <ref type="bibr" target="#b2">(Duan et al., 2012</ref>). However, there are no microblog corpora annotated with word bound- aries, POS tags, and normalized sentences.</p><p>There has been a surge of interest in lexical nor- malization with the advent of microblogs ( <ref type="bibr" target="#b5">Han and Baldwin, 2011;</ref><ref type="bibr" target="#b17">Liu et al., 2012;</ref><ref type="bibr" target="#b6">Han et al., 2012;</ref><ref type="bibr" target="#b24">Wang and Ng, 2013;</ref><ref type="bibr" target="#b28">Zhang et al., 2013;</ref><ref type="bibr" target="#b16">Ling et al., 2013;</ref><ref type="bibr" target="#b27">Yang and Eisenstein, 2013;</ref><ref type="bibr" target="#b16">Wang et al., 2013)</ref>. However, these studies did not address en- hancing word segmentation. <ref type="bibr" target="#b16">Wang et al. (2013)</ref> proposed a method of joint ill-spelled word recognition and word segmenta- tion. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the infor- mation obtainable from well-spelled equivalents to increase word segmentation accuracy.</p><p>Some studies also explored integrating the lexi- cal normalization process into word segmentation and POS tagging ( <ref type="bibr" target="#b9">Ikeda et al., 2009;</ref><ref type="bibr" target="#b21">Sasano et al., 2013)</ref>. A strength of our joint model is that it uses rich character-level and word-level features used in state-of-the-art models of joint word segmenta- tion and POS tagging ( <ref type="bibr" target="#b14">Kudo et al., 2004;</ref><ref type="bibr" target="#b18">Neubig et al., 2011;</ref><ref type="bibr" target="#b13">Kaji and Kitsuregawa, 2013)</ref>. Thanks to these features, our model performed much bet- ter than Sasano et al.'s system, which is the only publicly available system that jointly conducts lex- ical normalization, in the experiments (see Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Further- more, we present a comprehensive evaluation in terms of precision and recall on our microblog cor- pus. Such an evaluation has not been conducted in previous work due to the lack of annotated cor- pora. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexical Normalization Task</head><p>This section explains the task of lexical normal- ization addressed in this paper. Since lexical nor- malization is a relatively new research topic, there are no precise definitions of a lexical normaliza- tion task that are widely accepted by researchers. <ref type="table">Table 1</ref>: Examples of our target ill-spelled words and their well-spelled equivalents. Phonemes are shown between slashes. English translations are provided in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ill-spelled word</head><p>Well-spelled equivalent</p><formula xml:id="formula_0">/sugee/ ‚ · ‚ ² ‚ ¢ /sugoi/ (great) -ß‚ ë /modoro/ -ß‚ ë‚ ¤ /modorou/ (going to return) ‚ ¤ ‚ Ü‚ ¢ ‚ ¢ ‚ ¢ ‚ ¢ /umaiiii/ ‚ ¤ ‚ Ü‚ ¢ /umai/ (yummy)</formula><p>Therefore, it is important to clarify our task setting before discussing our joint model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Target ill-spelled words</head><p>Many studies on lexical normalization have pointed out that phonological factors are deeply involved in the process of deriving ill-spelled words. <ref type="bibr" target="#b26">Xia et al. (2006)</ref> investigated a Chi- nese chat corpus and reported that 99.2% of the ill-spelled words were derived by phonetic map- ping from well-spelled equivalents. <ref type="bibr" target="#b24">Wang and Ng (2013)</ref> analyzed 200 Chinese messages from Weibo and 200 English SMS messages from the NUS SMS corpus <ref type="bibr" target="#b8">(How and Kan, 2005</ref>). Their analysis revealed that most ill-spelled words were derived from well-spelled equivalents based on pronunciation similarity. On top of these investigations, we focused on ill-spelled words that are derived by phonologi- cal mapping from well-spelled words by assum- ing that such ill-spelled words are dominant in Japanese microblogs as well. We also assume that these ill-spelled words can be normalized into well-spelled equivalents on a word-to-word basis, as assumed in a previous study <ref type="bibr" target="#b5">(Han and Baldwin, 2011</ref>). The validity of these two assumptions is empirically assessed in Section 4. <ref type="table">Table 1</ref> lists examples of our target ill-spelled words, their well-spelled equivalents, and their phonemes. The ill-spelled word in the first row is formed by changing the continuous two vowels from /oi/ to /ee/. This type of change in pronun- ciation is often observed in Japanese spoken lan- guage. The second row presents contractions. The last vowel character "‚ ¤ " /u/ of the well-spelled word is dropped. The third row illustrates word lengthening. The ill-spelled word is derived by re- peating the vowel character "‚ ¢ " /i/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Terminology</head><p>We now introduce the terminology that will be used throughout the remainder of this paper. The term word surface form (or surface form for short) is used to refer to the word form observed in an actual text, while word normal form (or normal form) refers to the normalized word form. Note that surface forms of well-spelled words are al- ways identical to their normal forms.</p><p>It is possible that the word surface form and nor- mal form have distinct POS tags, although they are identical in most cases. Take the ill-spelled word " -ß‚ ë " /modoro/ as an example (the second row of <ref type="table">Table 1</ref>). According to the JUMAN POS tag set, 5 POS of its surface form is CONTRACTED VERB, while that of its normal form is VERB. <ref type="bibr">6</ref> To handle such a case, we strictly distinguish between these two POS tags by referring to them as surface POS tags and normal POS tags, respectively. Given these terms, the tasks addressed in this paper can be stated as follows. Word segmenta- tion is a task of segmenting a sentence into a se- quence of word surface forms, and POS tagging is a task of providing surface POS tags. The task of joint lexical normalization, word segmentation, and POS tagging is to map a sentence into a se- quence of quadruplets: word surface form, surface POS tag, normal form, and normal POS tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Microblog Corpus</head><p>This section introduces our microblog corpus. We first explain the process of developing the corpus then present the results of our agreement study and corpus analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data collection and annotation</head><p>The corpus was developed by manually annotating text messages posted to Twitter.</p><p>The posts to be annotated were collected as fol- lows. 171,386 Japanese posts were collected using the Twitter API 7 on December 6, 2013. Among these, 1000 posts were randomly selected then manually split into sentences. As a result, we ob- tained 1831 sentences as a source of the corpus.</p><p>Two human participants annotated the 1831 sentences with surface forms and surface POS tags. Since much effort has already been done to annotate corpora with this information, the anno- tation process here follows the guidelines used to develop such corpora in previous studies <ref type="bibr" target="#b15">(Kurohashi and Nagao, 1998;</ref><ref type="bibr" target="#b7">Hashimoto et al., 2011</ref>).</p><p>The two participants also annotated ill-spelled words with their normal forms and normal POS tags. Although this paper targets only infor- mal phonological variations (c.f., Section 3), other types of ill-spelled words were also anno- tated to investigate their frequency distribution in microblog texts. Specifically, besides infor- mal phonological variations, spelling errors and Twitter-specific abbreviations were annotated. As a result, 833 ill-spelled words were identified (Ta- ble 2). They were all annotated with normal forms and normal POS tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agreement study</head><p>We investigated the inter-annotator agreement to check the reliability of the annotation. During the annotation process, the two participants collabo- ratively annotated around 90% of the sentences (specifically, 1647 sentences) with normal forms and normal POS tags, and elaborated an annota- tion guideline through discussion. They then inde- pendently annotated the remaining 184 sentences (1431 words), which were used for the agreement study. Our annotation guideline is shown in the supplementary material.</p><p>We first explored the extent to which the two participants agreed in distinguishing between well-spelled words and ill-spelled words. For this task, we observed Cohen's kappa of 0.96 (almost perfect agreement). This results show that it is easy for humans to distinguish between these two types of words.</p><p>Next, we investigated whether the two partici- pants could give ill-spelled words with the same normal forms and normal POS tags. For this pur- pose, we regarded the normal forms and normal POS tags annotated by one participant as goldstan- dards and calculated precision and recall achieved by the other participant. We observed moder- ate agreement between the two participants: 70% (56/80) precision and 73% (56/76) recall. We manually analyzed the conflicted examples and found that there were more than one acceptable normal form in many of these cases. Therefore, we would like to note that the precision and recall reported above are rather pessimistic estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>We conducted corpus analysis to confirm the fea- sibility of our approach.  <ref type="table" target="#tab_0">Table 2</ref> illustrates that phonological variations constitute a vast majority of ill-spelled words in Japanese microblog texts. In addition, analysis of the 804 phonological variations showed that 793 of them can be normalized into single words. These represent the validity of the two assump- tions we made in Section 3.1.</p><p>We then investigated whether lexical normaliza- tion can decrease the number of out-of-vocabulary words. For the 793 ill-spelled words, we counted how many of their surface forms and normal forms were not registered in the JUMAN dictio- nary. <ref type="bibr">8</ref> The result suggests that 411 (51.8%) and 74 (9.3%) are not registered in the dictionary. This indicates the effectiveness of lexical normalization for decreasing out-of-vocabulary words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Overview of Joint Model</head><p>This section gives an overview of our joint model with lexical normalization for accurate word seg- mentation and POS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Lattice-based approach</head><p>A lattice-based approach has been commonly adopted to perform joint word segmentation and POS tagging ( <ref type="bibr" target="#b10">Jiang et al., 2008;</ref><ref type="bibr" target="#b14">Kudo et al., 2004;</ref><ref type="bibr" target="#b13">Kaji and Kitsuregawa, 2013)</ref>. In this approach, an input sentence is transformed into a word lattice in which the edges are labeled with surface POS tags ( <ref type="figure" target="#fig_0">Figure 1</ref>). Given such a lattice, word seg- mentation and POS tagging can be performed at the same time by traversing the lattice. A discrim- inative model is typically used for the traversal.</p><p>An advantage of this approach is that, while the lattice can represent an exponentially large num- ber of candidate analyses, it can be quickly tra- versed using dynamic programming ( <ref type="bibr" target="#b14">Kudo et al., 2004;</ref><ref type="bibr" target="#b13">Kaji and Kitsuregawa, 2013)</ref> or beam search ( <ref type="bibr" target="#b10">Jiang et al., 2008)</ref>. In addition, a discriminative model allows the use of rich word-level features to find the correct analysis. We propose extending the lattice-based ap- proach to jointly perform lexical normalization, word segmentation, and POS tagging. We trans- form an input sentence into a word lattice in which the edges are labeled with not only surface POS tags but normal forms and normal POS tags <ref type="figure">(Fig- ure 2)</ref>. By traversing such a lattice, the three tasks can be performed at the same time. This ap- proach can not only exploit rich information ob- tainable from word normal forms, but also achieve efficiency similar to the original lattice-based ap- proach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Issues</head><p>Issues on how to develop this lattice-based ap- proach is detailed in Sections 6 and 7.</p><p>Section 6 describes how to generate a word lat- tice from an input sentence. This is done us- ing a hybrid approach that combines a statistical model and normalization dictionary. The normal- ization dictionary is specifically a list of quadru-   <ref type="table" target="#tab_1">(Table 3)</ref>. Section 7 describes a discriminative model for the lattice traversal. Our feature design as well as two training methods are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Word Lattice Generation</head><p>In this section, we first describe a method of con- structing a normalization dictionary then present a method of generating a word lattice from an input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Construction of normalization dictionary</head><p>Although large-scale normalization dictionaries are difficult to obtain, tag dictionaries, which list pairs of word surface forms and their surface POS tags <ref type="table" target="#tab_2">(Table 4)</ref>, are widely available in many lan- guages including Japanese. Therefore, we use an existing tag dictionary to construct the normaliza- tion dictionary.</p><p>Due to space limitations, we give only a brief overview of our construction method, omitting its details. We note that our method uses hand-crafted rules similar to those used in ( <ref type="bibr" target="#b21">Sasano et al., 2013)</ref>; hence, the proposal of this method is not an im- portant contribution. To make our experimental results reproducible, our normalization dictionary, as well as a tool for constructing it, is released as supplementary material.</p><p>Our method of constructing the normalization dictionary takes three steps. The following ex- plains each step using <ref type="table" target="#tab_1">Tables 3 and 4</ref> as running examples.</p><p>Step 1 A tag dictionary generally contains a small number of ill-spelled words, although well- spelled words constitute a vast majority. We iden- tify such ill-spelled words by using a manually- tailored list of surface POS tags indicative of in- formal spelling (e.g., CONTRACTED VERB). For example, entry (c) in <ref type="table" target="#tab_2">Table 4</ref> is identified as an ill-spelled word in this step.</p><p>Step 2 The tag dictionary is augmented with normal forms and normal POS tags to construct a small normalization dictionary. For ill-spelled words identified in step 1, the normal forms and normal POS tags are determined by hand-crafted rules. For example, the normal form is derived by appending the vowel character "‚ ¤ " /u/ to the sur- face form, if the surface POS tag is CONTRACTED VERB. This rule derives entry (D) in <ref type="table" target="#tab_1">Table 3</ref> from entry (c) in <ref type="table" target="#tab_2">Table 4</ref>. For well-spelled words, on the other hand, the normal forms and normal POS tags are simply set the same as the surface forms and surface POS tags. For example, entries (A), (C), and (E) in <ref type="table" target="#tab_1">Table 3</ref> are generated from entries (a), (b), and (d) in <ref type="table" target="#tab_2">Table 4</ref>, respectively.</p><p>Step 3 Because the normalization dictionary constructed in step 2 contains only a few ill- spelled words, it is expanded in this step. For this purpose, we use hand-crafted rules to derive ill- spelled words from the entries already registered in the normalization dictionary. Some rules are taken from ( <ref type="bibr" target="#b21">Sasano et al., 2013)</ref>, while the others are newly tailored. In <ref type="table" target="#tab_1">Table 3</ref>, for example, entry (B) is derived from entry (A) by applying the rule that substitutes "‚ ² ‚ ¢ " /goi/ with "‚ ° ‚ ¦ " /gee/. A small problem that arises in step 3 is how to handle lengthened words, such as entry (F) in Ta- ble 3. While lengthened words can be easily de- rived using simple rules ( <ref type="bibr" target="#b0">Brody and Diakopoulos, 2011;</ref><ref type="bibr" target="#b21">Sasano et al., 2013</ref>), such rules infinitely increase the number of entries because an unlim- ited number of lengthened words can be derived by repeating characters. To address this problem, no lengthened words are added to the normaliza- tion dictionary in step 3. We instead use rules to skip repetitive characters in an input sentence when performing dictionary match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A hybrid approach</head><p>A word lattice is generated using both a statisti- cal method <ref type="bibr" target="#b13">(Kaji and Kitsuregawa, 2013</ref>) and the normalization dictionary.</p><p>We begin by generating a word lattice which en- codes only word surface forms and surface POS tags (c.f., <ref type="figure" target="#fig_0">Figure 1</ref>) using the statistical method proposed by <ref type="bibr" target="#b13">Kaji and Kitsuregawa (2013)</ref>. Inter- ested readers may refer to their paper for details.</p><p>Each edge in the lattice is then labeled with nor- mal forms and normal POS tags. Note that a sin- gle edge can have more than one candidate normal form and normal POS tag. In such a case, new edges are accordingly added to the lattice.</p><p>The edges are labeled with normal forms and normal POS tags in the following manner. First, every edge is labeled with a normal form and normal POS tag that are identical with the sur- face form and surface POS tag. This is based on our observation that most words are well-spelled ones. The edge is not provided with further nor- mal forms and normal POS tags, if the normaliza- tion dictionary contains a well-spelled word that has the same surface form as the edge. Otherwise, we allow the edge to have all pairs of normal forms and normal POS tags that are obtained by using the normalization dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discriminative Lattice Traversal</head><p>This section explains a discriminative model for traversing the word lattice. The lattice traversal with a discriminative model can formally be writ- ten as f (x, w, t, v, s) · θ.</p><p>Here, x denotes an input sentence, w, t, v, and s denote a sequence of word surface forms, surface POS tags, normal forms, and normal POS tags, re- spectively, L(x) represents a set of candidate anal- yses represented by the word lattice, and f (·) and θ are feature and weight vectors.</p><p>We now describe features, a decoding method, and two training methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Features</head><p>We use character-level and word-level features used for word segmentation and POS tagging in <ref type="bibr" target="#b13">(Kaji and Kitsuregawa, 2013)</ref>. To take advan- tage of joint model with lexical normalization, the word-level features are extracted from not only surface forms but also normal forms. See <ref type="bibr" target="#b13">(Kaji and Kitsuregawa, 2013</ref>) for the original features.</p><p>In addition, several new features are introduced in this paper. We use the quadruplets (w i , t i , v i , s i ) and pairs of surface and normal POS tags (t i , s i ) as binary features to capture probable mappings between ill-spelled words and their well-spelled equivalents. We use another binary feature indi- cating whether a quadruplet (w i , t i , v i , s i ) is reg- istered in the normalization dictionary. Also, we use a bigram language model feature, which pre- vents sentences from being normalized into un- grammatical and/or incomprehensible ones. The language model features are associated with nor- malized bigrams, <ref type="figure" target="#fig_0">(v i−1 , s i−1 , v i , s i )</ref>, and take as the values the logarithmic frequency log 10 (f + 1), where f represents the bigram frequency ( <ref type="bibr" target="#b12">Kaji and Kitsuregawa, 2011)</ref>. Since it is difficult to obtain a precise value of f , it is approximated by the fre- quency of the surface bigram, (w i−1 , t i−1 , w i , t i ), calculated from a large raw corpus automatically analyzed using a system of joint word segmenta- tion and POS tagging. See Section 8.1 for the raw corpus and system used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Decoding</head><p>It is easy to find the best analysis (w, t, v, s) among the candidates represented by the word lat- tice. Although we use several new features, we can still locate the best analysis by using the same dynamic programming algorithm as in previous studies ( <ref type="bibr" target="#b14">Kudo et al., 2004;</ref><ref type="bibr" target="#b13">Kaji and Kitsuregawa, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Training on a fully annotated corpus</head><p>It is straightforward to train the joint model pro- vided with a fully annotated corpus, which is la- beled with word surface forms, surface POS tags, normal forms, and normal POS tags.</p><p>We use structured perceptron <ref type="bibr" target="#b1">(Collins, 2002</ref>) for the training (Algorithm 1). The training be- gins by initializing θ as a zero vector (line 1). It then reads the annotated corpus C (line 2-9). Given a training example, (x, w, t, v, s) ∈ C, the algorithm locates the best analysis,</p><formula xml:id="formula_1">( ˆ w, ˆ t, ˆ v, ˆ s),</formula><p>based on the current weight vector (line 4). If the best analysis differs from the oracle analy- sis, (w, t, v, s), the weight vector is updated (line 5-7). After going through the annotated corpus m times (m=10 in our experiment), the averaged weight vector is returned (line 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Training on a partially annotated corpus</head><p>Although the training with the perceptron algo- rithm requires a fully annotated corpus, it is labor- intensive to fully annotate sentences. This consid-</p><formula xml:id="formula_2">Algorithm 1 Perceptron training 1: θ ← 0 2: for i = 1 . . . m do 3: for (x, w, t, v, s) ∈ C do 4: ( ˆ w, ˆ t, ˆ v, ˆ s) ← DECODING(x, θ) 5: if (w, t, v, s) = ( ˆ w, ˆ t, ˆ v, ˆ s) then 6: θ ← θ + f (x, w, t, v, s) − f (x, ˆ w, ˆ t, ˆ v, ˆ</formula><note type="other">s) 7: end if 8: end for 9: end for 10: return AVERAGE(θ) Algorithm 2 Latent perceptron training 1: θ ← 0 2: for i = 1 . . . m do 3:</note><p>for (x, w, t) ∈ C ′ do 4:</p><formula xml:id="formula_3">( ˆ w, ˆ t, ˆ v, ˆ s) ← DECODING(x, θ) 5: (w, t, ¯ v, ¯ s) ← CONSTRAINEDDECODING(x, θ) 6: if w = ˆ w or t = ˆ t then 7: θ ← θ + f (x, w, t, ¯ v, ¯ s) − f (x, ˆ w, ˆ t, ˆ v, ˆ s) 8: end if 9:</formula><p>end for 10: end for 11: return AVERAGE(θ) eration motivates us to explore training our model with less supervision. We specifically explore us- ing a corpus annotated with only word boundaries and POS tags.</p><p>We use the latent perceptron algorithm ( <ref type="bibr" target="#b22">Sun et al., 2013)</ref> to train the joint model from such a par- tially annotated corpus (Algorithm 2). In this sce- nario, a training example is a sentence x paired with a sequence of word surface forms w and sur- face POS tags t (c.f., line 3). Similarly to the perceptron algorithm, we locate the best analy- sis ( ˆ w, ˆ t, ˆ v, ˆ s) for a given training example, (line 4). We also locate the best analysis, (w, t, ¯ v, ¯ s), among those having the same surface forms w and surface POS tags t as the training example (line 5). If the surface forms and surface POS tags of the former analysis differ from the annotations of the training example, parameter is updated by re- garding the latter analysis as an oracle (line 6-8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>We conducted experiments to investigate how the microblog corpus and joint model contribute to improving accuracy of word segmentation and POS tagging in the microblog domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Setting</head><p>We constructed the normalization dictionary from the JUMAN dictionary 7.0. 9 While JUMAN dic-tionary contains 750,156 entries, the normaliza- tion dictionary contains 112,458,326 entries. Some features taken from the previous study <ref type="bibr" target="#b13">(Kaji and Kitsuregawa, 2013</ref>) are induced using a tag dictionary. For this we used two tag dictionar- ies. One is JUMAN dictionary 7.0 and the other is a tag dictionary constructed by listing surface forms and surface POS tags in the normalization dictionary.</p><p>To compute the language model features, one billion sentences from Twitter posts were analyzed using MeCab 0.996. <ref type="bibr">10</ref> We used all bigrams ap- pearing at least 10 times in the auto-analyzed sen- tences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Results of word segmentation and POS tagging</head><p>We first investigated the performance of models trained on an existing annotated corpus form news texts. For this experiment, our joint model as well as three state-of-the-art models ( <ref type="bibr" target="#b14">Kudo et al., 2004)</ref>   <ref type="bibr" target="#b15">and Nagao, 1998)</ref>.</p><p>Since this training corpus is not annotated with normal forms and normal POS tags, our model was trained using the latent perceptron. <ref type="table" target="#tab_4">Table  5</ref> summarizes the word-level F 1 -scores ( <ref type="bibr" target="#b14">Kudo et al., 2004</ref>) on our microblog corpus. The two columns represent the results for word segmenta- tion (Seg) and joint word segmentation and POS tagging (Seg+Tag), respectively. We also conducted 5-fold crossvalidation on our microblog corpus to evaluate performance im- provement when these models are trained on mi- croblog texts <ref type="table" target="#tab_5">(Table 6</ref>). In addition to the models in <ref type="table" target="#tab_4">Table 5</ref>, results of a rule-based system ( <ref type="bibr" target="#b21">Sasano et al., 2013)</ref>  <ref type="bibr">13</ref> and our joint model trained using the perceptron algorithm are also presented. No- tice that Proposed and Proposed (latent) repre- sent our model trained using perceptron and latent perceptron, respectively.</p><p>From <ref type="table" target="#tab_4">Tables 5 and 6</ref>, as expected, we see that the models trained on news texts performed poorly on microblog texts, while their performance sig- nificantly boosted when trained on the microblog texts. This demonstrates the importance of corpus annotation. An exception was Kudo04. Its perfor- mance improved only slightly, even when it was trained on the microblog texts. We believe this is because their model uses dictionary-based rules to prune candidate analyses; thus, it could not per- form well in the microblog domain, where out-of- vocabulary words are abundant. <ref type="table" target="#tab_5">Table 6</ref> also illustrates that our joint models achieved F 1 -score better than the state-of-the-art models trained on the microblog texts. This shows that modeling the derivation process of ill- spelled words makes training easier. We con- ducted bootstrap resampling (with 1000 samples) to investigate the significance of the improvements achieved with our joint model. The results showed that all improvements over the baselines were sta- tistically significant (p &lt; 0.01). The difference between Proposed and Proposed (latent) were also statistically significant (p &lt; 0.01).</p><p>The results of Proposed (latent) are interest- ing. <ref type="table" target="#tab_4">Table 5</ref> illustrates that our joint model per- forms well even when it is trained on a news cor- pus that rarely contains ill-spelled words and is not at all annotated with normal forms and nor- mal POS tags. This indicates the robustness of our training method and the importance of modeling word derivation process in the microblog domain. In <ref type="table" target="#tab_5">Table 6</ref>, we observed that Proposed (latent), which uses less supervision, performed better than Proposed. The reason for this will be examined later.</p><p>In summary, we can conclude that both the mi- croblog corpus and joint model significantly con- tribute to training accurate models for word seg- mentation and POS tagging in the microblog do- main.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Results of lexical normalization</head><p>While the main goal with this study was to en- hance word segmentation and POS tagging in the microblog domain, it is interesting to explore how well our joint model can normalize ill-spelled words. <ref type="table" target="#tab_6">Table 7</ref> illustrates precision, recall, and F 1 - score for the lexical normalization task. To put  the results into context, we report on the baseline results of a tagging model proposed by <ref type="bibr" target="#b18">Neubig et al. (2011)</ref>. This baseline conducts lexical normal- ization by regarding it as two independent tagging tasks (i.e., tasks of tagging normal forms and nor- mal POS tags). The result of the baseline model is also obtained using 5-fold crossvalidation. <ref type="table" target="#tab_6">Table 7</ref> illustrates that Proposed performed sig- nificantly better than the simple tagging model, Neubig11. This suggests the effectiveness of our joint model. On the other hand, Proposed (latent) performed poorly in this task. From this result, we can argue that Proposed (latent) can achieve su- perior performance in word segmentation and POS tagging <ref type="table" target="#tab_5">(Table 6</ref>) because it gave up correctly nor- malizing ill-spelled words, focusing on word seg- mentation and POS tagging.</p><p>The experimental results so far suggest the fol- lowing strategy for training our joint model. If ac- curacy of word segmentation and POS tagging is the main concern, we can use the latent percep- tron. This approach has the advantage of being able to use a partially annotated corpus. On the other hand, if performance of lexical normaliza- tion is crucial, we have to use the standard percep- tron algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Error analysis</head><p>We manually analyzed erroneous outputs and ob- served several tendencies.</p><p>We found that a word lattice sometimes missed the correct output. Such an error was, for example, observed in a sentence including many ill-spelled words, e.g., 'Ž üˆÍ‚üˆÍ‚ Ì-Ú‚ " • Aƒ Lƒ jƒ iƒ Š ƒ } ƒ X• I (be nervous about what other people think!)', where the part 'ƒ Lƒ jƒ iƒ Š ƒ } ƒ X ' is in ill-spelled words. Improving the lattice generation algorithm is con- sidered necessary to achieve further performance gain.</p><p>Even if the correct analysis appears in the word lattice, our model sometimes failed to handle ill-spelled words, incorrectly analyzing them as out-of-vocabulary words. For example, the pro- posed method treated the phrase '‚ ¨ ‚ â‚ Â‚ ½• [ ‚ ¢ ‚ Þ (snack time)' as a single out-of-vocabulary word, even though the correct analysis was found in the word lattice. More sophisticated features would be required to accurately distinguish between ill- spelled and out-of-vocabulary words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>We presented our attempts towards developing an accurate model for word segmentation and POS tagging in the microblog domain. To this end, we, for the first time, developed an annotated corpus of microblogs. We also proposed a joint model with lexical normalization to handle orthographic diversity in the microblog text. Intensive exper- iments demonstrated that we could successfully improve the performance of word segmentation and POS tagging on microblog texts. We believe this study will have a large practical impact on a various research areas that target microblogs.</p><p>One limitation of our approach is that it cannot handle certain types of ill-spelled words. For ex- ample, the current model cannot handle the cases in which there are no one-to-one-mappings be- tween well-spelled and ill-spelled words. Also, our model cannot handle spelling errors, which are considered relatively frequent in the microblog than news domains. The treatment of these prob- lems would require further research.</p><p>Another future research is to speed-up our model. Since the joint model with lexical normal- ization significantly increases the search space, it is much slower than the original lattice-based model for word segmentation and POS tagging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example lattice (Kudo et al., 2004; Kaji and Kitsuregawa, 2013). Circle and arrow represent node and edge, respectively. Bold edges represent correct analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Frequency distribution over three types of ill-spelled words in corpus.</head><label>2</label><figDesc></figDesc><table>Type 
Frequency 
Informal phonological variation 804 (92.9%) 
Spelling error 
27 (3.1%) 
Twitter-specific abbreviation 
34 (3.9%) 
Total 
865 (100%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 : Normalization dictionary. Columns rep- resent entry ID, surface form, surface POS, normal form, and normal POS, respectively.</head><label>3</label><figDesc></figDesc><table>ID Surf. 
Surf. POS 
Norm. Norm. POS 
A ‚ · 
‚ ² ‚ ¢ 
ADJECTIVE 

‚ · 
‚ ² ‚ ¢ 

ADJECTIVE 
B ‚ · 
‚ ° ‚ ¦ 
ADJECTIVE 

‚ · 
‚ ² ‚ ¢ 

ADJECTIVE 
C -ß‚ ë‚ ¤ 
VERB 

-ß‚ ë‚ ¤ 

VERB 
D -ß‚ ë 
CONTR. VERB -ß‚ ë‚ ¤ 
VERB 
E ‚ ¤ 
‚ Ü‚ ¢ 
ADJECTIVE 

‚ ¤ 
‚ Ü‚ ¢ 

ADJECTIVE 
F ‚ ¤ 
‚ Ü‚ ¢ 
‚ ¢ 
‚ ¢ 
‚ ¢ 
ADJECTIVE 

‚ ¤ 
‚ Ü‚ ¢ 

ADJECTIVE 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Tag dictionary. 

ID Surf. form 
Surf. POS 
a 
‚ · 
‚ ² ‚ ¢ 
(great) 
ADJECTIVE 
b 
-ß‚ ë‚ ¤ 
(going to return) VERB 
c 
-ß‚ ë 
(gonna return) 
CONTR. VERB 
d 
‚ ¤ 
‚ Ü‚ ¢ 
(yummy) 
ADJECTIVE 

plets: word surface form, surface POS tag, normal 
form, and normal POS tag </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Performance of models trained on the news articles.</head><label>5</label><figDesc></figDesc><table>Seg Seg+Tag 
Kudo04 
81.8 
71.0 
Neubig11 
80.5 
69.1 
Kaji13 
83.2 
73.1 
Proposed (latent) 83.0 
73.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 : Results of 5-fold cross-validation on mi- croblog corpus.</head><label>6</label><figDesc></figDesc><table>Seg Seg+Tag 
Kudo04 
82.7 
71.7 
Neubig11 
88.6 
75.9 
Kaji13 
90.9 
82.1 
Sasano13 
82.7 
73.3 
Proposed 
91.3 
83.2 
Proposed (latent) 91.4 
83.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results of lexical normalization task in 
terms of precision, recall, and F 1 -score. 

Precision Recall 
F1 
Neubig11 
69.2 
35.9 47.3 
Proposed 
77.1 
44.6 56.6 
Proposed (latent) 
53.7 
24.7 33.9 

</table></figure>

			<note place="foot" n="3"> Please contact the first author for this corpus.</note>

			<note place="foot" n="4"> Very recently, Saito et al. (2014) conducted similar empirical evaluation on microblog corpus. However, they used biased dataset, in which every sentence includes at least one ill-spelled words.</note>

			<note place="foot" n="5"> http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN 6 In this paper, we use simplified POS tags for explanation purposes. Remind that these tags are different from the original ones defined in JUMAN POS tag set. 7 https://stream.twitter.com/1.1/statuses/sample.json</note>

			<note place="foot" n="8"> http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN</note>

			<note place="foot" n="9"> http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN</note>

			<note place="foot" n="10"> https://code.google.com/p/mecab 11 https://code.google.com/p/mecab 12 http://www.phontron.com/kytea/ 13 http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Naoki Yoshinaga for his help in developing the microblog corpus as well as fruitful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Diakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The CIPS-SIGHAN CLP 2012 Chinese word segmentation on microblog corpora bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Huiming Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second CIPS-SIGHAN Joint Conrerence on Chinese Language Processing</title>
		<meeting>the Second CIPS-SIGHAN Joint Conrerence on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">#hardtoparse: POS tagging and parsing the twitterverse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Cetinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Workshop on Analysing Microtext</title>
		<meeting>AAAI Workshop on Analysing Microtext</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical normalization of short text messages: Makin sens a #twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Construction of a blog corpus with syntactic, anaphoric, and semantic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Shinzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="201" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing predictive text entry for short message service on mobile phones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiu</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Computer Interfaces International</title>
		<meeting>Human Computer Interfaces International</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised text normalization approach for morphological analysis of blog documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazushi</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Yanagihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunori</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Takishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Australasian Joint Conference on Advances in Artificial Intelligence</title>
		<meeting>Australasian Joint Conference on Advances in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word lattice reranking for Chinese word segmentation and part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Target-dependent Twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Splitting noun compounds via monolingual and bilingual paraphrasing: A study on Japanese Katakana words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="959" to="969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient word lattice generation for joint word segmentation and POS tagging in Japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Applying conditional random fields to Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a Japanese parsed corpus while improving the parsing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="719" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Paraphrasing 4 microblog normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint inference of named entity recognition and normalization for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="526" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust adaptable Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Morphological analysis for Japanese noisy text based on character-level and word-level normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kugatsu</forename><surname>Sadamitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1773" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Earthquak shakes Twitter users: real-time event detection by social sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple approach to unknown word processing in Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent structured perceptrons for large-scale learning with hidden information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2063" to="2075" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining informal language from Chinese microtext: Joint word recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="731" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A beam-search decoder for normalization of social media text with application to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chinese informal word normalization: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A phonetic-based approach to Chinese chat text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A log-linear model for unsupervised text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive parsercentric text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benny</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
