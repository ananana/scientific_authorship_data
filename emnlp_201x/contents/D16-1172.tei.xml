<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University † ‡</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country>China † ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University † ‡</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country>China † ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University † ‡</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country>China † ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Software School</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1660" to="1669"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However , one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses <ref type="bibr" target="#b38">(Wilson et al., 2009;</ref><ref type="bibr" target="#b26">O'Connor et al., 2010)</ref>, etc.</p><p>Recently, deep learning approaches ( <ref type="bibr" target="#b32">Socher et al., 2013;</ref><ref type="bibr" target="#b15">Kim, 2014;</ref><ref type="bibr" target="#b21">Liu et al., 2016)</ref> have gained encouraging results on sentiment clas- sification, which frees researchers from handcrafted feature engineering. Among these methods, Recur- rent Neural Networks (RNNs) are one of the most * Corresponding author. prevalent architectures because of the ability to han- dle variable-length texts.</p><p>Sentence-or paragraph-level sentiment analysis expects the model to extract features from limited source of information, while document-level senti- ment analysis demands more on selecting and stor- ing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps .</p><p>Efforts have been made to solve such a scalabil- ity problem on long texts by extracting semantic in- formation hierarchically <ref type="bibr" target="#b35">(Tang et al., 2015a;</ref><ref type="bibr" target="#b34">Tai et al., 2015)</ref>, which first obtain sentence representa- tions and then combine them to generate high-level document embeddings. However, some of these so- lutions either rely on explicit a priori structural as- sumptions or discard the order information within a sentence, which are vulnerable to sudden change or twists in texts especially a long-range one <ref type="bibr" target="#b22">(McDonald et al., 2007;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013</ref>). Re- current models match people's intuition of reading word by word and are capable to model the intrinsic relations between sentences. By keeping the word order, RNNs could extract the sentence representa- tion implicitly and meanwhile analyze the semantic meaning of a whole document without any explicit boundary.</p><p>Partially inspired by neural structure of human brain and computer system architecture, we present the Cached Long Short-Term Memory neural net- works (CLSTM) to capture the long-range senti- ment information. In the dual store memory model proposed by <ref type="bibr" target="#b0">Atkinson and Shiffrin (1968)</ref>, memo- ries can reside in the short-term "buffer" for a lim- ited time while they are simultaneously strengthen- ing their associations in long-term memory. Accord- ingly, CLSTM equips a standard LSTM with a sim- ilar cache mechanism, whose internal memory is di- vided into several groups with different forgetting rates. A group with high forgetting rate plays a role as a cache in our model, bridging and transiting the information to groups with relatively lower forget- ting rates. With different forgetting rates, CLSTM learns to capture, remember and forget semantics in- formation through a very long distance.</p><p>Our main contributions are as follows:</p><p>• We introduce a cache mechanism to diversify the internal memory into several distinct groups with different memory cycles by squashing their forgetting rates. As a result, our model can capture the local and global emotional informa- tion, thereby better summarizing and analyzing sentiment on long texts in an RNN fashion.</p><p>• Benefiting from long-term memory unit with a low forgetting rate, we could keep the gradi- ent stable in the long back-propagation process. Hence, our model could converge faster than a standard LSTM.</p><p>• Our model outperforms state-of-the-art meth- ods by a large margin on three document-level datasets <ref type="bibr">(Yelp 2013, Yelp 2014 and</ref><ref type="bibr">IMDB)</ref>. It worth noticing that some of the previous meth- ods have utilized extra user and product infor- mation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly introduce related work in two areas: First, we discuss the existing document- level sentiment classification approaches; Second, we discuss some variants of LSTM which address the problem on storing the long-term information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document-level Sentiment Classification</head><p>Document-level sentiment classification is a sticky task in sentiment analysis <ref type="bibr" target="#b29">(Pang and Lee, 2008)</ref>, which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally in- formative for inferring the sentiment of the whole document ( <ref type="bibr" target="#b28">Pang and Lee, 2004;</ref><ref type="bibr" target="#b39">Yessenalina et al., 2010)</ref>. Various methods have been investigated and explored over years ( <ref type="bibr" target="#b37">Wilson et al., 2005;</ref><ref type="bibr" target="#b29">Pang and Lee, 2008;</ref><ref type="bibr" target="#b27">Pak and Paroubek, 2010;</ref><ref type="bibr" target="#b39">Yessenalina et al., 2010;</ref><ref type="bibr" target="#b25">Moraes et al., 2013</ref>). Most of these methods depend on traditional machine learning al- gorithms, and are in need of effective handcrafted features.</p><p>Recently, neural network based methods are prevalent due to their ability of learning discrimina- tive features from data ( <ref type="bibr" target="#b32">Socher et al., 2013;</ref><ref type="bibr" target="#b17">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b35">Tang et al., 2015a</ref>).  and <ref type="bibr" target="#b34">Tai et al. (2015)</ref> integrate a tree-structured model into LSTM for better semantic composi- tion; <ref type="bibr" target="#b1">Bhatia et al. (2015)</ref> enhances document-level sentiment analysis by using extra discourse par- ing results. Most of these models work well on sentence-level or paragraph-level sentiment classifi- cation. When it comes to the document-level sen- timent classification, a bottom-up hierarchical strat- egy is often adopted to alleviate the model complex- ity <ref type="bibr" target="#b4">(Denil et al., 2014;</ref><ref type="bibr" target="#b36">Tang et al., 2015b;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory Augmented Recurrent Models</head><p>Although it is widely accepted that LSTM has more long-lasting memory units than RNNs, it still suffers from "forgetting" information which is too far away from the current point ( <ref type="bibr" target="#b18">Le et al., 2015;</ref><ref type="bibr" target="#b14">Karpathy et al., 2015)</ref>. Such a scalability problem of LSTMs is crucial to extend some previous sentence-level work to document-level sentiment analysis.</p><p>Various models have been proposed to increase the ability of LSTMs to store long-range informa- tion ( <ref type="bibr" target="#b18">Le et al., 2015;</ref><ref type="bibr" target="#b31">Salehinejad, 2016</ref>) and two kinds of approaches gain attraction. One is to aug- ment LSTM with an external memory ( <ref type="bibr" target="#b33">Sukhbaatar et al., 2015;</ref><ref type="bibr">Monz, 2016</ref>), but they are of poor per- formance on time because of the huge external mem- ory matrix. Unlike these methods, we fully exploit the potential of internal memory of LSTM by adjust- ing its forgetting rates.</p><p>The other one tries to use multiple time-scales to distinguish different states <ref type="bibr" target="#b8">(El Hihi and Bengio, 1995;</ref><ref type="bibr" target="#b16">Koutnik et al., 2014;</ref><ref type="bibr" target="#b20">Liu et al., 2015</ref>). They partition the hidden states into several groups and each group is activated and updated at different fre- quencies (e.g. one group updates every 2 time-step, the other updates every 4 time-step). In these meth- ods, different memory groups are not fully inter- connected, and the information is transmitted from faster groups to slower ones, or vice versa.</p><p>However, the memory of slower groups are not updated at every step, which may lead to senti- ment information loss and semantic inconsistency. In our proposed CLSTM, we assign different forget- ting rates to memory groups. This novel strategy enable each memory group to be updated at every time-step, and every bit of the long-term and short- term memories in previous time-step to be taken into account when updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Long Short-Term Memory Networks</head><p>Long short-term memory network (LSTM) <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) is a typical recurrent neural network, which alleviates the problem of gra- dient diffusion and explosion. LSTM can capture the long dependencies in a sequence by introducing a memory unit and a gate mechanism which aims to decide how to utilize and update the information kept in memory cell.</p><p>Formally, the update of each LSTM component can be formalized as:</p><formula xml:id="formula_0">i (t) = σ(W i x (t) + U i h (t−1) ),<label>(1)</label></formula><formula xml:id="formula_1">f (t) = σ(W f x (t) + U f h (t−1) ),<label>(2)</label></formula><formula xml:id="formula_2">o (t) = σ(W o x (t) + U o h (t−1) ),<label>(3)</label></formula><formula xml:id="formula_3">˜ c (t) = tanh(W c x (t) + U c h (t−1) ),<label>(4)</label></formula><formula xml:id="formula_4">c (t) = f (t) c (t−1) + i (t) ˜ c (t) ,<label>(5)</label></formula><formula xml:id="formula_5">h (t) = o (t) tanh(c (t) ),<label>(6)</label></formula><p>where σ is the logistic sigmoid function. Opera- tor is the element-wise multiplication operation. i (t) , f (t) , o (t) and c (t) are the input gate, forget gate, output gate, and memory cell activation vector at time-step t respectively, all of which have the same size as the hidden vector</p><formula xml:id="formula_6">h (t) ∈ R H . W i , W f , W o ∈ R H×d and U i , U f , U o ∈ R H×H are train- able parameters.</formula><p>Here, H and d are the dimension- ality of hidden layer and input respectively. <ref type="figure">Figure 1</ref>: (a) A standard LSTM unit and (b) a CIFG- LSTM unit. There are three gates in (a), the input gate, forget gate and output gates, while in (b), there are only two gates, the CIFG gate and output gate.</p><formula xml:id="formula_7">+ C σ + C ~ output forget input IN OUT C σ + C ~ output CIFG IN OUT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cached Long Short-Term Memory Neural Network</head><p>LSTM is supposed to capture the long-term and short-term dependencies simultaneously, but when dealing with considerably long texts, LSTM also fails on capturing and understanding significant sen- timent message ( <ref type="bibr" target="#b18">Le et al., 2015)</ref>. Specifically, the error signal would nevertheless suffer from gradient vanishing in modeling long texts with hundreds of words and thus the network is difficult to train. Since the standard LSTM inevitably loses valu- able features, we propose a cached long short-term memory neural networks (CLSTM) to capture in- formation in a longer steps by introducing a cache mechanism. Moreover, in order to better control and balance the historical message and the incoming in- formation, we adopt one particular variant of LSTM proposed by <ref type="bibr" target="#b12">Greff et al. (2015)</ref>, the Coupled Input and Forget Gate LSTM (CIFG-LSTM).</p><p>Coupled Input and Forget Gate LSTM Previous studies show that the merged version gives perfor- mance comparable to a standard LSTM on language modeling and classification tasks because using the input gate and forget gate simultaneously incurs re- dundant information ( <ref type="bibr" target="#b3">Chung et al., 2014;</ref><ref type="bibr" target="#b12">Greff et al., 2015</ref>).</p><p>In the CIFG-LSTM, the input gate and forget gate are coupled as one uniform gate, that is, let i (t) = 1 − f (t) . We use f (t) to denote the coupled gate. Formally, we will replace Eq. 5 as below: <ref type="figure">Figure 1</ref> gives an illustrative comparison of a stan- dard LSTM and the CIFG-LSTM.</p><formula xml:id="formula_8">c (t) = f (t) c (t−1) + (1 − f (t) ) ˜ c (t) (7)</formula><p>Cached LSTM Cached long short-term mem- ory neural networks (CLSTM) aims at capturing the long-range information by a cache mechanism, which divides memory into several groups, and dif- ferent forgetting rates, regarded as filters, are as- signed to different groups.</p><p>Different groups capture different-scale depen- dencies by squashing the scales of forgetting rates. The groups with high forgetting rates are short-term memories, while the groups with low forgetting rates are long-term memories.</p><p>Specially, we divide the memory cells into K groups {G 1 , · · · , G K }. Each group includes a in- ternal memory c k , output gate o k and forgetting rate r k . The forgetting rate of different groups are squashed in distinct ranges.</p><p>We modify the update of a LSTM as follows.</p><formula xml:id="formula_9">r (t) k = ψ k   σ(W k r x (t) + K j=1 U j→k f h (t−1) j )   ,<label>(8)</label></formula><formula xml:id="formula_10">o (t) k = σ(W k o x (t) + K j=1 U j→k o h (t−1) j ),<label>(9)</label></formula><formula xml:id="formula_11">˜ c (t) k = tanh(W k c x (t) + K j=1 U j→k c h (t−1) j ),<label>(10)</label></formula><formula xml:id="formula_12">c (t) k = (1 − r (t) k ) c (t−1) k + (r (t) k ) ˜ c (t) k , (11) h (t) k = o (t) k tanh(c (t) k ),<label>(12)</label></formula><p>where r (t) k represents forgetting rate of the k-th memory group at step t; ψ k is a squash function, which constrains the value of forgetting rate r k within a range. To better distinguish the different role of each group, its forgetting rate is squashed into a distinct area. The squash function ψ k (z) could be formalized as:</p><formula xml:id="formula_13">r k = ψ k (z) = 1 K · z + k − 1 K ,<label>(13)</label></formula><p>where z ∈ (0, 1) is computed by logistic sigmoid function. Therefore, r k can constrain the forgetting rate in the range of ( k−1 K , k K ). Intuitively, if a forgetting rate r k approaches to 0, the group k tends to be the long-term memory; if a r k approaches to 1, the group k tends to be the short- term memory. Therefore, group G 1 is the slowest, while group G K is the fastest one. The faster groups are supposed to play a role as a cache, transiting in- formation from faster groups to slower groups.</p><p>Bidirectional CLSTM Graves and Schmidhuber (2005) proposed a Bidirectional LSTM (B-LSTM) model, which utilizes additional backward informa- tion and thus enhances the memory capability. We also employ the bi-directional mechanism on CLSTM and words in a text will receive informa- tion from both sides of the context. Formally, the outputs of forward LSTM for the k-th group is</p><formula xml:id="formula_14">[ − → h (1) k , − → h (2) k , . . . , − → h (T ) k ]. The outputs of backward LSTM for the k-th group is [ ← − h (1) k , ← − h (2) k , . . . , ← − h (T )</formula><p>k ]. Hence, we encode each word w t in a given text</p><formula xml:id="formula_15">w 1:T as h (t) k : h (t) k = − → h (t) k ⊕ ← − h (t) k ,<label>(14)</label></formula><p>where the ⊕ indicates concatenation operation.</p><p>Task-specific Output Layer for Document-level Sentiment Classification With the capability of modeling long text, we can use our proposed model to analyze sentiment in a document. <ref type="figure" target="#fig_0">Figure 2</ref> gives an overview of the architecture.</p><p>Since the first group, the slowest group, is sup- posed to keep the long-term information and can bet- ter represent a whole document, we only utilize the final state of this group to represent a document. As for the B-CLSTM, we concatenate the state of the first group in the forward LSTM at T -th time-step and the first group in the backward LSTM at first time-step. Then, a fully connected layer followed by a soft- max function is used to predict the probability distri- bution over classes for a given input. Formally, the probability distribution p is:</p><formula xml:id="formula_16">p = softmax(W p × z + b p ),<label>(15)</label></formula><p>where</p><formula xml:id="formula_17">W p and b p are model's parameters. Here z is − → h (T ) 1</formula><p>in CLSTM, and</p><formula xml:id="formula_18">z is [ − → h (T ) 1 ⊕ ← − h<label>(1)</label></formula><p>1 ] in B- CLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>The objective of our model is to minimize the cross- entropy error of the predicted and true distributions. Besides, the objective includes an L 2 regularization term over all parameters. Formally, suppose we have m train sentence and label pairs (w</p><formula xml:id="formula_19">(i) 1:T i , y (i) ) m i=1</formula><p>, the object is to minimize the objective function J(θ):</p><formula xml:id="formula_20">J(θ) = − 1 m m i=1 log p (i) y (i) + λ 2 ||θ|| 2 ,<label>(16)</label></formula><p>where θ denote all the trainable parameters of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>In this section, we study the empirical result of our model on three datasets for document-level senti- ment classification. Results show that the proposed model outperforms competitor models from several aspects when modelling long texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>Most existing datasets for sentiment classification such as Stanford Sentiment Treebank ( <ref type="bibr" target="#b32">Socher et al., 2013</ref>) are composed of short paragraphs with sev- eral sentences, which cannot evaluate the effective- ness of the model under the circumstance of encod- ing long texts. We evaluate our model on three pop- ular real-world datasets, Yelp 2013, Yelp 2014 and IMDB. <ref type="table" target="#tab_0">Table 1</ref> shows the statistical information of the three datasets. All these datasets can be publicly accessed <ref type="bibr">1</ref> . We pre-process and split the datasets in the same way as <ref type="bibr" target="#b36">Tang et al. (2015b)</ref> did.</p><p>• Yelp 2013 and Yelp 2014 are review datasets derived from Yelp Dataset Challenge 2 of year 2013 and 2014 respectively. The sentiment po- larity of each review is 1 star to 5 stars, which reveals the consumers' attitude and opinion to- wards the restaurants.</p><p>• IMDB is a popular movie review dataset con- sists of 84919 movie reviews ranging from 1 to 10 ( <ref type="bibr" target="#b5">Diao et al., 2014</ref>). Average length of each review is 394.6 words, which is much larger than the length of two Yelp review datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics</head><p>We use Accuracy (Acc.) and MSE as evaluation metrics for sentiment classification. Accuracy is a standard metric to measure the overall classification result and Mean Squared Error (MSE) is used to fig- ure out the divergences between predicted sentiment labels and the ground truth ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baseline Models</head><p>We compare our model, CLSTM and B-CLSTM with the following baseline methods.</p><p>• CBOW sums the word vectors and applies a non-linearity followed by a softmax classifica- tion layer.  <ref type="figure" target="#fig_0">(Tang et al., 2015b)</ref> 43  • JMARS is one of the state-of-the-art recom- mendation algorithm ( <ref type="bibr" target="#b5">Diao et al., 2014</ref>), which leverages user and aspects of a review with col- laborative filtering and topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB Yelp 2014 Yelp 2013 Acc. (%) MSE Acc. (%) MSE Acc. (%) MSE</head><note type="other">.5 2.566 60.8 0.584 59.6 0.615 RNN 20.5 6.163 41.0 1.203 42.8 1.144 LSTM 37.8 2.597 56.3 0.592 53.9 0.656 CIFG-LSTM 39.1 2.467 55.2 0.598 57.3 0.558 CLSTM 42.1 2.399 59.2 0.539 59.4 0.587 BLSTM 43.3 2.231 59.2 0.538 58.4 0.583 CIFG-BLSTM 44.5 2.283 60.1 0.527 59.2 0.554 B-CLSTM 46.2 2.112 61.9 0.496 59.8 0.549</note><p>• CNN UPNN (CNN) (Tang et al., 2015b) can be regarded as a CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref>. Multiple fil- ters are sensitive to capture different semantic features during generating a representation in a bottom-up fashion.</p><p>• RNN is a basic sequential model to model texts <ref type="bibr" target="#b9">(Elman, 1991)</ref>.</p><p>• LSTM is a recurrent neural network with mem- ory cells and gating mechanism <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>).</p><p>• BLSTM is the bidirectional version of LSTM, and can capture more structural information and longer distance during looking forward and back ( <ref type="bibr" target="#b11">Graves et al., 2013</ref>).</p><p>• CIFG-LSTM &amp; CIFG-BLSTM are Coupled Input Forget Gate LSTM and BLSTM, de- noted as CIFG-LSTM and CIFG-BLSTM re- spectively ( <ref type="bibr" target="#b12">Greff et al., 2015)</ref>. They combine the input and forget gate of LSTM and require smaller number of parameters in comparison with the standard LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Hyper-parameters and Initialization</head><p>For parameter configuration, we choose parameters on validation set mainly according to classification accuracy for convenience because MSE always has strong correlation with accuracy. The dimension of pre-trained word vectors is 50. We use 120 as the dimension of hidden units, and choose weight de- cay among { 5e−4, 1e−4, 1e−5 }. We use Adagrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) as optimizer and its initial learn- ing rate is 0.01. Batch size is chosen among { 32, 64, 128 } for efficiency. For CLSTM, the number of memory groups is chosen upon each dataset, which will be discussed later. We remain the total number of the hidden units unchanged. Given 120 neurons in all for instance, there are four memory groups and each of them has 30 neurons. This makes model comparable to (B)LSTM. <ref type="table" target="#tab_3">Table 3</ref> shows the optimal hyper-parameter configurations for each dataset. the validation set are chosen for final evaluation on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results</head><p>The classification accuracy and mean square error (MSE) of our models compared with other competi- tive models are shown in <ref type="table" target="#tab_2">Table 2</ref>. When comparing our models to other neural network models, we have several meaningful findings.</p><p>1. Among all unidirectional sequential models, RNN fails to capture and store semantic fea- tures while vanilla LSTM preserves sentimen- tal messages much longer than RNN. It shows that internal memory plays a key role in text modeling. CIFG-LSTM gives performance comparable to vanilla LSTM. 2. With the help of bidirectional architecture, models could look backward and forward to capture features in long-range from global per- spective. In sentiment analysis, if users show their opinion at the beginning of their review, single directional models will possibly forget these hints. 3. The proposed CLSTM beats the CIFG-LSTM and vanilla LSTM and even surpasses the bidi- rectional models. In Yelp 2013, CLSTM achieves 59.4% in accuracy, which is only 0.4 percent worse than B-CLSTM, which reveals that the cache mechanism has successfully and effectively stored valuable information without the support from bidirectional structure. 4. Compared with existing best methods, our model has achieved new state-of-the-art re- sults by a large margin on all document- level datasets in terms of classification accu- racy. Moreover, B-CLSTM even has surpassed JMARS and CNN (UPNN) methods which uti- lized extra user and product information. 5. In terms of time complexity and numbers of pa- rameters, our model keeps almost the same as its counterpart models while models of hierar- chically composition may require more compu- tational resources and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Rate of Convergence</head><p>We compare the convergence rates of our mod- els, including CIFG-LSTM, CIFG-BLSTM and B- CLSTM, and the baseline models (LSTM and BLSTM). We configure the hyper-parameter to make sure every competing model has approxi- mately the same numbers of parameters, and vari- ous models have shown different convergence rates in <ref type="figure" target="#fig_1">Figure 3</ref>. In terms of convergence rate, B-CLSTM beats other competing models. The reason why B- CLSTM converges faster is that the splitting mem- ory groups can be seen as a better initialization and constraints during the training process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Effectiveness on Grouping Memory</head><p>For the proposed model, the number of memory groups is a highlight. In <ref type="figure">Figure 4</ref>, we plot the best prediction accuracy (Y-axis) achieved in validation set with different number of memory groups on all datasets. From the diagram, we can find that our model outperforms the baseline method. In Yelp 2013, when we split the memory into 4 groups, it achieves the best result among all tested memory group numbers. We can observe the dropping trends when we choose more than 5 groups.</p><p>For fair comparisons, we set the total amount of neurons in our model to be same with vanilla LSTM. Therefore, the more groups we split, the less the neu- rons belongs to each group, which leads to a worse capacity than those who have sufficient neurons for each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Sensitivity on Document Length</head><p>We also investigate the performance of our model on IMDB when it encodes documents of different lengths. Test samples are divided into 10 groups with regard to the length. From <ref type="figure">Figure 5</ref>, we can draw several thoughtful conclusions.</p><p>1. Bidirectional models have much better perfor- mance than the counterpart models. 2. The overall performance of B-CLSTM is bet- ter than CIFG-BLSTM. This means that our model is adaptive to both short texts and long documents. Besides, our model shows power in dealing with very long texts in comparison with CIFG-BLSTM. 3. CBOW is slightly better than CIFG-LSTM due to LSTM forgets a large amount of information during the unidirectional propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we address the problem of effectively analyzing the sentiment of document-level texts in an RNN architecture. Similar to the memory struc- ture of human, memory with low forgetting rate cap- tures the global semantic features while memory with high forgetting rate captures the local seman- tic features. Empirical results on three real-world document-level review datasets show that our model outperforms state-of-the-art models by a large mar- gin. For future work, we are going to design a strategy to dynamically adjust the forgetting rates for fine- grained document-level sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of the proposed architecture. Different styles of arrows indicate different forgetting rates. Groups with stars are fed to a fully connected layers for softmax classification. Here is an instance of B-CLSTM with text length equal to 4 and the number of memory groups is 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convergence speed experiment on Yelp 2013. X-axis is the iteration epoches and Y-axis is the classifcication accuracy(%) achieved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Classification accuracy on different number of memory group on three datasets. X-axis is the number of memory group(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Statistics of the three datasets used in this paper. The rating scale (Class) of Yelp2013 and Yelp2014 range from 1 to 5 and that of IMDB ranges from 1 to 10. Words/Doc is the average length of a sample and</head><label>1</label><figDesc></figDesc><table>Dataset 

Type 
Train Size Dev. Size Test Size Class Words/Doc Sents/Doc 
IMDB 
Document 
67426 
8381 
9112 
10 
394.6 
16.08 
Yelp 2013 Document 
62522 
7773 
8671 
5 
189.3 
10.89 
Yelp 2014 Document 
183019 
22745 
25399 
5 
196.9 
11.41 

Sents/Doc is the average number of sentences in a document. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 and 
Yelp 2013. Evaluation metrics are classification accuracy (Acc.) and MSE. Models with * use user and 
product information as additional features. Best results in each group are in bold. 

Dataset 
IMDB Yelp13 Yelp14 
Hidden layer units 120 
120 
120 
Number of groups 
3 
4 
4 
Weight Decay 
1e−4 1e−4 5e−4 
Batch size 
128 
64 
64 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Optimal hyper-parameter configuration for 
three datasets. 

</table></figure>

			<note place="foot" n="1"> http://ir.hit.edu.cn/ ˜ dytang/paper/ acl2015/dataset.7z 2 http://www.yelp.com/dataset_challenge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We appreciate the constructive work from Xinchi Chen. Besides, we would like to thank the anony-mous reviewers for their valuable comments. This work was partially funded by National Natural Sci-ence Foundation of China (No. 61532011 and 61672162), the National High Technology Re-search and Development Program of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human memory: A proposed system and its control processes. The psychology of learning and motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiffrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="89" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from rst discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing,(EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing,(EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentence modeling with gated recursive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3830</idno>
		<title level="m">Modelling, visualising and summarising documents with a single convolutional neural network</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="195" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">LSTM: A Search Space Odyssey. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR), Workshop Track</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton</editor>
		<imprint>
			<biblScope unit="page" from="2304" to="2314" />
			<date type="published" when="2015" />
			<publisher>The Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-timescale long shortterm memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured models for fine-to-coarse sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerry</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Neylon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Reynar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting-Association For Computational Linguistics</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Efficient Estimation of Word Representations in Vector Space. arXiv.org</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent memory networks for language modeling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<editor>Ke Tran Arianna Bisazza Christof Monz</editor>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Document-level sentiment classification: An empirical comparison between svm and ann</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">Francisco</forename><surname>Valiati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson P Gavião</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="621" to="633" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramnath</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bryan R Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>ICWSM</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Twitter as a corpus for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREc</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1320" to="1326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04</title>
		<meeting>the 42Nd Annual Meeting on Association for Computational Linguistics, ACL &apos;04<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Salehinejad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04335</idno>
		<title level="m">Learning over long time lags</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1422" to="1432" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Semantic Representations of Users and Products for Document Level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="1014" to="1023" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="433" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-level structured models for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1046" to="1056" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
