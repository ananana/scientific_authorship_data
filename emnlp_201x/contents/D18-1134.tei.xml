<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A dataset and baselines for sequential open-domain question answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A dataset and baselines for sequential open-domain question answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1077" to="1083"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous work on question-answering systems mainly focuses on answering individual questions , assuming they are independent and devoid of context. Instead, we investigate sequential question answering, asking multiple related questions. We present QBLink, a new dataset of fully human-authored questions. We extend existing strong question answering frameworks to include previous questions to improve the overall question-answering accuracy in open-domain question answering. The dataset is publicly available at http:// sequential.qanta.org.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The framework of combining information retrieval and neural reading comprehension has been the ba- sis of several systems for answering open-domain questions over unstructured text <ref type="bibr">(Chen et al., 2017;</ref><ref type="bibr" target="#b3">Wang et al., 2018;</ref><ref type="bibr">Clark and Gardner, 2018;</ref><ref type="bibr">Htut et al., 2018)</ref>. Typically, such systems take one in- put question at a time, retrieving and ranking mul- tiple paragraphs that potentially contain the answer. A reading comprehension model then produces a ranked list of candidate answer spans from each paragraph. The final answer is then selected from the produced spans.</p><p>In information-seeking dialogs, e.g., personal assistants, users interact with a question answering system by asking a sequence of related questions, where questions share the same predicate, entities, or at least a topic. Answering each question in isola- tion is sub-optimal as information from previously asked questions and previously obtained answers can help better answer the current question.</p><p>We study the task of sequential open-domain question answering. We ask how a standard open- domain question answering system can incorpo- rate connections between question-answer pairs in the same sequence. We introduce QBLink, a new dataset of about 18,000 question sequences <ref type="figure" target="#fig_0">(Figure 1</ref>), each sequence consists of three natu- rally occurring human-authored questions (totaling around 56,000 unique questions). The sequences themselves are also naturally occurring (i.e., we do not artificially combine individually-authored questions to form sequences), which allows us to focus more on the important connections between questions that should be incorporated to improve the end-to-end question answering accuracy.</p><p>We compare sequence-aware models to base- lines that process each question separately. For our sequence-aware models, we tweak the retrieval component by incorporating previous questions and their answers together with the current ques- tion to better rank the retrieved paragraphs. For the reader, we use the semantic relations between enti- ties in previous questions (or their corresponding answers) and entities mentioned in the paragraph being read (candidate answers) to better choose the answer entity. Both the retrieval and reading steps can be slightly improved by incorporating sequence information.</p><p>Our contributions are two-fold: first, we present a new dataset for sequential question answering. Our dataset is composed of complex questions about a variety of topics. We make the dataset pub- licly available to encourage future research. Sec- ond, we use our dataset to compare baselines in the open-domain question answering setup with the goal of showing that incorporating sequential connections between questions is advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sequential Question Answering Task</head><p>We define the task of open-domain sequential question answering: given a document collection D and questions grouped into disjoint sequences {S i | i = 1 . . . n} where each S i is an ordered se- quence of question, answer pairs, and a subset of documents</p><formula xml:id="formula_0">S i = ((q j i , a j i , D j i ) | j = 1 . . . m)</formula><p>, the task is to answer questions q ˆ j i with document evi- dences D ˆ j i given access to previously asked ques- tions in the same sequence and their corresponding answers {(q j i , a j i ) | j &lt; ˆ j}. Following <ref type="bibr">Chen et al. (2017)</ref>, we split the task into two steps-a retrieval step and a reading step. In the retrieval step the current question q ˆ j i and previous questions and answers {(q j i , a j i ) | j &lt; ˆ j} are used to retrieve a ranked list of paragraphs D </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Construction</head><p>This section describes QBLink's construction. QBLink is based on the bonus questions of Quiz Bowl tournaments. Unlike previous work that only uses the starter (or tossup) questions (Boyd-Graber et al., 2012), bonus questions are not interruptable (players always hear the complete question) and have greater variability in difficulty. Bonus ques- tions start with a lead-in text, which sets the stage for the rest of the question, followed by a sequence of related questions. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of a sequence of three questions.  Specifically, we collect bonus questions from http://quizdb.org for the tournaments in <ref type="bibr">[2008]</ref><ref type="bibr">[2009]</ref><ref type="bibr">[2010]</ref><ref type="bibr">[2011]</ref><ref type="bibr">[2012]</ref><ref type="bibr">[2013]</ref><ref type="bibr">[2014]</ref><ref type="bibr">[2015]</ref><ref type="bibr">[2016]</ref><ref type="bibr">[2017]</ref><ref type="bibr">[2018]</ref>. Each question is categorized by topic as history, literature, science, geography, fine arts, philosophy, religion, mythology, social sciences, current events or trash. We filter out too short questions (fewer than ten tokens), and only keep questions with exactly three sub-questions.</p><p>We map the answers to unambiguous Wikipedia pages using combination of rule based match- ing and fuzzy string matching, then filter out the questions whose answers are not mapped to any Wikipedia page (12.5% of the questions).</p><p>To keep our development and test set intact and and of a reasonable percentage of questions, we use the questions in 2014 tournament (the year with the largest number of questions) for devel- opment and testing, and the rest of the questions are used for training. <ref type="table" target="#tab_1">Table 1</ref> shows the number of question sequences and questions per split as well as tokens and linked Wikipedia mentions per question. We use TagMe <ref type="bibr">(Ferragina and Scaiella, 2010)</ref> for mention detection and linking question text to Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baselines</head><p>We build our baselines on the DrQA framework of <ref type="bibr">Chen et al. (2017)</ref> for open-domain question an- swering over Wikipedia. <ref type="bibr">1</ref> The framework operates in a retrieval phase followed by a reading phase.</p><p>The retrieval phase uses a simple tf-idf <ref type="bibr">(Salton and Buckley, 1987)</ref> ranking of Wikipedia articles given a question as the query.</p><p>The reading phase is a multi-layer recurrent neural network model that extracts an answer span from the top d retrieved paragraphs. The reader model computes a contextualized represen- tation of each token t i by running the token se- quence through a multi-layer bidirectional long short-term memory network (BiLSTM) (Hochre- iter and Schmidhuber, 1997) and taking the corre- sponding hidden state to each token at the top layer. The question itself is encoded in a vector q as a weighted average of the hidden states of a BiLSTM over the word embeddings of its individual tokens. The model then computes an unnormalized proba- bility score of t i as the start and end token of the answer span,</p><formula xml:id="formula_1">Start(i) = exp(t i T W start q);</formula><formula xml:id="formula_2">End(i) = exp(t i T W end q).<label>(1)</label></formula><p>To find the answer in multiple paragraphs at test time, we merge all paragraphs before feeding them to the reader (Clark and Gardner, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Answering Question in Isolation</head><p>We experiment with three models that ignore the sequential connections between questions and an- swer each question in isolation. Our first model is a simple information retrieval (IR) baseline that only uses the retrieval component: the title of the top-1 Wikipedia article is predicted as the answer. Our second baseline is the full DrQA base- line whose reader is trained/tuned on the train- ing/development questions of our dataset. To as- sign paragraphs to each of the training questions, we follow a similar distant-supervision approach to <ref type="bibr">Chen et al. (2017)</ref>. We retrieve the top twenty Wikipedia articles for each question, exclude the paragraphs that do not contain the gold answer, and then rank the remaining paragraphs using tf- idf. Each of the top ten paragraphs is paired with the question to form a data instance for train- ing the reader. <ref type="bibr">1</ref> We use the Wikipedia dump of 2017-09-20.</p><p>Finally, we tweak the DrQA reader to limit the candidate answer spans to entity mentions that are linked to Wikipedia. We set the pre-normalization start and end scores of spans that are not detected mentions to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incorporating Context in Retrieval</head><p>To incorporate the sequential connections between questions in the retrieval phase, we append the previously asked questions to the current question. We also compare appending the predicted answers (top-1 span) to each of the previous questions as well as the gold answers to the current question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Incorporating Context in Reader</head><p>In addition to encoding which entities have ap- peared in previous questions, we also want to provide our models with relationship information. However, pre-defined relationships from knowl- edge bases tend to be brittle. Instead, we use a continuous representation of relationships <ref type="bibr">(Iyyer et al., 2016)</ref>. For example, suppose we want to encode the relationships for an entity (answer can- didate) that starts at i and ends at j. We summarize that entities relationships from each of possible k relation-spans. A relation-span is a sequence of to- kens from Wikipedia that contains both the answer candidate and an answer to a previous question (For example, the correct answer in <ref type="figure" target="#fig_2">Figure 2</ref> has a relation-span "He is best known for defending President Ronald Reagan during the assassination attempt by John Hinckley Jr." with the previous answer "Ronald Reagan"). This is summarized in a vector r ij by merging all k relation-spans in a single span that is then fed through a BiLSTM whose hidden states are combined as a weighted sum where the weights are computed with self- attention ( <ref type="bibr">Lin et al., 2017)</ref>.</p><p>The stronger the similarity between the relation that the question is asking about and the relation- spans, the higher the score of the candidate answer should be. We estimate that similarity r by con- catenating the elementwise absolute difference and hadamard product between r ij and the question embedding q. Then, we use a trainable weight vector w rel to combine the components of the concatenation output and produce a single simi- larity score as</p><formula xml:id="formula_3">r = w rel T [|q − r ij |; q • r ij ].</formula><p>This can then influence the final selection of the answer span by adding the relation similarity <ref type="table">Table 2</ref>: Incorporating sequence information in the re- trieval and the reading step slightly improves overall accuracy compared to answering questions in isolation.</p><note type="other">Method EM Baselines: Questions in Isolation IR 15.6 DrQA 39.3 DrQA + limiting to entities 39.7 DrQA + Retrieval with context Previous questions 36.4 Previous predicted answers 39.8 Previous gold answers 40.1 DrQA + Reading with context Append relation descriptions w/ predicted answers 40.2 Append relation descriptions w/ gold answers 40.7 Explicit relation embedding w/ predicted answers 38.3 Explicit relation embedding w/ gold answers 39.5 IR w/ Previous gold answers + Reading w/ Append relation descriptions w/ gold answers 40.7</note><p>score r to the start and end scores of the candidate answer (Equation 1) as</p><formula xml:id="formula_4">Start(i) = exp(t i T W start q + r) End(j) = exp(t j T W end q + r).<label>(2)</label></formula><p>The relation embedding module is trained jointly with the reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baseline Results</head><p>We use QBLink to compare the baselines' ques- tion answering accuracy. Incorporating previous questions and answers slightly improves the accu- racy <ref type="table">(Table 2)</ref>. We set the maximum number of retrieved doc- uments to ten, and each document is divided into paragraphs each of 400 tokens. At test time, we merge the top ten ranked such paragraphs and feed them to the reader. We use the reader network of <ref type="bibr">Chen et al. (2017)</ref>. We limit the number of re- lation description spans for each entity pair to five. We used an LSTM of one hidden layer and 128 hid- den units for the paragraph, question, and relation description encoders. Each reader was trained for twenty epochs. <ref type="table">Table 2</ref> summarizes the results of the baselines (Section 4). Question-answering accuracy is exact- match accuracy since we limit the answer spans to entity mentions whose boundaries are fixed for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question: This man attempted to impress Jodie Foster by shooting Ronald Reagan, but he failed to kill the Pres- ident. At trial, he was found not guilty by reason of insanity. Gold answer to previous question: Ronald Reagan Predict without relation span: George H. W. Bush</head><p>Correct answer: John Hinckley Jr. Relation span: He is best known for defending Presi- dent Ronald Reagan during the assassination attempt by John Hinckley Jr. Incorporating the previous answer in the retrieval and the reading components slightly improves the overall question answering accuracy ( <ref type="table">Table 2</ref>). The accuracy drops by more than 3% when using the en- tire text of previous questions in the retrieval phase. Modeling relations reduces the accuracy slightly compared to augmenting paragraphs with relation spans. One possible explanation is that our rela- tion embedding model ends up being under-trained since we could not retrieve any relation-spans for many questions. Replacing Wikipedia with a larger corpus (e.g., ClueWeb) might help improve the training of the relation embedding model. Unsur- prisingly, the gold answers to previous questions are more useful than the predicted answers, which highlights a need for models that take into account the uncertainty about previous answers when gold previous answers are not available. However, pro- viding answers to previous questions is consistent for most Quiz Bowl tournament play. <ref type="figure" target="#fig_2">Figure 2</ref> gives an example of how explicit rela- tion embedding helps reader get a correct predic- tion. Without the relation span, the model predicts George H. W. Bush (vice president at that time) as correct answer. Including the direct relation span between Reagan and John Hinckley Jr., the model gets the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work and Discussion</head><p>We adopt the open-domain question answering framework ( <ref type="bibr" target="#b3">Wang et al., 2018;</ref><ref type="bibr">Chen et al., 2017)</ref>. Previous work considers improving that base framework itself <ref type="bibr">(Clark and Gardner, 2018;</ref><ref type="bibr">Swayamdipta et al., 2018, inter alia)</ref>. But retains the assumption of answering individual questions.</p><p>Aside from the open-domain setup, much of the recent work on question answering has focused on the sub-problem of reading-comprehension, where the gold answer to each question is assumed to exist in a given single paragraph for the model to read <ref type="bibr">(Hermann et al., 2015;</ref><ref type="bibr">Rajpurkar et al., 2016;</ref><ref type="bibr">Seo et al., 2017)</ref>. Another line of work on ques- tion answering is question answering over struc- tured knowledge-bases ( <ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr">Berant and Liang, 2014;</ref><ref type="bibr" target="#b5">Yao and Van Durme, 2014;</ref><ref type="bibr">Gardner and Krishnamurthy, 2017)</ref>. Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the reading- comprehension setup as well as the question an- swering over knowledge-bases setup.</p><p>Several question answering datasets have been proposed <ref type="bibr" target="#b1">(Berant et al., 2013;</ref><ref type="bibr">Joshi et al., 2017;</ref><ref type="bibr" target="#b2">Trischler et al., 2017;</ref><ref type="bibr">Rajpurkar et al., 2018, inter alia)</ref>. However, all of them were limited to answer- ing individual questions. <ref type="bibr">Saha et al. (2018)</ref> study the problem of sequen- tial question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset con- struction was overly synthetic: templates were col- lected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual ques- tions by predicate or subjects.</p><p>Both <ref type="bibr">Iyyer et al. (2017)</ref> and <ref type="bibr">Talmor and Berant (2018)</ref> answer complex questions by decomposing each into a sequence of simple questions. <ref type="bibr">Iyyer et al. (2017)</ref> adopt a semantic parsing approach to answer questions over semi-structured tables. They construct a dataset of around 6,000 question sequences by asking humans to rewrite a set of 2,000 complex questions into simple sequences. <ref type="bibr">Talmor and Berant (2018)</ref> consider the setup of open-domain question answering over unstructured text, but their dataset is constructed synthetically (with human paraphrasing) by combining simple questions with a few rules.</p><p>In parallel to our work, <ref type="bibr">Choi et al. (2018)</ref> and <ref type="bibr">Reddy et al. (2018)</ref> introduce sequential question answering datasets (QuAC and CoQA) that focus on the reading comprehension setup (i.e., a sin- gle text snippet is pre-specified for answering the given questions). QBLink is entirely naturally oc- curring (all questions and answers were authored independently from any knowledge sources) and is primarily designed to challenge human players.</p><p>The idea of our baseline to improving the reading step by incorporating additional relation descrip- tion spans is similar as <ref type="bibr" target="#b4">Weissenborn et al. (2017)</ref> and <ref type="bibr">Mihaylov and Frank (2018)</ref>, who integrate background commonsense knowledge into reading- comprehension systems. Both rely on structured knowledge bases to extract information about se- mantic relations that hold between entities. On the other hand, we extract text spans that mention each pair of entities and encoded them into vector representations of the relations between entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We introduce QBLink, a dataset of 56,000 natu- rally occurring sequential question, answer pairs. The questions are designed primarily to challenge human players in Quiz Bowl tournaments. We use QBLink to evaluate baselines for sequential open-domain question answering. We show that incorporating sequential information helps slightly improve question answering accuracy.</p><p>In the future, we would like to invest in building better sequential question answering models that push the accuracy beyond the presented baselines. Specifically, we will look at how to better model the interaction between the reader and the relation embedding model and how to improve the relation embedding model itself by adopting ideas from the relation extraction literature <ref type="bibr">(Miwa and Bansal, 2016;</ref><ref type="bibr">Peng et al., 2017;</ref><ref type="bibr" target="#b0">Ammar et al., 2017</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example sequence of questions from QBLink. The lead-in and question 1 are asking about the same object/answer. The subject of question 2 is the same as the object of question 1. All questions are about a narrow topic, Bitcoin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>from D that are likely to contain the correct answer to the current question q ˆ j i . The retrieved paragraphs D ˆ j i are the input to the reading step that selects a span from D ˆ j i as the answer to q ˆ j i . The reading step has access to previous questions and answers {(q j i , a j i ) | j &lt; ˆ j} as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Modeling the relation between President Ronald Reagan and John Hinckley Jr. expressed by relation span helps the reader select the correct answer entity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Statistics about QBLink. Most questions are fairly long and contain 2.5 entity mentions in average which demonstrates the complexity of the questions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Jonathan Berant and Percy Liang. 2014. Semantic pars-
ing via paraphrasing. In Association for Computa-
tional Linguistics. 

Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal 
Daumé III. 2012. Besting the quiz master: Crowd-
sourcing incremental classification games. In Em-
pirical Methods in Natural Language Processing. 

Danqi Chen, Adam Fisch, Jason Weston, and Antoine 
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Association for Computa-
tional Linguistics. 

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen 
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. Quac: Question answering in context. 
In Empirical Methods in Natural Language Process-
ing. 

Christopher Clark and Matt Gardner. 2018. Simple 
and effective multi-paragraph reading comprehen-
sion. In Association for Computational Linguistics. 

Paolo Ferragina and Ugo Scaiella. 2010. Tagme: 
on-the-fly annotation of short text fragments (by 
wikipedia entities). In ACM International Confer-
ence on Information and Knowledge Management. 

Matt Gardner and Jayant Krishnamurthy. 2017. Open-
vocabulary semantic parsing with both distributional 
statistics and formal knowledge. In AAAI Confer-
ence on Artificial Intelligence. 

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, 
and Phil Blunsom. 2015. Teaching machines to read 
and comprehend. In Advances in Neural Informa-
tion Processing Systems. 

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long 
short-term memory. Neural computation 9(8):1735-
1780. 

Phu Mon Htut, Samuel R Bowman, and Kyunghyun 
Cho. 2018. Training a ranking function for open-
domain question answering. In North American 
Chapter of the Association for Computational Lin-
guistics: Student Research Workshop. 

Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jor-
dan Boyd-Graber, and Hal Daumé III. 2016. Feud-
ing families and former friends: Unsupervised learn-
ing for dynamic fictional relationships. In North 
American Association for Computational Linguis-
tics. 

Mohit Iyyer, Wen tau Yih, and Ming-Wei Chang. 2017. 
Search-based neural structured learning for sequen-
tial question answering. In Association for Compu-
tational Linguistics. 

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke 
Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Association for Computational Lin-
guistics. 

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua 
Bengio. 2017. A structured self-attentive sentence 
embedding. In International Conference on Learn-
ing Representations. 

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: Enhancing cloze-style reading compre-
hension with external commonsense knowledge. In 
Association for Computational Linguistics. 

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree 
structures. In Association for Computational Lin-
guistics. 

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina 
Toutanova, and Wen-tau Yih. 2017. Cross-sentence 
n-ary relation extraction with graph LSTMs. Trans-
actions of the Association for Computational Lin-
guistics 5. 

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. 
SQuAD: 100,000+ questions for machine compre-
hension of text. In Empirical Methods in Natural 
Language Processing. 

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. 
Know what you don't know: Unanswerable ques-
tions for SQuAD. In Association for Computational 
Linguistics. 

Siva Reddy, Danqi Chen, and Christopher D Manning. 
2018. CoQA: A conversational question answering 
challenge. In Empirical Methods in Natural Lan-
guage Processing. 

Amrita Saha, Vardaan Pahuja, Mitesh M Khapra, 
Karthik Sankaranarayanan, and Sarath Chandar. 
2018. Complex sequential question answering: To-
wards learning to converse over linked question an-
swer pairs with a knowledge graph. In AAAI Confer-
ence on Artificial Intelligence. 

Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical 
report, Cornell University. 

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and 
Hannaneh Hajishirzi. 2017. Bidirectional attention 
flow for machine comprehension. In International 
Conference on Learning Representations. 

Swabha Swayamdipta, Ankur P Parikh, and Tom 
Kwiatkowski. 2018. Multi-mention learning for 
reading comprehension with neural cascades. In 
International Conference on Learning Representa-
tions. 

Alon Talmor and Jonathan Berant. 2018. The web as 
a knowledge-base for answering complex questions. 
In North American Association for Computational 
Linguistics. </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and members of the UMD CLIP lab for their comments and sug-gestions. Jordan Boyd-Graber is supported by NSF Grant IIS-1652666. Ahmed Elgohary is supported by an IBM PhD fellowship. Any opinions, find-ings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The AI2 system at SemEval-2017 task 10 (ScienceIE): semi-supervised end-to-end entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2nd Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evidence aggregation for answer re-ranking in opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic integration of background knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02596</idno>
	</analytic>
	<monogr>
		<title level="m">neural NLU systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
