<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disambiguated skip-gram model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Grzegorczyk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">AGH University of Science and Technology</orgName>
								<address>
									<settlement>Kraków</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Allegro, Pozna´nPozna´n</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Kurdziel</surname></persName>
							<email>{kgr,kurdziel}@agh.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">AGH University of Science and Technology</orgName>
								<address>
									<settlement>Kraków</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disambiguated skip-gram model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1445" to="1454"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1445</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present disambiguated skip-gram: a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a word sense disambiguation model. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neu-ral network. We also introduce an effective pruning strategy for the embeddings learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our model. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words find applica- tions in a broad range of tasks, from natural lan- guage parsing <ref type="bibr" target="#b20">(Socher et al., 2013</ref>) to image cap- tioning <ref type="bibr" target="#b8">(Karpathy and Fei-Fei, 2015)</ref>. Their use- fulness led to a renewed interest in word embed- ding algorithms. The most popular algorithms of this kind learn word vectors in an unsuper- vised manner, e.g., from word contexts ( <ref type="bibr" target="#b13">Mikolov et al., 2013a</ref>) or from statistics of word co- occurrence ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>). Unsuper- vised learning of word embeddings has a clear advantage: both general and domain-specific text corpora are available for a number of languages, which greatly reduces the cost of training. That said, unsupervised learning of word embeddings comes with its own challenges. One of the most important is word ambiguity: words in a natural language often have more than one meaning. The word mouse, for example, may mean a pointing device or an animal. Word embedding algorithms often do not recognize this language feature and estimate only one vector representation per word. This may lead to suboptimal word representations.</p><p>The main contribution of this work is disam- biguated skip-gram: a neural-probabilistic model for learning distributed representations of words that capture word ambiguity. Disambiguated skip- gram builds upon the skip-gram model introduced by <ref type="bibr">Mikolov et al. (2013a,b)</ref>. Skip-gram constructs word embeddings via an auxiliary prediction task: given a word in a sentence, skip-gram attempts to predict the surrounding words. To this end, skip- gram defines a simple softmax model for the con- ditional probability of observing a context word c given the center word w:</p><formula xml:id="formula_0">p (c | w) = e v T w uc c ∈D e v T w u c ,<label>(1)</label></formula><p>where D is the vocabulary. This log-bilinear model assigns two embedding vectors to every word w ∈ D: an input embedding vector v w and an output embedding vector u w . Skip-gram de- fines the training objective for a single example as the log-probability: log p (c | w). By maximizing this objective, skip-gram estimates input and out- put vectors that reflect semantic relations between words that occur in similar contexts. The input vectors are then used as word embeddings.</p><p>The main idea behind disambiguated skip-gram is to jointly learn to disambiguate words and pre- dict their contexts. We therefore extend skip- gram with a parametric word sense disambigua- tion model. This allows us to discover word senses in an unsupervised manner, while preserving the simplicity of the skip-gram approach. In particu- lar, unlike previous probabilistic models for multi- sense word embeddings, disambiguated skip-gram can be seen as a simple feed-forward neural net- work amendable to end-to-end training with back-propagation. Furthermore, disambiguated skip- gram admits an effective pruning strategy for the learned word sense embeddings. In particular, we control the granularity of the learned repre- sentations by penalizing the entropy of the prob- ability distributions learned by the disambiguation model. We then marginalize word sense probabil- ities over the training examples and prune embed- dings with low marginal probability.</p><p>We have carried out an extensive experimental evaluation of disambiguated skip-gram. Our re- sults demonstrate that the multi-sense word em- beddings learned by disambiguated skip-gram im- prove state-of-the-art results in the word sense in- duction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Disambiguated skip-gram model</head><p>Let X = [w 1 , . . . , w n ], where each w i ∈ D, be a sequence of words from a vocabulary D. By C w i we denote the context of the word w i in X. For example, C w i can be a set of words that are no fur- ther than l positions from w i and are in the same sentence as w i . To simplify notation we will usu- ally omit the sequence index i and write w ∈ X for an element of the input sequence X and C w for its context. We will also use notation y w for a vector y (from some set of vectors indexed by the vocabulary words) corresponding to the word w. Note that in this case we disregard the position of w in X and use just the word as the index. In particular, if some word w occurs multiple times in X, all occurrences share a single vector y w .</p><p>Similarly to skip-gram, the disambiguated skip- gram model constructs word embeddings by learn- ing to predict context words c ∈ C w given the center word w ∈ X. However, disambiguated skip-gram explicitly accounts for word ambiguity. To this end, we represent each word d ∈ D by a set of k sense embedding vectors v dz , indexed by z ∈ {1, . . . k}, and an output embedding vec- tor u d . We then parametrize the conditional prob- ability p (c | w, z = j) of observing a word c in the context of the word w in its j-th sense with a softmax model similar to the original skip-gram parametrization:</p><formula xml:id="formula_1">p (c | w, z = j) = e v T wj uc c ∈D e v T wj u c .<label>(2)</label></formula><p>Furthermore, in this work we assume that a sense of the word w ∈ X can be guessed from its con- text C w 1 , i.e. that:</p><formula xml:id="formula_2">z w ∼ p (z = j | w, C w ) , j = 1, . . . , k, (3)</formula><p>where z w is an index of the sense of the word w ∈ X. Given this assumption, we parametrize the probability distribution for z w using a softmax model similar to Eq. 2. That is, for each word d ∈ D we introduce k sense disambiguation vec- tors q ds , s ∈ {1, . . . , k}, and a context embed- ding vector r d . The conditional probability that the word w ∈ X occurs in its j-th sense is then modelled as:</p><formula xml:id="formula_3">p (z = j | w, C w ) = e q T wj ¯ rw s=1,...,k e q T ws ¯ rw ,<label>(4)</label></formula><p>where ¯ r w is a vector representation of C w . We rep- resent C w by an average of context embedding vec- tors:</p><formula xml:id="formula_4">¯ r w = 1 #C w c∈Cw r c .<label>(5)</label></formula><p>We can now define the training objective for a sin- gle word w ∈ X as the expected negative log- likelihood of observing the context C w under the distribution of senses of the center word w:</p><formula xml:id="formula_5">L (φ, w) =E zw∼p(z=j|w,Cw) − log c∈Cw p (c | w, z = z w ) ,<label>(6)</label></formula><p>with the parameters:</p><formula xml:id="formula_6">φ = {v dz , u d , q dz , r d |d ∈ D, z = 1, . . . , k}.</formula><p>The objective in Eq. 6 is inconvenient for gradient-based optimization, because the expecta- tion is taken with respect to a probability distri- bution that is a function of model parameters. In principle, we can estimate the gradient of this ob- jective with the score function estimator <ref type="bibr" target="#b2">(Glynn, 1990;</ref><ref type="bibr" target="#b24">Williams, 1992)</ref>. Unfortunately, the score function estimator suffers from a high variance, even when used with a control variate. One can also derive a low-variance unbiased gradient esti- mator for certain probability distributions, by ex- pressing the samples as a differentiable function of model parameters and a random variable from some independent fixed distribution ( <ref type="bibr" target="#b9">Kingma and Welling, 2013)</ref>. This approach is not directly ap- plicable to our case, because categorical distribu- tion do not admit reparametrization with a differ- entiable function. That said, a simple and effective biased gradient estimator for an expectation with respect to a categorical distribution was recently proposed by <ref type="bibr" target="#b6">Jang et al. (2016)</ref> and <ref type="bibr" target="#b12">Maddison et al. (2016)</ref>. The basic idea is to reparametrize the samples from the categorical distribution with the Gumbel-Max trick <ref type="bibr" target="#b3">(Gumbel, 1954)</ref> and then approximate the non-differentiable max operator with a softmax function with temperature hyper- parameter. This can be seen as a reparametriza- tion trick for a continuous relaxation to discrete samples from the categorical distribution.</p><p>In our case, the samples from p (z = j | w, C w ) take the form:</p><formula xml:id="formula_7">z wj =        1 if j = arg max s (ξ s + log p (z = s | w, C w )), 0 otherwise,</formula><p>where ξ s are i.i.d. samples from the standard Gumbel distribution f (0, 1). Note that the sam- ples z w = [z <ref type="bibr">w1</ref> , . . . , z wk ] are now one-hot en- coded. The continuous relaxation to z w is:</p><formula xml:id="formula_8">˜ z w = [˜ z wj (ξ 1 , . . . , ξ k , C w ) | j = 1 . . . k] , ˜ z wj (ξ 1 , . . . , ξ k , C w ) = e [ξ j +log p(z=j|w,Cw)]/τ s=1,...,k e [ξs+log p(z=s|w,Cw)]/τ , (7)</formula><p>where τ is the temperature hyper-parameter. When τ → 0, we recover the samples from p (z = j | w, C w ). However, for τ &gt; 0 the sam- ples˜zples˜ ples˜z w are no longer discrete. In this case we consider a relaxed sense embedding vector: <ref type="formula">(8)</ref> and model the conditional probability of observing a word c in the context of the word w as:</p><formula xml:id="formula_9">˜ v w = j=1,...,k ˜ z wj (ξ 1 , . . . , ξ k , C w ) v wj</formula><formula xml:id="formula_10">p (c | w, ˜ v w ) = e ˜ v T w uc c ∈D e ˜ v T w u c .<label>(9)</label></formula><p>Our training objective for a single word w ∈ X then takes the form:</p><formula xml:id="formula_11">L (φ, w) =E ξ 1 ,...,ξ k ∼f (0,1) − log c∈Cw p (c | w, ˜ v w ) .<label>(10)</label></formula><p>The relaxed objective in Eq. 10 is tractable and differentiable with respect to the parameters φ.  When τ → 0, it becomes equivalent to the ob- jective in Eq. 6. In practice, we approximate the expectation in Eq. 10 with a one-sample Monte Carlo estimator. In these settings disambiguated skip-gram can be seen as a simple feed-forward neural network pictured in <ref type="figure" target="#fig_0">Fig. 1</ref>. During training, the network jointly estimates a sense disambigua- tion model (Eq. 4) and a context word prediction model (Eq. 2), which we use to construct multi- sense word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pruning word senses</head><p>The disambiguated skip-gram model is paramet- ric, i.e. it allocates a fixed number of sense em- bedding vectors to each word, even though dif- ferent words have different number of discernible senses. That said, we can prune the sense embed- ding vectors by considering their probabilities ac- cording to the learned disambiguation model. In particular, after training we estimate the marginal probability:</p><formula xml:id="formula_12">p (d, j) = 1 m d w i ∈X, w i =d p (z = j | w i , C w i ) (11)</formula><p>for each word d ∈ D and each sense index j ∈ {1, . . . , k}. We then prune sense embedding vec- tors with low marginal probability, e.g., p (d, j) &lt; 0.05. The normalizing factor m d in Eq. 11 is the number of occurrences of the word d in X.</p><p>The above pruning technique can be extended to allow for an explicit control over the granular- ity of the learned sense representations. To this end, we use an entropy regularization term sim- ilar to the one studied by <ref type="bibr" target="#b17">Pereyra et al. (2017)</ref> in classification networks. In disambiguated skip-gram the granularity of the learned repre- sentations is controlled by the disambiguation model <ref type="bibr">(Eq. 4)</ref>. Therefore, we extend the objective of our model (Eq. 10) by adding to it an entropy S of the probability distribution p (z = j | w, C w ):</p><formula xml:id="formula_13">L r (φ, w) = L (φ, w) + γS (φ, w) ,<label>(12)</label></formula><p>where:</p><formula xml:id="formula_14">S (φ, w) = − k j=1 p (z = j | w, C w ) log p (z = j | w, C w ) .</formula><p>The hyper-parameter γ, which we further call en- tropy cost, controls the strength of the regulariza- tion and, in turn, the granularity of the learned sense representations. In particular, γ &gt; 0 encour- ages the model to learn more coarse-grained sense representations, whereas γ &lt; 0 increases the gran- ularity of the learned senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Algorithms for learning distributed multi-sense representations of words have been a focus of sev- eral recent works. Initial approaches to this task relied on clustering word contexts. One of the first algorithms of this kind was proposed by <ref type="bibr" target="#b4">Huang et al. (2012)</ref>. They learn multi-sense word repre- sentations in three steps. First, they estimate vec- tor representations of words using a feed-forward neural language model. Next, they calculate av- erage word vector for each context in the training corpus, cluster these context representations and relabel each word in the corpus to a word sense represented by the nearest cluster. Finally, they train the language model on the relabelled corpus and obtain vector representations for word senses. <ref type="bibr" target="#b15">Neelakantan et al. (2014)</ref> proposed the Multi- Sense Skip-gram (MSSG) model, that jointly learns context cluster prototypes and word sense embeddings. Their model extends skip-gram by maintaining context clusters for every word in the vocabulary. Given a training example with a cen- ter word w and its context representation c, they infer the word sense for w by a hard assignment of the context representation c to the cluster with the nearest centroid. Afterwards, they perform a skip- gram-like update on the vector representation of the selected word sense and the output vectors of the context words. Neelakantan et al. also pro- posed a non-parametric version of MSSG (NP- MSSG), in which the number of clusters, and in turn the number of word senses, increases during training. They use a simple heuristic to determine the number of word senses: NP-MSSG allocates a new sense for the center word w when the similar- ity between the context representation c and the nearest cluster centroid falls below some prede- fined threshold. Neelakantan et al. demonstrated that MSSG and NP-MSSG outperform the Huang et al. algorithm on a contextual word similarity task.</p><p>A disadvantage of the Huang et al. and Nee- lakantan et al. algorithms is that they do not fol- low a principled statistical approach, but instead rely on hard clustering of context vectors. This has been addressed in more recent algorithms, which learn multi-sense word representations in a probabilistic framework. Concurrent to Neelakan- tan et al. work, Tian et al. <ref type="formula" target="#formula_0">(2014)</ref> proposed a probabilistic Multi-Prototype Skip-Gram (MPSG) model. MPSG extends the skip-gram model by adding to each position in the input text a latent variable that encodes the index of the word sense at that position. Furthermore, for each word in the vocabulary MPSG maintains a fixed number of sense embedding vectors and a single output vec- tor. These parameters define a softmax model for the conditional probability of observing a context word given the center word and the latent sense in- dex. Finally, MPSG models the conditional prob- ability of observing a context word given the cen- ter word with a mixture model whose components correspond to the senses of the center word. Tian et al. derived an expectation maximization algo- rithm for estimation of softmax parameters and prior sense probabilities in their model. MPSG was evaluated in a contextual word similarity task, where its performance was similar to that of the Huang et al. algorithm. <ref type="bibr" target="#b1">Bartunov et al. (2016)</ref> proposed the AdaGram model, which can be seen as a non-parametric ver- sion of MPSG. Similarly to MPSG, AdaGram in- troduces latent variables for word sense indexes in the input text. However, unlike MPSG, AdaGram does not assume a fixed number of word senses.</p><p>Instead, it defines the prior over word senses via a Dirichlet process. As a result, AdaGram au- tomatically learns the number of senses for all words in the vocabulary. Unfortunately, defining the prior over word senses via a Dirichlet pro- cess gives an intractable model likelihood. <ref type="bibr">Bartunov</ref> </p><note type="other">et al. therefore optimize variational lower bound of the AdaGram model likelihood using a stochastic variational inference algorithm. Bar- tunov et al. evaluated AdaGram performance on several word sense induction benchmarks. They demonstrated that AdaGram consistently outper- forms MSSG, NP-MSSG and MPSG models in these benchmarks. AdaGram has been recently extended to handle parallel multilingual text cor- pora (Upadhyay et al., 2017).</note><p>For the prediction of context words (Eq. 2) dis- ambiguated skip-gram adopts the softmax model used in MPSG. However, in contrast to the previ- ous works, disambiguated skip-gram learns a para- metric model for the conditional probability dis- tribution over senses of the center word given the context words <ref type="bibr">(Eq. 4)</ref>. This allows us to define the training objective for disambiguated skip-gram as the expected negative log-likelihood of observing the context words under the distribution of senses of the center word. We use a biased low-variance gradient estimator for this objective, which en- ables stable end-to-end training with backpropa- gation.</p><p>The main goal of the AdaGram model is to auto- matically discover the number of word senses for the vocabulary words. This does not mean, how- ever, that the number of senses learned by Ada- Gram is independent of model hyper-parameters. On the contrary, the number of senses learned by AdaGram is directly controlled by the hy- per parameter α in the Dirichlet process used to define the prior over word meanings ( <ref type="bibr" target="#b1">Bartunov et al., 2016)</ref>. Disambiguated skip-gram controls the number of learned senses by penalizing the en- tropy of the conditional probability distribution in the sense disambiguation model (Eq. 12). The en- tropy cost γ in this approach performs a function similar to the hyper-parameter α in AdaGram.</p><p>In addition to the works discussed above, word ambiguity was also modelled using topic mod- els ( <ref type="bibr" target="#b11">Liu et al., 2015</ref>), large bi-directional language models ( <ref type="bibr" target="#b18">Peters et al., 2018)</ref> or subword informa- tion ( <ref type="bibr" target="#b0">Athiwaratkun et al., 2018)</ref>. Also, Li and Ju- rafsky (2015) evaluated multi-sense embeddings in several downstream tasks. They found that multi-sense embeddings improve performance in tasks such as POS tagging or identification of se- mantic relations. They also identify downstream tasks which do not benefit from sense disambigua- tion. In sentiment analysis, for example, word sentiment usually does not depend on the inferred sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted a number of experiments to eval- uate the quality of multi-sense word embeddings learned by disambiguated skip-gram. This section reports results from our evaluation. First, we re- port qualitative results from our model for several polysemous words. We then compare the perfor- mance of disambiguated skip-gram with several competing algorithms on a set of word sense in- duction tasks. Finally, we evaluate the effect of the entropy cost on the learned representations.</p><p>It is worth noting that the quality of multi- sense word embeddings was formerly assessed in contextual word similarity experiments. How- ever, <ref type="bibr" target="#b1">Bartunov et al. (2016)</ref> demonstrated that con- textual word similarity experiments do not reflect the quality of multi-sense representations. In par- ticular, the best performance in contextual word similarity task is often achieved by the baseline skip-gram model, which does not recognize word senses. This can be attributed to the fact that skip-gram objective directly optimizes similarity of vector representations of words that appear in similar contexts. Multi-sense models, on the other hand, solve a harder task: they disambiguate words in contexts and then model the similarities between the discovered senses. Bartunov et al. fo- cus, therefore, on the performance of multi-sense embeddings in the word sense induction task. We adopt their evaluation methodology in this work.</p><p>We trained our disambiguated skip-gram mod- els on the Westbury Lab Wikipedia corpus <ref type="bibr" target="#b19">(Shaoul and Westbury, 2010)</ref>. We optimized the models using mini-batch stochastic gradient descend with momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative results</head><p>We begin our evaluation by presenting senses discovered by disambiguated skip-gram for sev- eral ambiguous words. For the demonstration we trained four 300-dimensional models with three sense embedding vectors allocated to each word  <ref type="table">Table 1</ref>: Nearest neighbors and marginal probabilities p of word sense embedding vectors discovered by the disambiguated skip-gram model for several ambiguous words. Sense embedding vectors with a marginal probability p &lt; 0.05 are pruned from the learned model. and the entropy cost γ ranging from 0.0 to 0.5.</p><formula xml:id="formula_15">Word γ = 0.0 γ = 0.25 γ = 0.5 p Nearest</formula><p>For each of the evaluated words sense embeddings we calculated the cosine similarity to the remain- ing words and selected 5 nearest neighbours. The results are reported in Tab. 1.</p><p>Disambiguated skip-gram discovered main meanings of our test words. For example, the meanings discovered for the word fox correspond to a broadcasting company, an animal and a fam- ily name. The meanings discovered for the word mouse correspond to a cartoon character, a com- puter mouse and an animal.</p><p>Results in Tab. 1 also demonstrate that disam- biguated skip-gram will often expose an internal structure in a word meaning, if that meaning ap- pears in different contexts. For example, disam- biguated skip-gram learned two embeddings for the word plant corresponding to its factory mean-ing: one related to heavy industry and one related to a farm or a plant nursery. This is a consequence of the fact that disambiguated skip-gram discovers word senses using only the information about the contexts in which these words occur. In particular, it does not employ any supervision from an exter- nal knowledge base. <ref type="bibr" target="#b1">Bartunov et al. (2016)</ref> refer to a related phenomenon in the AdaGram embed- dings as the semantic resolution of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word-sense induction experiments</head><p>To compare disambiguated skip-gram with state- of-the-art competing algorithms we assessed its performance in a set of word sense induction tasks. In this evaluation we follow the experi- mental setup from ( <ref type="bibr" target="#b1">Bartunov et al., 2016)</ref>, allow- ing for direct comparison with the results reported therein. In particular, we evaluated disambiguated skip-gram on the datasets from the <ref type="bibr">SemEval-2007</ref><ref type="bibr">Task 2 competition (SE-2007</ref>, <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 14 competition (SE-2010</ref>, <ref type="bibr">SemEval-2013</ref><ref type="bibr">Task 13 competition (SE-2013</ref> and the Wikipedia Word-sense Induction (WWSI) dataset introduced by Bartunov et al. The test datasets consist of between 4664 and 36354 examples. Each ex- ample provides a ground truth sense of a cen- ter word and the context in which this sense ap- peared. The goal is to recognize the sense of the center word given the context. We use the prepro- cessed versions of SE-2007, SE-2010 and WWSI datasets made available by Bartunov et al. For the SemEval-2013 Task 13 we use the original com- petition dataset <ref type="bibr" target="#b7">(Jurgens and Klapaftis, 2013)</ref> and follow the preprocessing steps reported in <ref type="bibr" target="#b1">(Bartunov et al., 2016)</ref>.</p><p>We use a simple procedure for resolving word senses. That is, we average all sense embedding vectors of all context words and select the sense z w of the center word w whose embedding vector is most similar to the average vector:</p><formula xml:id="formula_16">z w = arg max j cos (v wj , ¯ c w ) ,<label>(13)</label></formula><p>where:</p><formula xml:id="formula_17">¯ c w = (k · #C w ) −1 c∈Cw k s=1 v cs .<label>(14)</label></formula><p>The intuition behind this procedure is that we ex- pect the average to preserve a shared component in the embedding vectors, namely embeddings for senses related to the sense of the center word, and cancel out embeddings of unrelated senses. In ad- dition to averaging sense embedding vectors we also experimented with averaging output vectors of context words. However, this approach usually gave slightly worse results. Following <ref type="bibr" target="#b1">Bartunov et al. (2016)</ref>, we use ad- justed rand index <ref type="bibr" target="#b5">(Hubert and Arabie, 1985)</ref> to compare ground truth senses for a given word with the senses inferred from disambiguated skip-gram embeddings. The final performance on a bench- mark task is the average of adjusted rand index values over all test words in the task.</p><p>For this evaluation we trained a 300- dimensional disambiguated skip-gram model with 5 sense embedding vectors allocated to each word and no entropy cost (γ = 0.0). The comparison between our model and MSSG, NP- MSSG, MPSG and AdaGram is reported in Tab. 2. The results for MSSG, NP-MSSG, MPSG and AdaGram are taken from <ref type="bibr" target="#b1">Bartunov et al. (2016)</ref>. Multi-sense embedding vectors learned by disambiguated skip-gram outperform baseline methods on the SE-2007, SE-2010 and WWSI benchmarks, and achieve the second best result on the SE-2013 benchmark. <ref type="bibr">SE-2007</ref><ref type="bibr">SE-2010</ref><ref type="bibr">SE-2013</ref>   <ref type="table">Table 2</ref>: Performance of multi-sense word embed- ding methods in word sense induction tasks. The reported performance metric is the adjusted rand index averaged over all test words in the bench- mark task. Results for all models except the dis- ambiguated skip-gram (Disamb. skip-gram) are taken from (Bartunov et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of the entropy cost</head><p>Results reported in Tab. 1 demonstrate that the en- tropy cost γ (Eq. 12) indeed allows for pruning senses learned by disambiguated skip-gram. In particular, when the entropy cost increases, disam- biguated skip-gram allocates more of the marginal probability mass (Eq. 11) to the frequent mean- ings of the modelled words and, in effect, learns coarser representations.</p><p>For a quantitative evaluation of the effect of entropy cost on the learned representations we trained 50-dimensional disambiguated skip-gram models with γ ranging from 0.0 to 1.0. All models allocate 5 sense embedding vectors to every word in the vocabulary. In Tab. 3 we report an average number of senses per word with marginal prob- ability p ≥ 0.05, depending on the value of the entropy cost. In <ref type="figure" target="#fig_1">Fig. 2</ref>    Histograms in <ref type="figure" target="#fig_1">Fig. 2</ref> confirm our observation from the qualitative evaluation: when the en- tropy cost increases, disambiguated skip-gram learns more peaked distributions for the condi- tional sense probability p (z = j | w, C w ). This translates to coarser sense representations. In par- ticular, the model with no entropy cost learned an average of 4.7 senses per word with marginal probability p ≥ 0.05 (Tab. 3). This number de- creases with an increasing entropy cost, reaching an average of 2.5 senses per word for γ = 1.0.</p><p>We also evaluated 50-and 300-dimensional models with different entropy costs in the word sense induction tasks. In each case we pruned senses with marginal probability p &lt; 0.05. Re-  <ref type="table">Table 4</ref>: Performance of disambiguated skip-gram models with different entropy costs in the word sense induction tasks. The reported performance metric is the adjusted rand index averaged over all test words in the benchmark task.</p><p>sults from this evaluation <ref type="bibr">(Tab. 4)</ref> indicate that the desired granularity of the learned sense rep- resentations depends on the underlying task. In the WWSI benchmark the best performing models had no entropy cost, while in the SemEval tasks small entropy cost usually improved results. The results agree for both model dimensionalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we developed disambiguated skip- gram: a novel neural-probabilistic model for learning multi-sense distributed representations of words. Unlike previous probabilistic models for multi-sense word embeddings, disambiguated skip-gram is a simple feed-forward neural network and can be trained end-to-end with backpropaga- tion. In experimental evaluation disambiguated skip-gram improved over the state-of-the-art re- sults in three out of four benchmark datasets and ranked second the fourth. Disambiguated skip-gram optimizes expected log-likelihood of the context prediction model un- der the distribution of word senses parametrized by the disambiguation model. We choose to opti- mize this objective with a biased but low-variance gradient estimator. However, parallel to this work there has been a significant progress in gradient- based training of models with discrete latent vari- ables. Specifically,  pro- posed an unbiased low-variance gradient estima- tor, called REBAR, that is applicable to models with categorical latent variables. REBAR may allow to efficiently optimize the original disam- biguated skip-gram objective (Eq. 6), instead of the relaxed objective (Eq. 10). This may further improve the quality of embeddings learned with our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Disambiguated skip-gram model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histograms of marginal probabilities of word senses learned by disambiguated skip-gram models with different values of the entropy cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>we also report histograms of marginal probabilities for selected entropy cost values.</figDesc><table>Entropy cost 
0.0 0.1 0.25 0.5 0.75 1.0 
Avg. sense num. 4.7 4.3 
3.7 
3.2 
2.8 
2.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average number of senses per word with 
marginal probability p ≥ 0.05, learned by disam-
biguated skip-gram models with different values 
of the entropy cost. 

</table></figure>

			<note place="foot" n="1"> This assumption follows from an observation that two different meanings of a given word will often have vastly different contexts.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by National Science Centre, Poland grant no. 2013/09/B/ST6/01549 "Interactive Visual Text Analytics (IVTA): De-velopment of novel, user-driven text mining and visualization methods for large text corpora ex-ploration". The paper was partially financed by AGH University of Science and Technology Statu-tory Fund. The paper was also partially financed by Allegro. Experiments for this work were supported by the PL-Grid infrastructure and by the "HPC Infrastructure for Grand Challenges of Science and Engineering" project co-financed by the European Regional Development Fund under the Innovative Economy Operational Programme. Last but not least, we would like to thank Professor Witold Dzwinel for overall guidance and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic FastText for multi-sense word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breaking sticks and ambiguities with adaptive skip-gram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Likelihood ratio gradient estimation for stochastic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical theory of extreme values and some practical applications: a series of lectures. 33. US Govt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><forename type="middle">Julius</forename><surname>Gumbel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
<note type="report_type">Print. Office</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with Gumbel-Softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international workshop on semantic evaluation</title>
		<meeting>the 7th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
	<note>Topical word embeddings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Westbury lab Wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shaoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Westbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2624" to="2633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond bilingual: Multi-sense word embeddings using multilingual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
