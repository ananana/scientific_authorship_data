<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Answer-focused and Position-aware Neural Question Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
							<email>wangshi@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Answer-focused and Position-aware Neural Question Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3930" to="3939"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3930</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we focus on the problem of question generation (QG). Recent neural network-based approaches employ the sequence-to-sequence model which takes an answer and its context as input and generates a relevant question as output. However, we observe two major issues with these approaches: (1) The generated interrogative words (or question words) do not match the answer type. (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. To address these two issues, we propose an answer-focused and position-aware neural question generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embedding, which can help generate an interrogative word matching the answer type. (2) By position-aware, we mean that we model the relative distance between the context words and the answer. Hence the model can be aware of the position of the context words when copying them to generate a question. We conduct extensive experiments to examine the effectiveness of our model. The experimental results show that our model significantly improves the baseline and outperforms the state-of-the-art system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of question generation (QG) aims to gen- erate questions for a given text, and it can benefit several real applications: (1) In the area of educa- tion, QG can help generate questions for reading comprehension materials ( <ref type="bibr" target="#b3">Du et al., 2017)</ref>. (2) QG can enable the machine to actively ask questions in a dialogue system. (3) QG can also aid in the de- velopment of question answering datasets ( . Typically, QG includes two sub- tasks: (1) what to say, i.e. determining the targets (e.g. sentences, phrases or words) that should be asked; (2) how to say, i.e. producing the surface- form of the question. In this paper, we focus on the sub-task of surface-form realization of ques- tions by assuming the targets are given.</p><p>Previous work of QG can be classified into two categories: rule-based and neural network-based. Compared to the rule-based approach, the neural network-based approach does not rely on hand- crafted rules, and it is instead data-driven and trainable in an end-to-end fashion. The recent re- lease of large-scale machine reading comprehen- sion datasets, e.g. SQuAD ( <ref type="bibr" target="#b15">Rajpurkar et al., 2016)</ref> and <ref type="bibr">MARCO (Nguyen et al., 2016)</ref>, also drives the development of neural question generation.</p><p>Recent neural question generation ap- proaches ( <ref type="bibr" target="#b3">Du et al., 2017;</ref> employ sequence-to-sequence model that takes an answer and its context as input and outputs a relevant question.  further enrich the sequence-to-sequence model with rich features (e.g. answer position and lexical features) to generate answer focused questions, and incor- porate copy mechanism that allows to copy words from the context when generating questions. To the best of our knowledge, it achieve the best re- sults on SQuAD dataset so far ( . In this paper, we implement this approach and carefully study its generation results. Specifically, we randomly sample 130 questions generated by the approach, and manually judge their quality by comparing them with the references. We find 54 out of 130 questions are ill generated, and we ob- serve two major issues with the 54 questions: (1) 20 (37.04% errors) questions contain the question words that do not match the answer type, though the answer position feature has been incorporated. Because the model does not pay much attention to the answer that is a key to question word genera- tion. <ref type="table">Table 1</ref> gives an example. A when-question Context: The tax collector who arrested him rose to higher political office , and Thoreau 's essay was not published until after the end of the Mexican War. Answer: the end of the Mexican War Question generated by the baseline: Why was Thoreau's essay published ? Reference: When was Thoreau's essay published ? <ref type="table">Table 1</ref>: A bad case where the generated question word does not match the answer type. A when-question should be triggered for answer "the end of the Mexican War", while a why-question is generated by the baseline.</p><p>Context: This mechanism is still the leading theory today; however, a second theory suggests that most cpdna is actually linear and replicates through homologous recombination. Answer: homologous recombination Question generated by the baseline: What is the leading theory today ? Reference: How does the second theory say most cpdna replicates ? <ref type="table">Table 2</ref>: A bad case where the model copies the context words far away from and irrelevant to the answer. The baseline copies "leading theory" that is far away from and unrelated to the answer "homologous recombination", but neglects the phrase "second theory" that is close and relevant to the answer.</p><p>should be triggered for answer "the end of the Mexican War" while a why-question is generated by the model. (2) 11 (20.37% errors) questions copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. Because the model is not aware of the positions of context words. <ref type="table">Table 2</ref> gives an example. The baseline model copies "leading theory" that is far away from and unrelated to the answer "homologous recombination", but neglects the phrase "second theory" that is close and relevant to the answer.</p><p>To address these two issues, we propose an answer-focused and position-aware neural ques- tion generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embed- ding, which can help generate a question word matching the answer type. (2) By position-aware, we mean that we model the relative distance be- tween the context words and the answer. The rela- tive distance is encoded as position embedding, on which a position-aware attention is generated. The position-aware attention help the model copy the context words that are relatively close and relevant to the answer. We further conduct extensive exper- iments on SQuAD and MARCO dataset to exam- ine the effectiveness of the answer-focused model and position-aware model, respectively. The ex- perimental results show that the combination of our proposed answer-focused model and position- aware model significantly improves the baseline and outperforms the state-of-the-art system.</p><p>The contributions of this paper can be summa- rized as follows:</p><p>• We analyze the generation results by the state- of-the-art neural model, and find two major is- sues with the model: (a) the generated question words do not match the answer type; (b) the model copies the context words that are far from and irrelevant to the answer.</p><p>• To deal with these two issues, we propose an answer-focused and position-aware neural ques- tion generation model. • We conduct extensive experiments to examine the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Question Generation Previous work of QG can be classified into two categories: rule-based and neural network-based. Regardless of the approach taken, QG usually includes two sub-tasks: <ref type="formula" target="#formula_0">(1)</ref> what to say, i.e. selecting the targets that should be asked. (2) how to say, i.e. formulating the struc- ture of the question and producing the surface re- alization. This is similar to other natural language generation tasks. In this paper, we focus on the second sub-task, i.e. surface-form realization of questions by assuming the targets are given. The rule-based approaches usually include the following steps: (1) Preprocess the given text by applying natural language processing techniques, including syntactic parsing, sentence simplifica- tion and semantic role labeling. (2) Identify the targets that should be asked by using rules or semantic roles. (3) Generate questions using transformation rules or templates. (4) Rank the over generated questions by well-designed fea- tures <ref type="bibr">Smith, 2009, 2010;</ref><ref type="bibr" target="#b2">Chali and Hasan, 2015)</ref>. The major drawbacks of rule-based approaches include: (1) they rely on rules or tem- plates that are expensive to manually create; (2) the rules or templates lack diversity; (3) the targets that they can deal with are limited.</p><p>To tackle the issues of rule-based approaches, the neural network-based approaches are applied to the task of QG. The neural network-based ap- proaches do not rely on hand-crafted rules, and they are instead data driven and trainable in an end-to-end fashion. <ref type="bibr" target="#b17">Serban et al. (2016)</ref> firstly in- troduce an encoder-decoder framework with atten- tion mechanism to generate factoid questions for the facts (i.e. each fact is a triple composed of a subject, a predicate and an object) from FreeBase. <ref type="bibr" target="#b3">Du et al. (2017)</ref> introduce sequence-to-sequence model with attention mechanism to generate ques- tions for the text from SQuAD dataset, which con- tains large-scale manually annotated triples com- posed of question, answer and the context (i.e. the passage).  enrich the sequence- to-sequence model with rich features, e.g. answer position and lexical features, and incorporate copy mechanism that allows it to copy words from the context when generating a question. Their ex- periments show the effectiveness of the rich fea- tures and the copy mechanism.  propose to combine templates and sequence-to- sequence model. Specifically, they mine question patterns from a question answering community and apply sequence-to-sequence to generate ques- tion patterns for a given text.  model question answering and question generation as dual tasks. It helps generate better questions when training these two tasks together.</p><p>In this paper, we observe two major issues with the exiting neural models: (1) The generated ques- tion words do not match the answer type, since the models do not pay much attention to the answers that are critical to generate question words. <ref type="formula" target="#formula_2">(2)</ref> The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer, since the models are not aware the positions of the context words. To address these two issues, we propose an answer-focused and position-aware neural question generation model. As to position- aware models, <ref type="bibr" target="#b21">Zeng et al. (2014)</ref>; <ref type="bibr" target="#b22">Zhang et al. (2017)</ref> introduce position feature in the task of re- lation extraction. They apply this feature to en- code the relative distance to the target noun pairs. In the task of QG,  apply BIO scheme to label answer position, which is a weak representation of relative distance between answer and its context words. Sequence-to-sequence In recent years, the sequence-to-sequence model has been widely used in the area of natural language generation, including the tasks of abstractive text summa- rization, response generation in dialogue, poetry generation, etc.  propose a sequence-to-sequence model and apply it to the task of machine translation. <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> introduce attention mechanism to the sequence-to-sequence model and it greatly improves the model performance on the task of machine translation. To deal with the out of vocabulary issue, several variants of the sequence- to-sequence model have been proposed to copy words from source text ( <ref type="bibr" target="#b7">Gu et al., 2016;</ref><ref type="bibr" target="#b1">Cao et al., 2017;</ref><ref type="bibr" target="#b16">See et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Models</head><p>In this section, we describe the details of our models. We first describe the baseline model, a feature-enriched pointer-generator model. Then, we elaborate the proposed answer-focused model and position-aware model to deal with the two is- sues discussed in previous section. Finally, a hy- brid model is introduced to combine these two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline (Feature-enriched</head><p>Pointer-generator Model)</p><p>Our baseline model is an attention-based pointer- generator model ( <ref type="bibr" target="#b16">See et al., 2017</ref>) enhanced with various rich features proposed by ( ). These features include: named entity (NE), part-of-speech (POS) and answer position in the embedding layer of the encoder. The encoder in our baseline is a bidirec- tional LSTM, which takes the joint embedding of word, answer position and lexical features (NE, POS) as input (w 1 , w 2 , ..., w Tx ) with w i ∈ R dw+da+dn+dp , where T x is the input length and d w , d a , d n , d p is the dimensionality of word em- bedding, answer position embedding, NE embed- ding and POS embedding respectively. It pro- duces a sequence of hidden states (h 1 , h 2 , ..., h Tx ) to represent its input, each of which is a concate- nation of a forward and a backward LSTM repre- sentation:</p><formula xml:id="formula_0">h i = [ ← − h i ; − → h i ], ← − h i = LSTM(w i , ← − h i+1 ), − → h i = LSTM(w i , − → h i−1 )<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">← − h i , − → h i are all d h -dimensional vectors.</formula><p>The decoder is a unidirectional LSTM condi- tioned on all encoded hidden states. At decoding step t, the decoder reads an input word embedding w t , previous attentional context vector c t−1 and its previous hidden state s t−1 to update its current hidden state s t ∈ R d h :</p><formula xml:id="formula_2">s t = LSTM([w t ; c t−1 ], s t−1 )<label>(2)</label></formula><p>The context vector c t together with an attention distribution α t are generated via attention mecha- nism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). Attention can be re- garded as a semantic match between encoder hid- den states and the decoder hidden state. It de- scribes how the model spread out the amount it cares about different encoder hidden states during decoding. At step t, the context vector c t and the attention distribution α t are calculated as follows:</p><formula xml:id="formula_3">c t = Tx i=1 α ti h i (3) α ti = softmax(e ti )<label>(4)</label></formula><formula xml:id="formula_4">e ti = v T tanh(W T h h i + W T s s t + b)<label>(5)</label></formula><p>where W h , W s , b and v are all trainable parame- ters.</p><p>Our baseline is based on a pointer-generator framework, which includes two complementary modes: a generation mode and a copy mode. The former mode generates words from a given vocab- ulary as the vanilla sequence-to-sequence model:</p><formula xml:id="formula_5">P vocab = softmax (g(s t , c t ))<label>(6)</label></formula><p>where g(·) is a two-layer feed-forward network with a maxout internal activation. P vocab ∈ R |V | denotes the vocabulary distribution with a vocab- ulary size of |V |. The latter mode copies words directly from the source sequence. As the attention weights already measure the relevance of each input word to the partial decoding state, we treat α t as the copy probability i.e. P copy (w) = α t . Both modes are switched via a generation probability p gen as fol- lows:</p><formula xml:id="formula_6">P (w) = p gen P vocab (w)+(1−p gen )P copy (w) (7)</formula><p>where p gen is computed from the context vector c t , decoder hidden state s t and the decoder input w t :</p><formula xml:id="formula_7">p gen = σ(f (c t , s t , w t ))<label>(8)</label></formula><p>f (·) indicates a simple feed-forward neural net- work that emits a single scalar value. The whole network is thus trained end-to-end according to the negative log likelihood loss of target word prob- ability P (w * ). In the baseline, we apply BIO scheme to label answer position, where B,I,O de- note the begin of an answer, the non-begin of the answer and the non-answer context words re- spectively. Besides, we introduce dropout <ref type="bibr" target="#b18">(Srivastava et al., 2014</ref>) with maxout ( <ref type="bibr" target="#b6">Goodfellow et al., 2013</ref>) to tackle over-fitting problem and pre- trained global vectors (Glove) for word represen- tation ( <ref type="bibr" target="#b14">Pennington et al., 2014</ref>). All these tech- niques have been verified in their effectiveness in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer-focused Model</head><p>The mismatch between generated question words and answer type is a major issue in neural ques- tion generation (NQG). Even though answer is a key to question word generation, most NQG mod- els do not focus on answer or weakly emphasis on it when generating question words. As <ref type="table">Ta- ble 1</ref> shows, a when-question should be triggered for the answer "the end of the Mexican War", but an answer-irrelevant why-question is generated by the baseline. According to our analysis in Section 1, we discover that nearly 37% bad cases from our baseline fall into this category. To deal with this issue, we develop an answer-focused model. We observe that the generation of question words is mainly related to the answer and its sur- rounding words. For example in <ref type="table">Table 1</ref>, the an- swer and its context "until after the end of the Mexican War" already involve the essential infor- mation to generate a question word "when". This  suggests that the answer and its context can bene- fit the generation of question words. We also ob- serve that the number of question words is limited. Therefore, we introduce a specific vocabulary of question words to directly and explicitly model the generation of question words.</p><p>As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, comparing to pointer- generator model, we introduce an additional ques- tion word generation mode to generate question words based on a restricted vocabulary of ques- tion words. This mode produces a question word distribution based on an answer embedding v ans , the decoder state s t and the context vector c t :</p><formula xml:id="formula_8">P question word = softmax (g(v ans , s t , c t )) (9)</formula><p>where P question word is a |V qw |-dimensional prob- ability distribution, and |V qw | is the size of vocab- ulary of question words. We employ the encoded hidden state at the answer start position as the an- swer embedding, i.e. v ans = h answer start . We argue that under bidirectional encoding, this an- swer embedding has already memorized both the left and the right contexts around the answer re- gion, making it a desired choice. To control the balance among different modes, we introduce a 3- dimensional switch probability:</p><formula xml:id="formula_9">p genv , p genq , p copy = softmax(f (c t ,s t ,w t )) (10)</formula><p>The final probability distribution is calculated via a weighted sum of the three mode probability dis- tributions:</p><formula xml:id="formula_10">P (w) = p genv P vocab (w) + p copy P copy (w) + p genq P question word (w)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Position-aware Model</head><p>Another main issue of NQG is that the generated question copies the context words that are distant from and irrelevant to the answer, instead of the words that are close and relevant to the answer. In other words, attention is distracted by irrelevant words far away from the answer. As shown in <ref type="table">Table 2</ref>, the baseline model copies "leading the- ory" that are far away from and unrelated to the answer "homologous recombination", but neglects the words "second theory" and "linear and repli- cates through" that are closer to the answer. In Section 1, we analyze the bad cases of our base- line, and find that about 20% suffer from this phe- nomenon. Further analysis on these cases indi- cates that the closer a word is to the answer, the more likely it should be copied. Based on these evidences, we believe that an important reason for the above phenomenon lies in the lack of word position information inside the NQG model. Following this direction, we propose a position-aware model. The model aims at en- forcing local attention, and adapting the attention weights so as to put more emphasis on answer- surrounded context words by incorporating word position embeddings.</p><p>A straightforward solution to inject position information is to directly incorporate relative word position embeddings. This can inform the model where the answer is and what context words are close to it. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, we achieve this by feeding position embeddings</p><formula xml:id="formula_11">(d p 1 , d p 2 , ..., d p Tx )</formula><p>into the computation of atten- tion distribution through a single layer network:</p><formula xml:id="formula_12">e ti = v T tanh(W d d p i + W h h i + W s s t + b) (12) α ti = softmax(e ti )<label>(13)</label></formula><p>where p i is the relative distance between the i-th word and the answer, d p i is the embedding of p i , which we call word position embedding. By opti- mizing the attention parameters, our model is ex- pected to discover the correlation between target words and their relative distance from the answer. As a result, distracted attention on irrelevant input words can be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Hybrid Model (Answer-focused and Position-aware)</head><p>In Section 3.2 and 3.3, we describe the answer- focused model and position-aware model respec- tively. In this section, we combine these two models to get a both answer-focused and position- aware hybrid model. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the hybrid model generates two types of attention dis- tribution: position-aware and non-position-aware distribution. Accordingly, the model has two types of context vector: position-aware and non- position-aware context vector. At time step t, we calculate position-aware attention distribution α t as equation <ref type="formula" target="#formula_0">(12)</ref>, <ref type="formula" target="#formula_0">(13)</ref>, and non-position-aware one α t as equation <ref type="formula" target="#formula_3">(4)</ref>, <ref type="formula" target="#formula_4">(5)</ref>. And we use c t , c t to repre- sent non-position-aware and position-aware con- text vector calculated as equation <ref type="formula">(3)</ref> respectively.</p><p>The hybrid model has three modes as in the answer-focused model. The question word dis- tribution is computed from non-position-aware at- tention distribution as equation <ref type="formula">(9)</ref>, while vocabu- lary distribution is calculated from position-aware one. The final distribution is weighted sum of the three mode probability distributions:</p><formula xml:id="formula_13">P vocab = softmax g(s t , c t ))<label>(14)</label></formula><p>P (w) = p genv P vocab (w) + p copy P copy (w)</p><formula xml:id="formula_14">+ p genq P question word (w)<label>(15)</label></formula><p>where P copy (w) is the position-aware attention distribution α t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>Dataset In this paper, we conduct the experiments on SQuAD and MARCO. Since the test sets of both data sets are not publicly available, we follow  to randomly split the develop- ment set into two parts and use them as the devel- opment set and test set for the task of question gen- eration. In SQuAD, there are 86, 635, 8, 965 and 8, 964 question-answer pairs in our training set, development set and test set, respectively. We di- rectly use the extracted features 1 shared by . In MARCO, there are 74, 097, 4, 539 and 4, 539 question-answer pairs in our training set, development set and test set, respectively. We use Stanford CoreNLP 2 to extract lexical features. We attach all the processed data sets in the supple- mental materials. Implementation Details In this paper, we set the cutoff length of the input sequence as 100 words. The vocabulary contains the most frequent 20, 000 words in each training set. The vocabulary of question words contains 20 words. We use the pre-trained Glove word vectors 3 with 300 dimen- sions to initialize the word embeddings that will be further fine-tuned in the training stage. The representations of answer position feature and lex- ical features at the embedding layer of the encoder DataSet SQuAD MARCO Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-1 BLEU-2 BLEU-3 BLEU-4 NQG++ (  - - - 13.29 - - - - Pointer-generator model <ref type="bibr" target="#b16">See et al. (2017)</ref> 32  are randomly initialized as 32 dimensional vectors that are trainable during training stage. The hid- den size of both the encoder and decoder is 512.</p><p>We use dropout only in the encoder with a dropout rate 0.5. The size of answer embedding in answer- focused model is 512. The position, that indicates the relative distance between the context words and the answer, ranges from 0 to 80 and its embed- ding size in position-aware model is 16. We use the optimization algorithm Adagrad (Duchi et al., 2011) with learning rate 0.15, an initial accumu- lator value of 0.1 and batch size as 64. We use gradient clipping with a maximum gradient norm of 2. During training, we select the best model on development set. Evaluation Metrics We report BLEU ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) as the main evaluation metric of the question generation systems. Baselines In the experiments, we have three base- lines for comparisons:</p><p>• NQG++ ( ) It is the state- of-the-art neural question generation system on SQuAD that incorporates rich features to the embedding layer of a sequence-to-sequence model and introduces copy mechanism pro- posed by .</p><p>• Pointer-generator model <ref type="bibr" target="#b16">(See et al., 2017</ref>) It is a sequence-to-sequence model with copy mech- anism that has different architecture from the one proposed by . We choose this model since its copy mechanism shows better performance ( <ref type="bibr" target="#b16">See et al., 2017)</ref>. Note that we do not enable the coverage mech- anism in this model to have a fair comparison.</p><p>• Feature-enriched pointer-generator model</p><p>We add the features to the embedding layer of the pointer-generator model as described in Sec- tion 3.1. <ref type="table" target="#tab_2">Table 3</ref> shows the main results, and we have the following observations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>• The feature-enriched pointer-generator model outperforms NQG++. Both of the two models employ the sequence-to-sequence model with copy mechanism and the same features. The major difference between them is that their copy mechanism has different architecture, and pointer-generator shows better performance.</p><p>• The pointer-generator model without the fea- tures does not perform well. This verifies the effectiveness of the features extracted by .</p><p>• Both answer-focused model and position-aware model outperform the feature-enriched pointer- generator model and NQG++. We will analyze the effectiveness of these two models in the fol- lowing two sections, respectively. • The hybrid model shows the best perfor- mance and it outperforms the two single mod- els, answer-focused model and position-aware model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Answer-focused Model Analysis</head><p>As we discussed in Section 1, one major issue with the neural question generation model is that the generated question word does not match the an- swer type. The design of answer-focused model is explicitly modeling question word generation by incorporating answer embedding. It is expected that the answer-focused model can reduce such er- rors. Recall the example shown in <ref type="table">Table 1</ref> (Section 1). The answer-focused model correctly predicts the question word, though it copies wrong context words that can be further corrected by the hybrid model. The outputs of the answer-focused model and the hybrid model for the case in <ref type="table">Table 1</ref> are as follows:</p><p>• Answer-focused model: When did Thoreau's essay come to higher political office ? • Answer-focused + Position-aware model:</p><p>When was Thoreau's essay published ?</p><p>We further evaluate different systems in terms of the same question word ratio (SQWR). This metric measures the ratio of the generated ques- tions that have the same question words as the ref- erence. <ref type="table" target="#tab_3">Table 4</ref> shows the SQWR of different sys- tems on SQuAD. We can see that answer-focused model outperforms the strong baseline, feature- enriched pointer-generator model.</p><p>Model SQWR Pointer-generator model <ref type="bibr" target="#b16">(See et al., 2017)</ref> 53.17% Feature-enriched pointer-generator model 71.58% Answer-focused model 73.91% We re-analyze the 20 (37% of errors) questions (discussed in Section 1) that have the answer type mismatching problem. Our answer-focused model can correct 7 out of the 20 bad cases. All the re- solved cases have a commonality that we can eas- ily figure out the answer type from the answer it- self, as the case shown in <ref type="table">Table 1</ref>. We also analyze the remaining 13 of the 20 unresolved cases. (1) 4 cases show that the answer type is closely re- lated to the context words other than the answer. But these context words are far from the answer, and the encoding of the answer by LSTM has lit- tle memory of them. As shown in <ref type="table">Table 5</ref>, the answer "an attempt to avoid responsibility for her actions" is useless to generate a question word "why", while the useful context word "because" is far from the answer. (2) 3 cases show that the answer itself has ambiguity to generate the right question words. (3) 3 cases contain the answers with the wrong named entity labels. (4) 2 cases contain answers which are out of vocabulary. <ref type="formula" target="#formula_4">(5)</ref> 1 case is hard even for human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Position-aware Model Analysis</head><p>As discussed in Section 1, another major issue with the neural question generation model is that the model copies the context words that are far from and irrelevant to the answer. The design of position-aware model is tackling this issue by modeling the relative distance between context words and the answer, so that the attention distri- bution for copy mechanism will be biased towards the context words that are close and relevant to the answer. Recall the example in <ref type="table">Table 2</ref> (Sec- tion 1). The question generated by the baseline copies the wrong context words "leading theory" instead of "second theory" and "linear and repli- Context: This action was upheld because , ac- cording to the U.S. court of appeals for the first circuit , her statement suggested a lack of re- morse , an attempt to avoid responsibility for her actions , and even a likelihood of repeating her illegal actions . Answer: an attempt to avoid responsibility for her actions Reference: Why is giving a defiant speech sometimes more harmful for the individual ? Question generated by the answer-focused model: What did the court of appeals reject ? <ref type="table">Table 5</ref>: A bad case of baseline remains unresolved by applying answer-focused model because answer type is closely related to the context word "because" instead of the answer itself, but "because" is far from the an- swer. Thus, the encoding of answer has little memory of "because".</p><p>cates through". The outputs of our position-aware and hybrid model for this case are as follows.</p><p>• Position-aware model: The second theory suggests that most cpdna is actually linear and replicates through what ? • Position-aware + Answer-focused model:</p><p>Most cpdna is actually linear and replicates through what ?</p><p>We can observe that the model can copy the correct context words after introducing the posi- tion embedding to the attention distribution. <ref type="figure" target="#fig_1">Fig- ure 2</ref> illustrates the attention distributions for copy mechanism before and after introducing po- sition embedding of context words. We can see that "second" has much higher probability in the position-aware attention distribution.  <ref type="table">Table 2</ref>. The position-aware model emphasizes more on the "second" that is close to the answer.</p><p>The design of position-aware model aims to help copy the context words close and relevant to the answer. We analyze the effects of position- aware model on copying out of vocabulary (OOV) words from the source sequence. We measure the effects from two perspectives: average precision and average recall. For one generated sequence, the precision is defined as the ratio of the num- ber of OOV words appearing in both the gener- ated question and the reference, and the number of OOV words in the generated question. The recall is defined as the ratio of the number of OOV words appearing in both the generated question and the reference, and the number of OOV words in the reference. The average precision (AP) is the mean of precision of all generated questions, and the av- erage recall (AR) is the mean of recall of all gen- erated questions. As <ref type="table">Table 6</ref> shows, our position- aware model can significantly improve the AP and AR, which indicates our position-aware model can help on copying OOV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>AP AR Feature-enriched pointer-generator model 21.22% 18.87% Position-aware model 22.79% 20.62% <ref type="table">Table 6</ref>: Our position-aware model can significantly improve the average precision and recall of copied OOV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we find two major issues with the ex- isting neural question generation model. To tackle the two issues, we propose an answer-focused and position-aware model. We further conduct exten- sive experiments on SQuAD and MARCO dataset. The experimental results show that the combina- tion of our proposed answer-focused model and position-aware model significantly improves the baseline and outperforms the state-of-the-art sys- tem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The modules marked with A and colored yellow describe the answer-focused model, while the ones marked with P and colored green describe the position-aware model. In answer-focused model, comparing to pointer-generator model, we introduce a question word generation mode and generate question words in a restricted vocabulary of question words. In position-aware model, we incorporate word position embeddings to gain a position-aware attention for further generation. The hybrid model has two types of attention distribution: positionaware and non-position-aware attention distribution and two types of context vector accordingly. Question word distribution is generated from non-position-aware context vector while vocabulary distribution is calculated from position-aware one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The two attention distributions for copy mechanism when generating questions for the case in Table 2. The position-aware model emphasizes more on the "second" that is close to the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The main experimental results of baselines, answer-focused model, position-aware model and a hybrid 
model on SQuAD and MARCO. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The answer-focused model has the highest 
same question word ratio. 

</table></figure>

			<note place="foot" n="1"> https://res.qyzhou.me/redistribute. zip 2 https://nlp.stanford.edu/software/ 3 http://nlp.stanford.edu/data/glove. 6B.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work is supported by the National Basic Research Program of China (973 program, No. 2014CB340505). We thank the anonymous re-viewers for their constructive criticism of the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint copying and restricted generation for paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3152" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards topicto-question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question generation for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="866" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08148</idno>
		<title level="m">Pointing the unknown words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Question generation via overgenerating transformations and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A Smith ; Carnegie-Mellon Univ Pittsburgh Pa Language Technologies</forename><surname>Inst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Good question! statistical ranking for question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06807</idno>
		<title level="m">Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Question answering and question generation as dual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02027</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural question generation from text: A preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National CCF Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="662" to="671" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
