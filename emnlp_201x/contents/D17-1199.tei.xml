<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenisbek</forename><surname>Assylbekov</surname></persName>
							<email>zhassylbekov@nu.edu.kz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Science and Technology</orgName>
								<orgName type="department" key="dep3">Linguistics Department Swarthmore College</orgName>
								<orgName type="laboratory">National Laboratory Astana Nazarbayev University</orgName>
								<orgName type="institution" key="instit1">Nazarbayev University</orgName>
								<orgName type="institution" key="instit2">Nazarbayev University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rustem</forename><surname>Takhanov</surname></persName>
							<email>rustem.takhanov@nu.edu.kz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Science and Technology</orgName>
								<orgName type="department" key="dep3">Linguistics Department Swarthmore College</orgName>
								<orgName type="laboratory">National Laboratory Astana Nazarbayev University</orgName>
								<orgName type="institution" key="instit1">Nazarbayev University</orgName>
								<orgName type="institution" key="instit2">Nazarbayev University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bagdat</forename><surname>Myrzakhmetov</surname></persName>
							<email>bagdat.myrzakhmetov@nu.edu.kz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Science and Technology</orgName>
								<orgName type="department" key="dep3">Linguistics Department Swarthmore College</orgName>
								<orgName type="laboratory">National Laboratory Astana Nazarbayev University</orgName>
								<orgName type="institution" key="instit1">Nazarbayev University</orgName>
								<orgName type="institution" key="instit2">Nazarbayev University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">N</forename><surname>Washington</surname></persName>
							<email>jonathan.washington@swarthmore.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Science and Technology</orgName>
								<orgName type="department" key="dep3">Linguistics Department Swarthmore College</orgName>
								<orgName type="laboratory">National Laboratory Astana Nazarbayev University</orgName>
								<orgName type="institution" key="instit1">Nazarbayev University</orgName>
								<orgName type="institution" key="instit2">Nazarbayev University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1866" to="1872"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Syllabification does not seem to improve word-level RNN language model-ing quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in neural language modeling (NLM) are connected with character-aware mod- els ( <ref type="bibr" target="#b10">Kim et al., 2016;</ref><ref type="bibr" target="#b13">Ling et al., 2015b;</ref><ref type="bibr" target="#b23">Verwimp et al., 2017)</ref>. This is a promising approach, and we propose the following direction related to it: We would like to make sure that in the pursuit of the most fine-grained representations one has not missed possible intermediate ways of segmenta- tion, e.g., by syllables. Syllables, in our opinion, are better supported as linguistic units of language than single characters. In most languages, words can be naturally split into syllables: ES: el par-la-men-to a-po-yó la en-mien-da RU: пар-ла-мент под-дер-жал по-прав-ку (EN: the parliament supported the amendment)</p><p>Based on this observation, we attempted to de- termine whether syllable-aware NLM has any ad- vantages over character-aware NLM. We exper- imented with a variety of models but could not find any evidence to support this hypothesis: split- ting words into syllables does not seem to improve the language modeling quality when compared to splitting into characters. However, there are some positive findings: while our best syllable-aware language model achieves performance comparable to the competitive character-aware model, it has 18%-33% fewer parameters and is 1.2-2.2 times faster to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Much research has been done on subword-level and subword-aware 1 neural language modeling when subwords are characters ( <ref type="bibr" target="#b13">Ling et al., 2015b;</ref><ref type="bibr" target="#b10">Kim et al., 2016;</ref><ref type="bibr" target="#b23">Verwimp et al., 2017)</ref> or mor- phemes ( <ref type="bibr" target="#b0">Botha and Blunsom, 2014;</ref><ref type="bibr" target="#b18">Qiu et al., 2014;</ref><ref type="bibr" target="#b2">Cotterell and Schütze, 2015</ref>). However, not much work has been done on syllable-level or syllable-aware NLM. <ref type="bibr" target="#b16">Mikolov et al. (2012)</ref> show that subword-level language models outperform character-level ones. <ref type="bibr">2</ref> They keep the most frequent words untouched and split all other words into syllable-like units. Our approach differs mainly in the following aspects: we make predictions at the word level, use a more linguistically sound syllab- ification algorithm, and consider a variety of more advanced neural architectures.</p><p>We have recently come across a concurrent paper <ref type="bibr" target="#b22">(Vania and Lopez, 2017)</ref> where the au- thors systematically compare different subword units (characters, character trigrams, BPE <ref type="bibr" target="#b19">(Sennrich et al., 2016)</ref>, morphemes) and different rep- resentation models (CNN, Bi-LSTM, summation) on languages with various morphological typol- ogy. However, they do not consider syllables, and they experiment with relatively small models on small data sets (0.6M-1.4M tokens).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Syllable-aware word embeddings</head><p>Let W and S be finite vocabularies of words and syllables respectively. We assume that both words unconstitutional conditions on and syllables have already been converted into in- dices. Let E S ∈ R |S|×d S be an embedding ma- trix for syllables -i.e., it is a matrix in which the sth row (denoted as s) corresponds to an embed- ding of the syllable s ∈ S. Any word w ∈ W is a sequence of its syllables (s 1 , s 2 , . . . , s nw ), and hence can be represented as a sequence of the cor- responding syllable vectors:</p><formula xml:id="formula_0">[s 1 , s 2 , . . . , s nw ].<label>(1)</label></formula><p>The question is: How shall we pack the sequence (1) into a single vector x ∈ R d W to produce a better embedding of the word w? 3 In our case "better" means "better than a character-aware em- bedding of w via the Char-CNN model of <ref type="bibr" target="#b10">Kim et al. (2016)</ref>". Below we present several viable ap- proaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent sequential model (Syl-LSTM)</head><p>Since the syllables are coming in a sequence it is natural to try a recurrent sequential model:</p><formula xml:id="formula_1">h t = f (s t , h t−1 ), h 0 = 0,<label>(2)</label></formula><p>which converts the sequence of syllable vectors (1) into a sequence of state vectors h 1:nw . The last state vector h nw is assumed to contain the information on the whole sequence (1), and is therefore used as a word embedding for w. There is a big variety of transformations from which one can choose f in (2); however, a recent thorough evaluation <ref type="bibr" target="#b9">(Jozefowicz et al., 2015)</ref> shows that the LSTM (Hochre- iter and Schmidhuber, 1997) with its forget bias initialized to 1 outperforms other popular architec- tures on almost all tasks, and we decided to use it for our experiments. We will refer to this model as Syl-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional model (Syl-CNN)</head><p>Inspired by recent work on character-aware neural language models ( <ref type="bibr" target="#b10">Kim et al., 2016</ref>) we decided to try this approach (Char-CNN) on syllables. Our case differs mainly in the following two aspects:</p><p>1. The set of syllables S is usually bigger than the set of characters C, 4 and also the dimen- sionality d S of syllable vectors is expected to be greater than the dimensionality d C of char- acter vectors. Both of these factors result in allocating more parameters on syllable em- beddings compared to character embeddings. 2. On average a word contains fewer syllables than characters, and therefore we need nar- rower convolutional filters for syllables. This results in spending fewer parameters per con- volution. This means that by varying d S and the maximum width of convolutional filters L we can still fit the parameter budget of <ref type="bibr" target="#b10">Kim et al. (2016)</ref> to allow fair comparison of the models.</p><p>Like in Char-CNN, our syllable-aware model, which is referred to as Syl-CNN- <ref type="bibr">[L]</ref>, utilizes max- pooling and highway layers ( <ref type="bibr" target="#b21">Srivastava et al., 2015)</ref> to model interactions between the syllables. The dimensionality of a highway layer is denoted by d HW .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linear combinations</head><p>We also considered using linear combinations of syllable-vectors to represent the word embedding:</p><formula xml:id="formula_2">x = ∑ nw t=1 α t (s t ) · s t .<label>(3)</label></formula><p>The choice for α t is motivated mainly by the ex- isting approaches (discussed below) which proved to be successful for other tasks. Syl-Sum: Summing up syllable vectors to get a word vector can be obtained by setting α t (s t ) = 1. This approach was used by <ref type="bibr" target="#b0">Botha and Blunsom (2014)</ref> to combine a word and its morpheme em- beddings into a single word vector.</p><p>Syl-Avg: A simple average of syllable vectors can be obtained by setting α t (s t ) = 1/n w . This can be also called a "continuous bag of syllables" in an analogy to a CBOW model ( <ref type="bibr" target="#b15">Mikolov et al., 2013)</ref>, where vectors of neighboring words are averaged to get a word embedding of the current word. Syl-Avg-A: We let the weights α t in (3) be a function of parameters (a 1 , . . . , a n ) of the model, which are jointly trained together with other pa- rameters. Here n = max w {n w } is a maxi- mum word length in syllables. In order to have a weighted average in <ref type="formula" target="#formula_2">(3)</ref> we apply a softmax nor- malization:</p><formula xml:id="formula_3">α t = softmax(a) t = exp(a t ) ∑ n τ =1 exp(a τ )<label>(4)</label></formula><p>Syl-Avg-B: We can let α t depend on syllables and their positions:</p><formula xml:id="formula_4">α t = α t (s t ) = softmax(a st + b) t</formula><p>where A ∈ R d S ×n (with elements a s,t ) is a set of parameters that determine the importance of each syllable type in each (relative) position, b ∈ R n is a bias, which is conditioned only on the rela- tive position. This approach is motivated by re- cent work on using an attention mechanism in the CBOW model ( <ref type="bibr" target="#b12">Ling et al., 2015a</ref>). We feed the resulting x from (3) into a stack of highway layers to allow interactions between the syllables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Concatenation (Syl-Concat)</head><p>In this model we simply concatenate syllable vec- tors (1) into a single word vector:</p><formula xml:id="formula_5">x = [s 1 ; s 2 ; . . . ; s nw ; 0; 0; . . . ; 0 n−nw ]</formula><p>We zero-pad x so that all word vectors have the same length n · d S to allow batch processing, and then we feed x into a stack of highway layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word-level language model</head><p>Once we have word embeddings x 1:k for a se- quence of words w 1:k we can use a word-level RNN language model to produce a sequence of states h 1:k and then predict the next word accord- ing to the probability distribution</p><formula xml:id="formula_6">Pr(w k+1 |w 1:k ) = softmax(h k W + b),</formula><p>where W ∈ R d LM ×|W| , b ∈ R |W| , and d LM is the hidden layer size of the RNN. Training the model involves minimizing the negative log-likelihood over the corpus w 1:K :</p><formula xml:id="formula_7">− ∑ K k=1 log Pr(w k |w 1:k−1 ) −→ min (5)</formula><p>As was mentioned in Section 3.1 there is a huge variety of RNN architectures to choose from. The most advanced recurrent neural architectures, at the time of this writing, are recurrent highway net- works ( <ref type="bibr" target="#b26">Zilly et al., 2017</ref>) and a novel model which was obtained through a neural architecture search with reinforcement learning <ref type="bibr" target="#b27">(Zoph and Le, 2017</ref>). These models can be spiced up with the most re- cent regularization techniques for RNNs ( <ref type="bibr" target="#b4">Gal and Ghahramani, 2016)</ref> to reach state-of-the-art. How- ever, to make our results directly comparable to those of <ref type="bibr" target="#b10">Kim et al. (2016)</ref> we select a two-layer LSTM and regularize it as in <ref type="bibr">Zaremba et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We search for the best model in two steps: first, we block the word-level LSTM's architecture and pre-select the three best models under a small pa- rameter budget (5M), and then we tune these three best models' hyperparameters under a larger bud- get (20M). Pre-selection: We fix d LM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set <ref type="bibr" target="#b14">(Marcus et al., 1993)</ref>, keeping the total parameter bud- get at 5M. The architectural choices are specified in Appendix A.</p><p>Hyperparameter tuning: The hyperparameters of the three best-performing models from the pre- selection step are then thoroughly tuned on the same English PTB data through a random search according to the marginal distributions: <ref type="formula" target="#formula_1">(2000)</ref>), with the restriction d S &lt; d LM . The total parameter budget is kept at 20M to allow for easy comparison to the results of <ref type="bibr" target="#b10">Kim et al. (2016)</ref>. Then these three best models (with their hyperparameters tuned on PTB) are trained and evaluated on small-(DATA- S) and medium-sized (DATA-L) data sets in six languages.</p><formula xml:id="formula_8">• d S ∼ U (20, 650), 5 • log(d HW ) ∼ U (log(160), log(2000)), • log(d LM ) ∼ U (log(300), log</formula><p>Optimizaton is performed in almost the same way as in the work of <ref type="bibr">Zaremba et al. (2014)</ref>. See Ap- pendix B for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPL Model PPL LSTM-Word 88.0 Char-CNN 92.3 Syl-LSTM</head><p>88.7 Syl-Avg 88.5 Syl-CNN-2 86.6 Syl-Avg-A 91.4 Syl-CNN-3 84.6 Syl-Avg-B 88.5 <ref type="table" target="#tab_2">Syl-CNN-4</ref> 86.8 Syl-Concat 83.7 Syl-Sum 84.6  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syllabification:</head><p>The true syllabification of a word requires its grapheme-to-phoneme conversion and then splitting it into syllables based on some rules. Since these are not always available for less- resourced languages, we decided to utilize Liang's widely-used hyphenation algorithm <ref type="bibr" target="#b11">(Liang, 1983)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results of the pre-selection are reported in <ref type="table" target="#tab_0">Table 1</ref>. All syllable-aware models comfortably outperform the Char-CNN when the budget is limited to 5M parameters. Surprisingly, a pure word-level model, <ref type="bibr">6</ref> LSTM-Word, also beats the character-aware one under such budget. The three best configurations are Syl-Concat, Syl-Sum, and Syl-CNN-3 (hereinafter referred to as Syl-CNN), and tuning their hyperparameters under 20M pa- rameter budget gives the architectures in <ref type="table" target="#tab_1">Table  2</ref>. The results of evaluating these three models on small (1M tokens) and medium-sized (17M- 57M tokens) data sets against Char-CNN for dif- ferent languages are provided in <ref type="table">Table 3</ref>. The models demonstrate similar performance on small data, but Char-CNN scales significantly better on medium-sized data. From the three syllable-aware models, Syl-Concat looks the most advantageous as it demonstrates stable results and has the least number of parameters. Therefore in what follows we will make a more detailed comparison of Syl- Concat with Char-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model EN FR ES DE CS RU Char-CNN 78.9 184 165 239 371 261 DATA-S Syl-CNN 80.5 191 172 239 374 269 Syl-Sum 80.3 193 170 243 389 273 Syl-Concat 79.4 188 168 244 383 265 Char-CNN 160 124 118 198 392 190</head><p>DATA-L <ref type="table" target="#tab_0">Syl-CNN 7  - - - - - - Syl-Sum  170 141 129 212 451 233  Syl-Concat 176 139 129 225 449 225   Table 3</ref>: Evaluation of the syllable-aware mod- els against Char-CNN. In each case the smallest model, Syl-Concat, has 18%-33% less parameters than Char-CNN and is trained 1.2-2.2 times faster (Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared errors: It is interesting to see whether</head><p>Char-CNN and Syl-Concat are making similar er- rors. We say that a model gives an error if it as- signs a probability less than p * to a correct word from the test set. <ref type="figure" target="#fig_1">Figure 2</ref> shows the percentage of errors which are shared by Syl-Concat and Char- CNN depending on the value of p * . We see that the vast majority of errors are shared by both mod- els even when p * is small (0.01). PPL breakdown by token frequency: To find out how Char-CNN outperforms Syl-Concat, we partition the test sets on token frequency, as com- puted on the training data. We can observe in <ref type="figure" target="#fig_2">Figure 3</ref> that, on average, the more frequent the word is, the bigger the advantage of Char-CNN over Syl-Concat. The more Char-CNN sees a word in different contexts, the more it can learn about this word (due to its powerful CNN filters). Syl- Concat, on the other hand, has limitations -it can- not see below syllables, which prevents it from ex- tracting the same amount of knowledge about the word. PCA of word embeddings: The intrinsic advan- tage of Char-CNN over Syl-Concat is also sup-  ported by the following experiment: We took word embeddings produced by both models on the En- glish PTB, and applied PCA to them. 8 Regard- less of the threshold percentage of variance to re- tain, the embeddings from Char-CNN always have more principal components than the embeddings from Syl-Concat (see <ref type="table" target="#tab_2">Table 4</ref>). This means that Char-CNN embeds words into higher dimensional space than Syl-Concat, and thus can better distin- guish them in different contexts. LSTM limitations: During the hyperparameters tuning we noticed that increasing d S , d HW and d LM from the optimal values (in <ref type="table" target="#tab_1">Table 2</ref>) did not result in better performance for Syl-Concat. Could it be due to the limitations of the word-level LSTM (the topmost layer in <ref type="figure">Fig. 1</ref>)? To find out whether this was the case we replaced the LSTM by a Varia- tional RHN ( <ref type="bibr" target="#b26">Zilly et al., 2017)</ref>, and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat <ref type="table" target="#tab_4">(Table 5)</ref>. More- over, increasing d LM from 439 to 650 did result in better performance for Syl-Concat. Optimization details are given in Appendix B. Comparing syllable and morpheme embed- dings: It is interesting to compare morphemes and syllables. We trained Morfessor 2.0 ( <ref type="bibr" target="#b3">Creutz and Lagus, 2007</ref>) in its default configuration on the PTB training data and used it instead of the syl-  labifier in our models. Interestingly, we got ≈3K unique morphemes, whereas the number of unique syllables was ≈6K. We then trained all our models on PTB under 5M parameter budget, keeping the state size of the word-level LSTM at 300 (as in our pre-selection step for syllable-aware models). The reduction in number of subword types allowed us to give them higher dimensionality d M = 100 (cf. d S = 50). 9</p><p>Convolutional (Morph-CNN-3) and additive (Morph-Sum) models performed better than oth- ers with test set PPLs 83.0 and 83.9 respectively. Due to limited amount of time, we did not per- form a thorough hyperparameter search under 20M budget. Instead, we ran two configurations for Morph-CNN-3 and two configurations for Morph- Sum with hyperparameters close to those, which were optimal for Syl-CNN-3 and Syl-Sum corre- spondingly. All told, our best morpheme-aware model is Morph-Sum with d M = 550, d HW = 1100, d LM = 550, and test set PPL 79.5, which is practically the same as the result of our best syllable-aware model <ref type="bibr">Syl-Concat (79.4)</ref>. This makes Morph-Sum a notable alternative to Char- CNN and Syl-Concat, and we defer its thorough study to future work. Source code: The source code for the models discussed in this paper is available at https:// github.com/zh3nis/lstm-syl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pre-selection</head><p>In all models with highway layers there are two of them and the non-linear activation of any highway layer is a ReLU.</p><formula xml:id="formula_9">LSTM-Word: d W = 108, d LM = 300. Syl-LSTM: d S = 50, d LM = 300. Syl-CNN-[L]: d S = 50, convolutional filter widths are [1, . . . , L], the corresponding convolu- tional filter depths are [c·l] L l=1 , d HW = c·(1+. . .+ L).</formula><p>We experimented with L = 2, 3, 4. The corre- sponding values of c are chosen to be 120, 60, 35 to fit the total parameter budget. CNN activation is tanh. Linear combinations: We give higher dimension- ality to syllable vectors here (compared to other models) since the resulting word vector will have the same size as syllable vectors (see <ref type="formula" target="#formula_2">(3)</ref>). d S = 175, d HW = 175 in all models except the Syl-Avg- B, where we have d S = 160, d HW = 160. Syl-Concat: d S = 50, d HW = 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Optimization</head><p>LSTM-based models: We perform the training (5) by truncated BPTT <ref type="bibr" target="#b24">(Werbos, 1990;</ref><ref type="bibr" target="#b6">Graves, 2013)</ref>. We backpropagate for 70 time steps on DATA-S and for 35 time steps on DATA-L using stochastic gradient descent where the learning rate is initially set to 1.0 and halved if the perplex- ity does not decrease on the validation set after an epoch. We use batch sizes of 20 for DATA-S and 100 for DATA-L. We train for 50 epochs on DATA-S and for 25 epochs on DATA-L, picking the best-performing model on the validation set. Parameters of the models are randomly initialized uniformly in [−0.05, 0.05], except the forget bias of the word-level LSTM, which is initialized to 1. For regularization we use dropout ( <ref type="bibr" target="#b20">Srivastava et al., 2014</ref>) with probability 0.5 between word- level LSTM layers and on the hidden-to-output softmax layer. We clip the norm of the gradi- ents (normalized by minibatch size) at 5. These choices were guided by previous work on word- level language modeling with LSTMs ( <ref type="bibr">Zaremba et al., 2014)</ref>.</p><p>To speed up training on DATA-L we use a sam- pled softmax <ref type="bibr" target="#b8">(Jean et al., 2015</ref>) with the number of samples equal to 20% of the vocabulary size ( <ref type="bibr" target="#b1">Chen et al., 2016)</ref>. Although <ref type="bibr" target="#b10">Kim et al. (2016)</ref> used a hierarchical softmax <ref type="bibr" target="#b17">(Morin and Bengio, 2005</ref>) for the same purpose, a recent study <ref type="bibr" target="#b5">(Grave et al., 2016)</ref> shows that it is outperformed by sam- pled softmax on the Europarl corpus, from which DATA-L was derived <ref type="bibr" target="#b0">(Botha and Blunsom, 2014</ref>). RHN-based models are optimized as in <ref type="bibr" target="#b26">Zilly et al. (2017)</ref>, except that we unrolled the networks for 70 time steps in truncated BPTT, and dropout rates were chosen to be as follows: 0.2 for the embed- ding layer, 0.7 for the input to the gates, 0.7 for the hidden units and 0.2 for the output activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Sizes and speeds</head><p>On DATA-S, Syl-Concat has 28%-33% fewer pa- rameters than Char-CNN, and on DATA-L the re- duction is 18%-27% (see <ref type="figure" target="#fig_3">Fig. 4</ref>). Training speeds are provided in the  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Syllable-aware language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of errors shared by both Syl-Concat and Char-CNN on DATA-S (left) and DATA-L (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PPL reduction by token frequency, CharCNN relative to Syl-Concat on DATA-L. Model 80% 90% 95% 99% Char-CNN 568 762 893 1038 Syl-Concat 515 729 875 1035</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model sizes on DATA-S (left) and DATA-L, in millions of trainable variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Pre-selection results. PPL stands for test 
set perplexity, all models have ≈ 5M parameters. 

Model 
dS 
dHW 
dLM Size PPL 
Syl-CNN 
242 1170 380 15M 80.5 
Syl-Sum 
438 1256 435 18M 80.3 
Syl-Concat 228 
781 
439 13M 79.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hyperparameters tuning. In Syl-CNN, 
d HW is a function of the primary hyperparameter 
c = 195 (see Appendix A). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Number of principle components when PCA is applied to word embeddings produced by each model, depending on % of variance to retain.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Replacing LSTM with Variational RHN.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Mod-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Training speeds, in thousands of tokens 
per second. </table></figure>

			<note place="foot" n="1"> Subword-level LMs rely on subword-level inputs and make predictions at the level of subwords; subword-aware LMs also rely on subword-level inputs but make predictions at the level of words. 2 Not to be confused with character-aware ones, see the previous footnote.</note>

			<note place="foot" n="3"> The same question applies to any model that segments words into a sequence of characters or other subword units.</note>

			<note place="foot" n="4"> In languages with alphabetic writing systems.</note>

			<note place="foot" n="5"> U (a, b) stands for a uniform distribution over (a, b).</note>

			<note place="foot" n="6"> When words are directly embedded into R d W through an embedding matrix EW ∈ R |W|×d W. 7 Syl-CNN results on DATA-L are not reported since computational resources were insufficient to run these configurations.</note>

			<note place="foot" n="8"> We equalized highway layer sizes dHW in both models to have same dimensions for embeddings. In both cases, word vectors were standardized using the z-score transformation.</note>

			<note place="foot" n="7"> Conclusion It seems that syllable-aware language models fail to outperform competitive character-aware ones. However, usage of syllabification can reduce the total number of parameters and increase the training speed, albeit at the expense of languagedependent preprocessing. Morphological segmentation is a noteworthy alternative to syllabification: a simple morpheme-aware model which sums morpheme embeddings looks promising, and its study is deferred to future work. 9 M stands for morphemes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the NVIDIA Corpo-ration for their donation of the Titan X Pascal GPU used for this research. The work of Bagdat Myrzakhmetov has been funded by the Commit-tee of Science of the Ministry of Education and Science of the Republic of Kazakhstan under the targeted program O.0743 (0115PK02473). The authors would like to thank anonymous review-ers and Aibek Makazhanov for valuable feedback, Makat Tlebaliyev and Dmitriy Polynin for IT sup-port, and Yoon Kim for providing the preprocessed datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strategies for training large vocabulary neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised models for morpheme segmentation and morphology learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04309</idno>
		<title level="m">Efficient softmax approximation for gpus</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Word Hy-phen-a-tion by Com-put-er</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franklin Mark</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chu-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-learning of word representations and morpheme representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From characters to words to in between: Do we capture morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Character-word lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyan</forename><surname>Verwimp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Pelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wambacq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul J Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
