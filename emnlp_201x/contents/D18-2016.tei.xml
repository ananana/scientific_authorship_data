<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from YouTube Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Knowledge Technology University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Magg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Knowledge Technology University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Knowledge Technology University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Knowledge Technology University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from YouTube Videos</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations)</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations) <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="90" to="95"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>90</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we describe KT-Speech-Crawler: an approach for automatic dataset construction for speech recognition by crawling YouTube videos. We outline several filtering and post-processing steps, which extract samples that can be used for training end-to-end neural speech recognition systems. In our experiments , we demonstrate that a single-core version of the crawler can obtain around 150 hours of transcribed speech within a day, containing an estimated 3.5% word error rate in the transcriptions. Automatically collected samples contain reading and spontaneous speech recorded in various conditions including background noise and music, distant microphone recordings, and a variety of accents and reverberation. When training a deep neural network on speech recognition, we observed around 40% word error rate reduction on the Wall Street Journal dataset by integrating 200 hours of the collected samples into the training set. The demo 1 and the crawler code 2 are publicly available.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end neural networks significantly simpli- fied the development of automatic speech recog- nition (ASR) systems <ref type="bibr" target="#b6">(Graves and Jaitly, 2014</ref>). Traditionally, ASR systems are based on Gaus- sian Mixture Models (GMM) or Deep Neural Net- works (DNN) for acoustic state representations followed by the Hidden Markov Model (HMM) for sequence-level learning. Though such systems are successful and achieve high performance <ref type="bibr" target="#b9">(Hinton et al., 2012)</ref>, they require word-or phoneme- level alignments between the acoustic signal and the transcription. As a result, dataset prepara- tion for such hybrid systems is a labor-intensive <ref type="figure">Figure 1</ref>: Architecture of the proposed system crawl- ing YouTube to find videos with closed captions. Sev- eral filtering and post-processing steps are applied to select high-quality speech candidates. As a result, pairs of speech and corresponding transcriptions are col- lected. and error-prone process as the performance of the whole system is sensitive to the quality of the alignment. Also, each component is trained indi- vidually, which makes the whole process complex and difficult to maintain. Recently, Connectionist Temporal Classification (CTC) loss ( <ref type="bibr" target="#b5">Graves et al., 2006</ref>) has been introduced, which allows relax- ing the constraint of having alignment between the spoken text and audio by introducing a sequence- level criterion. Also, recurrent neural network- based architectures that are state-of-the-art mod- els in machine translation have been applied to speech recognition ( <ref type="bibr" target="#b2">Chan et al., 2016)</ref>. Conse- quently, neural networks can be trained end-to-end via backpropagation ( <ref type="bibr" target="#b6">Graves and Jaitly, 2014</ref>). CTC maximizes the log-likelihood of the ground truth transcription and thus only the spoken text is required without an explicit alignment, which is easier and cheaper to obtain.</p><p>Previous work outlined the importance of hav- ing large amounts of annotated data to train deep neural networks. For example, a ten times in- crease of the training data size from 1,200 hours to 12,000 hours resulted in improving the word error rate from 13.9% to 8.46% for clean and from 22.99% to 13.59% for noisy speech <ref type="bibr" target="#b0">(Amodei et al., 2016)</ref>. Collecting such large datasets is an expensive and labor-intensive process, which re- quires a significant amount of resources, usually not available for the research community com- pared to large industrial companies. For exam- ple, Baidu's internal speech dataset ( <ref type="bibr" target="#b0">Amodei et al., 2016)</ref> contains around 10,000 hours of speech, while the largest dataset available for the re- search community does not exceed 2,000 hours ( <ref type="bibr" target="#b4">David et al., 2004</ref>). We propose to utilize a vast amount of videos available on YouTube with user- provided closed captions as a source to extract speech datasets comparable in size to the ones available in the industry.</p><p>Our contribution in this paper is two-fold: 1) we provide a crawler that automatically ex- tracts speech samples with transcriptions from YouTube and filters high-quality samples with several heuristic measures, and 2) we extend the training data of two benchmark datasets with the extracted samples and validate the benefit of the collected data by training a deep neural network on the original and the combined data to mea- sure test performance difference. We also evalu- ate the amount of noise in transcriptions by manu- ally checking the word error rate of a random sub- set of the dataset. We hope that our developed tool will foster research of large-scale automatic speech recognition systems 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Crowdsourcing has been successfully used to con- struct speech datasets like VoxForge 4 or Mozilla's Common Voice <ref type="bibr">5</ref> , where users recorded them- selves through the provided web-interface, and up- loaded samples can be checked by other partic-ipants. While such an approach, in theory, can be a viable strategy to acquire a large number of diverse speech samples, it has several draw- backs. The main limitation of this approach is the difficulty of engaging and acquiring users to donate samples to achieve a large and di- verse dataset in terms of the number of differ- ent speakers, accents, environments and recording conditions. Another approach, which is widely adopted by the research community, is to make use of a vast amount of available multi-modal data which contains transcribed speech. For exam- ple, TED talks ( <ref type="bibr" target="#b16">Rousseau et al., 2014</ref>) are care- fully transcribed and contain around 200 hours of speech from around 1,500 speakers (TED-LIUM v2 dataset). LibriSpeech ( <ref type="bibr" target="#b14">Panayotov et al., 2015</ref>) is composed of a large number of audiobooks and is the largest freely available dataset: around 960 hours of English read speech. Google re- leased their Speech Commands dataset 6 contain- ing around 65,000 one-second-long utterances.</p><p>It has already been demonstrated that YouTube captions can be successfully used as a ground truth spoken text transcription to train large-scale ASR systems ( <ref type="bibr" target="#b12">Liao et al., 2013;</ref><ref type="bibr" target="#b11">Lecouteux et al., 2012</ref>). Users upload closed captions for vari- ous reasons: to make video accessible for peo- ple having some degree of hear loss, or to help non-native speakers, or to increase the number of views (YouTube search ranking algorithm in- dexes closed captions content 7 ). Nevertheless, some videos contain inaccurate or even unrelated to speech captions, for example, advertisements. Several heuristics were proposed to remove low- quality samples: removing captions containing advertisements, language mismatch detection and using forced alignment to detect confident align- ment regions between the caption and the audio. In addition, YouTube has been used previously in multiple ways to automatically collect multi- modal datasets, e.g. emotion recognition datasets by <ref type="bibr" target="#b1">Barros et al. (2018)</ref> and <ref type="bibr" target="#b17">Zadeh et al. (2016)</ref>, or opinion mining (Marrese-Taylor et al., 2017), or video classification (YouTube-8M 8 , or human ac- tion recognition ( <ref type="bibr" target="#b10">Kay et al., 2017)</ref>).</p><p>In this work, we combine several known heuris-tics and propose some additional ones to select high quality samples in an automatic way. We integrate it into an easy to use tool KT-Speech- Crawler, which can continuously scan new videos uploaded to YouTube and update the speech database. To our knowledge, this is the first open- source tool available for automatic speech dataset construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Crawler</head><p>In this section, we describe the sample selection strategy, followed by several filtering and post- processing heuristics to locate high-quality sam- ples and discard noisy ones from YouTube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate selection</head><p>Firstly, we download candidate videos with En- glish closed captions, which are usually uploaded by the channel owner. To reach as many videos as possible we use the YouTube Search API, where one of the top 100 most common English words is used as a search keyword to match the video title (for example, the, but, have, not, and, ...).</p><p>Such frequent keywords allow us to match many videos, even though, as a side effect, non-English videos with closed captions in English might be captured. The YouTube Search API allows to download the 600 most recent videos for each key- word, and since many videos are constantly be- ing uploaded to YouTube it is possible to continu- ously collect speech samples. Also, we memorize YouTube channels containing samples that passed all the filtering steps (see section 3.2) and use other videos from this channel. This leads to many di- verse candidates coming from TV shows and TV series, video blogs, news, and live recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Filtering steps</head><p>We perform several filtering steps to select suitable candidates:</p><p>• we discard a caption if it overlaps with an- other caption, which sometimes happens due to incorrectly closed caption auto syncing,</p><p>• we filter out captions that indicate that there is music content in this sample and captions containing non-ASCII characters or URLs,</p><p>• we remove text chunks which do not corre- spond to the actual spoken text, like the infor- mation of the speaker name (Speaker 1: ...), annotations ([laughs], *laughs*, (laughs)), and punctuation,</p><p>• we spell out numbers which are within the range from 1 to 100 as they have non- ambiguous pronunciation (in contrast, for ex- ample, 1,500 can be uttered as fifteen hundred or one thousand and five hundred),</p><p>• we discard captions if they contain any char- acter that is not an English letter, apostrophe or a white space,</p><p>• we filter segments which have less than one second duration or more than ten seconds,</p><p>• in addition, we select randomly three phrases from the video and measure the Levenshtein similarity between the provided closed cap- tion and the transcription generated by the Google ASR API. If the similarity is below a 70% threshold, we discard all the samples in this video. This step allows filtering videos which have English subtitles for non-English spoken text or videos with a bad alignment. Also, this filter removes videos with com- pletely misaligned captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-processing steps</head><p>During our experiments on evaluating the quality of the extracted samples, we spotted that one of the major problems is imprecise alignments be- tween caption and audio. For example, the first or the last word can be omitted on the recording due to incorrect caption timings. One possible way to reduce the number of samples with mis- aligned borders is to group together nearby cap- tions if they are at a distance of less than one sec- ond. We stop grouping adjacent utterances if the overall length exceeds ten seconds. In addition, we perform a forced alignment 9 between the caption and the corresponding audio using Kaldi (Povey et al., 2011) and if the first or the last word is not successfully mapped, we try to extend the caption boundaries (up to 500 milliseconds) until the bor- der word becomes mapped. If we cannot align the border word, we keep the caption boundaries un- changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and analysis</head><p>To evaluate the usefulness of the collected sam- ples we conducted three types of experiments. We trained the deep neural network-based model on different training datasets:</p><p>• on the original training data,</p><p>• on the mix of the original and with the crawled samples,</p><p>• only on the crawled samples.</p><p>For benchmarking, we selected two well-known datasets for training ASR systems: The Wall Street Journal and TED-LIUM v2. In all experiments, we kept the same size and architecture of the neu- ral model and its hyperparameters. In this section, we outline the details of benchmark data used in our experiments, neural model architecture and the evaluation protocol and metrics, followed by the evaluation results and comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ASR model</head><p>Our ASR model (see <ref type="figure" target="#fig_0">Figure 2</ref>) is a combination of convolutional and recurrent layers inspired by the DeepSpeech 2 ( <ref type="bibr" target="#b0">Amodei et al., 2016</ref>) architecture. Our model contains two 2D convolutional layers for feature extraction from power FFT spectro- grams. Power spectrograms are extracted using a Hamming window of 20ms width and 10ms stride, resulting in 161 features for each speech frame. Convolutional layers are followed by five recur- rent bi-directional Gated Recurrent Units ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>) layers with a size of 1,024 followed by a softmax layer on top, predicting the char- acter distribution for each speech frame. Over- all, our model has around 61 million parameters. Connectionist Temporal Classification (CTC) loss ( <ref type="bibr" target="#b5">Graves et al., 2006</ref>) is used as a loss criterion to measure how good the alignment produced by the network is compared to the ground truth transcrip- tion. The Stochastic Gradient Descent optimizer is used in all experiments with a learning rate of 0.0003, clipping the norm of the gradient at the <ref type="table">Table 1</ref>: Evaluation results. We evaluated the effect of adding samples extracted from YouTube by our tool on two benchmarking datasets: WSJ and TED-LIUM v2. We trained the deep neural network on the origi- nal training data, then combined the data with YouTube samples (WSJ+YouTube, for example), and, finally, only on the YouTube samples. We report word and character error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>Test level of 400 with a batch size of 32. During the training, we apply learning rate annealing with a factor of 1.1. We apply the SortaGrad algorithm ( <ref type="bibr" target="#b0">Amodei et al., 2016</ref>) during the first epoch by sorting utterances by their duration ( <ref type="bibr" target="#b7">Hannun et al., 2014a)</ref>. We select the model with the best word er- ror rate measured on the validation set to prevent model overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data and evaluation measure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">WSJ</head><p>The Wall Street Journal (WSJ) dataset is a well- known dataset for evaluating ASR systems, con- taining utterances of read speech coming from the news domain. The WSJ training set (train-si284) consists of 81 hours containing 37,318 sentences from 284 speakers (142 male and 142 female). We used the dev93 development set for validation and report the word error rate on the eval92 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">TED talks</head><p>We also evaluated our approach on the TED- LIUM v2 dataset, which contains around 200 hours of transcribed TED 10 talks of 1,495 speak- ers. In contrast to the WSJ dataset, it contains spontaneous speech rather than read speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We summarize our results in <ref type="table">Table 1</ref>. Note that we did not use a language model for decoding in our experiments but used greedy decoding, where the most probable character at each timestep was emitted. It is well known that decoding with the language model and beam search significantly im- proves the performance on the test set of character- based end-to-end models ( <ref type="bibr" target="#b8">Hannun et al., 2014b</ref>), but as our goal was to demonstrate the impact of adding extracted samples within the same neural model and test set, we left it out. We observed that adding samples from YouTube positively con- tributed to the overall performance in both met- rics: word (WER) and character error rates (CER for the WSJ and the TED datasets, respectively (compared to 7.2% and 10.4% using original train- ing data), indicating that having a domain-specific training set plays an important role and there is a room for improvement in designing better filtering and post-processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transcriptions quality</head><p>We manually investigated samples by using devel- oped a web-based demo, see <ref type="figure" target="#fig_1">Fig. 3</ref> and analyzed the quality of the collected samples and their tran- scriptions. Our developed web-service presents random eight utterances and their corresponding transcriptions to the user and allows to load more samples if necessary. We also integrated a simple functionality to validate the extracted samples: a user can confirm that the caption is correct or if not enter the right transcription.</p><formula xml:id="formula_0">W ER = S + D + I S + D + C<label>(1)</label></formula><p>We computed the word error rate using equation 1, where S, I, D, C is number of substitutions, insertions, deletions and correct words, respec- tively. We estimated 3.5% word error rate on the small randomly selected subset of 600 samples. The most common type of error was missing or wrongly added one or two words at the beginning or at the end of the utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>In this work, we presented an open-source system that automatically constructs datasets for training end-to-end neural speech recognition systems. We demonstrated the usefulness of the collected sam- ples on the WSJ and TED datasets. We provide the code for the crawler and metadata and a script to easily construct a dataset of 500 hours. Future work includes extending the script to support other languages. A more sophisticated approach to identify wrongly added or missing words in transcriptions could also be used by using attention-based neural networks like pointer net- works. We are also aware that some collected sam- ples may contain automatically generated utter- ances with Text-To-Speech software, which may require performing speaker recognition to balance the dataset. Furthermore, domain-specific speech datasets can be collected by selecting samples af- ter analyzing captions and video metadata (for ex- ample, in the financial domain). In addition, sam- ples with several people talking at the same time and noisy samples with low signal-to-noise ratio need to be filtered, which could be implemented as neural network-based modules.</p><p>We believe that having large, free and high- quality speech datasets available to the research community will foster the development of new architectures and applications for speech under- standing, and we hope that our presented tool will contribute to that. vation programme under the Marie Sklodowska- Curie grant agreement No 642667 (SECURE) and partial support from the German Research Foun- dation DFG under project CML (TRR 169).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the ASR model used in this work, following the DeepSpeech 2 architecture.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,218.27,62.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A screenshot of the web-based demo to browse the collected samples, presenting the extracted utterance and the corresponding transcription.</figDesc><graphic url="image-3.png" coords="5,72.00,62.81,218.27,133.13" type="bitmap" /></figure>

			<note place="foot" n="1"> http://emnlp-demo.lakomkin.me/ 2 https://github.com/EgorLakomkin/ KTSpeechCrawler</note>

			<note place="foot" n="3"> The code and the Dockerfile are available by this link https://github.com/EgorLakomkin/ KTSpeechCrawler 4 http://www.voxforge.org 5 https://voice.mozilla.org/</note>

			<note place="foot" n="6"> https://ai.googleblog.com/2017/08/ launching-speech-commands-dataset.html 7 https://www.3playmedia.com/customers/ case-studies/discovery-digital-networks/ 8 https://research.google.com/ youtube8m/</note>

			<note place="foot" n="9"> https://github.com/lowerquality/ gentle</note>

			<note place="foot" n="10"> https://www.ted.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from the Eu-ropean Union's Horizon 2020 research and inno-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The OMG-Emotion Behavior Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Churamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Siqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To appear in International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Fisher Corpus: a Resource for the Next Generations of Speech-to-Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Cieri</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">Cieri</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 4th International Conference On Language Resources and Evaluation</title>
		<meeting>4th International Conference On Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning-ICML &apos;06</title>
		<meeting>the 23rd international conference on Machine learning-ICML &apos;06<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards endto-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1764</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Recurrent DNNs. CoRR, abs/1408.2873</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating imperfect transcripts into speech recognition systems for building high-quality corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Oger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="89" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining fine-grained opinions on closed captions of YouTube videos with an attention-RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Balazs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing the TEDLIUM corpus with selected data for language modeling and more TED talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 9th International Conference On Language Resources and Evaluation</title>
		<meeting>9th International Conference On Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
