<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Inference for Generative Neural Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Inference for Generative Neural Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1695" to="1700"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these gen-erative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Char-niak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A recent line of work has demonstrated the success of generative neural models for constituency pars- ing ( <ref type="bibr" target="#b4">Dyer et al., 2016;</ref><ref type="bibr" target="#b2">Choe and Charniak, 2016)</ref>. As with discriminative neural parsers, these mod- els lack a dynamic program for exact inference due to their modeling of unbounded dependencies. However, while discriminative neural parsers are able to obtain strong results using greedy search <ref type="bibr" target="#b4">(Dyer et al., 2016)</ref> or beam search with a small beam ( <ref type="bibr" target="#b13">Vinyals et al., 2015)</ref>, we find that a simple action-level approach fails outright in the genera- tive setting. Perhaps because of this, the applica- tion of generative neural models has so far been re- stricted to reranking the output of external parsers.</p><p>Intuitively, because a generative parser defines a joint distribution over sentences and parse trees, probability mass will be allocated unevenly be- tween a small number of common structural ac- tions and a large vocabulary of lexical items. This imbalance is a primary cause of failure for search procedures in which these two types of ac- tions compete directly. A notion of equal com- petition among hypotheses is then desirable, an idea that has previously been explored in gener- ative models for constituency parsing <ref type="bibr" target="#b6">(Henderson, 2003)</ref> and dependency parsing <ref type="bibr" target="#b12">(Titov and Henderson, 2010;</ref><ref type="bibr" target="#b0">Buys and Blunsom, 2015</ref>), among other tasks. We describe a related state-augmented beam search for neural generative constituency parsers in which lexical actions compete only with each other rather than with structural actions. Ap- plying this inference procedure to the generative model of <ref type="bibr" target="#b2">Choe and Charniak (2016)</ref>, we find that it yields a self-contained generative parser that achieves high performance.</p><p>Beyond this, we propose an enhanced candi- date selection strategy that yields significant im- provements for all beam sizes. Additionally, mo- tivated by the look-ahead heuristic used in the top-down parsers of <ref type="bibr" target="#b9">Roark (2001) and</ref><ref type="bibr" target="#b1">Charniak (2010)</ref>, we also experiment with a simple coarse pruning function that allows us to reduce the num- ber of states expanded per candidate by several times without compromising accuracy. Using our final search procedure, we surpass prior state-of- the-art results among single-model parsers on the Penn Treebank, obtaining an F1 score of 92.56.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Common Framework</head><p>The generative neural parsers of <ref type="bibr" target="#b4">Dyer et al. (2016)</ref> and <ref type="bibr" target="#b2">Choe and Charniak (2016)</ref> can be unified un- der a common shift-reduce framework. Both sys- tems build parse trees in left-to-right depth-first or- der by executing a sequence of actions, as illus- trated in <ref type="figure">Figure 1</ref>. These actions can be grouped Figure 1: A parse tree and the action sequence that produced it, corresponding to the sentence "He had an idea." The tree is constructed in left-to- right depth-first order. The tree contains only non- terminals and words; part-of-speech tags are not included. OPEN(X) and CLOSE(X) are rendered as "(X" and "X)" for brevity.</p><p>into three major types: OPEN(X) and CLOSE(X), which open and close a constituent with nontermi- nal X, 1 respectively, and SHIFT(x), which adds the word x to the current constituent. The proba- bility of an action sequence (a 1 , . . . , a T ) is</p><formula xml:id="formula_0">P (a 1 , . . . , a T ) = T t=1 P (a t | a 1 , . . . , a t−1 ) = T t=1 [softmax(Wu t + b)] at ,</formula><p>where u t is a continuous representation of the parser's state at time t, and <ref type="bibr">[v]</ref> j denotes the jth component of a vector v. We refer readers to the respective authors' papers for the parameterization of u t in each model.</p><p>In both cases, the decoding process reduces to a search for the most probable action sequence that represents a valid tree over the input sentence. For a given hypothesis, this requirement implies sev- eral constraints on the successor set ( <ref type="bibr" target="#b4">Dyer et al., 2016</ref>); e.g., SHIFT(x) can only be executed if the next word in the sentence is x, and CLOSE(X) cannot be executed directly after OPEN(X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model and Training Setup</head><p>We reimplemented the generative model described in <ref type="bibr" target="#b2">Choe and Charniak (2016)</ref> and trained it on the Penn Treebank ( <ref type="bibr" target="#b8">Marcus et al., 1993</ref>) using (S (NP He NP) (VP had (NP an idea NP) VP) . S) -0.1 -0.5</p><formula xml:id="formula_1">-3.2 -0.0 -0.1 -4.6 -1.4 -3.9 -4.0 -0.1 -0.8 -0.0 -0.0</formula><p>Figure 2: A plot of the action log probabilities log P (a t | a 1 , . . . , a t−1 ) for the example in <ref type="figure">Fig- ure 1</ref> under our main model. We observe that OPEN and CLOSE actions have much higher prob- ability than SHIFT actions. This imbalance is re- sponsible for the failure of standard action-level beam search.</p><p>their published hyperparameters and preprocess- ing. However, rather than selecting the final model based on reranking performance, we instead per- form early stopping based on development set per- plexity. We use sections 2-21 of the Penn Tree- bank for training, section 22 for development, and section 23 for testing. The model's action space consists of 26 matching pairs of OPEN and CLOSE actions, one for each nonterminal, and 6,870 SHIFT actions, one for each preprocessed word type. While we use this particular model for our experiments, we note that our subsequent dis- cussion of inference techniques is equally appli- cable to any generative parser that adheres to the framework described above in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Action-Level Search</head><p>Given that ordinary action-level search has been applied successfully to discriminative neural parsers ( <ref type="bibr" target="#b13">Vinyals et al., 2015;</ref><ref type="bibr" target="#b4">Dyer et al., 2016)</ref>, it offers a sensible starting point for decoding in generative models. However, even for large beam sizes, the following pathological behavior is en- countered for generative decoding, preventing rea- sonable parses from being found. Regardless of the sequence of actions taken so far, the generative model tends to assign much higher probabilities to structural OPEN and CLOSE actions than it does lexical SHIFT actions, as shown in <ref type="figure">Figure 2</ref>. The model therefore prefers to continually open new constituents until a hard limit is reached, as the al- ternative at each step is to take the low-probability action of shifting the next word. The resulting sequence typically has much lower overall prob- ability than a plausible parse, but the model's my- opic comparison between structural and lexical ac- tions prevents reasonable candidates from staying on the beam. Action-level beam search with beam size 1000 obtains an F1 score of just 52.97 on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Word-Level Search</head><p>The imbalance between the probabilities of struc- tural and lexical actions suggests that the two kinds of actions should not compete against each other within a beam. This leads us to consider an augmented state space in which they are kept separate by design, as was done by <ref type="bibr" target="#b5">Fried et al. (2017)</ref>. In conventional action-level beam search, hypotheses are grouped by the length of their ac- tion history |A|. Letting A i denote the set of ac- tions taken since the ith shift action, we instead group hypotheses by the pair (i, |A i |), where i ranges between 0 and the length of the sentence.</p><p>Let k denote the target beam size. The search process begins with the empty hypothesis in the (0, 0) bucket. Word-level steps are then taken according to the following procedure for i = 0, 1, . . . , up to the length of the sentence (inclu- sive). Beginning with the (i, 0) bucket, the suc- cessors of each hypothesis are pooled together, sorted by score, and filtered down to the top k. Of those that remain, successors obtained by tak- ing an OPEN or CLOSE action advance to the (i, 1) bucket, whereas successors obtained from a SHIFT action are placed in the (i + 1, 0) bucket if i is less than the sentence length, or the completed list if i is equal to the sentence length. This process is re- peated for the (i, 1) bucket, the (i, 2) bucket, and so forth, until the (i + 1, 0) bucket contains at least k hypotheses. If desired, a separate word beam size k w &lt; k can be used at word boundaries, in which case each word-level step terminates when the (i + 1, 0) bucket has k w candidates instead of k. This introduces a bottleneck that can help to promote beam diversity.</p><p>Development set results for word-level search </p><note type="other">an an (i, 0) (i, 1) (i, 2) (i + 1, 0)</note><p>top-k fast-track <ref type="figure">Figure 3</ref>: One step of word-level search with fast- track candidate selection (Sections 5 and 6) for the example in <ref type="figure">Figure 1</ref>. Grouping candidates by the current word i ensures that low-probability lexi- cal actions are kept separate from high-probability structural actions at the beam level. Fast-track selection mitigates competition between the two types of actions within a single pool of successors.</p><p>with a variety of beam sizes and with k w = k or k w = k/10 are given in <ref type="table">Table 1</ref>. We observe that performance in both cases increases steadily with beam size. Word-level search with k w = k/10 consistently outperforms search without a bottle- neck at all beam sizes, indicating the utility of this simple diversity-inducing modification. The top result of 92.93 F1 is already quite strong compared to other single-model systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Fast-Track Candidate Selection</head><p>The word-level beam search described in Section 5 goes one step toward ameliorating the issue that causes action-level beam search to fail, namely the direct competition between common structural actions with high probabilities and low-frequency shift actions with low probabilities. However, the issue is still present to some extent, in that succes- sors of both types from a given bucket are pooled <ref type="bibr">400</ref>   <ref type="table">Table 2</ref>: Development F1 scores using the settings from <ref type="table">Table 1</ref>, together with the fast-track selection strategy from Section 6 with k s = k/100. together and filtered down as a single collection before being routed to their respective destina- tions. We therefore propose a more direct solution to the problem, in which a small number k s k of SHIFT successors are fast-tracked to the next word-level bucket before any filtering takes place. These fast-tracked candidates completely bypass competition with potentially high-scoring OPEN or CLOSE successors, allowing for higher-quality results in practice with minimal overhead. See <ref type="figure">Figure 3</ref> for an illustration.</p><p>We repeat the experiments from Section 5 with k s = k/100 and report the results in <ref type="table">Table 2</ref>. Note that the use of fast-tracked candidates offers sig- nificant gains under all settings. The top result im- proves from 92.93 to 93.18 with the use of fast- tracked candidates, surpassing prior single-model systems on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">OPEN Action Pruning</head><p>At any point during the trajectory of a hypothesis, either 0 or all 26 of the OPEN actions will be avail- able, compared with at most 1 CLOSE action and at most 1 SHIFT action. Hence, when available, OPEN actions comprise most or all of a candidate's successor actions. To help cut down on this por- tion of the search space, it is natural to consider whether some of these actions could be ruled out using a coarse model for pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Coarse Model</head><p>We consider a class of simple pruning models that condition on the c ≥ 0 most recent actions and the next word in the sentence, and predict a probabil- ity distribution over the next action. In the interest of efficiency, we collapse all SHIFT actions into a single unlexicalized SHIFT action, significantly reducing the size of the output vocabulary.</p><p>The input v t to the pruning model at time t is the concatenation of a vector embedding for each action in the context (a t−c , a t−c+1 , . . . , a t−1 ) and a vector embedding for the next word w:</p><formula xml:id="formula_2">v t = [e a t−c ; e a t−c+1 ; . . . ; e a t−1 ; e w ],</formula><p>where each e j is a learned vector embedding. The pruning model itself is implemented by feeding the input vector through a one-layer feedforward network with a ReLU non-linearity, then applying a softmax layer on top:</p><formula xml:id="formula_3">P (a t = a | a 1 , . . . , a t−1 , next-word = w) = P (a t = a | a t−c , . . . , a t−1 , next-word = w) = [softmax(W 2 max(W 1 v t + b 1 , 0) + b 2 )] a .</formula><p>The pruning model is trained separately from the main parsing model on gold action sequences de- rived from the training corpus, with log-likelihood as the objective function and a cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Strategy and Empirical Lower Bound</head><p>Once equipped with a coarse model, we use it for search reduction in the following manner. As men- tioned above, when a hypothesis is eligible to open a new constituent, most of its successors will be obtained through OPEN actions. Accordingly, we use the coarse model to restrict the set of OPEN actions to be explored. When evaluating the pool of successors for a given collection of hypothe- ses during beam search, we run the coarse model on each hypothesis to obtain a distribution over its next possible actions, and gather together all the coarse scores of the would-be OPEN succes- sors. We then discard the OPEN successors whose coarse scores lie below the top 1 − p quantile for a fixed 0 &lt; p &lt; 1, guaranteeing that no more than a p-fraction of OPEN successors are considered for evaluation. Taking p = 1 corresponds to the un- pruned setting.</p><p>This strategy gives us a tunable hyperparameter p that allows us to trade off between the amount of search we perform and the quality of our re- sults. Before testing our procedure, however, we would first like to investigate whether there is a principled bound on how low we can expect to set p without a large drop in performance. A simple estimate arises from noting that the pruning frac- tion p should be set to a value for which most or all of the outputs encountered in the training set are retained. Otherwise, the pruning model would prevent the main model from even recreating the training data, let alone producing good parses for new sentences.</p><p>To this end, we collect training corpus statis- tics on the occurrences of inputs to the pruning function and their corresponding outputs. We then c 1 2 3 4 5 6 7 8 9 10 0 20.0 58.4 82.4 91.0 94.9 96.8 97.9 98.6 98.9</p><p>99.2 1 54.9 80.5 91.1 95.9 97.7 98.8 99.5 99.8 99.9 100.0 2 61.2 85.0 93.8 97.4 98.6 99.5 99.8 99.9 100.0 100.0  <ref type="table">Table 4</ref>: Results when the best setting from Sec- tion 6 is rerun with OPEN action pruning with con- text size c = 2 and various pruning fractions p. Lower values of p indicate more aggressive prun- ing, while p = 1 means no pruning is performed.</p><p>compute the number of unique OPEN actions as- sociated with inputs occurring at least 20 times, and restrict our attention to inputs with at least one OPEN output. The resulting cumulative dis- tributions for context sizes c = 0, 1, 2 are given in <ref type="table" target="#tab_2">Table 3</ref>. If we require that our pruning frac- tion p be large enough to recreate at least 99% of the training data, then since there are 26 to- tal nonterminals, approximate <ref type="bibr">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Pruning Results</head><p>We reran our best experiment from Section 6 with an order-2 pruning function and pruning fractions p = 6/26, . . . , 11/26. The results are given in <ref type="table">Table 4</ref>. We observe that performance is on par with the unpruned setup (at most 0.1 absolute dif- ference in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Final Results and Conclusion</head><p>We find that the best overall settings are a beam size of k = 2000, a word beam size of k w = 200, and k s = 20 fast-track candidates per step, as this  setup achieves both the highest probabilities un- der the model and the highest development F1. We report our test results on section 23 of the Penn Treebank under these settings in <ref type="table" target="#tab_4">Table 5</ref> both with and without pruning, as well as a number of other recent results. We achieve F1 scores of 92.56 on the test set without pruning and 92.53 when 1 − 8/26 ≈ 69.2% of OPEN successors are pruned, obtaining performance well above the previous state-of-the-art scores for single-model parsers. This demonstrates that the model of <ref type="bibr" target="#b2">Choe and Charniak (2016)</ref> works well as an accurate, self-contained system. The fact that we match the performance of their reranking parser using the same generative model confirms the efficacy of our approach. We believe that further refinements of our search procedure can continue to push the bar higher, such as the use of a learned heuristic function for forward score estimation, or a more sophisticated approximate decoding scheme mak- ing use of specific properties of the model. We look forward to exploring these directions in fu- ture work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>S (NP He NP) (VP had (NP an idea NP) VP) . S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>lower bounds for p are 10/26 ≈ 0.385 for c = 0, 7/26 ≈ 0.269 for c = 1, and 6/26 ≈ 0.231 for c = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>F1 score) for p as low as 8/26 ≈ 0.308. Setting p to 7/26 ≈ 0.269 results in a drop of 0.18, and setting p to 6/26 ≈ 0.231 results in a drop of 0.40. Hence, degradation begins to occur right around the empirically-motivated threshold of 6/26 given above, but we can prune 1 − 8/26 ≈ 69.2% of OPEN successors with minimal changes in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>k/10 89.25 91.16 91.83 92.12 92.38 92.93 Table 1: Development F1 scores using word-level search with various beam sizes k and two choices of word beam size k w .</head><label></label><figDesc></figDesc><table>Beam Size k 

200 
400 
600 
800 
1000 2000 
k w = k 
87.47 89.86 90.98 91.62 91.97 92.74 
k w = </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>600 800 1000 2000 k w = k 91.33 92.17 92.51 92.73 92.89 93.05 k w = k/10 91.41 92.34 92.70 92.94 93.09 93.18</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Cumulative distributions of the number of unique OPEN outputs per input for an order- c pruning function, computed over pruning</head><label>3</label><figDesc></figDesc><table>inputs 
with at least one OPEN output. 

p 
6/26 
7/26 
8/26 
9/26 10/26 11/26 
1 
Dev F1 92.78 93.00 93.08 93.13 93.19 93.19 93.18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of F1 scores on section 23 of 
the Penn Treebank. Here we only include models 
trained without external silver training data. Re-
sults in the first two sections are for single-model 
systems. 

</table></figure>

			<note place="foot" n="1"> The model described in Dyer et al. (2016) has only a single CLOSE action, whereas the model described in Choe and Charniak (2016) annotates CLOSE(X) actions with their nonterminals. We present the more general version here.</note>

			<note place="foot" n="2"> These thresholds are not exact due to the fact that our pruning procedure operates on collections of multiple hypotheses&apos; successors at inference time rather than the successors of an individual hypothesis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>MS is supported by an NSF Graduate Research Fellowship. DF is supported by an NDSEG fel-lowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative incremental dependency parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="863" to="869" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Top-down nearly-contextsensitive parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="674" to="683" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
	<note>California</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving neural parsing by disentangling model combination and reranking effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inducing history representations for broad coverage statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shift-reduce constituent parsing with neural lookahead features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Parsing Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="35" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
